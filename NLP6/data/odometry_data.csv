name,text,chunks,link
15.pdf,"see discussion , st at , and author pr ofiles f or this public ation at : http : //www .researchgate.ne t/public ation/316948653 flowdometry : an optical flow and deep learning based approach to visual odometry conf erence paper · mar ch 2017 doi : 10.1109/wa cv.2017.75 citations 103reads 1,987 2 author s , include : andr eas sav akis rochest er instit ute of t echnolog y 199 publica tions 3,173 citations see profile all c ontent f ollo wing this p age be uplo aded b y andr eas sav akis on 04 oct ober 2019 . the user have r equest ed enhanc ement of the do wnlo aded file.flowdometry : an optical flow and deep learning based approach to visual odometry peter muller rochester institute of technology pmm5983 @ rit.eduandreas savakis rochester institute of technology andreas.savakis @ rit.edu abstract visual odometry be a challenge task relate to simul- taneous localization and mapping that aim to generate a map travel from a visual data stream . based on one or two camera , motion be estimate from feature and pixel difference between frame . because of the frame rate of the camera , there be generally small , incremental change between subsequent frame where optical ﬂow can be as- sum to be proportional to the physical distance move by an egocentric reference , such a a camera on a vehicle . in this paper , a visual odometry system call flowdometry be propose base on optical ﬂow and deep learning . optical ﬂow image be use a input to a convolutional neural net- work , which calculate a rotation and displacement for each image pixel . the displacement and rotation be apply incrementally to construct a map of where the camera have travel . the proposed system be train and test on the kitti visual odometry dataset , and accuracy be measure by the difference in distance between ground truth and predict drive trajectory . different convolutional neu- ral network architecture conﬁgurations be test for accu- racy , and then result be compare to other state-of-the-art monocular odometry system use the same dataset . the average translation error from the flowdometry system be 10.77 % and the average rotation error be 0.0623 degree per meter . the total execution time of the system per optical ﬂow frame be 0.633 second , which offer a 23.796x speedup over state-of-the-art method use deep learning . 1 . introduction visual odometry ( vo ) be a challenge task that gener- ate a map of where a vehicle have travel give an input video stream from camera ( s ) mount on the vehicle . this type of vo capability can assist in the development of au- tonomous vehicle and robot and create map of various type of space , among many other application . there be a need for autonomous vehicle and robot to be able totrack where they have be , because this knowledge help to create predictive model for route understanding and nav- igation . an important step in accumulate the mapping over the entire length of the video sequence be estimate the movement strictly between two consecutive frame in the sequence . this paper propose flowdometry , a system for inter-frame odometry estimation base on optical ﬂow and deep learning . in traditional simultaneous localization and mapping ( slam ) approach , the ﬁnal result be reﬁned over the course of the run-time of the algorithm . particle ﬁltering [ 7 ] , bundle adjustment [ 9 ] , and feature track [ 10 ] have all show highly accurate result in this ﬁeld . each be capa- ble of relocalizing within the generated map , which allow for them to correct scale and rotation drift from previously visit place in the map . however , this be not the case for system that use inter-frame estimation only . when only consider delta from one frame to the next , error be ac- cumulate over the length of the driving path , which mean that early error will propagate all the way to the end of the video sequence in addition to any other error along the way . some example of these type of system be viso2-m [ 6 ] , svr vo [ 2 ] , and p-cnn [ 3 ] , which will all be cover in more detail in section 2 . one way to estimate motion between two image or frame in a video sequence be to relate physical movement to the optical ﬂow between the image . optical ﬂow be a method of densely compute how far a pixel move from one image to the next . there be several highly accurate al- gorithms for this calculation , such a the brox algorithm [ 1 ] , deepmatching [ 11 ] , epicflow [ 12 ] , and deepflow [ 13 ] . one major issue with the optical ﬂow algorithm be that they take a signiﬁcant amount of time per image pair to run . thus , computational efﬁciency be an important considera- tion , especially for vehicular application where there be an ever-increasing need for fast computation . flownet [ 4 ] be a deep learning approach to calculat- ing optical ﬂow between image . due to the use of graph- ic process unit ( gpus ) , the run time of a calculationpreprintfigure 1 : flownets architecture with the contractive side of the network show . the upconvolutional stage take intermediate result from previous convolution to assist in ultimately form a dense optical ﬂow output image . be reduce to fraction of a second , make it the fast run optical ﬂow system , but at the cost of not be as accurate a the state-of-the-art method . two different network architecture , flownetc and flownets , be in- troduced , and it be show that flownetc generalize bet- ter to synthetic scene while flownets generalize good to naturalistic scene . this paper contribute an end-to-end deep learn so- lution for visual odometry inspire by flownets . the pro- pose system utilize a novel network input of raw optical ﬂow in a repurposed neural network architecture that do not require semi-supervised pretraining . due to implemen- tation choice , this system have a greatly reduced execution time from raw camera data to odometry result compare to any other similar system . the rest of this paper be organize a follow . section 2 describes related work , section 3 detail the flowdometry architecture , section 4 present the result , and section 5 summarize the work . 2 . related works the visual odometry task differs from other slam work in that strictly inter-frame odometry be calculate and accumulate over the sequence , with no other method of error correction or post processing to relocalize the vehicle within the map . these system be subject to integration er- ror from the accumulation of small error over the length of the video sequence be process . the flownets architecture [ 4 ] be show to generalize natural scene well when compute optical ﬂow . the con- tractive architecture of flownets be show in fig . 1 . using ten layer of convolution with and without stride , two in- put image be process down into an activation volume that be then upconvolved into the optical ﬂow estimation . the upconvolution be deﬁned a a convolution in which the output activation volume have great height and width dimension than the input volume . to assist in the upconvo- lutions , activation from the contractive convolution be append to the upconvolution input and process at the same time . intermediate ﬂow estimation be also usedto increase the amount of data available for each upconvo- lution . all convolution use a leaky rectiﬁed linear unit ( leaky relu ) activation function . the output of the convo- lutions be then upscaled use linear interpolation to the full resolution of the input image to create the ﬁnal optical ﬂow prediction . in order to provide a sufﬁcient amount of training data for such a large network , the author of flownet devise the flying chairs dataset , which include over 20,000 im- age pair of synthetic image to train on . all example pair include a chair overlay on a random background , with the second image be a transformation of the ﬁrst so that op- tical ﬂow could be compute between the two image . in other work on vo , a frame-to-frame movement esti- mation system be develop in [ 6 ] , which algorithmically calculate a three dimensional disparity between frame , in a system call viso2-m . the original intent of this work be to generate 3d map in real-time base on stereo video sequence . this be do by compute the optical ﬂow between image pair and relate the dense result to depth measurement . movement for a slam algorithm , a adapt in [ 3 ] , could be derive by correspond the dis- parity to the camera source over the sequence of video . in [ 2 ] , a support vector machine ( svm ) approach , call svr vo , be take so that the system could ” learn ” how to relate optical ﬂow calculation to physical movement of the camera source . this be one of the ﬁrst know meth- od that use a non-geometric approach to visual odometry . it be show that with labeled data , learn approach , such a svm or gaussian process regression , can perform as well or good than the geometric system before them . in addition , the learned system run much more quickly and efﬁciently than the geometric one . in [ 8 ] , a convolutional neural network model be use for produce odometry information between frame pair in video sequence . unable to regress accurate value with that method , the problem be convert into a classiﬁcation approach by discretizing the ground truth data into range of rotation and displacement . this could be due to the fact that there be very few parameter to train in the network architecture that be use . it only contain two convolu-figure 2 : the flowdometry system overview . raw camera frame be input into the flownets network to produce raw optical ﬂow image . these raw ﬂow image be use a input for the odometry network , which be base on the flownets architecture . the odometry result be then use for mapping and evaluation of the system . figure 3 : the flowdometry convolutional neural network architecture base on the contractive part of flownets . the archi- tectures be the same except for the input data and the output past the last convolution in the contractive network . tional layer and a fully-connected layer , so it do not have the learning capacity that a deep network would have . later , in [ 3 ] , a convolutional neural network that could regress these value be introduce , and this surpass the accuracy of other inter-frame odometry estimation system . the p-cnn vo system , a the author call it , be a com- bination of ﬁve convolutional neural network , one for each quadrant of an image and one for the image a a whole , work together to produce translation and rotation data . although the individual network only have two convolu- tional layer , the best result be achieve when combin- ing the ﬁve network together , mean that the number of parameter be , at the minimum , ﬁve time great than that of a single network with two convolutional layer . this in- crease the learning capacity of the model , which result in the potential for more accurate result . 3 . flowdometry architecture the proposed system calculate odometry estimation in three step . first , raw optical ﬂow be compute from the flownets architecture . next , the ﬂow image be use a in- put to the repurposed flownets network , which will hence- forth be refer to a flowdometry , to produce inter-frame odometry estimation . in a ﬁnal post-processing step , the estimation be accumulate to generate map and accuracy measurement . this system be represent in fig . 2.3.1 . network architecture the convolutional neural network use to regress odom- etry value be base on the contractive half of the flownets [ 4 ] network . unlike other work , flowdometry use raw optical ﬂow , a two-channel image that describe horizon- tal and vertical pixel movement , a input . the other work use color image that be convert from the raw opti- cal ﬂow to color representation of the optical ﬂow vec- tor . flownets be use to generate optical ﬂow image for each pair of consecutive video frame . the result optical ﬂow image be use a input to the flowdometry architecture . flownets be choose to produce the optical ﬂow input be- cause it be show to generalize good to natural scene and also because it run much fast than other optical ﬂow algorithm . for the regression part of the system , a modiﬁed flownets architecture be choose . based on fig . 1 , the change be that the input be now a two-channel , instead of six-channel image , and the reﬁnement network be re- place by a fully-connected layer follow by the output node for regress rotation and displacement information between frame . it be hypothesize that the modiﬁed ar- chitecture could produce the physical motion result be- cause the flownet network be capable of capture mo- tion information . the regression network structure con- tributed in this work be show in fig . 3 . to train this network , the ground truth data from the kitti odometry benchmark be transform into incre-mental movement value of an angle and a displacement . a raw optical ﬂow image , represent two consecutive frame in the video sequence , be associate with the incremental odometry value , and then the network be train with over 114,000 training sample , include the mirrored video sequence . unlike p-cnn , this network do not require semi-supervised weight pretraining with an auto-encoder network before train it to regress the ﬁnal value . this speed up the training time , and simpliﬁes the training process a a whole . for each consecutive frame pair , the incremental move- ments be calculate by the neural network . the rotation and displacement be accumulate over the length of the video sequence , and the result be mapped and reformat- ted into the rotation matrix style that the kitti odometry evaluation require . because each measurement be relative to the current orientation of the vehicle , the movement be project onto a two dimensional plane , and the relative ro- tation be apply , follow by the displacement in that direc- tion . in this application , there be an assumption of linearity for each optical ﬂow image because it be not possible for the vehicle to move too far in the span of the 10hz refresh rate of the camera . 4 . results 4.1 . dataset the kitti odometry benchmark [ 5 ] be a dataset that pro- vides color or grayscale video from mount stereo camera on a vehicle . the video contain drive scene in suburban and highway environment . corresponding to each frame of the video be a ground truth transformation matrix that trans- form the current frame into the coordinate system of the ﬁrst frame . assuming that the vehicle start at the origin of a map , the rotation matrix translates and rotate the vehicle in the current frame , relative to the initial start point . the evaluation generally give 11 sequence with ground truth data to tune an odometry system and 11 sequence with- out ground truth to use for online evaluation of the system . by nature of the integration error introduce in inter-frame odometry application , these system be not evaluate on- line against the state-of-the-art slam system . instead , these system be trained or tune on the ﬁrst 8 video se- quences with ground truth and evaluate on the remain three sequence with ground truth , ensure that those re- sults be not use a part of the training process . not all of the 11 sequence have the same number of video frame . table 1 show the number of frame in each video sequence . the video sequence in this dataset do not contain nearly as many example of the vehicle turn a it go straight , which make regress accurate odometry value difﬁcult in these situation . to provide more data in this work , thephase sequence frames training 00 4541 01 1101 02 4661 03 801 04 271 05 2761 06 1101 07 1101 testing 08 4071 09 1591 10 1201 table 1 : number of frame per video sequence in the kitti odometry benchmark . adding mirror sequence double the amount of training data available to the system . entire dataset be mirror and grind truth data be ad- justed in addition to the original data to give the network twice as much data to train with . 4.2 . evaluation for evaluation of the flowdometry application , result be compare to the result of the regression model intro- duced in [ 2 , 3 , 6 ] . it should be note that flowdometry be develop independently of these work . using the kitti odometry evaluation software kit , the map be plot against the ground truth , and average er- rors for each sequence and all sequence combine be calculate . figs . 4 , 5 , and 6 show the driving path that the application generate from the optical ﬂow input and incre- mental odometry output . due to the integration error accu- mulated over the sequence length , the result path tend to look ” stretch ” away from the ground truth value , but base on the shape and scale of the estimate path , it show that many of the small , incremental estimate be good ap- proximations . the large source of error be around turn , where the rotation estimate be either too large or too small to accurately navigate those area . average error result for flowdometry , as well a com- parisons to viso2-m , svr vo , and p-cnn , be present in table 2 . these result show that flowdometry sur- pass the translational accuracy of the non-deep learning approach of viso2-m and svr vo , and it be compa- rable to the state-of-the-art deep learn model of p-cnn . this system produce more consistent error across each video sequence , and exceed the state-of-the-art model in sequence 10 . the rotational error be high for each se- quence in flowdometry because only one axis of rotation be consider , even though the vehicle be subject to all three ax of rotation in reality . our system be less complex than p-cnn in term of the training method and numberviso2-m [ 6 ] svr vo [ 2 ] p-cnn vo [ 3 ] flowdometry seq trans rot trans rot trans rot trans rot [ % ] [ deg/m ] [ % ] [ deg/m ] [ % ] [ deg/m ] [ % ] [ deg/m ] 08 19.39 0.0393 14.44 0.0300 7.60 0.0187 9.98 0.0544 09 9.26 0.0279 8.70 0.0266 6.75 0.0252 12.64 0.0804 10 27.55 0.0409 18.81 0.0265 21.23 0.0405 11.65 0.0728 avg . 18.55 0.0376 13.81 0.0302 8.96 0.0235 10.77 0.0623 table 2 : average translation and rotation error for each video sequence and method of visual odometry . results for the work in comparison be provide from the p-cnn vo [ 3 ] work . figure 4 : sequence 08 odometry result compare to the ground truth path . of different network to concatenate together for regress the ﬁnal result . the odometry benchmark calculate rotation and trans- lation error a function of sequence length and vehicle speed . fig . 7 show graph of each combination of er- ror and cause of error for the average of all test sequence . when measure against the video sequence length , the er- ror tend to decrease a the length of the video subsequence increase . this show that there may be large spike in er- ror at certain point in the sequence , but they get average to small error over a long period of time . this be espe- cially evident in fig . 7a . contrasting those chart , when the error be compare to the vehicle speed , there be large error when the vehicle be travel at it fast . this be most likely due to the fact that there be minimal data in each sequence where the vehicle be travel that quickly . with figure 5 : sequence 09 odometry result compare to the ground truth path . few sample of fast driving , there be more signiﬁcance to the amount of error that be present when the car do achieve those speed . due to the different number of frame in each of the test- ing video sequence , any result aggregate over sequence length will be bias , especially toward sequence 08 which have many more frame than the other two sequence com- bin . in contrast , the small sequence have more vari- ation , however brief , in car speed , which lead to the spike see in figs . 7b and 7d . because of the limited data in these bucket , the result be weight to the available data , which in these case , correspond to high error at these speed . with few data point to average with , the high error con- tributes most to the shape of the error chart . to show that the flowdometry system be train to generalize to the give result , an example plot from se-figure 6 : sequence 10 odometry result compare to the ground truth path . quence 00 , one of the training sequence , be show in fig . 8 . just a in the test sequence plot , integration error propagate throughout the length of the sequence , but dis- tinct shape of the drive path can be see from the vo system . the model do not overﬁt to the training data be- cause even in sequence 00 , turn be difﬁcult for the sys- tem to accommodate , especially the long , gradual leave turn at the end of the sequence . in this case , if more recording of drive path be take with the same camera setup , it would be expect that this system provide result consis- tent with what have be measure with sequences 08 , 09 , and 10 . although the model do not overﬁt the data , it be likely that there be not enough training data to create the most accurate model . as visualize in the driving path , the scale of movement be accurate , but the turning angle be the large source of error . this suggest that the flowdom- etry model learn how to accurately determine the distance travel between video frame , but it be not able to learn the magnitude of a turn angle , most likely because there be not many example of turn in the training dataset . 4.3 . timing one of the advantage of the flowdometry system be it overall computation time from input image to odometry output . because it use raw optical ﬂow a input to the odometry neural network , it skip a timing penalty associ- ated with convert optical ﬂow to rgb image a do in the compare work . in addition , use flownets instead ( a ) average rotation error vs. video sequence length . ( b ) average rotation error vs. vehicle speed . ( c ) average translation error vs. video sequence length . ( d ) average translation error vs. vehicle speed . figure 7 : average error across all sequence . of the brox algorithm greatly increase the rate at which input image be convert to optical ﬂow , but this comesoptical flow rgb odometry total flowdometry calculation conversion calculation execution speedup [ s/frame ] [ s/frame ] [ s/frame ] [ s/frame ] viso2-m [ 6 ] 14.812 0.210 0.063 15.085 23.831 svr-vo [ 2 ] 14.812 0.210 0.112 15.134 23.908 p-cnn [ 3 ] 14.812 0.210 0.041 15.063 23.796 flowdometry 0.271 0.000 0.362 0.633 — table 3 : timing information for each system . the speedup column express how much fast the total execution time of flowdometry be in comparison to the total execution time of each system . figure 8 : sequence 00 odometry result compare to the ground truth path . this sequence be one of the training sequence for the flowdometry system , and it show that the system do not overﬁt to train data . with the cost of have less accurate optical ﬂow image . table 3 show the comparison of execution time between each of the system . despite the long execution time of the odometry calculation for flowdometry , use flownets and raw optical ﬂow input greatly reduce the total amount of time require to produce odometry result . the brox al- gorithm be use to calculate optical ﬂow for image in the kitti dataset and the low observed time for 1000 differ- ent image pair be record for the comparison system . for measure the flownets optical ﬂow calculation for the flowdometry result , the average execution time for 1000 image pair be measure . conversion to rgb image from optical ﬂow be measure a the average execution time over 1000 different optical ﬂow image . flowdome-try skip this step so it measured time be 0.00 second . the odometry timing for the comparison system be the best recorded value provide in [ 3 ] . for flowdometry , the average execution time over 1000 input optical ﬂow image be measure . it be find that flowdometry have 23.796x speedup over the next fastest system . all measurement that be not provide by costante et al . be calculate on an ubuntu 15.10 x86 64 desktop computer with a 3.00ghz in- tel core 2 duo cpu and nvidia geforce gtx 950 gpu . 5 . conclusions the flowdometry application introduces several contri- butions . an end-to-end convolutional neural network sys- tem take two consecutive video frame a input , convert them to an optical ﬂow image , then regress odometry in- formation base on the raw ﬂow data . the architecture re- purpose the flownets architecture to perform the odom- etry regression . semi-supervised pretraining be not re- quired to achieve result on par with the state-of-the-art inter-frame odometry system . because of the use of neural network for all aspect of the system , flowdometry be also the fast run odometry system of it kind . the result of this system show that it be comparable with other system of similar application . its rotation and trans- lation accuracy surpass those of non-deep learn ap- proaches , and be similar to the result of the deep learn- ing approach . as in other system of inter-frame odometry estimation , integration error play a large role in the ﬁnal shape of the map generate from the neural network output data . although this system tend to follow simple prac- tices than the state of the art model , it be still comparable in translation accuracy , even surpass the state of the art in one of the video sequence . our system use raw opti- cal ﬂow input instead of a color image to produce odometry data that be comparable to the state of the art . this be the ﬁrst system , a of our knowledge , to use neural network all the way from generate the optical ﬂow information to produce the odometry output . there be several opportunity for future improvement in follow up work . the cnn architecture may be reduce in size so that similar result may be achieve with fewerparameters and low network training time . more training data along with dataset augmentation could help to more ac- curately regress turn . as it currently stand , there be many variation of turn with few example of each , make it dif- ﬁcult for the system to accurately regress value for them . it be worth explore other type of network architecture , that may be able to more accurately regress turn , particularly a more frame data become available for training . references [ 1 ] t. brox , a. bruhn , n. papenberg , and j. weickert . high accuracy optical flow estimation based on a theory for warping . in computer vision - eccv 2004 , volume 3024 , page 25–36 . springer berlin heidelberg , 2004 . [ 2 ] t. a. ciarfuglia , g. costante , p. valigi , and e. ricci . evalua- tion of non-geometric method for visual odometry . robotics and autonomous systems , 62 ( 12 ) :1717–1730 , dec 2014 . [ 3 ] g. costante , m. mancini , p. valigi , and t. a. ciarfuglia . ex- ploring representation learning with cnns for frame-to- frame ego-motion estimation . ieee robotics and automa- tion letters , 1 ( 1 ) :18–25 , jan 2016 . [ 4 ] p. fischer , a. dosovitskiy , e. ilg , ... , and t. brox . flownet : learning optical flow with convolutional networks . iccv , page 8 , 2015 . [ 5 ] a. geiger , p. lenz , and r. urtasun . are we ready for autonomous driving ? the kitti vision benchmark suite . inproceedings of the ieee computer society conference on computer vision and pattern recognition , page 3354– 3361 , 2012 . [ 6 ] a. geiger , j. ziegler , and c. stiller . stereoscan : dense 3d reconstruction in real-time . in ieee intelligent vehicles sym- posium , proceedings , page 963–968 . ieee , jun 2011 . [ 7 ] g. grisetti , c. stachniss , and w. burgard . improved tech- niques for grid mapping with rao-blackwellized particle ﬁl- ters . ieee transactions on robotics , 23 ( 1 ) :34–46 , 2007 . [ 8 ] k. konda and r. memisevic . learning visual odometry with a convolutional network . in proc . international con- ference on computer vision theory and applications , 2015 . [ 9 ] k. konolige and m. agrawal . frameslam : from bundle adjustment to real-time visual mapping . ieee transactions on robotics , 24 ( 5 ) :1066–1077 , 2008 . [ 10 ] r. mur-artal , j. m. m. montiel , and j. d. tardos . orb- slam : a versatile and accurate monocular slam system . ieee transactions on robotics , 31 ( 5 ) :1147–1163 , 2015 . [ 11 ] j. revaud , p. weinzaepfel , z. harchaoui , and c. schmid . deepmatching : hierarchical deformable dense matching . international journal of computer vision , ( lowe 2004 ) :1– 24 , 2016 . [ 12 ] j. revaud , p. weinzaepfel , z. harchaoui , c. schmid , j. re- vaud , p. weinzaepfel , z. harchaoui , and c. s. e. edge . epicflow : edge-preserving interpolation of correspon- dences for optical flow . cvpr 2015 , 2015 . [ 13 ] p. weinzaepfel , z. harchaoui , c. schmid , p. weinzaepfel , z. harchaoui , c. schmid , z. harchaoui , and c. schmid . deepflow : large displacement optical ﬂow with deep matching . page 1385–1392 , 2013 . view publication stats","['see discussion , st at , and author pr ofiles f or this public ation at : http : //www .researchgate.ne t/public ation/316948653 flowdometry : an optical flow and deep learning based approach to visual odometry conf erence paper · mar ch 2017 doi : 10.1109/wa cv.2017.75 citations 103reads 1,987 2 author s , include : andr eas sav akis rochest er instit ute of t echnolog y 199 publica tions 3,173 citations see profile all c ontent f ollo wing this p age be uplo aded b y andr eas sav akis on 04 oct ober 2019 . the user have r equest ed enhanc ement of the do wnlo aded file.flowdometry : an optical flow and deep learning based approach to visual odometry peter muller rochester institute of technology pmm5983 @ rit.eduandreas savakis rochester institute of technology andreas.savakis @ rit.edu abstract visual odometry be a challenge task relate to simul- taneous localization and mapping that aim to generate a map travel from a visual data stream . based on one or two camera , motion be estimate from feature and pixel difference between frame . because of the frame rate of the camera , there be generally small , incremental change between subsequent frame where optical ﬂow can be as- sum to be proportional to the physical distance move by an egocentric reference , such a a camera on a vehicle . in this paper , a visual odometry system call flowdometry be propose base on optical ﬂow and deep learning . optical ﬂow image be use a input to a convolutional neural net- work , which calculate a rotation and displacement for each image pixel . the displacement and rotation be apply incrementally to construct a map of where the camera have travel . the proposed system be train and test on the kitti visual odometry dataset , and accuracy be measure by the difference in distance between ground truth and predict drive trajectory . different convolutional neu- ral network architecture conﬁgurations be test for accu- racy , and then result be compare to other state-of-the-art monocular odometry system use the same dataset . the', 'different convolutional neu- ral network architecture conﬁgurations be test for accu- racy , and then result be compare to other state-of-the-art monocular odometry system use the same dataset . the average translation error from the flowdometry system be 10.77 % and the average rotation error be 0.0623 degree per meter . the total execution time of the system per optical ﬂow frame be 0.633 second , which offer a 23.796x speedup over state-of-the-art method use deep learning . 1 . introduction visual odometry ( vo ) be a challenge task that gener- ate a map of where a vehicle have travel give an input video stream from camera ( s ) mount on the vehicle . this type of vo capability can assist in the development of au- tonomous vehicle and robot and create map of various type of space , among many other application . there be a need for autonomous vehicle and robot to be able totrack where they have be , because this knowledge help to create predictive model for route understanding and nav- igation . an important step in accumulate the mapping over the entire length of the video sequence be estimate the movement strictly between two consecutive frame in the sequence . this paper propose flowdometry , a system for inter-frame odometry estimation base on optical ﬂow and deep learning . in traditional simultaneous localization and mapping ( slam ) approach , the ﬁnal result be reﬁned over the course of the run-time of the algorithm . particle ﬁltering [ 7 ] , bundle adjustment [ 9 ] , and feature track [ 10 ] have all show highly accurate result in this ﬁeld . each be capa- ble of relocalizing within the generated map , which allow for them to correct scale and rotation drift from previously visit place in the map . however , this be not the case for system that use inter-frame estimation only . when only consider delta from one frame to the next , error be ac- cumulate over the length of the driving path , which mean that early error will propagate all the way to the end of the video sequence in addition to any', 'from one frame to the next , error be ac- cumulate over the length of the driving path , which mean that early error will propagate all the way to the end of the video sequence in addition to any other error along the way . some example of these type of system be viso2-m [ 6 ] , svr vo [ 2 ] , and p-cnn [ 3 ] , which will all be cover in more detail in section 2 . one way to estimate motion between two image or frame in a video sequence be to relate physical movement to the optical ﬂow between the image . optical ﬂow be a method of densely compute how far a pixel move from one image to the next . there be several highly accurate al- gorithms for this calculation , such a the brox algorithm [ 1 ] , deepmatching [ 11 ] , epicflow [ 12 ] , and deepflow [ 13 ] . one major issue with the optical ﬂow algorithm be that they take a signiﬁcant amount of time per image pair to run . thus , computational efﬁciency be an important considera- tion , especially for vehicular application where there be an ever-increasing need for fast computation . flownet [ 4 ] be a deep learning approach to calculat- ing optical ﬂow between image . due to the use of graph- ic process unit ( gpus ) , the run time of a calculationpreprintfigure 1 : flownets architecture with the contractive side of the network show . the upconvolutional stage take intermediate result from previous convolution to assist in ultimately form a dense optical ﬂow output image . be reduce to fraction of a second , make it the fast run optical ﬂow system , but at the cost of not be as accurate a the state-of-the-art method . two different network architecture , flownetc and flownets , be in- troduced , and it be show that flownetc generalize bet- ter to synthetic scene while flownets generalize good to naturalistic scene . this paper contribute an end-to-end deep learn so- lution for visual odometry inspire by flownets . the pro- pose system utilize a novel network input of raw optical ﬂow in a repurposed neural network architecture that do not require semi-supervised', 'lution for visual odometry inspire by flownets . the pro- pose system utilize a novel network input of raw optical ﬂow in a repurposed neural network architecture that do not require semi-supervised pretraining . due to implemen- tation choice , this system have a greatly reduced execution time from raw camera data to odometry result compare to any other similar system . the rest of this paper be organize a follow . section 2 describes related work , section 3 detail the flowdometry architecture , section 4 present the result , and section 5 summarize the work . 2 . related works the visual odometry task differs from other slam work in that strictly inter-frame odometry be calculate and accumulate over the sequence , with no other method of error correction or post processing to relocalize the vehicle within the map . these system be subject to integration er- ror from the accumulation of small error over the length of the video sequence be process . the flownets architecture [ 4 ] be show to generalize natural scene well when compute optical ﬂow . the con- tractive architecture of flownets be show in fig . 1 . using ten layer of convolution with and without stride , two in- put image be process down into an activation volume that be then upconvolved into the optical ﬂow estimation . the upconvolution be deﬁned a a convolution in which the output activation volume have great height and width dimension than the input volume . to assist in the upconvo- lutions , activation from the contractive convolution be append to the upconvolution input and process at the same time . intermediate ﬂow estimation be also usedto increase the amount of data available for each upconvo- lution . all convolution use a leaky rectiﬁed linear unit ( leaky relu ) activation function . the output of the convo- lutions be then upscaled use linear interpolation to the full resolution of the input image to create the ﬁnal optical ﬂow prediction . in order to provide a sufﬁcient amount of training data for such a large network , the author', 'interpolation to the full resolution of the input image to create the ﬁnal optical ﬂow prediction . in order to provide a sufﬁcient amount of training data for such a large network , the author of flownet devise the flying chairs dataset , which include over 20,000 im- age pair of synthetic image to train on . all example pair include a chair overlay on a random background , with the second image be a transformation of the ﬁrst so that op- tical ﬂow could be compute between the two image . in other work on vo , a frame-to-frame movement esti- mation system be develop in [ 6 ] , which algorithmically calculate a three dimensional disparity between frame , in a system call viso2-m . the original intent of this work be to generate 3d map in real-time base on stereo video sequence . this be do by compute the optical ﬂow between image pair and relate the dense result to depth measurement . movement for a slam algorithm , a adapt in [ 3 ] , could be derive by correspond the dis- parity to the camera source over the sequence of video . in [ 2 ] , a support vector machine ( svm ) approach , call svr vo , be take so that the system could ” learn ” how to relate optical ﬂow calculation to physical movement of the camera source . this be one of the ﬁrst know meth- od that use a non-geometric approach to visual odometry . it be show that with labeled data , learn approach , such a svm or gaussian process regression , can perform as well or good than the geometric system before them . in addition , the learned system run much more quickly and efﬁciently than the geometric one . in [ 8 ] , a convolutional neural network model be use for produce odometry information between frame pair in video sequence . unable to regress accurate value with that method , the problem be convert into a classiﬁcation approach by discretizing the ground truth data into range of rotation and displacement . this could be due to the fact that there be very few parameter to train in the network architecture that be use . it only contain two', 'the ground truth data into range of rotation and displacement . this could be due to the fact that there be very few parameter to train in the network architecture that be use . it only contain two convolu-figure 2 : the flowdometry system overview . raw camera frame be input into the flownets network to produce raw optical ﬂow image . these raw ﬂow image be use a input for the odometry network , which be base on the flownets architecture . the odometry result be then use for mapping and evaluation of the system . figure 3 : the flowdometry convolutional neural network architecture base on the contractive part of flownets . the archi- tectures be the same except for the input data and the output past the last convolution in the contractive network . tional layer and a fully-connected layer , so it do not have the learning capacity that a deep network would have . later , in [ 3 ] , a convolutional neural network that could regress these value be introduce , and this surpass the accuracy of other inter-frame odometry estimation system . the p-cnn vo system , a the author call it , be a com- bination of ﬁve convolutional neural network , one for each quadrant of an image and one for the image a a whole , work together to produce translation and rotation data . although the individual network only have two convolu- tional layer , the best result be achieve when combin- ing the ﬁve network together , mean that the number of parameter be , at the minimum , ﬁve time great than that of a single network with two convolutional layer . this in- crease the learning capacity of the model , which result in the potential for more accurate result . 3 . flowdometry architecture the proposed system calculate odometry estimation in three step . first , raw optical ﬂow be compute from the flownets architecture . next , the ﬂow image be use a in- put to the repurposed flownets network , which will hence- forth be refer to a flowdometry , to produce inter-frame odometry estimation . in a ﬁnal post-processing step , the estimation be', 'use a in- put to the repurposed flownets network , which will hence- forth be refer to a flowdometry , to produce inter-frame odometry estimation . in a ﬁnal post-processing step , the estimation be accumulate to generate map and accuracy measurement . this system be represent in fig . 2.3.1 . network architecture the convolutional neural network use to regress odom- etry value be base on the contractive half of the flownets [ 4 ] network . unlike other work , flowdometry use raw optical ﬂow , a two-channel image that describe horizon- tal and vertical pixel movement , a input . the other work use color image that be convert from the raw opti- cal ﬂow to color representation of the optical ﬂow vec- tor . flownets be use to generate optical ﬂow image for each pair of consecutive video frame . the result optical ﬂow image be use a input to the flowdometry architecture . flownets be choose to produce the optical ﬂow input be- cause it be show to generalize good to natural scene and also because it run much fast than other optical ﬂow algorithm . for the regression part of the system , a modiﬁed flownets architecture be choose . based on fig . 1 , the change be that the input be now a two-channel , instead of six-channel image , and the reﬁnement network be re- place by a fully-connected layer follow by the output node for regress rotation and displacement information between frame . it be hypothesize that the modiﬁed ar- chitecture could produce the physical motion result be- cause the flownet network be capable of capture mo- tion information . the regression network structure con- tributed in this work be show in fig . 3 . to train this network , the ground truth data from the kitti odometry benchmark be transform into incre-mental movement value of an angle and a displacement . a raw optical ﬂow image , represent two consecutive frame in the video sequence , be associate with the incremental odometry value , and then the network be train with over 114,000 training sample , include the mirrored video sequence .', 'consecutive frame in the video sequence , be associate with the incremental odometry value , and then the network be train with over 114,000 training sample , include the mirrored video sequence . unlike p-cnn , this network do not require semi-supervised weight pretraining with an auto-encoder network before train it to regress the ﬁnal value . this speed up the training time , and simpliﬁes the training process a a whole . for each consecutive frame pair , the incremental move- ments be calculate by the neural network . the rotation and displacement be accumulate over the length of the video sequence , and the result be mapped and reformat- ted into the rotation matrix style that the kitti odometry evaluation require . because each measurement be relative to the current orientation of the vehicle , the movement be project onto a two dimensional plane , and the relative ro- tation be apply , follow by the displacement in that direc- tion . in this application , there be an assumption of linearity for each optical ﬂow image because it be not possible for the vehicle to move too far in the span of the 10hz refresh rate of the camera . 4 . results 4.1 . dataset the kitti odometry benchmark [ 5 ] be a dataset that pro- vides color or grayscale video from mount stereo camera on a vehicle . the video contain drive scene in suburban and highway environment . corresponding to each frame of the video be a ground truth transformation matrix that trans- form the current frame into the coordinate system of the ﬁrst frame . assuming that the vehicle start at the origin of a map , the rotation matrix translates and rotate the vehicle in the current frame , relative to the initial start point . the evaluation generally give 11 sequence with ground truth data to tune an odometry system and 11 sequence with- out ground truth to use for online evaluation of the system . by nature of the integration error introduce in inter-frame odometry application , these system be not evaluate on- line against the state-of-the-art slam system', 'for online evaluation of the system . by nature of the integration error introduce in inter-frame odometry application , these system be not evaluate on- line against the state-of-the-art slam system . instead , these system be trained or tune on the ﬁrst 8 video se- quences with ground truth and evaluate on the remain three sequence with ground truth , ensure that those re- sults be not use a part of the training process . not all of the 11 sequence have the same number of video frame . table 1 show the number of frame in each video sequence . the video sequence in this dataset do not contain nearly as many example of the vehicle turn a it go straight , which make regress accurate odometry value difﬁcult in these situation . to provide more data in this work , thephase sequence frames training 00 4541 01 1101 02 4661 03 801 04 271 05 2761 06 1101 07 1101 testing 08 4071 09 1591 10 1201 table 1 : number of frame per video sequence in the kitti odometry benchmark . adding mirror sequence double the amount of training data available to the system . entire dataset be mirror and grind truth data be ad- justed in addition to the original data to give the network twice as much data to train with . 4.2 . evaluation for evaluation of the flowdometry application , result be compare to the result of the regression model intro- duced in [ 2 , 3 , 6 ] . it should be note that flowdometry be develop independently of these work . using the kitti odometry evaluation software kit , the map be plot against the ground truth , and average er- rors for each sequence and all sequence combine be calculate . figs . 4 , 5 , and 6 show the driving path that the application generate from the optical ﬂow input and incre- mental odometry output . due to the integration error accu- mulated over the sequence length , the result path tend to look ” stretch ” away from the ground truth value , but base on the shape and scale of the estimate path , it show that many of the small , incremental estimate be good ap- proximations . the large source', '” stretch ” away from the ground truth value , but base on the shape and scale of the estimate path , it show that many of the small , incremental estimate be good ap- proximations . the large source of error be around turn , where the rotation estimate be either too large or too small to accurately navigate those area . average error result for flowdometry , as well a com- parisons to viso2-m , svr vo , and p-cnn , be present in table 2 . these result show that flowdometry sur- pass the translational accuracy of the non-deep learning approach of viso2-m and svr vo , and it be compa- rable to the state-of-the-art deep learn model of p-cnn . this system produce more consistent error across each video sequence , and exceed the state-of-the-art model in sequence 10 . the rotational error be high for each se- quence in flowdometry because only one axis of rotation be consider , even though the vehicle be subject to all three ax of rotation in reality . our system be less complex than p-cnn in term of the training method and numberviso2-m [ 6 ] svr vo [ 2 ] p-cnn vo [ 3 ] flowdometry seq trans rot trans rot trans rot trans rot [ % ] [ deg/m ] [ % ] [ deg/m ] [ % ] [ deg/m ] [ % ] [ deg/m ] 08 19.39 0.0393 14.44 0.0300 7.60 0.0187 9.98 0.0544 09 9.26 0.0279 8.70 0.0266 6.75 0.0252 12.64 0.0804 10 27.55 0.0409 18.81 0.0265 21.23 0.0405 11.65 0.0728 avg . 18.55 0.0376 13.81 0.0302 8.96 0.0235 10.77 0.0623 table 2 : average translation and rotation error for each video sequence and method of visual odometry . results for the work in comparison be provide from the p-cnn vo [ 3 ] work . figure 4 : sequence 08 odometry result compare to the ground truth path . of different network to concatenate together for regress the ﬁnal result . the odometry benchmark calculate rotation and trans- lation error a function of sequence length and vehicle speed . fig . 7 show graph of each combination of er- ror and cause of error for the average of all test sequence . when measure against the video sequence length , the er- ror tend to', 'and vehicle speed . fig . 7 show graph of each combination of er- ror and cause of error for the average of all test sequence . when measure against the video sequence length , the er- ror tend to decrease a the length of the video subsequence increase . this show that there may be large spike in er- ror at certain point in the sequence , but they get average to small error over a long period of time . this be espe- cially evident in fig . 7a . contrasting those chart , when the error be compare to the vehicle speed , there be large error when the vehicle be travel at it fast . this be most likely due to the fact that there be minimal data in each sequence where the vehicle be travel that quickly . with figure 5 : sequence 09 odometry result compare to the ground truth path . few sample of fast driving , there be more signiﬁcance to the amount of error that be present when the car do achieve those speed . due to the different number of frame in each of the test- ing video sequence , any result aggregate over sequence length will be bias , especially toward sequence 08 which have many more frame than the other two sequence com- bin . in contrast , the small sequence have more vari- ation , however brief , in car speed , which lead to the spike see in figs . 7b and 7d . because of the limited data in these bucket , the result be weight to the available data , which in these case , correspond to high error at these speed . with few data point to average with , the high error con- tributes most to the shape of the error chart . to show that the flowdometry system be train to generalize to the give result , an example plot from se-figure 6 : sequence 10 odometry result compare to the ground truth path . quence 00 , one of the training sequence , be show in fig . 8 . just a in the test sequence plot , integration error propagate throughout the length of the sequence , but dis- tinct shape of the drive path can be see from the vo system . the model do not overﬁt to the training data be- cause even in sequence 00 , turn', 'throughout the length of the sequence , but dis- tinct shape of the drive path can be see from the vo system . the model do not overﬁt to the training data be- cause even in sequence 00 , turn be difﬁcult for the sys- tem to accommodate , especially the long , gradual leave turn at the end of the sequence . in this case , if more recording of drive path be take with the same camera setup , it would be expect that this system provide result consis- tent with what have be measure with sequences 08 , 09 , and 10 . although the model do not overﬁt the data , it be likely that there be not enough training data to create the most accurate model . as visualize in the driving path , the scale of movement be accurate , but the turning angle be the large source of error . this suggest that the flowdom- etry model learn how to accurately determine the distance travel between video frame , but it be not able to learn the magnitude of a turn angle , most likely because there be not many example of turn in the training dataset . 4.3 . timing one of the advantage of the flowdometry system be it overall computation time from input image to odometry output . because it use raw optical ﬂow a input to the odometry neural network , it skip a timing penalty associ- ated with convert optical ﬂow to rgb image a do in the compare work . in addition , use flownets instead ( a ) average rotation error vs. video sequence length . ( b ) average rotation error vs. vehicle speed . ( c ) average translation error vs. video sequence length . ( d ) average translation error vs. vehicle speed . figure 7 : average error across all sequence . of the brox algorithm greatly increase the rate at which input image be convert to optical ﬂow , but this comesoptical flow rgb odometry total flowdometry calculation conversion calculation execution speedup [ s/frame ] [ s/frame ] [ s/frame ] [ s/frame ] viso2-m [ 6 ] 14.812 0.210 0.063 15.085 23.831 svr-vo [ 2 ] 14.812 0.210 0.112 15.134 23.908 p-cnn [ 3 ] 14.812 0.210 0.041 15.063 23.796 flowdometry 0.271', '] [ s/frame ] [ s/frame ] [ s/frame ] viso2-m [ 6 ] 14.812 0.210 0.063 15.085 23.831 svr-vo [ 2 ] 14.812 0.210 0.112 15.134 23.908 p-cnn [ 3 ] 14.812 0.210 0.041 15.063 23.796 flowdometry 0.271 0.000 0.362 0.633 — table 3 : timing information for each system . the speedup column express how much fast the total execution time of flowdometry be in comparison to the total execution time of each system . figure 8 : sequence 00 odometry result compare to the ground truth path . this sequence be one of the training sequence for the flowdometry system , and it show that the system do not overﬁt to train data . with the cost of have less accurate optical ﬂow image . table 3 show the comparison of execution time between each of the system . despite the long execution time of the odometry calculation for flowdometry , use flownets and raw optical ﬂow input greatly reduce the total amount of time require to produce odometry result . the brox al- gorithm be use to calculate optical ﬂow for image in the kitti dataset and the low observed time for 1000 differ- ent image pair be record for the comparison system . for measure the flownets optical ﬂow calculation for the flowdometry result , the average execution time for 1000 image pair be measure . conversion to rgb image from optical ﬂow be measure a the average execution time over 1000 different optical ﬂow image . flowdome-try skip this step so it measured time be 0.00 second . the odometry timing for the comparison system be the best recorded value provide in [ 3 ] . for flowdometry , the average execution time over 1000 input optical ﬂow image be measure . it be find that flowdometry have 23.796x speedup over the next fastest system . all measurement that be not provide by costante et al . be calculate on an ubuntu 15.10 x86 64 desktop computer with a 3.00ghz in- tel core 2 duo cpu and nvidia geforce gtx 950 gpu . 5 . conclusions the flowdometry application introduces several contri- butions . an end-to-end convolutional neural network sys- tem take two consecutive video', 'cpu and nvidia geforce gtx 950 gpu . 5 . conclusions the flowdometry application introduces several contri- butions . an end-to-end convolutional neural network sys- tem take two consecutive video frame a input , convert them to an optical ﬂow image , then regress odometry in- formation base on the raw ﬂow data . the architecture re- purpose the flownets architecture to perform the odom- etry regression . semi-supervised pretraining be not re- quired to achieve result on par with the state-of-the-art inter-frame odometry system . because of the use of neural network for all aspect of the system , flowdometry be also the fast run odometry system of it kind . the result of this system show that it be comparable with other system of similar application . its rotation and trans- lation accuracy surpass those of non-deep learn ap- proaches , and be similar to the result of the deep learn- ing approach . as in other system of inter-frame odometry estimation , integration error play a large role in the ﬁnal shape of the map generate from the neural network output data . although this system tend to follow simple prac- tices than the state of the art model , it be still comparable in translation accuracy , even surpass the state of the art in one of the video sequence . our system use raw opti- cal ﬂow input instead of a color image to produce odometry data that be comparable to the state of the art . this be the ﬁrst system , a of our knowledge , to use neural network all the way from generate the optical ﬂow information to produce the odometry output . there be several opportunity for future improvement in follow up work . the cnn architecture may be reduce in size so that similar result may be achieve with fewerparameters and low network training time . more training data along with dataset augmentation could help to more ac- curately regress turn . as it currently stand , there be many variation of turn with few example of each , make it dif- ﬁcult for the system to accurately regress value for them . it be worth', 'ac- curately regress turn . as it currently stand , there be many variation of turn with few example of each , make it dif- ﬁcult for the system to accurately regress value for them . it be worth explore other type of network architecture , that may be able to more accurately regress turn , particularly a more frame data become available for training . references [ 1 ] t. brox , a. bruhn , n. papenberg , and j. weickert . high accuracy optical flow estimation based on a theory for warping . in computer vision - eccv 2004 , volume 3024 , page 25–36 . springer berlin heidelberg , 2004 . [ 2 ] t. a. ciarfuglia , g. costante , p. valigi , and e. ricci . evalua- tion of non-geometric method for visual odometry . robotics and autonomous systems , 62 ( 12 ) :1717–1730 , dec 2014 . [ 3 ] g. costante , m. mancini , p. valigi , and t. a. ciarfuglia . ex- ploring representation learning with cnns for frame-to- frame ego-motion estimation . ieee robotics and automa- tion letters , 1 ( 1 ) :18–25 , jan 2016 . [ 4 ] p. fischer , a. dosovitskiy , e. ilg , ... , and t. brox . flownet : learning optical flow with convolutional networks . iccv , page 8 , 2015 . [ 5 ] a. geiger , p. lenz , and r. urtasun . are we ready for autonomous driving ? the kitti vision benchmark suite . inproceedings of the ieee computer society conference on computer vision and pattern recognition , page 3354– 3361 , 2012 . [ 6 ] a. geiger , j. ziegler , and c. stiller . stereoscan : dense 3d reconstruction in real-time . in ieee intelligent vehicles sym- posium , proceedings , page 963–968 . ieee , jun 2011 . [ 7 ] g. grisetti , c. stachniss , and w. burgard . improved tech- niques for grid mapping with rao-blackwellized particle ﬁl- ters . ieee transactions on robotics , 23 ( 1 ) :34–46 , 2007 . [ 8 ] k. konda and r. memisevic . learning visual odometry with a convolutional network . in proc . international con- ference on computer vision theory and applications , 2015 . [ 9 ] k. konolige and m. agrawal . frameslam : from bundle adjustment to real-time', 'a convolutional network . in proc . international con- ference on computer vision theory and applications , 2015 . [ 9 ] k. konolige and m. agrawal . frameslam : from bundle adjustment to real-time visual mapping . ieee transactions on robotics , 24 ( 5 ) :1066–1077 , 2008 . [ 10 ] r. mur-artal , j. m. m. montiel , and j. d. tardos . orb- slam : a versatile and accurate monocular slam system . ieee transactions on robotics , 31 ( 5 ) :1147–1163 , 2015 . [ 11 ] j. revaud , p. weinzaepfel , z. harchaoui , and c. schmid . deepmatching : hierarchical deformable dense matching . international journal of computer vision , ( lowe 2004 ) :1– 24 , 2016 . [ 12 ] j. revaud , p. weinzaepfel , z. harchaoui , c. schmid , j. re- vaud , p. weinzaepfel , z. harchaoui , and c. s. e. edge . epicflow : edge-preserving interpolation of correspon- dences for optical flow . cvpr 2015 , 2015 . [ 13 ] p. weinzaepfel , z. harchaoui , c. schmid , p. weinzaepfel , z. harchaoui , c. schmid , z. harchaoui , and c. schmid . deepflow : large displacement optical ﬂow with deep matching . page 1385–1392 , 2013 . view publication stats']",http://dx.doi.org/10.3390/robotics11010024
5.pdf,"an improved multi-state constraint kalman filter for visual-inertial odometry m.r . abdollahi† , a , seid h. pourtakdoustia , m.h . yoosefian nooshabadi† , b , h.n . pishkenarib adepartment of aerospace engineering , sharif university of technology , tehran , iran bdepartment of mechanical engineering , sharif university of technology , tehran , iran abstract fast pose estimation ( pe ) play a vital role for agile autonomous robot in success- fully carry out their task . while global navigation satellite systems ( gnss ) such a global positinoing system ( gps ) have be traditionally use along with in- ertial navigation systems ( ins ) for pe , their viability be compromise in indoor and urban environment due to their low update rate and inadequate signal coverage . visual-inertial odometry ( vio ) be gain popularity a a practical alternative to gnss/ins system in gnss-denied environment . among various vio-based meth- od , the multi-state constraint kalman filter ( msckf ) have garner significant attention due to it robustness , speed and accuracy . nevertheless , high computa- tional cost of image processing be still challenge for real-time implementation on resource-constrained vehicle . in this paper , an enhanced version of the msckf be propose . the propose approach differs from the original msckf in the feature marginalization and state pruning step of the algorithm . this new design result in a faster algorithm with comparable accuracy . we validate the propose algorithm use both an open-source dataset and real-world experiment . it be demonstrate that the propose fast-msckf ( refer to a fmsckf ) be approximately six time faster and at least 20 % more accurate in final position estimation compare to the standard msckf . keywords : msckf , fast msckf , visual-inertial odometry , agile motion , kalman filter . video link : experimental test †these author contribute equally to this work . preprint submit to elsevier october 29 , 2024arxiv:2210.08117v2 [ cs.ro ] 25 oct 20241 . introduction in recent year , the utilization of quadrotors have witness a significant upsurge across various application ( abbasi et al. , 2018b , a ; abdollahi et al. , 2019 ) . accurate and reliable pose estimation serf a a crucial step in design guidance and control system for quadrotors . gps and imu data fusion have be widely use for pose estimation in autonomous vehicle ( sukkarieh et al. , 1999 ; cen et al. , 2020 ; macias et al. , 2021 ; cheng et al. , 2022 ) . however , this method have some drawback in certain scenario . for instance , satellite signal may not be readily available in various setting , such a forest , indoor area , and urban environment with tall building . moreover , the update rate of many satellite receiver be notably low and with a slight delay . as a result , alternative approach have be explore to replace gnss/ins system and address their limitation . although it do not provide absolute pose ( i.e. , the pose of the system with respect to an earth-fixed frame ) , camera-imu fusion be one of the emerge method for relative pe , due to the low cost and high quality of the sensor . this approach be commonly refer to a visual inertial odometry ( vio ) in the relevant literature . vio approach be consider safe , cost-effective and robust replacement for gnss/ins system in application where it suffice to have relative pe . there be various method for vio in the literature , include orb-slam ( mur- artal et al. , 2015 ) , svo+gtsam ( forster et al. , 2016a ) , cnn-svo ( loo et al. , 2019 ) , dso ( engel et al. , 2017 ) , vins-mono ( qin et al. , 2018 ) , and kimera ( rosi- nol et al. , 2020 ) . vio method can be categorize into two major group , namely , filter-based andoptimization-based approach . optimization-based method tend to be more computationally expensive , which make them suitable for offline and post- processing application . on the other hand , the low computational load and relatively high accuracy of filter-based method render them well-suited for real-time imple- mentation ( huang , 2019 ) . given that this paper ’ s focus be on fast pe , we opt for the latter approach . in certain filter-based method , landmark position be store in the state vector ( see rovio ( bloesch et al. , 2017 ) , ekf- and ukf- slam ( brossard et al. , 2018 ) ) . however , a the number of landmark position increase , the size of the state vector grows proportionally , result in a considerable rise in computational demand ( delmerico and scaramuzza , 2018 ) . therefore , for real-time application , only a limited number of landmark position can be use . on the other hand , some other filter-based approach , such a the msckf , store a number of camera pose in the state vector . the multi-state constraint kalman filter ( msckf ) be originally develop by mourikis and roumeliotis in their seminal paper ( mourikis and roumeliotis , 2007 ) . the msckf use an error-state extended kalman filter to fuse imu and camera data and , unlike other kf-based method , do not store the position of landmark 2in the state vector . instead , a series of recent position and orientation ( i.e. , pose ) of the camera be keep in the state vector . this alternative strategy significantly reduce the computational cost while enhance estimation accuracy and robustness . in general , to improve the performance of a vio algorithm , three main charac- teristics need to be consider , namely , accuracy , robustness , and output rate . there be many study in the literature that attempt to improve these three feature . to improve accuracy , a patch-based msckf ( zheng et al. , 2017 ) , have be develop . unlike the original msckf algorithm , in which image feature be utilized , in the patch-based msckf , a direct method be employ . in this method , the light inten- sities of pixel in distinct patch be use for estimation . this approach achieve , on average , 23 % enhancement in accuracy compare to the original msckf ( zheng et al. , 2017 ) . the local-optimal msckf ( lomsckf ) have be develop in an attempt to improve estimation accuracy use nonlinear optimization ( forster et al. , 2016a ; heo et al. , 2018 ) . the lomsckf employ pre-integrated imu and camera measurement . moreover , stereo msckf ( s-msckf ) , which use a stereo camera instead of a single camera , have be propose to enhance the performance of the original msckf . the s-msckf have show more robustness with a modest increase in computational cost ( sun et al. , 2018 ) . one limitation of ekf-based method lie in the inherent unobservability of the yaw angle , render them intrinsically inconsistent . huang have consider this prob- lem and propose potential solution , include observability-constrained ekf ( oc- ekf ) ( hesch et al. , 2013 ) and first-estimates jacobian ( fej-ekf ) ( huang et al. , 2009 ) . furthermore , ma have introduce ackermann msckf ( ack-msckf ) algo- rithm ( ma et al. , 2019 ) , in which the msckf have be modify to be use for ground vehicle . in this work , the unobservable state of ground robot be investigate , and an algorithm be present to resolve the unobservability problem . there be also numerous recent work focus on the integration of machine learn technique into visual-inertial navigation to enhance robustness and accuracy ( chen et al. , 2020 ; li and waslander , 2020 ) . for instance , a convolutional neural network have be implement in the update step of the msckf algorithm , which have result in high robustness compare to the original msckf ( zuo et al. , 2021 ) . however , learning-based approach often lack formal convergence guarantee , which make them unreliable for safety-critical application . as discuss above , numerous study have be conduct with the goal of en- hancing the original msckf algorithm . the majority of these endeavor have aim to improve the algorithm ’ s accuracy or address the observability issue associate with the yaw angle . however , relatively few study have try to decrease the computa- tional cost of the algorithm . yet , the high computational cost remain the primary challenge for resource-constrained robot . therefore , reduce this computational 3cost be of great importance . to this end , this paper attempt to reduce the computa- tional cost of the msckf algorithm use a new feature management method . the rest of the paper be organize a follow . the next section cover the for- mulation of the msckf . section 3 constitute the core contribution of this work , elucidate the feature management methodology of both the original msckf and the propose fmsckf . the implementation result obtain from both the msckf and fmsckf on an open-source dataset and in real-world experiment be present in section 4 . in section 5 , a detailed discussion of the result be provided . finally , conclusion be provide in section 6 . 2 . the multi state constraint kalman filter ( msckf ) the msckf be an error-state extended kalman filter that can be use to estimate the position and orientation of a camera-imu system . more precisely , the algorithm estimate the pose of the frame attach to the imu , { i } , with respect to a global ( reference ) frame , { g } . the equation in this section be base on ( mourikis and roumeliotis , 2007 ) . the state vector of the filter consist of two part . the first part , which evolve during the propagation step , be the imu state xi=iq⊤ gb⊤ ggv⊤ ib⊤ agp⊤ i⊤ , ( 1 ) in whichiqg∈r4is the unit quaternion indicate the rotation from the frame { g } to the frame { i } , gvi∈r3andgpi∈r3are the velocity and position vector of the frame { i } with respect to and express in the frame { g } , and bg∈r3and ba∈r3are gyroscope and accelerometer bias , respectively . the second part of the state vector include a number of pose of the camera frame { c } with respect to the global frame { g } . this part be add to the state vector during the augmentation step ( section 2.2 ) . the algorithm consists of three step , which be explain in the following section . 2.1 . the propagation step every time a new imu measurement be receive , the first part of the state vector and it corresponding covariance matrix be propagate . considering a calibrate imu ( i.e. , misalignment , temperature effect , scale-factor , etc . be account for ) , the measurement model for the gyroscope and the accelerometer be ωm=ω+bg+ng , ( 2 ) am=irg","['an improved multi-state constraint kalman filter for visual-inertial odometry m.r . abdollahi† , a , seid h. pourtakdoustia , m.h . yoosefian nooshabadi† , b , h.n . pishkenarib adepartment of aerospace engineering , sharif university of technology , tehran , iran bdepartment of mechanical engineering , sharif university of technology , tehran , iran abstract fast pose estimation ( pe ) play a vital role for agile autonomous robot in success- fully carry out their task . while global navigation satellite systems ( gnss ) such a global positinoing system ( gps ) have be traditionally use along with in- ertial navigation systems ( ins ) for pe , their viability be compromise in indoor and urban environment due to their low update rate and inadequate signal coverage . visual-inertial odometry ( vio ) be gain popularity a a practical alternative to gnss/ins system in gnss-denied environment . among various vio-based meth- od , the multi-state constraint kalman filter ( msckf ) have garner significant attention due to it robustness , speed and accuracy . nevertheless , high computa- tional cost of image processing be still challenge for real-time implementation on resource-constrained vehicle . in this paper , an enhanced version of the msckf be propose . the propose approach differs from the original msckf in the feature marginalization and state pruning step of the algorithm . this new design result in a faster algorithm with comparable accuracy . we validate the propose algorithm use both an open-source dataset and real-world experiment . it be demonstrate that the propose fast-msckf ( refer to a fmsckf ) be approximately six time faster and at least 20 % more accurate in final position estimation compare to the standard msckf . keywords : msckf , fast msckf , visual-inertial odometry , agile motion , kalman filter . video link : experimental test †these author contribute equally to this work . preprint submit to elsevier october 29 , 2024arxiv:2210.08117v2 [ cs.ro ] 25 oct 20241 . introduction in recent year , the', 'link : experimental test †these author contribute equally to this work . preprint submit to elsevier october 29 , 2024arxiv:2210.08117v2 [ cs.ro ] 25 oct 20241 . introduction in recent year , the utilization of quadrotors have witness a significant upsurge across various application ( abbasi et al. , 2018b , a ; abdollahi et al. , 2019 ) . accurate and reliable pose estimation serf a a crucial step in design guidance and control system for quadrotors . gps and imu data fusion have be widely use for pose estimation in autonomous vehicle ( sukkarieh et al. , 1999 ; cen et al. , 2020 ; macias et al. , 2021 ; cheng et al. , 2022 ) . however , this method have some drawback in certain scenario . for instance , satellite signal may not be readily available in various setting , such a forest , indoor area , and urban environment with tall building . moreover , the update rate of many satellite receiver be notably low and with a slight delay . as a result , alternative approach have be explore to replace gnss/ins system and address their limitation . although it do not provide absolute pose ( i.e. , the pose of the system with respect to an earth-fixed frame ) , camera-imu fusion be one of the emerge method for relative pe , due to the low cost and high quality of the sensor . this approach be commonly refer to a visual inertial odometry ( vio ) in the relevant literature . vio approach be consider safe , cost-effective and robust replacement for gnss/ins system in application where it suffice to have relative pe . there be various method for vio in the literature , include orb-slam ( mur- artal et al. , 2015 ) , svo+gtsam ( forster et al. , 2016a ) , cnn-svo ( loo et al. , 2019 ) , dso ( engel et al. , 2017 ) , vins-mono ( qin et al. , 2018 ) , and kimera ( rosi- nol et al. , 2020 ) . vio method can be categorize into two major group , namely , filter-based andoptimization-based approach . optimization-based method tend to be more computationally expensive , which make them suitable for offline and post- processing', 'major group , namely , filter-based andoptimization-based approach . optimization-based method tend to be more computationally expensive , which make them suitable for offline and post- processing application . on the other hand , the low computational load and relatively high accuracy of filter-based method render them well-suited for real-time imple- mentation ( huang , 2019 ) . given that this paper ’ s focus be on fast pe , we opt for the latter approach . in certain filter-based method , landmark position be store in the state vector ( see rovio ( bloesch et al. , 2017 ) , ekf- and ukf- slam ( brossard et al. , 2018 ) ) . however , a the number of landmark position increase , the size of the state vector grows proportionally , result in a considerable rise in computational demand ( delmerico and scaramuzza , 2018 ) . therefore , for real-time application , only a limited number of landmark position can be use . on the other hand , some other filter-based approach , such a the msckf , store a number of camera pose in the state vector . the multi-state constraint kalman filter ( msckf ) be originally develop by mourikis and roumeliotis in their seminal paper ( mourikis and roumeliotis , 2007 ) . the msckf use an error-state extended kalman filter to fuse imu and camera data and , unlike other kf-based method , do not store the position of landmark 2in the state vector . instead , a series of recent position and orientation ( i.e. , pose ) of the camera be keep in the state vector . this alternative strategy significantly reduce the computational cost while enhance estimation accuracy and robustness . in general , to improve the performance of a vio algorithm , three main charac- teristics need to be consider , namely , accuracy , robustness , and output rate . there be many study in the literature that attempt to improve these three feature . to improve accuracy , a patch-based msckf ( zheng et al. , 2017 ) , have be develop . unlike the original msckf algorithm , in which image feature be utilized , in the', 'improve these three feature . to improve accuracy , a patch-based msckf ( zheng et al. , 2017 ) , have be develop . unlike the original msckf algorithm , in which image feature be utilized , in the patch-based msckf , a direct method be employ . in this method , the light inten- sities of pixel in distinct patch be use for estimation . this approach achieve , on average , 23 % enhancement in accuracy compare to the original msckf ( zheng et al. , 2017 ) . the local-optimal msckf ( lomsckf ) have be develop in an attempt to improve estimation accuracy use nonlinear optimization ( forster et al. , 2016a ; heo et al. , 2018 ) . the lomsckf employ pre-integrated imu and camera measurement . moreover , stereo msckf ( s-msckf ) , which use a stereo camera instead of a single camera , have be propose to enhance the performance of the original msckf . the s-msckf have show more robustness with a modest increase in computational cost ( sun et al. , 2018 ) . one limitation of ekf-based method lie in the inherent unobservability of the yaw angle , render them intrinsically inconsistent . huang have consider this prob- lem and propose potential solution , include observability-constrained ekf ( oc- ekf ) ( hesch et al. , 2013 ) and first-estimates jacobian ( fej-ekf ) ( huang et al. , 2009 ) . furthermore , ma have introduce ackermann msckf ( ack-msckf ) algo- rithm ( ma et al. , 2019 ) , in which the msckf have be modify to be use for ground vehicle . in this work , the unobservable state of ground robot be investigate , and an algorithm be present to resolve the unobservability problem . there be also numerous recent work focus on the integration of machine learn technique into visual-inertial navigation to enhance robustness and accuracy ( chen et al. , 2020 ; li and waslander , 2020 ) . for instance , a convolutional neural network have be implement in the update step of the msckf algorithm , which have result in high robustness compare to the original msckf ( zuo et al. , 2021 ) . however , learning-based approach', 'network have be implement in the update step of the msckf algorithm , which have result in high robustness compare to the original msckf ( zuo et al. , 2021 ) . however , learning-based approach often lack formal convergence guarantee , which make them unreliable for safety-critical application . as discuss above , numerous study have be conduct with the goal of en- hancing the original msckf algorithm . the majority of these endeavor have aim to improve the algorithm ’ s accuracy or address the observability issue associate with the yaw angle . however , relatively few study have try to decrease the computa- tional cost of the algorithm . yet , the high computational cost remain the primary challenge for resource-constrained robot . therefore , reduce this computational 3cost be of great importance . to this end , this paper attempt to reduce the computa- tional cost of the msckf algorithm use a new feature management method . the rest of the paper be organize a follow . the next section cover the for- mulation of the msckf . section 3 constitute the core contribution of this work , elucidate the feature management methodology of both the original msckf and the propose fmsckf . the implementation result obtain from both the msckf and fmsckf on an open-source dataset and in real-world experiment be present in section 4 . in section 5 , a detailed discussion of the result be provided . finally , conclusion be provide in section 6 . 2 . the multi state constraint kalman filter ( msckf ) the msckf be an error-state extended kalman filter that can be use to estimate the position and orientation of a camera-imu system . more precisely , the algorithm estimate the pose of the frame attach to the imu , { i } , with respect to a global ( reference ) frame , { g } . the equation in this section be base on ( mourikis and roumeliotis , 2007 ) . the state vector of the filter consist of two part . the first part , which evolve during the propagation step , be the imu state xi=\\x02iq⊤ gb⊤ ggv⊤ ib⊤ agp⊤ i\\x03⊤ , ( 1 ) in', 'and roumeliotis , 2007 ) . the state vector of the filter consist of two part . the first part , which evolve during the propagation step , be the imu state xi=\\x02iq⊤ gb⊤ ggv⊤ ib⊤ agp⊤ i\\x03⊤ , ( 1 ) in whichiqg∈r4is the unit quaternion indicate the rotation from the frame { g } to the frame { i } , gvi∈r3andgpi∈r3are the velocity and position vector of the frame { i } with respect to and express in the frame { g } , and bg∈r3and ba∈r3are gyroscope and accelerometer bias , respectively . the second part of the state vector include a number of pose of the camera frame { c } with respect to the global frame { g } . this part be add to the state vector during the augmentation step ( section 2.2 ) . the algorithm consists of three step , which be explain in the following section . 2.1 . the propagation step every time a new imu measurement be receive , the first part of the state vector and it corresponding covariance matrix be propagate . considering a calibrate imu ( i.e. , misalignment , temperature effect , scale-factor , etc . be account for ) , the measurement model for the gyroscope and the accelerometer be ωm=ω+bg+ng , ( 2 ) am=irg\\x00ga−gg\\x01 +ba+na , ( 3 ) in which ωm∈r3andω∈r3are the measured and true angular velocity , am∈r3 be the measured body acceleration , irg∈r3×3is the rotation matrix correspond 4to the quaternioniqg , ng∈r3andna∈r3are zero mean white process noise vector , ga∈r3is the acceleration of the frame { i } express in { g } , and finally , gg∈r3is the local gravity vector express in { g } . bias vector be model a random walk process , hence , the time derivative of entry of the imu state vector be i˙qg ( t ) =1 2ω\\x00iω ( t ) \\x01iqg ( t ) , ( 4 ) g˙pi ( t ) =gvi ( t ) , ( 5 ) g˙vi ( t ) =gai ( t ) , ( 6 ) ˙bg ( t ) =nbg ( t ) , ( 7 ) ˙ba ( t ) =nba ( t ) . ( 8 ) in ( 7 ) and ( 8 ) , nbg∈r3andnba∈r3are random walk noise vector and ω ( ω ) =\\x14− [ ω× ] ω −ω⊤0\\x15 , [ ω× ] =\\uf8ee \\uf8f00−ωzωy ωz0−ωx ωyωx 0\\uf8f9 \\uf8fb . ( 9 ) applying the expectation operator to equation ( 4 ) to ( 8 ) yield i˙ˆqg=1 2ω ( ˆω ) iˆqg , ( 10 )', 'random walk noise vector and ω ( ω ) =\\x14− [ ω× ] ω −ω⊤0\\x15 , [ ω× ] =\\uf8ee \\uf8f00−ωzωy ωz0−ωx ωyωx 0\\uf8f9 \\uf8fb . ( 9 ) applying the expectation operator to equation ( 4 ) to ( 8 ) yield i˙ˆqg=1 2ω ( ˆω ) iˆqg , ( 10 ) g˙ˆpi=gˆvi , ( 11 ) g˙ˆ vi=ir⊤ gˆ a+gg , ( 12 ) ˙ˆbg=0 , ( 13 ) ˙ˆba=0 , ( 14 ) where ˆω=ωm−ˆbgandˆ a=am−ˆba . these equation can be solve either by continuous-time integration ( use differential equation ) or discrete-time integration ( use method such a fourth order runge-kutta ) to obtain the imu state propaga- tion equation . the covariance matrix be also propagate . this matrix , at time step k+ 1 , can be partition a p=\\x14piik+1 |kpick+1 |k pcik+1 |kpcck|k\\x15 , ( 15 ) 5in which the piik+1 |kis the covariance matrix relate to the imu state ( the correlation between imu state variable ) and can be calculate through numerical integration of the following lyapunov equation ˙pii=fpii+piif⊤+gq ig⊤ , ( 16 ) in which f=\\uf8ee \\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0−\\x02iˆω×\\x03 −i3030303 03 03030303 −gˆri [ ˆ a× ] 0303−gˆri03 03 03030303 03 03i30303\\uf8f9 \\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb , ( 17 ) g=\\uf8ee \\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0−i303 03 03 03−i303 03 0303−gˆri03 0303 03−i3 0303 03 03\\uf8f9 \\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb , ( 18 ) wheregˆriand [ ˆ a× ] be clear from the context , and qiis the covariance matrix correspond to the noise vector ni=\\x02n⊤ gn⊤ bgn⊤ an⊤ ba\\x03⊤ . ( 19 ) the next term in the covariance matrix in ( 15 ) be pick+1 |k , which represent the correlation between the imu and the camera state and be propagate use pick+1 |k=φ ( tk+t , tk ) pick|k , ( 20 ) where tis the sampling time of the imu , and φis the state transition matrix that evolve over time accord to the following equation ˙φ ( tk+τ , tk ) =fφ ( tk+τ , tk ) , ( 21 ) withfgiven in ( 17 ) . 2.2 . the augmentation step upon record an image , the camera pose be calculate base on the most recent estimate of the imu pose . assume that the filter be currently at the ( k+ 1 ) -th time 6step , and the state vector already include lcamera pose . when the ( l+ 1 ) -th image be receive , it pose be calculate use cl+1ˆqg=cqi⊗ik+1ˆqg , ( 22 ) gˆpcl+1=gˆpik+1+gˆrik+1ipc , ( 23 ) in', ""time 6step , and the state vector already include lcamera pose . when the ( l+ 1 ) -th image be receive , it pose be calculate use cl+1ˆqg=cqi⊗ik+1ˆqg , ( 22 ) gˆpcl+1=gˆpik+1+gˆrik+1ipc , ( 23 ) in whichipcandcqiare the translation vector and the quaternion between the imu frame and the camera frame , respectively . since the two sensor be rigidly attach to each other , ipcandcqiare two constant , and can be compute offline . in the next step , the state vector be augment accord to bxaug=h ˆx⊤cl+1ˆq⊤ ggˆp⊤ cl+1i⊤ , ( 24 ) in which bxis the state vector prior to augmentation . in addition to the state vector , the covariance matrix should be augment use p=\\x14i6l+15 j\\x15 p\\x14i6l+15 j\\x15⊤ , ( 25 ) in which jis the jacobian matrix , that be calculate use j= '' cri 03×90303×6l h\\x10gˆrik+1ipc\\x11 ×i 03×9i303×6l # . ( 26 ) 2.3 . the update step the update step be carry out use feature extract from image , the pose of which be available in the state vector . the policy base on which these feature be choose be the subject of section 3 , and be the key distinction between the propose method and the original msckf . assume mfeatures be observe in njimages ( l−nj , ··· , l−1 ) with lbeing the index of the most recent record image . each image contain pixel coordinate correspond to different feature . therefore , the camera measurement model for the j-th ( j= 1 , ··· , m ) feature observe in the i-th ( i= 1 , ··· , nj ) image be cizfj=1 cizfj\\x14cixfj ciyfj\\x15 +cinfj , ( 27 ) wherecinfj∈r2is the measurement noise with the covariancecirfj=σ2 imi2 , and cipfj=\\x02cixfjciyfjcizfj\\x03⊤is the position of the j-th feature in the i-th camera frame . utilizing all njmeasurements of the feature , it be possible to estimate the 7position of the feature in the { g } frame via triangulation ( see ( triggs et al. , 2000 ; nousias et al. , 2019 ) ) . assume that the estimated position of the j-th feature in the global frame be denote asgˆ pfj . the estimate of the position of the j-th feature in thei-th image frame be calculate via ciˆpfj=ciˆrg\\x00gˆ pfj−gˆ pci\\x01"", ""estimated position of the j-th feature in the global frame be denote asgˆ pfj . the estimate of the position of the j-th feature in thei-th image frame be calculate via ciˆpfj=ciˆrg\\x00gˆ pfj−gˆ pci\\x01 =\\uf8ee \\uf8ef\\uf8f0ciˆxfj ciˆyfj ciˆzfj\\uf8f9 \\uf8fa\\uf8fb . ( 28 ) the residual be calculate use the measurement of the feature cirfj=cizfj−ciˆ zfj , ( 29 ) in which ciˆ zfj=1 ciˆzfjh ciˆxfjciˆyfji⊤ . ( 30 ) subsequently , the jacobians of the measurement model with respect to the posi- tion of the feature , cihfj , and with respect to the state vector , cihxj , be calculate use cihfj=cijfjciˆrg , ( 31 ) cihxj=\\x02 02×15···cihfj\\x02\\x00gˆpfj−gˆpci\\x01 ×\\x03 −cihfj\\x03 , ( 32 ) where cijfj= 1 ciˆzfj ! 2 '' ciˆzfj 0−ciˆxfj 0ciˆzfj−ciˆyfj # . ( 33 ) these calculation be perform for all njimages in which the j-th feature have be observe . concatenating all calculation for the j-th feature yield rfj=h c1r⊤ fj···cnjr⊤ fji⊤ , ( 34 ) hfj=h c1h⊤ fj···cnjh⊤ fji⊤ , ( 35 ) hxj=h c1h⊤ xj···cnjh⊤ xji⊤ . ( 36 ) letafjbe the left null-space of hfj . define hoj=a⊤ fjhxj , ( 37 ) roj=σ2 ima⊤ fjafj , ( 38 ) 8roj=a⊤ fjrfj . ( 39 ) matrices hoj , roj , and rojare calculate for all mfeatures , which be to be use to carry out the update step . stacking the calculated matrix for all mfeatures yield ho=\\x02 h⊤ o1···h⊤ om\\x03⊤ , ( 40 ) ro=\\x02 r⊤ o1···r⊤ om\\x03⊤ , ( 41 ) ro= diag ( ro1 , . . . , rom ) , ( 42 ) where ro∈rdis the residual compute for all mfeatures observe in njimages , and d=mx j=1 ( 2nj−3 ) . since the number of feature can be large , an additional step can be take to reduce the computational cost . specifically , reduce qr-decomposition can be utilize to decompose the homatrix a ho=\\x02 q1q2\\x03\\x14th 0\\x15 , ( 43 ) where q1andq2are unitary matrix , and this an upper triangular matrix . as a result , the matrix calculate use ( 40 ) to ( 42 ) can be reduce to hn=th , ( 44 ) rn=q⊤ 1ro , ( 45 ) rn=q⊤ 1roq1 . ( 46 ) next , the kalman gain be compute via k=pk+1|kh⊤ ns−1 , ( 47 ) where s=hnpk+1|kh⊤ n. ( 48 ) finally , the kalman gain in ( 47 ) be use to update the covariance matrix and the state vector"", '. ( 46 ) next , the kalman gain be compute via k=pk+1|kh⊤ ns−1 , ( 47 ) where s=hnpk+1|kh⊤ n. ( 48 ) finally , the kalman gain in ( 47 ) be use to update the covariance matrix and the state vector accord to pk+1|k+1=pk+1|k−ksk⊤ , ( 49 ) ex=krn , ( 50 ) 9bxk+1|k+1=bxk+1|k⊕ex , ( 51 ) where the ⊕represents the general form of addition , which for the quaternion entry of the state vector be equivalent to the quaternion multiplication , and for other entry be the same a normal addition . the updated state vector and covariance matrix will be use in the next propagation step of the subsequent iteration of the filter . note that since the msckf be an error-state extended kalman filter , the ori- entation error-state be minimal , a we define δq=\\x141 2δθ 1\\x15 , where δθ∈r3represents the angular error . to enhance numerical stability and ensure that the quaternion norm constraint be meet , we normalize the quaternion vector after both the propagation step and the update step . in addition to quaternion normalization , we make the covariance matrix symmetric . for more information , the reader be refer to ( sola , 2017 , §5 ) . 3 . feature marginalization and state pruning in section 2 , it be assume that mnumber of feature be use to perform the update step of the msckf algorithm . in this section , the approach use to select these feature be explain . in the conventional msckf , the update step be carry out when one of the two follow case happen . •features be no longer visible . this case , which happen most often , occur when at least one feature be no longer visible in the new image . each time an image be capture , some of the previously visible feature may fall out of the field of view and therefore can not be track in the new image . these feature be then use to perform the update step in the algorithm . to maintain the number of feature at a constant level , an equivalent number of new feature be extract in the late image . over time , when all feature from an image have be utilize in the update step or be no longer', 'of feature at a constant level , an equivalent number of new feature be extract in the late image . over time , when all feature from an image have be utilize in the update step or be no longer trackable , that image no longer provide any useful infor- mation for the algorithm . at this point , the pose associate with that image be remove from the state vector . additionally , the corresponding row and col- umn relate to this pose be prune from the covariance matrix . this state and covariance pruning be essential a it prevent the state vector and the covariance matrix from grow indefinitely , which would otherwise lead to computational inefficiency and increase memory usage . 10•state vector reach a certain size . this situation happen when the number of camera pose available in the state vector reach it maximum , npmax . this case usually occur when the camera be not move fast enough . consequently , when the camera move slowly , although some feature may fall outside the field of view and be use to carry out an update ( previous case ) , the state vector and the covariance matrix be not prune . this be due to the fact that the rate at which the feature fall outside the field of view be not sufficiently high , so , the size of the state vector and the covariance matrix keep increase . as a result , over time , the number of stored camera pose reach it maximum , npmax . in this case , at least one image should be remove . before remove that image , all it feature and their track across precede image be use to perform an update . this ensure that the information contain in those feature be not lose . after carry out the update , the image , along with it correspond entry in the state vector and the covariance matrix , be remove . instead of remove only one image , in the original msckf , npmax/3 image be remove . starting from the second-oldest pose , these npmax/3 image be evenly space in time ( mourikis and roumeliotis , 2007 ) . the reason for keep the old image be that the geometric constraint involve this', 'from the second-oldest pose , these npmax/3 image be evenly space in time ( mourikis and roumeliotis , 2007 ) . the reason for keep the old image be that the geometric constraint involve this image usually correspond to a large baseline , make it carry more valuable information about positioning . according to ( mourikis and roumeliotis , 2007 ) , this method perform well in practice . hence , in the conventional msckf there be only one tuning parameter which be npmax . on the other hand , within the fast msckf ( fmsckf ) framework , we introduce an additional scenario center on the minimum number of feature that be track . in this case , in contrast to the conventional msckf approach , where feature be extract from every record image , our fmsckf methodology focus solely on a subset of image , which we term a keyframes . more specifically , in this approach , once feature be extract from a keyframe , they be track in the subsequent frame , and no new feature be extract until the number of tracked feature fall below a specific threshold , denote a nfmin . when the number of tracked feature be less than the threshold , all feature track be use to perform an update . post- update , all image , except the last image ( the keyframe ) be remove , and the state vector and covariance matrix be prune accordingly . following that , the final image in the sequence be utilize for the extraction of new feature , which will be track in forthcoming image . it be worth mention that the concept of keyframes use in this work differs significantly from the common keyframe-based method find in the literature . tra- ditionally , keyframe selection be base on the change in the baseline . this mean that when the positional change between two consecutive image be large enough , these 11is it the first or the second image ? are there any feature in the image ? is the number of stored camera pose more than n p ( max ) ? image no yes extract new or track perviously extract feature no maintian imu propagation yes delete first camera pose', 'in the image ? is the number of stored camera pose more than n p ( max ) ? image no yes extract new or track perviously extract feature no maintian imu propagation yes delete first camera pose and use any feature in this frame for triangulation update use triangulation data use lose feature for triangulation yes fast msckf feature managment msckf feature managment yes use all feature for triangulation state pruning extract feature from last image fast msckf update step is the number of tracked feature low than n f ( min ) ? no nofigure 1 : the flowchart of feature management approach in the msckf and the fmsckf algorithm . image be designate a keyframes ( azimi et al. , 2022 ) . however , our work adopt a different criterion for define keyframes . instead of rely on the change in the baseline , we determine keyframes base on the trackability of feature within the frame . specifically , in our method , a frame l ( where l > 1 ) be consider a keyframe if the number of feature that can be track in it fall below a certain threshold , nfmin . this threshold be a predefined minimum number of feature to be track , and when the number of these feature in a frame drop below this threshold , that frame be mark a a keyframe . therefore , within the propose strategy , three distinct scenario can trigger an update in the filter , which be a follow in descend order of frequency , from the most frequent to the least frequent . •features be no longer visible . 12the first case be similar to the first case in the conventional msckf and occur when at least one feature exit the field of view of the camera . this case happen most frequently . •feature track reach a certain number . the second scenario , which happen second most often , occur when the num- ber of tracked feature fall below a minimum threshold , nfmin . as explain before , in this case , all feature track be use to perform an update . then , all image in the sequence , except for the most recent one ( i.e. , the keyframe ) , be remove . this removal process involve', ', in this case , all feature track be use to perform an update . then , all image in the sequence , except for the most recent one ( i.e. , the keyframe ) , be remove . this removal process involve prune the state vector and the covariance matrix to maintain a manageable size and prevent computational inefficiency . following the prune process , the final image in the sequence , be then use for the extraction of new feature . these newly extract feature will be track in subsequent image . •state vector reach a certain size . lastly , the third case , which be equivalent to the second scenario in the con- ventional msckf , happen when the number of camera pose available in the state vector exceed a maximum number , npmax . this case happen least often . it be imperative to note that the introduced feature marginalization method have prove to be significantly faster and more accurate in practical application than the original approach . a schematic of the two feature management method be show in figure 1 . the pseudocode for the update step of the fmsckf be present in algorithm 1 . to provide a visual comparison between the two feature management policy , figure 2 show four consecutive frame and their corresponding feature extract in the msckf ( right column ) and the fmsckf ( left column ) . as can be see from figure 2 , the number of tracked feature in the first three frame of the fmsckf algorithm be decrease . in the fourth frame , this number fall below the threshold , and a a result , the algorithm extract new feature . on the other hand , the number of feature track in the msckf algorithm remain constant throughout all four frame . 4 . results in this section , the result of the propose algorithm be present . for the original msckf ( mourikis and roumeliotis , 2007 ) , the maximum allowable number of pose be choose to be npmax= 20 . for the fmsckf , the minimum number of feature be set tonfmin= 8 , and similar to the original msckf , the maximum allowable number of 13figure 2 : comparison of the number of', 'to be npmax= 20 . for the fmsckf , the minimum number of feature be set tonfmin= 8 , and similar to the original msckf , the maximum allowable number of 13figure 2 : comparison of the number of extracted feature in the fmsckf and the msckf . in the fourth frame on the left column , the number of tracked feature fall below the threshold , and therefore , the algorithm extract new feature . however , the number of feature in the original msckf algorithm , show in the right column , be constant . the image be part of mh 01 dataset ( burri et al. , 2016 ) . 14algorithm 1 : pseudocode for the update step in the propose algorithm . input : augmented state vector , augment covariance matrix , mfeatures track in njimages . output : updated state vector and covariance matrix . 1ifnumber of store camera pose < n pmaxthen 2forj= 1 , ··· , mdo 3 calculate the position of the j-th feature in the global frame use triangulation ; 4 fori= 1 , ··· , njdo 5 calculate the position of the j-th feature in the i-th frame use ( 28 ) ; 6 estimate the measurement use ( 30 ) , then calculate the residual in ( 29 ) ; 7 use ( 31 ) , ( 32 ) , and ( 33 ) to calculatecihfj , cihxj , andcijfj ; 8 end 9 use ( 34 ) to ( 36 ) to compute rfj , hfj , and hxj ; 10 calculate the left null-space of hfj ; 11 calculate h oj , roj , and rojusing ( 37 ) to ( 39 ) ; 12 perform a mahalanobis gate test to reject the outlier ; 13 end 14 calculate h o , ro , and rousing ( 40 ) to ( 42 ) ; 15 perform a qr-decomposition and calculate hn , rn , and rn , use ( 44 ) to ( 46 ) ; 16 update the covariance matrix and the state vector use ( 49 ) and ( 51 ) ; 17else if number of tracked feature < n fminthen 18 forall feature run 2 - 16 ; 19 prune the state vector and the covariance matrix ; 20 extract new feature from the last image ; 21else 22 delete the first camera pose from the state vector , and it correspond entry from the covariance matrix ; 23 forall feature in the deleted frame run 2 - 16 ; 24end 15table 1 : the update rate ( in hz ) for various algorithm', 'pose from the state vector , and it correspond entry from the covariance matrix ; 23 forall feature in the deleted frame run 2 - 16 ; 24end 15table 1 : the update rate ( in hz ) for various algorithm implement on the euroc mav dataset ( burri et al. , 2016 ) . the code be execute on matlab . dataset rovio svo ( vio ) svo+gtsam msckf vins mono fmsckf mh01 38.75 32.51 64.67 28.49 35.31 106.38 mh02 41.98 35.13 70.06 30.86 36.12 117.65 mh03 37.36 31.45 62.36 27.47 35.06 93.46 mh04 44.01 37.1 73.46 32.36 35.31 101.01 mh05 40.72 34.35 67.96 29.94 35.82 114.94 vr1 01 34.43 29.23 57.47 25.32 32.87 84.75 vr1 02 39.08 32.91 65.23 28.74 35.46 107.53 vr1 03 37.06 31.12 61.85 27.25 34.73 99.01 vr2 01 38.64 33.19 64.49 28.41 35.73 111.11 vr2 02 34.78 30.54 58.06 25.58 35.04 96.15 vr2 03 36.66 31.43 61.19 26.95 35.17 103.09 pose be choose to be npmax= 20 . the rationale for select nfmin= 8 be elaborate in section 5 . in the keyframes , a maximum of 350 harris corner be extract and track in consecutive frame . to track the extracted feature , the well-known klt algorithm ( tomasi and kanade , 1993 ) be use . the random sample consensus ( ransac ) algorithm be use to reject outlier in feature tracking ( fischler and bolles , 1981 ) . 4.1 . public dataset the euroc mav dataset ( burri et al. , 2016 ) be use to evaluate the algorithm . to provide a comprehensive comparison of the update rate , the result of svo ( vio ) ( forster et al. , 2016b ) , svo+gtsam ( forster et al. , 2016a ) , vins-mono ( qin et al. , 2018 ) and rovio ( bloesch et al. , 2017 ) algorithm be present . default setting be apply for these algorithm . all computation be conduct on a personal macbook air equip with an m1 chip and 8gb memory . the update rate for different algorithm be present in table 1 . the superior performance of the fmsckf compare to other algorithm in term of update rate be evident from table 1 . to visually compare the algorithm , the root mean square error ( rmse ) plot in position and orientation estimation for one dataset ( mh 01 ) be', 'in term of update rate be evident from table 1 . to visually compare the algorithm , the root mean square error ( rmse ) plot in position and orientation estimation for one dataset ( mh 01 ) be show in figures 3 and 4 , respectively . from figure 3 , it can be see that the 16figure 3 : orientation estimation rmse for different algorithm . figure 4 : position estimation rmse for different algorithm . fmsckf outperform all the other algorithm in term of orientation estimation . one should note that the ascend behavior of orientation error in figure 3 be due to the fact that in ekf-based visual-inertial estimator , the yaw angle be not observable ( li and mourikis , 2013 ) and therefore , it drift over time . from figure 4 , it can be deduce that the fmsckf have a low error in position estimation compare to other algorithm . additionally , in figure 5 , the 3d path for one dataset ( mh 01 ) use the msckf and the fmsckf , alongside the ground-truth be show . final error in orientation and position estimation for these algorithm be provide in table 2 . frame processing time for mh 01 be show in figure 6 . as expect , the fmsckf process the frame significantly faster than the original msckf . moreover , box plot illustrate absolute trajectory error ( ate ) and relative trajectory error ( rte ) be depict in figures 7 and 8 , respectively . as can be see in figures 7 and 8 , the fmsckf demonstrate accuracy on par with other algorithm . notably , it frequently surpass the msckf in both ate and rte , while concurrently achieve a significantly high output rate , a show in table 1 . note that we do not include the ate and rte result of svo+gtsam for mh 03 in figures 7 and 8 , a it error be notably high than compare to the other 17figure 5 : estimated 3d path for mh 01 use the msckf ( blue ) and the fmsckf ( red ) . the ground-truth path be plot in green ( burri et al. , 2016 ) . figure 6 : frame processing time for the fmsckf ( red ) and the msckf ( blue ) . algorithm . furthermore , the number of tracked feature , camera', 'path be plot in green ( burri et al. , 2016 ) . figure 6 : frame processing time for the fmsckf ( red ) and the msckf ( blue ) . algorithm . furthermore , the number of tracked feature , camera pose , and feature use for update be show in figures 9 , 10 , and 11 , respectively . as evident , both the number of tracked feature and the number of feature use for update in the fmsckf be low than those in the msckf , which , a claim , result in a high update rate 18figure 7 : absolute trajectory error ( ate ) for various algorithm . 19figure 8 : relative trajectory error ( rte ) for various algorithm . 20figure 9 : number of tracked feature in each image for the msckf ( blue ) and the fmsckf ( red ) . of the fmsckf . 4.2 . real-world experiments an experimental setup be develop and utilize to test the propose algorithm . the main processor utilize be an nvidia jetson xavier nx . the setup be show in figure 12 . further detail regard the sensor employ in the platform can be find in table 1 3 . the setup be calibrate use kalibr library ( rehder et al. , 2016 ) . the algorithm be not execute in real-time ; instead , the acquire data be later use on the personal laptop to run the filter . three experiment be conduct , the result of which will be present in the following subsection . 4.2.1 . short-range experiment in the first experiment , a path span 45 meter be traverse in approximately 90 second . a sample image record during this experiment be show in figure 21figure 10 : number of camera pose store in the state vector for the fmsckf ( blue ) and the msckf ( red ) . 22figure 11 : number of feature use in the update step of the filter the msckf ( blue ) and the fmsckf ( red ) . 23table 2 : results of various algorithm for mh 01 ( burri et al. , 2016 ) . the final point error be calculate with respect to the traveled distance . methodfinal point er- ror ( % ) final orientation error ( o ) rovio 0.57 2.09 svo ( vio ) 0.98 4.86 svo+gtsam 0.49 1.59 msckf 0.49 1.52 fmsckf 0.25 1.39 table 3 : the sensor setup use in this', '. methodfinal point er- ror ( % ) final orientation error ( o ) rovio 0.57 2.09 svo ( vio ) 0.98 4.86 svo+gtsam 0.49 1.59 msckf 0.49 1.52 fmsckf 0.25 1.39 table 3 : the sensor setup use in this paper . sensor type output rate ( hz ) adis 16467 −2 mems imu 100 basler aca1300 −200uc global shutter camera 10 figure 12 : the customized sensor setup . the camera be mount on the front side . an stm32 f103c8t6 receive data from the imu , and after an initial processing , send it to the nvidia jetson board . 24figure 13 : a sample image of the short-range dataset . figure 14 : estimated path in xz plane use the fmsckf ( blue ) and the msckf ( red ) in the short-range experiment . the start point be indicate by a yellow square and final point be mark with circle . 13 . for a more precise and explicit comparison , only the original msckf and the propose fmsckf be employ for this experiment . the setup be move by a person along a random path , make it impossible to recover the ground-truth trajectory . as a result , to evaluate the performance of the two algorithm , we attempt to create a loop-shaped path and return precisely to the start point . this enable u to measure and compare the final point error for each algorithm . the estimated trajectory be show in figure 14 . as evident from figure 14 , both algorithms exhibit good performance in position estimation . it be worth mention that the difference in the estimated trajectory of the two algorithm be primarily due to the low quality of the camera . the final position estimate of the msckf and the fmsckf , as well a the ground truth , be list in table 4 . accordingly , the final point error of the fmsckf be 0 .14 ( m ) , while that of the msckf be 0 .24 ( m ) . in other word , for the 45-meter-long trajectory , the final point error of the fmsckf be 0 .31 % and the final point error of the msckf be 0 .53 % of the traveled distance . these error be in good compliance with the final point error 25table 4 : results of the short-range experiment . algorithm xf ( m ) yf ( m', 'error of the msckf be 0 .53 % of the traveled distance . these error be in good compliance with the final point error 25table 4 : results of the short-range experiment . algorithm xf ( m ) yf ( m ) zf ( m ) final point error ( % ) fmsckf 0.0806 0.0355 0.1085 0.31 msckf 0.0899 −0.1385 0.1747 0.53 ground truth 0 0 0 n.a . figure 15 : estimated velocity during the short-range experiment use the fmsckf ( solid ) and the msckf ( dash ) . figure 16 : estimated gyroscope bias during the short-range experiment use the fmsckf ( solid ) and the msckf ( dash ) . of the algorithm run on the euroc mav dataset . for instance , for mh 01 , these number be 0 .25 % and 0 .49 % , respectively . estimated velocity and bias be plot in figures 15 to 17 . 4.2.2 . mid-range experiment in the second experiment , the setup be mount on a car , which then traverse a 900-meter-long path . a sample image record during this experiment be show 26figure 17 : estimated accelerometer bias during the short-range experiment use the fmsckf ( solid ) and the msckf ( dash ) . figure 18 : a sample image of the mid-range dataset . in figure 18 . the estimated path use the fmsckf and the msckf along with the ground-truth path be show in figure 19 . note that , similar to the short-range test , gps signal be inaccurate due to presence of tall building . however , unlike the short-range test , the ground-truth be obtain by reconstruct the trajectory of the vehicle base on the lane it travel in . it be worth note that since this ground- truth reconstruction be prone to various error , it shall not be use for numerical evaluation of the algorithm . it be provide solely for visual comparison purpose . the final position estimate of the msckf and the fmsckf , along with the ground truth , be list in table 5 . 4.2.3 . long-range experiment in the last experiment , we traverse a path spanning 2500 meter in a crowded neighborhood with a large number of move car and people . a sample image record during this experiment be show in figure 20 . the ground-truth path', 'we traverse a path spanning 2500 meter in a crowded neighborhood with a large number of move car and people . a sample image record during this experiment be show in figure 20 . the ground-truth path along with the estimate path use both the fmsckf and the msckf algorithm be show in figure 21 . similar to the mid-range experiment , the ground-truth be obtain by reconstruct the trajectory of the vehicle base on the lane it travel 27figure 19 : estimated path in yz plane use the msckf ( blue ) and the fmsckf ( red ) along with the ground-truth path ( black ) in the mid-range experiment . table 5 : results of the mid-range experiment . algorithm xf ( m ) yf ( m ) zf ( m ) final point error ( % ) fmsckf 0−3.3210 −0.7822 0.38 msckf 0 −4.512 −1.128 0.51 ground truth 0 0 0 n.a . figure 20 : a sample image of the long-range dataset . in . 28figure 21 : estimated path in yz plane use the fmsckf ( red ) and the msckf ( blue ) along with the ground-truth path ( black ) in the long-range experiment . the final position estimate of the msckf and the fmsckf , as well a the ground truth , be list in table 6 . using the final point estimate in table 6 , it be straightforward to calculate the final point error for the algorithm . for the fmsckf it be 0 .41 % , and for the msckf , it be 1 .02 % of the traveled distance . we observe that both algorithms perform well in long distance and in the presence of dynamic object . table 6 : results of the long-range experiment . algorithm xf ( m ) yf ( m ) zf ( m ) final point error ( % ) fmsckf 0−28.5850 −55.5651 0.41 msckf 0−30.3751 −27.3251 0.51 ground truth 0 −20 −50 n.a . 295 . discussion the result in section 4 demonstrate the superiority of the fmsckf over the original msckf in term of both speed and accuracy . this superiority be explain in this section . the enhanced update rate in the fmsckf can be attribute to two key factor . firstly , the second scenario in the fmsckf , where the number of feature reach it minimum , happen more frequently than the third one . consequently ,', 'the fmsckf can be attribute to two key factor . firstly , the second scenario in the fmsckf , where the number of feature reach it minimum , happen more frequently than the third one . consequently , our method conduct more frequent pruning of the state vector and the covariance matrix compare to the original msckf , lead to a noticeable enhancement in algorithm speed . secondly , in the conventional msckf , feature be extract from all frame , but in the fmsckf , feature be only extract from the keyframes , when the number of tracked feature be less than nfmin . this strategic feature extraction methodology significantly reduce the computational cost associate with image processing in the fmsckf . as illustrate in section 4 , our approach exhibit comparable accuracy to the msckf in orientation estimation while surpass it in position estimation . the reason for the high accuracy in position in the fmsckf be twofold . firstly , the rate at which update occur in the fmsckf be high compare to the original msckf . to elaborate , a mention in section 2.2 , in the augmentation step , camera pose be calculate use the propagated imu state . we know that the propagated state drift quickly , a imu reading be integrate in that step . as a result , the long the filter delay the update step , the less accurate camera pose will become , and consequently , the less accurate the triangulated point will be . secondly , in the fmsckf , when the number of tracked feature fall below nfmin , a relatively large number of feature ( nfmin ) be use to do the update . hence , the update state be more accurate . it be worth note that this particular event do not occur within the original msckf methodology . figure 22 : the effect of nfminon the error to speed ratio in the fmsckf . as discuss earlier , nfminis a tuning parameter and must be set appropriately . setting nfminto high value cause high state pruning rate , thereby reduce the alge- 30braic computational cost . however , a high value of nfminincreases feature extraction frequency', 'appropriately . setting nfminto high value cause high state pruning rate , thereby reduce the alge- 30braic computational cost . however , a high value of nfminincreases feature extraction frequency , which lead to a significantly high image processing cost . our extensive test show that the algebraic cost constitute only 10 % of the total computational cost , and the remain 90 % pertains to the image process part . moreover , a low value of nfmincorresponds to increase accuracy . this phenomenon occur because when the minimum number of feature be limited , the algorithm gain more information about individual feature over consecutive image frame . in essence , the algorithm ’ s knowledge about a feature increase a it be observe across a great number of frame . consequently , the triangulated point associate with the feature tend to be more precise . as a result of the above discussion , it can be conclude that the low nfmin be set , the high both the update rate and the accuracy will be . to validate the above-mentioned claim in practice , we analyze the performance of the fmsckf under different value of nfminusing the first 10 second of the euroc mav dataset mh01 ( burri et al. , 2016 ) . we calculate the ratio of position error tooutput rate for different value of nfmin . a small ratio correspond to either a small position error , a high output rate , or both , all of which be desirable . as a result , the intro- duced measure can be use to compare the performance of the propose algorithm for different value of the parameter nfmin . the small the ration , the good the performance . the result be show in figure 22 . in accordance with our assertion , it be obvious from figure 22 that when a high nfminis use , the error-to-speed ratio be high . the reason for choose the value 8 for nfminin our implementation be that in our algorithm , we use the well-known 8-point ransac algorithm for outlier rejection ( longuet-higgins , 1981 ) . to compute the fundamental matrix use this approach , we need at least 8 point .', 'in our algorithm , we use the well-known 8-point ransac algorithm for outlier rejection ( longuet-higgins , 1981 ) . to compute the fundamental matrix use this approach , we need at least 8 point . however , one can decide not to use this outlier rejection algorithm and choose a small value for nfmin . according to figure 22 , this will result in an even high accuracy and output rate . 6 . conclusion in this paper , we aim to address the challenge of fast and precise pose estima- tion ( pe ) for agile autonomous robot . we introduce an enhanced variant of the well-known multi-state constraint kalman filter ( msckf ) , name fast msckf ( fmsckf ) , design to tackle the high computational cost associate with real-time implementation on resource-constrained robot . the fmsckf leverage innovative feature marginalization and state pruning technique to achieve computational ef- ficiency , make it approximately six time faster than the standard msckf , all while deliver a substantial improvement of at least 20 % in final position estima- tion accuracy . in section 5 , we thoroughly discuss the difference between the 31msckf and the fmsckf and highlight the reason why the fmsckf outper- form the msckf . our extensive evaluation of the fmsckf on both public datasets and in real-world experiment demonstrate the remarkable performance of the fm- sckf compare to the state-of-the-art algorithm . future work include explore machine-learning-based feature extraction and track algorithm and evaluate their performance and robustness in vio application . another direction could be use additional sensor , such a lidar , to enhance the performance of the propose algorithm . references abbasi , e. , ghayour , m. , danesh , m. , amiri , p. , yoosefian , m.h. , 2018a . formation flight control and path tracking of a multi-quadrotor system in the presence of measurement noise and disturbance , in : 2018 6th rsi international conference on robotics and mechatronics ( icrom ) , ieee . pp . 273–279 . abbasi , e. , ghayour , m. , danesh ,', 'in the presence of measurement noise and disturbance , in : 2018 6th rsi international conference on robotics and mechatronics ( icrom ) , ieee . pp . 273–279 . abbasi , e. , ghayour , m. , danesh , m. , yoosefian , m.h. , 2018b . optimal path tracking of a quadrotor in the presence of obstacle use the league championship algorithm , in : 2018 6th rsi international conference on robotics and mechatronics ( icrom ) , ieee . pp . 236–242 . abdollahi , m. , banazadeh , a. , pourtakdoust , s. , 2019 . e \\u200cx \\u200cp \\u200ce \\u200cr \\u200ci \\u200cm \\u200ce \\u200cn \\u200ct \\u200ca \\u200cl i \\u200cn \\u200cv \\u200ce \\u200cs \\u200ct \\u200ci \\u200cg \\u200ca \\u200ct \\u200ci \\u200co \\u200cn o \\u200cf v \\u200ce \\u200cr \\u200ct \\u200ci \\u200cc \\u200ca \\u200cl c \\u200cg p \\u200co \\u200cs \\u200ci \\u200ct \\u200ci \\u200co \\u200cn c \\u200ch \\u200ca \\u200cn \\u200cg \\u200ce \\u200cs o \\u200cn q \\u200cu \\u200ca \\u200cd \\u200cr \\u200co \\u200ct \\u200co \\u200cr ’ s p \\u200ce \\u200cr \\u200cf \\u200co \\u200cr \\u200cm \\u200ca \\u200cn \\u200cc \\u200ce v \\u200ci \\u200ca f \\u200cr \\u200ce \\u200cq \\u200cu \\u200ce \\u200cn \\u200cc \\u200cy-d \\u200co \\u200cm \\u200ca \\u200ci \\u200cn i \\u200cd \\u200ce \\u200cn \\u200ct \\u200ci \\u200cf \\u200ci \\u200cc \\u200ca \\u200ct \\u200ci \\u200co \\u200cn t \\u200ce \\u200cc \\u200ch \\u200cn \\u200ci \\u200cq \\u200cu \\u200ce \\u200cs . sharif journal of mechanical engineering 35 , 129–138 . azimi , a. , ahmadabadian , a.h. , remondino , f. , 2022 . pks : a photogrammetric key-frame selection method for visual-inertial system build on orb-slam3 . isprs journal of photogrammetry and remote sensing 191 , 18–32 . bloesch , m. , burri , m. , omari , s. , hutter , m. , siegwart , r. , 2017 . iterated extended kalman filter base visual-inertial odometry use direct photometric feedback . the international journal of robotics research 36 , 1053–1072 . brossard , m. , bonnabel , s. , barrau , a. , 2018 . invariant kalman filter for visual inertial slam , in : 2018 21st international conference on information fusion ( fu- sion ) , ieee . pp . 2021–2028 . burri , m. , nikolic , j. , gohl , p. , schneider , t. , rehder , j. , omari , s. , achtelik , m.w. , siegwart , r. , 2016 . the euroc micro aerial vehicle datasets . the international journal of robotics research 35 , 1157–1163 . 32cen , r. , zhang , x. , tao , y. , xue , f. , zhang , y. , 2020 . temporal delay estimation of sparse direct visual inertial odometry for mobile robot . journal of the franklin institute 357 , 3893–3906 . chen , c. , wang , b. , lu , c.x. , trigoni ,', ', y. , 2020 . temporal delay estimation of sparse direct visual inertial odometry for mobile robot . journal of the franklin institute 357 , 3893–3906 . chen , c. , wang , b. , lu , c.x. , trigoni , n. , markham , a. , 2020 . a survey on deep learning for localization and mapping : towards the age of spatial machine intelli- gence . arxiv preprint arxiv:2006.12567 . cheng , c. , li , x. , xie , l. , li , l. , 2022 . autonomous dynamic docking of uav base on uwb-vision in gps-denied environment . journal of the franklin institute 359 , 2788–2809 . delmerico , j. , scaramuzza , d. , 2018 . a benchmark comparison of monocular visual- inertial odometry algorithm for fly robot , in : 2018 ieee international confer- ence on robotics and automation ( icra ) , ieee . pp . 2502–2509 . engel , j. , koltun , v. , cremers , d. , 2017 . direct sparse odometry . ieee transaction on pattern analysis and machine intelligence 40 , 611–625 . fischler , m.a. , bolles , r.c. , 1981 . random sample consensus : a paradigm for model fit with application to image analysis and automate cartography . communi- cation of the acm 24 , 381–395 . forster , c. , carlone , l. , dellaert , f. , scaramuzza , d. , 2016a . on-manifold preinte- gration for real-time visual–inertial odometry . ieee transactions on robotics 33 , 1–21 . forster , c. , zhang , z. , gassner , m. , werlberger , m. , scaramuzza , d. , 2016b . svo : semidirect visual odometry for monocular and multicamera system . ieee trans- action on robotics 33 , 249–265 . heo , s. , cha , j. , park , c.g. , 2018 . ekf-based visual inertial navigation use slide window nonlinear optimization . ieee transactions on intelligent transportation systems 20 , 2470–2479 . hesch , j.a. , kottas , d.g. , bowman , s.l. , roumeliotis , s.i. , 2013 . consistency anal- ysis and improvement of vision-aided inertial navigation . ieee transactions on robotics 30 , 158–176 . huang , g. , 2019 . visual-inertial navigation : a concise review , in : 2019 international conference on robotics and automation (', 'inertial navigation . ieee transactions on robotics 30 , 158–176 . huang , g. , 2019 . visual-inertial navigation : a concise review , in : 2019 international conference on robotics and automation ( icra ) , ieee . pp . 9572–9582 . 33huang , g.p. , mourikis , a.i. , roumeliotis , s.i. , 2009 . a first-estimates jacobian ekf for improve slam consistency , in : experimental robotics : the eleventh interna- tional symposium , springer . pp . 373–382 . li , c. , waslander , s.l. , 2020 . towards end-to-end learning of visual inertial odometry with an ekf , in : 2020 17th conference on computer and robot vision ( crv ) , ieee . pp . 190–197 . li , m. , mourikis , a.i. , 2013 . high-precision , consistent ekf-based visual-inertial odom- etry . the international journal of robotics research 32 , 690–711 . longuet-higgins , h.c. , 1981 . a computer algorithm for reconstruct a scene from two projection . nature 293 , 133–135 . loo , s.y. , amiri , a.j. , mashohor , s. , tang , s.h. , zhang , h. , 2019 . cnn-svo : improv- ing the mapping in semi-direct visual odometry use single-image depth prediction , in : 2019 international conference on robotics and automation ( icra ) , ieee . pp . 5218–5223 . ma , f. , shi , j. , yang , y. , li , j. , dai , k. , 2019 . ack-msckf : tightly-coupled ackermann multi-state constraint kalman filter for autonomous vehicle localization . sensors 19 , 4816 . macias , v. , becerra , i. , martinez , e. , murrieta-cid , r. , becerrra , h.m. , 2021 . single landmark feedback-based time optimal navigation for a differential drive robot . journal of the franklin institute 358 , 4761–4792 . mourikis , a.i. , roumeliotis , s.i. , 2007 . a multi-state constraint kalman filter for vision-aided inertial navigation , in : proceedings 2007 ieee international confer- ence on robotics and automation , ieee . pp . 3565–3572 . mur-artal , r. , montiel , j.m.m. , tardos , j.d. , 2015 . orb-slam : a versatile and accu- rate monocular slam system . ieee transaction on robotics 31 , 1147–1163 . nousias , s. ,', 'pp . 3565–3572 . mur-artal , r. , montiel , j.m.m. , tardos , j.d. , 2015 . orb-slam : a versatile and accu- rate monocular slam system . ieee transaction on robotics 31 , 1147–1163 . nousias , s. , lourakis , m. , bergeles , c. , 2019 . large-scale , metric structure from motion for unordered light field , in : proceedings of the ieee/cvf conference on computer vision and pattern recognition , pp . 3292–3301 . qin , t. , li , p. , shen , s. , 2018 . vins-mono : a robust and versatile monocular visual- inertial state estimator . ieee transactions on robotics 34 , 1004–1020 . rehder , j. , nikolic , j. , schneider , t. , hinzmann , t. , siegwart , r. , 2016 . extending kalibr : calibrating the extrinsics of multiple imu and of individual ax , in : 2016 34ieee international conference on robotics and automation ( icra ) , ieee . pp . 4304–4311 . rosinol , a. , abate , m. , chang , y. , carlone , l. , 2020 . kimera : an open-source library for real-time metric-semantic localization and mapping , in : 2020 ieee interna- tional conference on robotics and automation ( icra ) , ieee . pp . 1689–1696 . sola , j. , 2017 . quaternion kinematics for the error-state kalman filter . arxiv preprint arxiv:1711.02508 . sukkarieh , s. , nebot , e.m. , durrant-whyte , h.f. , 1999 . a high integrity imu/gps navigation loop for autonomous land vehicle application . ieee transaction on robotics and automation 15 , 572–578 . sun , k. , mohta , k. , pfrommer , b. , watterson , m. , liu , s. , mulgaonkar , y. , taylor , c.j. , kumar , v. , 2018 . robust stereo visual inertial odometry for fast autonomous flight . ieee robotics and automation letters 3 , 965–972 . tomasi , c. , kanade , t. , 1993 . shape and motion from image stream : a factorization method . proceedings of the national academy of sciences 90 , 9795–9802 . triggs , b. , mclauchlan , p.f. , hartley , r.i. , fitzgibbon , a.w. , 2000 . bundle adjust- ment—a modern synthesis , in : vision algorithms : theory and practice : inter- national workshop on vision algorithms corfu', ', p.f. , hartley , r.i. , fitzgibbon , a.w. , 2000 . bundle adjust- ment—a modern synthesis , in : vision algorithms : theory and practice : inter- national workshop on vision algorithms corfu , greece , september 21–22 , 1999 proceedings , springer . pp . 298–372 . zheng , x. , moratto , z. , li , m. , mourikis , a.i. , 2017 . photometric patch-based visual- inertial odometry , in : 2017 ieee international conference on robotics and au- tomation ( icra ) , ieee . pp . 3264–3271 . zuo , x. , merrill , n. , li , w. , liu , y. , pollefeys , m. , huang , g. , 2021 . codevio : visual-inertial odometry with learned optimizable dense depth , in : 2021 ieee international conference on robotics and automation ( icra ) , ieee . pp . 14382– 14388 . 35']",https://www.mdpi.com/2072-4292/14/13/3010
8.pdf,"review of visual odometry : type , approach , challenge , and application mohammad o . a. aqel1* , mohammad h. marhaban2 , m. iqbal saripan3 and napsiah bt . ismail4 background accurate localization of a vehicle be a fundamental challenge in mobile robot applica - tions . a robot must maintain knowledge of it position over time to achieve autono - mous navigation . therefore , various sensor , technique , and system for mobile robot positioning , such a wheel odometry , laser/ultrasonic odometry , global position system ( gps ) , global navigation satellite system ( gnss ) , inertial navigation system ( ins ) , and visual odometry ( vo ) , have be develop by researcher and engineer . however , each technique have it own weakness . although wheel odometry be the simple tech - nique available for position estimation , it suffer from position drift due to wheel slip - page ( fernandez and price 2004 ) . ins be highly prone to accumulate drift , and a highly precise ins be expensive and an unviable solution for commercial purpose . although gps be the most common solution to localization a it can provide absolute position without error accumulation , it be only effective in place with a clear view of the sky . moreover , it can not be use indoors and in confined space ( gonzalez et al . 2012 ) . the commercial gps estimate position with error in the order of meter . this error be con - sidered too large for precise application that require accuracy in centimeter , such a autonomous parking . differential gps and real time kinematic gps can provide position with centimeter accuracy , but these technique be expensive.abstract accurate localization of a vehicle be a fundamental challenge and one of the most important task of mobile robot . for autonomous navigation , motion tracking , and obstacle detection and avoidance , a robot must maintain knowledge of it position over time . vision-based odometry be a robust technique utilize for this purpose . it allow a vehicle to localize itself robustly by use only a stream of image capture by a camera attach to the vehicle . this paper present a review of state-of-the-art visual odometry ( vo ) and it type , approach , application , and challenge . vo be compare with the most common localization sensor and technique , such a inertial navigation system , global positioning system , and laser sensor . several area for future research be also highlight . keywords : visual odometry , localization sensor , image stream , global positioning system , inertial navigation systemopen access © the author ( s ) 2016 . this article be distribute under the term of the creative commons attribution 4.0 international license ( http : //creativecommons.org/licenses/by/4.0/ ) , which permit unrestricted use , distribution , and reproduction in any medium , provide you give appropriate credit to the original author ( s ) and the source , provide a link to the creative commons license , and indicate if change be made.reviewaqel et al . springerplus ( 2016 ) 5:1897 doi 10.1186/s40064-016-3573-7 *correspondence : aqel2001 @ hotmail.com 1 department of engineering , faculty of engineering and information technology , al-azhar university-gaza , gaza , palestine full list of author information be available at the end of the articlepage 2 of 26 aqel et al . springerplus ( 2016 ) 5:1897 the term “ odometry ” originate from the two greek word hodos ( mean “ journey ” or “ travel ” ) and metron ( mean “ measure ” ) ( fernandez and price 2004 ) . this derivation be relate to the estimation of the change in a robot ’ s pose ( translation and orientation ) over time . mobile robot use data from motion sensor to estimate their position rela - tive to their initial location ; this process be call odometry . vo be a technique ( show in fig . 1 ) use to localize a robot by use only a stream of image acquire from a single or multiple camera attach to the robot ( scaramuzza and fraundorfer 2011 ) . the image contain a sufficient amount of meaningful information ( color , texture , shape , etc . ) to estimate the movement of a camera in a static environment ( rone and ben-tzvi 2013 ) . the article be organize a follow . the next section present the six most common sensor and technology utilize for localization in robotic application and compare their advantage and disadvantage . “ vo ” section provide a detailed discussion on vo and it type , approach , application , and challenge . prior related work be present and discuss in “ prior vo work ” section . finally , the conclusion for this article be pre - sented in “ conclusions ” section . localization sensor and technique wheel odometry the simple and most widely utilized method to estimate the position of mobile robot be wheel odometry . it be use to estimate wheeled vehicle position by count the num - ber of revolution of the wheel that be in contact with the ground . wheel revolution can be translate accurately into linear displacement relative to the ground ( borenstein et al . 1996 ) . encoders be use to measure wheel rotation , a show in fig . 2 . fig . 1 visual odometry [ aqel et al . 2016 ] fig . 2 wheel odometry with an optical encoder [ pololu corporation 2016 ] page 3 of 26 aqel et al . springerplus ( 2016 ) 5:1897 wheel odometry be a relative positioning technique . it suffer from position drift and inaccuracy because of wheel slippage , which lead to error accumulation over time ( fer - nandez and price 2004 ; nourani-vatani et al . 2009 ) . translation and orientation error in wheel odometry increase proportionally with the total travel distance . wheel odometry be simple and inexpensive , allow for high sampling rate , and exhibit good short-term accuracy ( borenstein et al . 1997 ; aboelmagd et al . 2013 ) . ins ins be a relative positioning technique that provide the position and orientation of an object relative to a know starting point , orientation , and velocity . as show in fig . 3 , it be a navigation aid that use a computer , motion sensor ( accelerometer ) , and rota - tion sensor ( rate gyroscope ) to continuously calculate the position , orientation , and velocity of a move vehicle , which could be a ground vehicle , an airplane , a spaceship , a rocket , a surface ship , or a submarine . the advantage of ins be that it be self-contained , that be , it do not require external reference ( wang et al . 2014 ; woodman 2007 ) . however , ins be highly prone to drift accumulation because calculation of the change in velocity and position be implement by perform successive mathematical inte - grations of acceleration with respect to time . accelerometer data need to be integrate twice to yield the position , whereas rate-gyro data be only integrated once to track the orientation . therefore , any small error in the measurement of acceleration and angu - lar velocity be integrate into large error in velocity , which be compound into still large error in position ( rone and ben-tzvi 2013 ; wang et al . 2014 ; woodman 2007 ) . the error be cumulative and increase with time . thus , the position need to be peri - odically correct with the input of another navigation system . consequently , inertial sensor be inaccurate and unsuitable for position applica - tions over an extended period of time and be usually utilized to supplement other navi - gation system , such a gps , to provide a high degree of accuracy than be possible with the use of any single system ( maklouf and adwaib 2014 ) . moreover , accurate inertial navigation require high-cost equipment . thus , the high cost of a highly precise ins fig . 3 inertial navigation system . a block diagram of ins . b miniature ins [ sbg systems 2016 ] page 4 of 26 aqel et al . springerplus ( 2016 ) 5:1897 make the method an unviable solution for commercial purpose ( borenstein et al . 1996 , 1997 ) . gps/gnss gnss be use a an umbrella term for all current and future global radio-navigation sys - tems include the u.s. gps , the russian global navigation satellite system ( glonass ) , and the european georgia library learning online system ( galileo ) . at present , there be two navigation satellite system in orbit which be gps and glonass . gali - leo be plan to be deploy and operational by 2013 ( rizos et al . 2010 ) . before gps be invent in the early 1970s by the u.s. department of defense ( dod ) , the primary method of navigation revolve around the map and compass . gps be a sat - ellite-based navigation system that allow user to accurately determine their location anywhere on or slightly above the surface of the earth ( el-rabbany 2002 ; cook 2011 ) . gps be utilize for more than simple outdoor navigational exercise ; it be use in geol - ogy , agriculture , landscaping , construction , and public transportation . gps provide accurate position , navigation , and time information free of charge to anyone who have a gps receiver . gps consist of a nominal constellation of 24 operational satellite orbit the earth and transmitting encode radio frequency ( rf ) signal . they be arrange so that four satellite be place in each of six orbital plane to ensure continuous world - wide coverage , a show in fig . 4a ( el-rabbany 2002 ; aboelmagd et al . 2013 ) . only four satellite be need to provide positioning or location information . through trilateration , ground receiver can calculate their position by use the travel time of the satellite ’ s signal and information about their current location that be include in the transmitted signal . each satellite be equip with a radio transmitter and receiver and atomic clock . the receiver clock be not as precise a the atomic clock and normally exhibit bias . this bias generate error in the travel time of the signal and lead to error in the calculation of the distance to the satellite . theoretically , by use the principle of trilateration/triangulation , a gps receiver require the range to three satellite only to calculate the 3d position ( latitude , longitude , and altitude ) , but a fourth satellite be require to estimate the offset of the receiver ’ s clock from the system clock and to cor - rect clock bias in the receiver . figure 4b show the concept of gps positioning by trilat - eration use three satellite ( aboelmagd et al . 2013 ; cook 2011 ) . fig . 4 global positioning system [ aboelmagd et al . 2013 ] . a gps satellite constellation . b concept of posi- tioning by trilateration ( red dot represent user ’ s position ) page 5 of 26 aqel et al . springerplus ( 2016 ) 5:1897 gps provide the absolute position with a know ratio of error . its main advantage be it immunity to error accumulation over time and it long-term stability . gps be a revolutionary technology for outdoor navigation ; it be effective in area with a clear view of the sky but be unusable for indoor , confine , underground , and underwater space . the limitation of gps include outage cause by satellite signal blockage , occasional high noise content , multipath effect , low bandwidth , and interference or jamming . gps outages occur in urban canyon , tunnel , and other gps-denied environment and con - fin place ( gonzalez et al . 2012 ; maklouf and adwaib 2014 ; cook 2011 ; wang et al . 2014 ) . common standalone gps be use for positioning and have an accuracy within 10 m. differential gps ( dgps ) and real-time kinematic gps ( rtk-gps ) be invent to improve gps accuracy and allow for localization in outdoor open-field environment within a sub-meter or centimeter order . they be relative position technique that employ two or more receiver simultaneously to track the same satellite . dgps mainly consist of three element : one gps receiver ( base station ) locate at a known location , one gps receiver ( user receiver ) call a rover , and a radio communication medium between these two receiver ( fig . 5 ) . dgps can correct bias error of the user receiver by use measure bias error at the base station ( aboelmagd et al . 2013 ; morales and tsubouchi 2007 ) . rtk-gps provide real-time measurement in centimeter accuracy . it provide two solution , namely , float and fix . the first solution require a minimum of four common satellite and provide an accuracy range of approximately 20 cm to 1 m. the second rtk-gps solution require at least five common satellite and provide accuracy within 2 cm ( aboelmagd et al . 2013 ; cook 2011 ) . sonar/ultrasonic sensor sonar/ultrasonic sensor utilize acoustic energy to detect object and measure distance from the sensor to the target object . they have two main part , namely , transmitter and receiver . the transmitter send a short ultrasonic pulse , and the receiver receive what come back of the signal after it have reflect off nearby object . the sensor measure the time-of-flight ( tof ) , which be the time from signal transmission to reception . given that the transmission rate of an ultrasonic signal be know , the distance to the target that reflect the signal can be compute . sonar sensor can be utilize to localize mobile fig . 5 real-time differential global positioning systempage 6 of 26 aqel et al . springerplus ( 2016 ) 5:1897 robot through model matching or triangulation by compute the pose change between every two sensor input acquire at two different pose . by combine many sonar sen - sors , a sonar array can obtain a detailed picture of the environment and exhibit high positioning accuracy ( jiménez and seco 2005 ; kreczmer 2010 ) . the major drawback of these sensor be the reflection of signal wave that be highly dependent on the material and the orientation of the object surface . moreover , they be sensitive to noise from the environment and other robot use ultrasound with the same frequency . many object in the environment be assume to be specular reflec - tor for ultrasonic wave , which cause a sonar sensor to receive a multi-reflected echo instead of the first one ( kreczmer 2010 ; rone and ben-tzvi 2013 ; sanchez et al . 2012 ) . laser sensor laser sensor can be utilize in several application relate to position . it be a remote sensing technology for distance measurement that involve transmit a laser toward the target and then analyze the reflected light . laser-based range measurement depend on either tof or phase-shift technique . similar to the sonar sensor , in a tof system , a short laser pulse be send out , and the time until it return be measure . this type of sensor be often refer to a laser radar or light detection and range sensor ( lidar ) . however , in phase-shift system , a continuous signal be transmit . the phase of the returned signal be compare with a reference signal generate by the same source . the velocity of the target and the distance to it be measure with the doppler shift ( horn and schmidt 1995 ; takahashi 2007 ) . lidar be mostly use in obstacle detection and avoidance , mapping , and 3d motion capture . lidar can be integrate with gps and ins to enhance the accuracy of outdoor positioning application . although sonar sensor have a large beam width that allow for increase coverage , the angular resolution with a laser scanner be much good than that with an ultrasonic one ( aboelmagd et al . 2013 ; lingemann et al . 2005 ) . a drawback of lidar when compare with sonar sensor be that it entail a highly expensive solution . moreover , analysis of lidar data have a high computational cost and may affect the response of real-time application . the iterative manner of calculate the optimal match between two laser scan increase the computational cost . furthermore , scan can fail when the material appear a transparent for the laser , such a glass , because the reflection on these surface lead to suspicious data ( takahashi 2007 ; horn and schmidt 1995 ; lingemann et al . 2005 ) . optical camera cameras and vision system can be employ in mobile robotic application for locali - zation and to perform various task . recently , many researcher have be show interest in visual-based localization system because these system be more robust and reliable than other sensor-based localization system . camera image can be utilize for indoor and outdoor vehicle navigation , such a to detect road edge , lane , and their transition as well a road intersection . the image capture by a camera can provide a large amount of information to be use for several purpose , include localization . compared with proximity sensor , optical camera be low-cost sensor that provide a large amount of meaningful information . moreover , they be passive ; that be , visual page 7 of 26 aqel et al . springerplus ( 2016 ) 5:1897 localization system do not suffer from the interference often encounter when active ultrasonic or laser proximity sensor be use ( frontoni 2012 ; rone and ben-tzvi 2013 ) . vision-based navigation of mobile robot be one of the main goal of computer vision and robotics research ( campbell et al . 2005 ) . this approach be a non-contact method for the effective positioning of mobile robot , particularly in outdoor application ( naga - tani et al . 2010 ) . for autonomous navigation , a robot need to track it own position and motion . vo provide an incremental online estimation of the vehicle position by analyze the image sequence capture by a camera ( campbell et al . 2005 ; gonzalez et al . 2012 ) . vision-based odometry be an inexpensive alternative technique that be rela - tively more accurate than conventional technique , such a gps , ins , and wheel odome - try ( howard 2008 ) . vo have a good trade-off among cost , reliability , and implementation complexity ( nistér et al . 2004 ) . it can estimate robot location inexpensively by use a consumer-grade camera instead of expensive sensor or system , such a gps and ins ( nistér et al . 2006 ; nourani-vatani et al . 2009 ) . however , image analysis be typically computationally expensive . in visual localization , the computation involve several step , namely , ( 1 ) acquisition of camera image , ( 2 ) extraction of several image feature ( edge , corner , line , etc . ) , ( 3 ) matching between image frame , and ( 4 ) calculation of the position by calculate the pixel displacement between frame . moreover , vision algorithm be highly sensitive to operate and envi - ronmental condition , such a lightning , texture , illumination change throughout the day , presence of blur in image , presence of shadow , and presence of water or snow on the ground . therefore , these algorithm may perform well under several condition , but in other environmental condition , it will not work well and thus become unreliable ( aboelmagd et al . 2013 ) . table 1 show a summary of the feature and drawback of the six most commonly use localization technology . the process of estimate ego-motion ( translation and orientation of an agent ( e.g. , vehicle , human , and robot ) ) by use only the input of a single or multiple camera attach to it be call vo ( scaramuzza and fraundorfer 2011 ) . vo localization be the main task for autonomous vehicle to be able to track their path and properly detect and avoid obstacle . vision-based odometry be one of the robust tech - niques use for vehicle localization . this section comprehensively discuss vo and it type , approach , application , and challenge . what be vo ? vo be the pose estimation process of an agent ( e.g. , vehicle , human , and robot ) that involve the use of only a stream of image acquire from a single or from multiple cam - era attach to it ( scaramuzza and fraundorfer 2011 ) . the core of vo be camera pose estimation ( ni and dellaert 2006 ) . it be an ego-motion online estimation process from a video input ( munguia and gra 2007 ) . this approach be a non-contact method for the effective positioning of mobile robot ( nagatani et al . 2010 ) . vo provide an incremen - tal online estimation of a vehicle ’ s position by analyze the image sequence capture by a camera ( campbell et al . 2005 ; gonzalez et al . 2012 ) .page 8 of 26 aqel et al . springerplus ( 2016 ) 5:1897 the idea of estimate a vehicle ’ s pose from visual input alone be introduce and describe by moravec in the early 1980s ( nistér et al . 2004 ; scaramuzza and fraundorfer 2011 ) . from 1980 to 2000 , vo research be dominate by nasa in preparation for the 2004 mars mission . the term “ visual odometry ” be coin by nistér et al . ( 2004 ) . the term be select because vision-based localization be similar to wheel odometry in that it incrementally estimate the motion of a vehicle by integrate the number of turn of it wheel over time ( scaramuzza and fraundorfer 2011 ) . in the same manner , vo inte - grate pixel displacement between image frame over time . why vo ? vo be an inexpensive and alternative odometry technique that be more accurate than conventional technique , such a gps , ins , wheel odometry , and sonar localization system , with a relative position error range from 0.1 to 2 % ( scaramuzza and fraun - dorfer 2011 ) . this method be characterize by good balance among cost , reliability , and implementation complexity ( nistér et al . 2004 ) . the use of a consumer-grade camera instead of expensive sensor or system , such a gps , ins , and laser-based localization system , be a straightforward and inexpensive method to estimate robot location ( nis - tér et al . 2006 ; gonzalez et al . 2012 ; nourani-vatani et al . 2009 ) . although gps can be utilize for outdoor localization , lose gps information cause significant error ( taka - hashi 2007 ) . images store large amount of meaningful information , which be sufficient to esti - mate the movement of a camera ( rone and ben-tzvi 2013 ) . vo be unaffected by wheel slippage in uneven terrain or other unfavorable condition . furthermore , vo work table 1 comparison of commonly use localization sensor sensor/technology advantages disadvantages wheel odometry simple to determine position/orientation short term accuracy , and allow high sample rate low cost solutionposition drift due to wheel slippageerror accumulation over timevelocity estimation require numerical differentiation that produce additional noise ins provides both position and orientation use 3-axis accelerometer and gyro - scope not subject to interference outagesposition drift ( position estimation require second-order integral ) have long-term drift error gps/gnss provides absolute position with known value of error no error accumulation over timeunavailable in indoor , underwater , and close area affected by rf interference ultrasonic sensor provides a scalar distance measurement from sensor to object inexpensive solutionreflection of signal wave be dependent on material or orientation of obstacle surface suffer from interference if multiple sensor be use low angular resolution and scan rate laser sensor similar to sonar sensor but have high accuracy and scan rate return the distance to a single point ( rangefinder ) or an array of distance ( scanner ) reflection of signal wave be dependent on material or orientation of obstacle surface expensive solution optical camera images store a huge meaningful informa- tion provide high localization accuracyinexpensive solutionrequires image-processing and data- extraction technique high computational-cost to process imagespage 9 of 26 aqel et al . springerplus ( 2016 ) 5:1897 effectively in gps-denied environment ( scaramuzza and fraundorfer 2011 ) . the rate of local drift under vo be small than the drift rate of wheel encoders and low-precision ins ( howard 2008 ) . vo can be integrate with gps and ins for maximum accuracy . different from laser and sonar localization system , vo do not emit any detecta - ble energy into the environment . moreover , compare with gps , vo do not require the existence of other signal ( ni and dellaert 2006 ) . compared with the use of other sensor , the use of camera for robot localization have the advantage of cost reduction , allow for a simple integration of ego-motion data into other vision-based algorithm , such a obstacle , pedestrian and lane detection , and without the need for calibration between sensor ( wang et al . 2011 ) . cameras be small , cheap , lightweight , low pow - ered , and versatile . thus , they can also be employ in any vehicle ( land , underwater , air ) and for other robotic task ( e.g. , object detection and recognition ) . vo challenge although indoor robot localization have be implement successfully , robot locali - zation in outdoor environment remain a challenging problem . many factor , ( e.g. , terrain be usually not flat , direct sunlight , shadow , and dynamic change in the environment cause by wind and sunlight ) make localization difficult in outdoor envi - ronments ( takahashi 2007 ) . the main challenge in vo system be mainly relate to computational cost and light and imaging condition ( gonzalez et al . 2013 ; nagatani et al . 2010 ; nourani-vatani and borges 2011 ; yu et al . 2011 ) . for vo to work efficiently , sufficient illumination and a static scene with enough tex - ture should be present in the environment to allow apparent motion to be extract ( scaramuzza and fraundorfer 2011 ) . in area that have a smooth and low-textured sur - face floor , directional sunlight and lighting condition be highly consider , lead to non-uniform scene lighting . moreover , shadow from static or dynamic object or from the vehicle itself can disturb the calculation of pixel displacement and thus result in erro - neous displacement estimation ( gonzalez et al . 2012 ; nourani-vatani and borges 2011 ) . monocular vision system suffer from scale uncertainty ( kitt et al . 2011 ; cumani 2011 ; zhang et al . 2014 ) . if the surface be uneven , the image scale will fluctuate , and the image scale factor will be difficult to estimate . according to kitt et al . ( 2011 ) , estimation of the scaling factor may become erroneous when a large change in the road slope occur , which may lead to incorrect estimation of the result trajectory . vo application vo have a wide range of application and have be effectively apply in several field . its application domain include robotics , automotive , and wearable computing ( scara - muzza and fraundorfer 2011 ; fraundorfer and scaramuzza 2012 ) . vo be apply in many type of mobile robotic system , such a ground , underwater , aerial , and space robot . in space exploration , for example , vo be use to estimate the ego-motion of the nasa mars rover ( maimone et al . 2007 ) . nasa utilizes vo to track the motion of the rover a a supplement to dead reckon . vo be mainly use for navigation and to reach target efficiently as well a to avoid obstacle while drive . it be also apply in unmanned aerial vehicle ( uavs ) to perform autonomous take-off and landing and point-to-point navigation . moreover , vo play a page 10 of 26 aqel et al . springerplus ( 2016 ) 5:1897 significant role in autonomous underwater vehicle and coral-reef inspection system ( dunbabin et al . 2005 ) . given that the gps signal degrades or becomes unavailable in underwater environment , underwater vehicle can not rely on gps for pose estimation ; therefore , vo be consider a cost-effective solution for underwater localization system . in the automotive industry , vo also play a big role . it be apply in numerous driver assistance system , such a vision-based assisted brake system . vo be consider a cost-effective solution compare with lidar system ( fraundorfer and scaramuzza 2012 ) . in ground vehicle robotics , the effective use of visual sensor for navigation and obstacle detection be the main goal ( nistér et al . 2006 ) . vo be employ in case where the gps signal be unavailable ( in planetary environment ) , too heavy to carry ( on a small air vehicle ) , or insufficiently accurate at a low cost ( in agricultural application ) ( zhang et al . 2014 ; jiang et al . 2014a ) . it be also use in agricultural field robot to estimate the robot ’ s position relative to the crop ( ericson and astrand 2008 ; jiang et al . 2014a ) . types of camera use in vo vo can be classify accord to the type of camera/data sensor utilize to estimate the robot trajectory ( valiente garcía et al . 2012 ) . various type of camera , such a stereo , monocular , stereo or monocular omnidirectional , and rgb-d camera ( fig . 6 ) , can be use for vo purpose . most vo method that have be propose in exist literature use either stereo or monocular camera and can be roughly classify a stereo or monocular vo sys - tems . the system that utilize a binocular camera be consider stereo vo system , a implement by nistér et al . ( 2006 ) , howard ( 2008 ) , azartash et al . ( 2014 ) , golban et al . ( 2012 ) , soltani et al . ( 2012 ) , siddiqui and khatibi ( 2014 ) , mouats et al . ( 2014 ) , alonso et al . ( 2012 ) , mcmanus et al . ( 2013 ) , jiang et al . ( 2013 ) , garcía-garcía et al . ( 2008 ) , mar - tinez ( 2015 ) ; those that use a monocular camera be consider monocular vo system , fig . 6 different type of camera use in vo system . a stereo camera [ courtesy of voltrium ] . b stereo omnidirectional [ courtesy of occam ] . c monocular camera [ courtesy of microsoft ] . d monocular omnidirec- tional [ courtesy of occam ] page 11 of 26 aqel et al . springerplus ( 2016 ) 5:1897 a apply by yu et al . ( 2011 ) , gonzalez et al . ( 2012 , 2013 ) , lovegrove et al . ( 2011 ) , mar - tinez ( 2013 ) , nagatani et al . ( 2010 ) , nourani-vatani et al . ( 2009 ) , royer et al . ( 2007 ) , jiang et al . ( 2014b ) . a binocular camera have two lens , with a separate image sensor for each lens . it have be use on mars to estimate robot motion since early 2004 ( nistér et al . 2006 ) . given that information on the third dimension ( i.e. , depth ) can be extract from a sin - gle frame , the image scale can be immediately and instantaneously retrieve because the size of the stereo baseline be fix and know , thereby result in an efficient and accurate triangulation process . moreover , the various feature present in both type of camera increase the track ability in subsequent frame ( gonzalez et al . 2012 ; nistér et al . 2006 ) . however , stereo camera be more expensive than conventional camera . in addition , binocular camera require more calibration effort than monocular camera , and error in calibration directly affect the motion estimation process ( kitt et al . 2011 ) . furthermore , it be very important for stereo vo that the two image of the stereo pair to be acquire at exactly the same time interval . that can be achieve by synchronize the shutter speed of the two camera of stereo vision or by synchronize the two camera by an external trigger signal provide by the controlling pc through serial or parallel port ( krešo et al . 2013 ; cumani and guiducci 2008 ) . much more effort be require to maintain a calibrated constant baseline between the pair of camera than to maintain a single calibrate camera . stereo vo can be degrade to the monocular case when the stereo baseline be much small than the distance to the scene from the camera . ste - reo vision become ineffective in this case , and monocular method be recommend ( scaramuzza and fraundorfer 2011 ; sünderhauf and protzel 2007 ) . using a monocular camera mitigate the effect of calibration error in motion estima - tion . low cost and easy deployment be the main motivation for use the monocular camera in many common application , such a cellular phone and laptop . however , monocular vision system suffer from scale uncertainty ( kitt et al . 2011 ; cumani 2011 ) . as discuss by nagatani et al . ( 2010 ) , kitt et al . ( 2011 ) , nourani-vatani et al . ( 2009 ) , gonzalez et al . ( 2012 ) , cumani ( 2011 ) , if the surface be uneven , the image scale will fluc - tuate , and the image scale factor will become difficult to estimate . according to ( kitt et al . 2011 ) , estimation of the scaling factor may become erroneous if a large change in the road slope occur , which may lead to incorrect estimation of the result trajec - tory . monocular vo system , compare with stereo vo system , be essentially good for small robotics because they conserve the space of the baseline between the pair of stereo camera . moreover , interfacing and synchronization be more difficult with stereo cam - era than with monocular camera . several vo system utilize omni-directional camera , a present by scaramuzza and siegwart ( 2008a ) , valiente garcía et al . ( 2012 ) , bunschoten and krose ( 2003 ) , scara - muzza and siegwart ( 2008b ) , scaramuzza et al . ( 2010 ) , tardif et al . ( 2008 ) , and several others employ rgb-d camera that provide both color and dense depth image , a pre - sented by fabian and clayton ( 2014a ) , steinbrücker et al . ( 2011 ) , huang et al . ( 2011 ) , fang and zhang ( 2015 ) , fabian and clayton ( 2014b ) , dryanovski et al . ( 2013 ) , kerl et al . ( 2013 ) , whelan et al . ( 2013 ) . omni-directional camera can represent a scene with a very wide field of vision ( fov ) ( up to 360° fov ) . given that omni-directional camera can provide more information than common camera and their feature stay in the camera page 12 of 26 aqel et al . springerplus ( 2016 ) 5:1897 fov for a long period of time , a well refine 3d model of the world structure can be generate ( valiente garcía et al . 2012 ) . table 2 show a summary of the feature and drawback of the three most commonly use camera for vo . each type of camera have advantage and disadvantage , so no sin - gle type can provide a 100 % perfect solution . approaches of vo estimating the position of a mobile robot with vision-based odometry can generally be approach in three way : through a feature-based approach , an appearance-based approach , or a hybrid of feature- and appearance-based approach ( scaramuzza and fraundorfer 2011 ; valiente garcía et al . 2012 ) . feature‑based approach the feature-based approach , a use by nistér et al . ( 2006 ) , howard ( 2008 ) , cumani ( 2011 ) , benseddik et al . ( 2014 ) , naroditsky et al . ( 2012 ) , jiang et al . ( 2013 ) , villanueva- escudero et al . ( 2014 ) , parra et al . ( 2010 ) , involve extract image feature ( such a corner , line , and curve ) between sequential image frame , match or track the distinctive one among the extracted feature , and finally estimate the motion . in this approach , match an image with a previous one be accomplish by compare each feature in both image and calculate the euclidean distance of feature vector to find the candidate matching feature . afterward , the displacement be obtain by calculat - ing the velocity vector between the identified pair of point ( lowe 2004 ; nistér et al . 2004 , 2006 ) . if stereo vo be implement , the extracted feature from the first frame be match with the corresponding point in the second frame , thus provide the 3d position of the point in space . the camera motion be estimate base on feature displacement where relative pose of camera can be estimate by find the geomet - ric transformation between two image acquire by the camera use a set of corre - sponding feature point . to compute the matching between the feature point of two image , near neighbour pair among their feature descriptor have to be determine . an 8-point algorithm be propose by longuet-higgins to compute the pose via the table 2 comparison between type of camera use for vo type of vo camera pros cons monocular low cost and easy deployment light weight : good for small roboticssimple calibrationsuffer from image scale uncertainty stereo image scale and depth information be easy to be retrieve provide 3d visionmore expensive and need more calibration effort than monocular camera it be degrade to the monocular case when the stereo baseline be much small than the distance to the scene from the camera difficult interfacing and synchronization . omnidirectional provides very wide field of vision ( fov ) ( up to 360° fov ) can generate well refine 3d model of the world structure rotational invariancecomplex systemmultiple camera calibrate and synchro - nizing needs high bandwidthexpensivepage 13 of 26 aqel et al . springerplus ( 2016 ) 5:1897 essential matrix ( longuet-higgins 1981 ) . this method be similar to the structure from motion ( sfm ) method ( kicman and narkiewicz 2013 ) . many work have be imple - mented to improve the robustness of longuet-higgins approach ( hartley 1997 , wu et al . 2005 ) or to solve it efficiently in a closed-form algorithm with the minimal set of five point ( nistér 2004 ) . in ( nistér 2004 ) , the relative camera pose be estimate from five matching feature point . however , several algorithm use 6 , 7 , and 8 feature pair for relative motion estimation ( stewenius et al . 2006 ) . feature-based vo have be success - fully utilize a the navigation system of mars exploration rover ( maimone et al . 2007 ) as well a in the mission of the mars science laboratory ( johnson et al . 2008 ) . kalman filter be one of the important bayesian filter use to improve the accuracy and refine the vo estimation result ( van hamme et al . 2015 ) . it use a prior vehicle state estimate to predict current feature location and then compare this prediction to current observation to calculate an updated vehicle state . the state estimate deliv - ered by the kalman filter utilize any available information to minimize the mean of the squared error of the estimate with regard to the available information ( lin et al . 2013 ) . in helmick et al . ( 2004 ) , a kalman filter pose estimator have be implement with vo system for autonomous rover in high slip environment . in this helmick work , salient feature in stereo image be track and a maximum likelihood motion estima - tion algorithm be use to estimate rover motion between successively acquire stereo image pair . the kalman filter merges data from an inertial measurement unit ( imu ) and vo . this merged estimate be then compare to the kinematic estimate to determine if and how much slippage have occur . if slippage have occur then a slip vector be cal - culated by differencing the current kalman filter estimate from the kinematic estimate to be use for calculate the necessary wheel velocity and steer angle to compen - sate for slip and follow the desired path . appearance‑based approach the appearance-based approach , a implement by gonzalez et al . ( 2012 , 2013 ) , lovegrove et al . ( 2011 ) , yu et al . ( 2011 ) , nourani-vatani et al . ( 2009 ) , nourani-vatani and borges ( 2011 ) , mcmanus et al . ( 2013 ) , zhang and kleeman ( 2009 ) , bellotto et al . ( 2008 ) , monitor the change in the appearance of acquire image and the intensity of pixel information therein instead of extract and track feature . it focus on the information extract from the pixel intensity ( valiente garcía et al . 2012 ) . the camera motion and vehicle speed can be estimate use optical flow ( of ) . of algorithm use the intensity value of the neighboring pixel to compute the displacement of brightness pattern from one image frame to another ( campbell et al . 2004 ; barron et al . 1994 ) . algorithms that estimate the displacement for all the image pixel be know a dense of algorithm such a the horn-schunck algorithm which calculate the displacement at each pixel by use global constraint ( horn and schunck 1982 ) . however , algorithms that calculate the displacement for a selected number of pixel in the image be call sparse optical flow algorithm such a the lucas-kanade method ( lucas and kanade 1981 ) . dense algorithms avoid feature extraction but be less robust to noise compare to sparse of algorithm . therefore , sparse of algorithm be desirable over dense of algorithm for many vo application ( campbell et al . 2005 , corke et al . 2004 , nourani- vatani and pradalier 2010 ) . in sparse algorithm , the feature should be choose carefully , page 14 of 26 aqel et al . springerplus ( 2016 ) 5:1897 consider that pixel in region with more variance between the neighbor will pro - duce more reliable displacement estimation . the commonly use method in appearance-based approach be the template matching method . the template match method select a patch or a template from the current image frame and attempt to match this patch in the next frame . vehicle displacement and rotation angle be retrieve by match a template between two consecutive image frame . template matching be a main task in various computer vision application . it be extensively apply in various area , such a object detection , video compression , and automatic inspection ( yoo et al . 2014 ; brunelli 2009 ) . template matching be the process of determine the existence and position of a sub-image or an object inside a large scene image ( choi and kim 2002 ; goshtasby et al . 1984 ) . the sub-image be call the template , and the large image be call the search area . template match decides whether the template exist in the search area and determine it location if it do . it compute the degree of similarity between the template and search area by shift the template over the search area and calculate the degree of similarity in each location base on various similarity measure . the shift position that have the large similarity degree be the likely position of the template find in the search area ( yoo et al . 2014 ; jurie and dhome 2002 ; goshtasby et al . 1984 ; choi and kim 2002 ) . the main similarity measure that be widely use in template matching be sum of square or absolute difference ( ssd/sad ) and normalize cross correlation ( ncc ) ( yoo et al . 2014 ; goshtasby et al . 1984 ) . ncc a a measure be more accurate than ssd/ sad . however , ncc algorithm be computationally slow than ssd/sad algorithm ( goshtasby et al . 1984 ; choi and kim 2002 ; yoo et al . 2014 ) . given that ncc-based tem - plate matching compute the normalized cross correlation of intensity value between two window , it be consider one of the most common template matching method that be invariant to linear brightness and contrast variation ( mahmood and khan 2012 ; zhao et al . 2006a , b ) . figure 7 present the flowchart of the required step of the vo system algorithm use correlation-based template matching ( aqel et al . 2016 ) . the algorithm begin by fig . 7 flowchart of visual odometry system algorithmpage 15 of 26 aqel et al . springerplus ( 2016 ) 5:1897 acquire a pair of consecutive image frame . thereafter , the template be select from the first frame and then match with the next frame through normalize cross correla - tion . then , the pixel displacement between the template and the maximum correlation point be calculate . once the horizontal and vertical pixel displacement ( δu and δv ) be measure , these pixel displacement be convert to the physical horizontal and verti - cal camera displacement ( in meter ) by use the intrinsic and extrinsic camera calibra - tion parameter through the following equation : the 2d image coordinate frame be convert to the camera coordinate frame by reverse the direction of the x and y ax because image and camera coordinate be opposite each other . for the camera coordinate plane ( x c , yc , zc ) to be convert to the vehicle coordinate plane ( xv , yv , zv ) with the application of euler angle , rotation matrix rc be calculate by rotate the camera coordinate plane 180° around the z-axis and then by 180° around the new y-axis , a depict in eqs . ( 2 ) and ( 3 ) . as the motion model be assume to be a an ackerman-steered model , the physical vehicle displacement ( translation δx and rotation δθ ) in the vehicle coordinate plane be then calculate use eq . ( 4 ) . where δx v and δyv be the vehicle displacement in the vehicle coordinate frame , and lcam be the distance between the camera center and the vehicle ’ s center of rotation . finally , the new position ( pnew ) of the vehicle in the world coordinate plane be calculate use eq . ( 5 ) which be equal to the previous position ( pprevious ) plus the incremental trans - lation ( tincremental ) in the x-axis direction and , use the rotation matrix rz-axis , rotate around the z-axis by a head angle equal to θi + 1 . hybrid of feature‑ and appearance‑based approach the feature-based approach be suitable for textured scenario , such a rough and urban environment ( johnson et al . 2008 ; gonzalez et al . 2012 ) . however , this approach fail to ( 1 ) /delta1xc=−/delta1u/parenleftbiggzc fx/parenrightbigg /delta1yc=−/delta1v/parenleftbiggzc fy/parenrightbigg ( 2 ) rc=rz×ry= cos ( θ z ) −sin ( θz ) 0 sin ( θz ) co ( θ z ) 0 0 01 × cos ( θ y ) 0 sin ( θy ) 0 10 −sin ( θy ) 0 co ( θ y )   , ( 3 )  /delta1xvi /delta1yvi /delta1zvi =rc× /delta1xci /delta1yci /delta1zci  . ( 4 ) �xi=�xvi , �θi=tan−1/parenleftbigg�yvi lcam/parenrightbigg , ( 5 ) pnew=pprevious+rz−axis×tincremental , page 16 of 26 aqel et al . springerplus ( 2016 ) 5:1897 deal with texture-less or low-textured environment of a single pattern ( e.g. , sandy soil , asphalt , and concrete ) . the few salient feature that can be detect and track in these low-textured environment make the feature-based approach inefficient in such envi - ronments ( nourani-vatani et al . 2009 ; nourani-vatani and borges 2011 ; gonzalez et al . 2012 ; johnson et al . 2008 ) . by contrast , the appearance-based approach be more robust and superior to feature track method in low-textured ( kicman and narkiewicz 2013 ; nourani-vatani and borges 2011 ) . given that a large template can be employ in the matching process with this method , the probability of successful match between two consecutive image frame be high . in some scenario , hybrid approach be the best solution which be a combination of fea - ture- and appearance- base approach . they combine between track salient fea - tures over the frame and use the pixel intensity information of the whole or batch of image . for example , in scaramuzza and siegwart ( 2008a ) , the hybrid approach be implement because the appearance-based approach alone be not very robust to image occlusion . therefore , in their work , image feature from the ground plane be use to estimate the vehicle translation while the image appearance be use to estimate the rotation of the vehicle . prior vo work vision-based odometry can estimate robot location inexpensively by use a consumer-grade camera instead of expensive sensor or system , such a gps and ins ( nistér et al . 2006 ; gonzalez et al . 2012 ; nourani-vatani et al . 2009 ) . it provide an incremental online estimation of the vehicle position by analyze the image sequence capture by a cam - era ( campbell et al . 2005 ; gonzalez et al . 2012 ) . vo a an effective non-contact position - ing method , particularly in outdoor application , be one of the main goal in computer vision and robotics research ( campbell et al . 2005 ; nagatani et al . 2010 ) . it be character - ized by good trade-off among cost , reliability , and implementation complexity ( nistér et al . 2004 ) . camera attachment to vehicle in exist literature , most vo system have cameras mount and attach to the vehi - cle , either orient toward the ground or face forward . a downward-facing camera be utilize by nourani-vatani et al . ( 2009 ) , yu et al . ( 2011 ) , nourani-vatani and borges ( 2011 ) , lovegrove et al . ( 2011 ) , nagatani et al . ( 2010 ) , kadir et al . ( 2015 ) , zienkiewicz and davison ( 2014 ) for vehicle position estimation with an appearance-based template matching approach . two monocular camera be use by gonzalez et al . ( 2012 ) : a downward-facing monocular camera for displacement and a front-facing camera a a visual compass to estimate the vehicle orientation . although the forward-facing cam - era provide more information than the downward-facing camera , template matching or feature track with the forward-facing camera can be disturb by shadow and dynamic change in the environment cause by wind and sunlight ( piyathilaka and munasinghe 2010 ; dille et al . 2010 ) . moreover , a forward-facing vo system under low- light condition require the surround environment to be illuminate and possibly require more power than the vehicle can provide.page 17 of 26 aqel et al . springerplus ( 2016 ) 5:1897 stereo vo estimating a vehicle ’ s ego-motion by use only visual input be introduce in the early 1980s by moravec ( 1980 ) . most of the early vo research be drive by nasa to develop a vo system for planetary rover with the capability to estimate motion in mars , which have uneven and rough terrain . moravec use a planetary rover equip with a single camera slide on a rail , which be call a slider stereo . the rover move in a stop-and-go manner . in each stop location , the camera slide horizontally and captured nine image at equidistant interval . by use his propose corner detector , the corner be detect in an image and match through ncc . finally , motion be estimate by triangulation of the 3d point see at two consecutive robot position . although moravec utilize a single sliding camera , his work be relate to the class of stereo vo algorithm . matthies and shafer ( 1987 ) utilize a stereo system and moravec ’ s approach to detect and track corner ; he obtain good result with 2 % relative error on a 5.5 m trajec - tory for a planetary rover . nistér et al . ( 2004 ) coin the term “ visual odometry ” and demonstrate the first real-time long-run implementation with a robust outlier rejection scheme . they do not use moravec ’ s approach to track feature among stereo frame , but they detect feature independently in all frame and only allow match between feature . this scenario avoid feature drift during cross correlation-based tracking . in cheng et al . ( 2005 ) , the importance of stereo vo during nasa ’ s mission with the rov - er spirit and opportunity be present . other recent work on stereo vo for different type of robot in different environment be present by nistér et al . ( 2006 ) , howard ( 2008 ) , azartash et al . ( 2014 ) , golban et al . ( 2012 ) , soltani et al . ( 2012 ) , li et al . ( 2013 ) . a real-time stereo vo system be implement by howard ( 2008 ) for ground vehicle through feature match rather than track and employ stereo range data for inlier detection . stereo vo be implement by helmick et al . ( 2004 ) to allow a mars rover to accu - rately follow path in high-slip environment and to estimate it travel motion . it depend on track distinctive scene feature in stereo imagery and estimate the change in the position and altitude of two or more pair of stereo image by use maxi - mum likelihood motion estimation . a correlation-based search and track base on an affine template be implement to precisely determine the 2d position of select feature in the second image pair and to eliminate the track error cause by a large roll and scale change between image . stereo matching be then perform on these track feature in the second pair to determine their new 3d position . the slippage rate be compute with the kalman filter , which merge the estimate from vo and imu and compare the estimate with the motion estimate from vehicle kinematics . monocular vo when the distance to the scene from the stereo camera be much large than the stereo baseline , stereo vo can be degrade to the monocular case , and stereo vision become ineffectual ( scaramuzza and fraundorfer 2011 , sünderhauf and protzel 2007 ) . in monocular vo , both the relative motion and 3d structure be compute from 2d bear - ing data ( scaramuzza and fraundorfer 2011 ) . successful work that employ vo with a single camera have be conduct in the last decade by use both monocular ( yu et al . 2011 ; gonzalez et al . 2012 , 2013 ; lovegrove et al . 2011 ; martinez 2013 ; nagatani page 18 of 26 aqel et al . springerplus ( 2016 ) 5:1897 et al . 2010 , nourani-vatani et al . 2009 ; van hamme et al . 2015 ; lee et al . 2015 ; forster et al . 2014 ; villanueva-escudero et al . 2014 ) and omnidirectional camera ( yu et al . 2011 ; gonzalez et al . 2012 ; 2013 ; lovegrove et al . 2011 ; martinez 2013 ; nagatani et al . 2010 ; nourani-vatani et al . 2009 ; scaramuzza and siegwart 2008a ; corke et al . 2004 ; bunscho - ten and krose 2003 ; valiente garcía et al . 2012 ) . in nistér et al . ( 2004 ) , a real-time vo that can estimate motion from a monocular or stereo camera have be develop . furthermore , the first real-time large-scale vo with a monocular camera be present . it use feature track approach and random sample consensus ( ransac ) for outlier rejection . the new upcoming camera pose be compute through 3d to 2d camera-pose estimation . the developed algorithm , which consist of three phase ( feature detection , feature tracking , and motion estima - tion ) , can be apply to either monocular or stereo vision system , with a slight change in the motion estimation phase . the algorithm begin by extract corner from each image frame and then track the detected feature between frame . a match cri - terion be implement to successfully track feature from one image to the next . finally , the motion estimation phase be execute . in the case of a monocular vision system , the motion estimation phase calculate the pose for each tracked feature by use a five- point pose algorithm . afterward , the 3d position of each detected feature be calculate with the first and last acquired image . next , 3d point information be use for the esti - mation of the 3d pose of the camera . in a stereo vision system , the 3d position of each extracted feature be calculate through stereo matching of the feature between the two image obtain by each of the camera . van hamme et al . propose a monocular vo algorithm which use planar tracking of feature point on the world ground plane surround the vehicle rather than traditional 3d pose estimation ( van hamme et al . 2015 ) . for easy consistency of motion among fea - tures , feature tracking be apply not in the image coordinate of the perspective cam - era but in the ground plane coordinate . an online self-learning approach of monocular vo and ground classification for ground vehicle be present by lee et al . ( 2015 ) . a constrained kinematic model be utilize to solve the motion and structure problem and to estimate the ground surface . a probabilistic appearance-based ground classi - fier that be learn online be use for effective sampling in the geometric search for the ground point . thus , a combination of geometric estimate with appearance-based classification be perform to achieve an online self-learning scheme from monocular vision . forster et al . present a semi-direct monocular vo algorithm that be apply to a micro aerial vehicle ( forster et al . 2014 ) . this algorithm operate directly on pixel inten - sities and eliminate the need for feature extraction and match technique in motion estimation . it use a probabilistic mapping method that explicitly model outlier meas - urements to estimate 3d point . a monocular omnidirectional vo use a hybrid combination of feature- and appear - ance-based approach be develop by scaramuzza and siegwart ( 2008a ) . the fea - tures from the ground plane be use by track scale-invariant feature transform ( sift ) point to estimate the translation and absolute scale . an image appearance visual compass be use to estimate the rotation of the vehicle . the feature-based approach be also utilized to detect failure of the appearance-based method because it be not robust to obstructions.page 19 of 26 aqel et al . springerplus ( 2016 ) 5:1897 in piyathilaka and munasinghe ( 2010 ) , an experimental study on the use of vo for short-run self-localization of field robot be present . fast fourier transform ( fft ) base on image registration technique be apply to calculate the relative translation and orientation between consecutive frame capture from a ground surface by a down - ward-facing monocular camera . the result of this study show that fft fail when the ground surface be low-textured and have repeat feature , such a cut grass , gravel , and sand . simultaneous localization and mapping ( slam ) be a technique allow robot to oper - ate in an environment without a priori knowledge of a map ( souici et al . 2013 ) . by slam , robot can localize itself in an unknown environment and incrementally gener - eat a map of this environment while at the same time use this map to estimate it new pose relative to this map . visual slam use camera sensor to acquire observation data to be use in build the map . in features-based slam , slam use environment to update the position of the robot by extract feature from the environment and re-observing when the robot move around . for example , lsd-slam and orb-slam be real-time algorithm for simultaneous localization and mapping with a monocular freely- move camera ( engel et al . 2014 ; mur-artal et al . 2015 ) . orb-slam be a feature-based approach robust to severe motion clutter , allow wide baseline loop closing and re-local - ization , and include full automatic initialization . orb feature have enough recognition power to enable place recognition from severe viewpoint change and very fast to extract and match ( without the need of multithreading acceleration ) that enable real-time accu - rate tracking and mapping . however , lsd-slam us direct approach which do not need feature extraction and thus avoid the corresponding artefact . it be able to generate semi-dense reconstruction of the environment , while the camera be localize by opti - mizing directly over image pixel intensity . moreover , it be robust to blur , low-texture environment like asphalt . vo limitation according to gonzalez et al . ( 2013 ) , nagatani et al . ( 2010 ) , nourani-vatani and borges ( 2011 ) , yu et al . ( 2011 ) , the main limitation of vo system be relate to the computa - tional cost and light and imaging condition ( i.e. , direct sunlight , shadow , image blur , and image scale variance ) . in area that have a smooth and low-textured surface floor , the directional sunlight and lighting condition be highly consider , which lead to non-uniform scene lighting . moreover , the shadow from static or dynamic object and from the vehicle itself can disturb the calculation of pixel displacement , which cause error in displacement estimation ( gonzalez et al . 2012 , nourani-vatani and borges 2011 ) . in gonzalez et al . ( 2012 , 2013 ) , yu et al . ( 2011 ) , nourani-vatani et al . ( 2009 ) , nou - rani-vatani and borges ( 2011 ) , siddiqui and khatibi ( 2014 ) , a monocular vo be imple - mented through ncc template match for ground car-like vehicle . in these study , the best positioning accuracy be achieve with less than 3 % error of the total travelling distance . the limitation of these system be relate to the negative effect of shadow , image blur , and deficiency in deal with scale variance at uneven surface . these limi - tations lead to false matching , which increase the estimation errors.page 20 of 26 aqel et al . springerplus ( 2016 ) 5:1897 scale uncertainty according to kitt et al . ( 2011 ) , cumani ( 2011 ) , choi et al . ( 2015 ) , monocular vision system be negatively affected and may fail because of scale uncertainty . in stereo vo system , the scale of motion can be recover by use the baseline between the two camera a a reference . however , in monocular vo system , scale ambiguity be unsolva - ble when camera motion be unconstrained ( zhang et al . 2014 ) . as discuss by nagatani et al . ( 2010 ) , kitt et al . ( 2011 ) , nourani-vatani et al . ( 2009 ) , gonzalez et al . ( 2012 ) , cum - ani ( 2011 ) , estimate the fluctuate image scale factor on an uneven terrain be difficult . according to kitt et al . ( 2011 ) , when a large change in road slope occur , estimation of the scaling factor may become erroneous , which may lead to incorrect estimation of the result trajectory . the relative scale with respect to the previous frame be determine use either knowledge of the 3d structure or the trifocal tensor because the absolute image scale be unknown . therefore , the absolute scale can be determine from direct measurement ( e.g. , measure the size of an object in the scene ) , motion constraint , or integration with other sensor , such a inertial measurement unit ( imu ) and range sen - sors ( scaramuzza and fraundorfer 2011 ; hartley and zisserman 2004 ) . scale ambiguity can be overcome by use independent information on the observed scene , such a the actual size of know object ( cumani 2011 ) . as discuss by ( nagatani et al . 2010 , nou - rani-vatani et al . 2009 ) , image scale variance occur when a robot move on non-smooth or loose soil floor that make the wheel go up or down ; then , the distance between the camera and the ground change , and the image zoom in and out . this image scale fluc - tuation affect the image ( make them short and wide than the actual scene ) , pre - vent correct match for visual tracking , and result in poor and unreliable motion estimation . several sensor , such a laser range finder , acceleration , and imu sensor , can be utilize to measure the camera height fluctuation ( gonzalez et al . 2012 ) . recovering the image scale be possible when the camera motion be constrain to a surface . for example , in kitt et al . ( 2011 ) , image scale ambiguity be solve by use the ackermann steer model and assume that the vehicle drive on a planar road sur - face . in nourani-vatani and borges ( 2011 ) , the planar motion of a vehicle be estimate by use a downward-facing camera and the ackermann steer model for estimation . moreover , an ins system be use to obtain vehicle pitch and roll angle . to resolve the image scale variation problem , ( nourani-vatani and borges 2011 ; gonzalez et al . 2012 ) regard the distance between the downward-facing camera and the ground a almost constant because the difference in camera height be cancel throughout the experi - ment a zero mean . in scaramuzza et al . ( 2009 ) , a monocular camera position with an offset to the vehicle rotation center be use to recover scale a the vehicle turn . however , the formulation degenerate in straight driving , and the scale be no longer recoverable . in zhang et al . ( 2014 ) , a method that do not require the imaged terrain to be flat be demonstrate . the method can simultaneously recover the inclination angle of the ground and estimate the motion . wheel odometry deal with case in which the detect terrain be not flat . recently , a new approach be design and apply by naga - tani et al . ( 2010 ) . the author develop a telecentric camera by use a ccd camera and telecentric lens that maintain the same fov regardless of the variation in camera height from the ground.page 21 of 26 aqel et al . springerplus ( 2016 ) 5:1897 table 3 summary table of some vo work in literature reference camera type approach vo estimation accuracy limitations gonzalez et al . ( 2013 ) two monocular camera : downward- facing camera for displacement and front-facing camera for orientation estimationappearance-based approach ( ncc tem- plate matching ) error < 3 % of the total travelling distance and < 8° average orientation errorfalse match due to shadow and blur at velocity > 1.5 m/s can ’ t deal with scale variance on non- smooth surface van hamme et al . ( 2015 ) monocular camera feature-based approach ( inverse per - spective projection and kalman filter for tracking of feature in the ground plane ) > 8.5 % translation error ( for 800 m ) significant rotational bias on some estimate trajectory segment due to non-planarity of the road environment in those segment scaramuzza and siegwart ( 2008a ) omnidirectional camera hybrid approach ( track sift feature point from ground plane to estimate translation . image appearance similarity measure ( ncc , manhattan and euclid- ean distance ) be use to estimate the rotation of the car ) error be < 2 % of the distance traverse 5° average orientation errorunavoidable visual odometry drift and deviation due to road hump that violate the planar motion assumption nistér et al . ( 2004 ) stereo camera feature-based approach ( detection of feature independently in all frame and only allow match between salient feature ) 1.63 % error over 380 m of the distance traversedno mention howard ( 2008 ) stereo camera feature-based approach ( feature match- ing and employ stereo range data for inlier detection ) 0.25 % error over 400 m of the distance traversedself-shadow lead to false-matchesit do not work effectively on vegetated environment nourani-vatani and borges ( 2011 ) monocular camera appearance-based approach ( ncc multi- template matching which select best template base on entropy ) error < 5 % of total travel distance5° average head errordeficiency in deal with scale variance at uneven surface system can ’ t deal with sunny/shadow region yu et al . ( 2011 ) monocular camera appearance-based approach ( ncc rotate template matching ) 1.38 % distance error and 2.8° head errorcannot deal with image scale variance , shadow and blur nagatani et al . ( 2010 ) telecentric camera ( which maintain the same field of ground area view , regardless of variation in camera height from groundappearance-based approach ( cross cor - relation template matching ) < 3 % error indoor experiment1.5 % ( for 100 m trajectory ) at 0.4 m/s speedcannot estimate the camera height from ground variation zhang et al . ( 2014 ) monocular camera feature-based approach [ tracking of fea- tures use lucas kanade tomasi ( lkt ) ] error be < 1 % of the distance traverse image scale uncertainty at complicate ground condition for example loose soil floorspage 22 of 26 aqel et al . springerplus ( 2016 ) 5:1897 in guo and meng ( 2012 ) , a system for vo and obstacle detection that involve the use of only a single camera be propose . the kanade–lucas–tomasi ( klt ) feature tracker be utilize for feature extraction , and the ransac algorithm be use for outlier rejection . the relative pose between two consecutive frame be extract from the essential matrix through svd decomposition . given that the absolute scale of the translation can not be derive from monocular motion estimation , the scale ambiguity problem be solve by use the constraint of camera mounting and ground planar assumption . to detect obstacle and separate the ground and obstacle area from each other , the image be segment into region . each region be classify a either ground or off-ground accord to three criterion : homography constraint , feature point distribu - tion , and boundary point reconstruction . table 3 be a summary table of some vo prior work which illustrate in “ prior vo work ” section . conclusions vo and it type , approach , and challenge be present and discuss . the most common positioning sensor and technique be present , and their feature and limitation be discuss and compare . different sensor and technique , such a wheel odometry , gps , ins , sonar and laser sensor , and visual sensor , can be utilize for localization task . each technique have it own drawback . vo be the localization of a robot use only a stream of image acquire from a camera attach to the robot . vo be a highly accurate solution to estimate the ego-motion of robot ; it can avoid most of the drawback of other sensor . vo be an inexpensive solution and be unaffected by wheel slippage in uneven terrain . although gps be the most common solution to localization because it can determine the absolute position without error accumulation , it be only effective in area with a clear view of the sky . it can not be use indoors and in confined space . the commercial gps estimate position with error in the order of meter . these error be consider too large for precise application that require accuracy in centimeter , such a autonomous parking . differential gps and real-time kinematic gps can determine the position with centimeter accuracy , but these technique be expensive . meanwhile , vo work effec - tively in gps-denied environment . ins be highly prone to accumulate drift , and a highly precise ins be an expensive and unviable solution for commercial purpose . the rate of local drift under vo be small than the drift rate of wheel encoders and low-precision ins . generally , estimate the position of a mobile robot use the vision-based odome - try technique can be approach in three way : through a feature-based approach , an appearance-based approach , or a hybrid of feature- and appearance-based approach . the feature-based approach be suitable for textured scenario . template match method be highly appropriate for low-textured scenario and be superior to feature track - ing method because it work robustly on almost texture-less surface . the main challenge in vo system be relate to computational cost and light and imaging condition ( i.e. , directional sunlight , shadow , image blur , and image scale/rota - tion variance ) . most of the vo system propose in exist literature fail or can not work effectively in outdoor environment with shadow and directional sunlight . shadows and page 23 of 26 aqel et al . springerplus ( 2016 ) 5:1897 directional sunlight have negative effect that disturb the estimation of pixel displace - ment between image frame and lead to error in vehicle position estimation . authors ’ contribution all author contribute equally to this work . all author read and approve the final manuscript . author detail 1 department of engineering , faculty of engineering and information technology , al-azhar university-gaza , gaza , pales- tine . 2 department of electrical and electronic engineering , faculty of engineering , universiti putra malaysia , 43400 ser - dang , selangor , malaysia . 3 department of computer and communication engineering , faculty of engineering , universiti putra malaysia , 43400 serdang , selangor , malaysia . 4 department of mechanical and manufacturing engineering , faculty of engineering , universiti putra malaysia , 43400 serdang , selangor , malaysia . competing interest the author declare that they have no compete interest . received : 7 april 2016 accepted : 18 october 2016 references aboelmagd n , karmat tb , georgy j ( 2013 ) fundamentals of inertial navigation , satellite-based positioning and their integration . springer , berlin alonso ip , llorca df , gavilán m et al ( 2012 ) accurate global localization use visual odometry and digital map on urban environment . ieee trans intell transp syst 13 ( 4 ) :1535–1545 aqel m , marhaban m , iqbal m et al ( 2016 ) adaptive-search template matching technique base on vehicle acceleration for monocular visual odometry system . ieej trans electr electr eng 11 ( 6 ) :739–752 azartash h , banai n , nguyen tq ( 2014 ) an integrate stereo visual odometry for robotic navigation . robot auton syst 62 ( 4 ) :414–421 barron jl , fleet dj , beauchemin ss ( 1994 ) performance of optical flow technique . int j comput vis 12:43–77bellotto n , burn k , fletcher e et al ( 2008 ) appearance-based localization for mobile robot use digital zoom and visual compass . robot auton syst 56 ( 2 ) :143–156 benseddik he , djekoune o , belhocine m ( 2014 ) sift and surf performance evaluation for mobile robot-monocular visual odometry . j image gr 2 ( 1 ) :7 borenstein j , everett h , feng l ( 1996 ) where be i ? sensors and method for mobile robot positioning . univ mich 119 ( 120 ) :27 borenstein j , everett hr , feng l et al ( 1997 ) mobile robot positioning-sensors and technique . naval command , control and ocean surveillance center rdt and e division , san diego brunelli r ( ed ) ( 2009 ) template match technique in computer vision : theory and practice , 1st edn . wiley , new yorkbunschoten r , krose b ( 2003 ) visual odometry from an omnidirectional vision system . in : proceedings ieee international conference on anonymous robotics and automation , 2003. vol 1 , ieee , piscataway , p 577–583 campbell j , sukthankar r , nourbakhsh i ( 2004 ) techniques for evaluate optical flow for visual odometry in extreme terrain . int conf intell robot syst 4:3704–3711 campbell j , sukthankar r , nourbakhsh i et al ( 2005 ) a robust visual odometry and precipice detection system use consumer-grade monocular vision . in : proceedings of the 2005 ieee international conference on anonymous robotics and automation 2005 . ieee , piscataway , p 3421–3427 cheng y , maimone m , matthies l ( 2005 ) visual odometry on the mars exploration rover . in : ieee international conference on anonymous system , man and cybernetics , 2005. vol 1 , ieee , piscataway , p 903–910 choi m , kim w ( 2002 ) a novel two stage template matching method for rotation and illumination invariance . pattern recognit 35 ( 1 ) :119–129 choi s , park j , yu w ( 2015 ) simplified epipolar geometry for real-time monocular visual odometry on road . int j control autom syst 13 ( 6 ) :1454–1464 cook g ( 2011 ) mobile robot : navigation , control and remote sensing . wiley , new yorkcorke p , strelow d , singh s ( 2004 ) omnidirectional visual odometry for a planetary rover . in : proceedings of the ieee/rsj international conference on anonymous intelligent robot and system , 2004. vol 4 , ieee , piscataway , p 4007–4012 cumani a ( 2011 ) feature localization refinement for improved visual odometry accuracy . int j circuits syst signal process 5 ( 2 ) :151–158 cumani a , guiducci a ( 2008 ) fast stereo-based visual odometry for rover navigation . wseas trans circuits syst 7 ( 7 ) :648–657 dille m , grocholsky b , singh s ( 2010 ) outdoor downward-facing optical flow odometry with commodity sensor . in : howard a , iagnemma k , kelly a ( ed ) field and service robotics . springer tracts in advanced robotics , vol 62 . springer , berlin , p 183–193 dryanovski i , valenti rg , xiao j ( 2013 ) fast visual odometry and mapping from rgb-d data . in : international conference on ieee anonymous robotics and automation 2013 . ieee , piscataway , p 2305–2310 dunbabin m , roberts j , usher k et al ( 2005 ) a hybrid auv design for shallow water reef navigation . in : proceedings of the 2005 ieee international conference on anonymous robotics and automation , 2005 . ieee , piscataway , p 2105–2110 el-rabbany a ( 2002 ) introduction to gps : the global positioning system . artech house , londonengel j , schops t , cremers d ( 2014 ) lsd-slam : large-scale direct monocular slam . in : european conference on com- puter vision , zurich , p 834–849page 24 of 26 aqel et al . springerplus ( 2016 ) 5:1897 ericson e , astrand b ( 2008 ) visual odometry system for agricultural field robot . in : anonymous proceedings of the world congress on engineering and computer science fabian jr , clayton gm ( 2014a ) adaptive visual odometry use rgb-d camera . in : international conference on anony- mous advanced intelligent mechatronics , 2014 . ieee , piscataway , p 1533–1538 fabian j , clayton gm ( 2014b ) error analysis for visual odometry on indoor , wheel mobile robot with 3-d sensor . ieee/ asme trans mechatron 19 ( 6 ) :1896–1906 fang z , zhang y ( 2015 ) experimental evaluation of rgb-d visual odometry method . int j adv robot syst 12 ( 3 ) :26 fernandez d , price a ( 2004 ) visual odometry for an outdoor mobile robot . in : anonymous 2004 ieee conference on robotics , automation and mechatronics . ieee , piscataway , p 816–821 forster c , pizzoli m , scaramuzza d ( 2014 ) svo : fast semi-direct monocular visual odometry . in : 2014 ieee international conference on anonymous robotics and automation . ieee , piscataway , p 15–22 fraundorfer f , scaramuzza d ( 2012 ) visual odometry : part ii : matching , robustness , optimization , and application . ieee robot autom mag 19 ( 2 ) :78–90 frontoni e ( 2012 ) vision base mobile robotics : mobile robot localization use vision sensor and active probabilistic approach . lulu . com . isbn-13:978-1471069772 garcía-garcía r , sotelo ma , parra i et al ( 2008 ) 3d visual odometry for road vehicle . j intell robot syst 51 ( 1 ) :113–134golban c , istvan s , nedevschi s ( 2012 ) stereo base visual odometry in difficult traffic scene . in : anonymous intelligent vehicle symposium ( iv ) , 2012 ieee . ieee , piscataway , p 736–741 gonzalez r , rodriguez f , guzman jl et al ( 2012 ) combined visual odometry and visual compass for off-road mobile robot localization . robotica 30 ( 6 ) :865–878 gonzalez r , rodriguez f , guzman jl et al ( 2013 ) control of off-road mobile robot use visual odometry and slip com- pensation . adv robot 27 ( 11 ) :893–906 goshtasby a , gage sh , bartholic jf ( 1984 ) a two-stage cross correlation approach to template matching . ieee trans pat - tern anal mach intell 3:374–378 guo s , meng c ( 2012 ) monocular visual odometry and obstacle detection system base on ground constraint . in : ge ss , khatib o , cabibihan j-j , simmons r , williams m-a ( ed ) social robotics . springer , berlin , p 516–525 hartley r ( 1997 ) in defense of the eight-point algorithm . ieee trans pattern anal mach intell 19 ( 6 ) :580–593hartley r , zisserman a ( 2004 ) multiple view geometry in computer vision , 2nd edn . cambridge university press , cambridge helmick dm , cheng y , clouse ds , et al ( 2004 ) path follow use visual odometry for a mar rover in high-slip environ- ments . in : proceedings 2004 anonymous aerospace conference on ieee , 2004. vol 2 , ieee , piscataway , p 772–789 horn j , schmidt g ( 1995 ) continuous localization of a mobile robot base on 3d-laser-range-data , predict sensor image , and dead-reckoning . robot auton syst 14 ( 2 ) :99–118 horn bk , schunck bg ( 1982 ) determining optical flow . artif intell 17:185–203howard a ( 2008 ) real-time stereo visual odometry for autonomous ground vehicle . in : anonymous 2008 ieee/rsj international conference on intelligent robot and system . p 3946–3952 huang as , bachrach a , henry p et al ( 2011 ) visual odometry and mapping for autonomous flight use an rgb-d camera . in : anonymous international symposium on robotics research . p 1–16 jiang y , xu y , liu y ( 2013 ) performance evaluation of feature detection and matching in stereo visual odometry . neuro - compute 120:380–390 jiang d , yang l , li d et al ( 2014a ) development of a 3d ego-motion estimation system for an autonomous agricultural vehicle . biosyst eng 121:150–159 jiang y , xiong g , chen h et al ( 2014b ) incorporating a wheeled vehicle model in a new monocular visual odometry algorithm for dynamic outdoor environment . sensors 14 ( 9 ) :16159–16180 jiménez a , seco f ( 2005 ) ultrasonic localization method for accurate positioning . instituto de automatica industrial , madrid johnson ae , goldberg sb , cheng y et al ( 2008 ) robust and efficient stereo feature track for visual odometry . in : ieee international conference on anonymous robotics and automation , 2008 . ieee , piscataway , p 39–46 jurie f , dhome m ( 2002 ) real time robust template matching . in : proceedings of the 13th british machine vision confer - ence ( bmvc 2002 ) , the british machine vision association , cardiff , uk , p 123–132 kadir ha , arshad m , aghdam hh , et al ( 2015 ) monocular visual odometry for in-pipe inspection robot . jurnal teknologi 74 ( 9 ) :35–40 kerl c , sturm j , cremers d ( 2013 ) robust odometry estimation for rgb-d camera . in : 2013 ieee international conference on anonymous robotics and automation ( icra ) , ieee , piscataway , p 3748–3754 kicman p , narkiewicz j ( 2013 ) concept of integrate ins/visual system for autonomous mobile robot operation . marine navigation and safety of sea transportation : navigational problem , crc press , p 35–40 . isbn : 978-1-138-00107-7 kitt bm , rehder j , chambers ad et al ( 2011 ) monocular visual odometry use a planar road model to solve scale ambi- guity . in : proceedings of the european conference on mobile robot , örebro university , sweden , p 43–48 kreczmer b ( 2010 ) objects localization and differentiation use ultrasonic sensor . intech open access publisher , west palm beach krešo i , ševrović m , šegvić s ( 2013 ) a novel georeferenced dataset for stereo visual odometry . in : ‏ proceedings of the croa- tian computer vision workshop , ccvw 2013 , university of zagreb , croatia , p 43–48‏ lee b , daniilidis k , lee dd ( 2015 ) online self-supervised monocular visual odometry for ground vehicle . in : ieee interna- tional conference on anonymous robotics and automation , 2015 . ieee , piscataway , p 5232–5238 li l , lian j , guo l et al ( 2013 ) visual odometry for planetary exploration rover in sandy terrains . int j adv robot syst 10:234. doi:10.5772/56342 lin lh , lawrence pd , hall r ( 2013 ) robust outdoor stereo vision slam for heavy machine rotation sense . mach vis appl 24 ( 1 ) :205–226 lingemann k , nüchter a , hertzberg j et al ( 2005 ) high-speed laser localization for mobile robot . robot auton syst 51 ( 4 ) :275–296 longuet-higgins hc ( 1981 ) a computer algorithm for reconstruct a scene from two projection . nature 293:133–135page 25 of 26 aqel et al . springerplus ( 2016 ) 5:1897 lovegrove s , davison aj , ibañez-guzmán j ( 2011 ) accurate visual odometry from a rear parking camera . in : proceedings of anonymous ieee intelligent vehicle symposium . p 788–793 lowe dg ( 2004 ) distinctive image feature from scale-invariant keypoints . int j comput vis 60 ( 2 ) :91–110 lucas bd , kanade t ( 1981 ) an iterative image registration technique with an application to stereo vision . int jt conf artif intell 3:674–679 mahmood a , khan s ( 2012 ) correlation-coefficient-based fast template match through partial elimination . ieee trans image process 21 ( 4 ) :2099–2108 maimone m , cheng y , matthies l ( 2007 ) two year of visual odometry on the mar exploration rover . j field robot 24 ( 3 ) :169–186 maklouf o , adwaib a ( 2014 ) performance evaluation of gps ins main integration approach . world acad sci eng technol int j mech aerosp ind mechatron eng 8 ( 2 ) :476–484 martinez g ( 2013 ) monocular visual odometry from frame to frame intensity difference for planetary exploration mobile robot . in : ieee workshop on robot vision ( worv ) , p 54–59 martinez g ( 2015 ) intensity-difference base monocular visual odometry for planetary rover . in : sun y , behal a , ronald chung c-k ( ed ) new development in robot vision , springer , berlin , p 181–198 matthies l , shafer s ( 1987 ) error model in stereo navigation . ieee j robot autom 3 ( 3 ) :239–248mcmanus c , furgale p , barfoot td ( 2013 ) towards lighting-invariant visual navigation : an appearance-based approach use scan laser-rangefinders . robot auton syst 61 ( 8 ) :836–852 morales y , tsubouchi t ( 2007 ) dgps , rtk-gps and starfire dgps performance under tree shade environment . in : ieee international conference on anonymous integration technology , 2007 . ieee , piscataway , p 519–524 moravec h ( 1980 ) obstacle avoidance and navigation in the real world by a see robot rover . stanford univ. , stanford ( ph.d. dissertation ) mouats t , aouf n , sappa ad et al ( 2014 ) multispectral stereo odometry . ieee trans transp syst 16:1210–1224munguia r , gra a ( 2007 ) monocular slam for visual odometry . in : ieee international symposium on anonymous intel- ligent signal processing , 2007 . ieee , piscataway , p 1–6 mur-artal r , montiel jm , tardós jd et al ( 2015 ) orb-slam : a versatile and accurate monocular slam system . ieee trans robot 31 ( 5 ) :1147–1163 nagatani k , ikeda a , ishigami g et al ( 2010 ) development of a visual odometry system for a wheeled robot on loose soil use a telecentric camera . adv robot 24 ( 8–9 ) :1149–1167 naroditsky o , zhou xs , gallier j et al ( 2012 ) two efficient solution for visual odometry use directional correspondence . ieee trans pattern anal mach intell 34 ( 4 ) :818–824 ni k , dellaert f ( 2006 ) stereo track and three-point/one-point algorithms-a robust approach in visual odometry . in : ieee international conference on anonymous image processing , 2006 . ieee , piscataway , p 2777–2780 nistér d ( 2004 ) an efficient solution to the five-point relative pose problem . ieee trans pattern anal mach intell 26 ( 6 ) :756–770 nistér d , naroditsky o , bergen j ( 2004 ) visual odometry . in : anonymous proceedings of the ieee computer society confer - ence on computer vision and pattern recognition , vol 1 , ieee , piscataway , p i652–i659 nistér d , naroditsky o , bergen j ( 2006 ) visual odometry for ground vehicle application . j field robot 23 ( 1 ) :3–20nourani-vatani n , borges pvk ( 2011 ) correlation-based visual odometry for ground vehicle . j field robot 28 ( 5 ) :742–768nourani-vatani n , pradalier c ( 2010 ) scene change detection for vision-based topological mapping and localization . in : ieee/rsj international conference on robot and system . ieee , piscataway , p 3792–3797 nourani-vatani n , roberts j , srinivasan mv ( 2009 ) practical visual odometry for car-like vehicle . in : ieee international conference on anonymous robotics and automation , 2009 . ieee , piscataway , p 3551–3557 parra i , sotelo m , llorca df et al ( 2010 ) robust visual odometry for vehicle localization in urban environment . robotica 28 ( 03 ) :441–452 piyathilaka l , munasinghe r ( 2010 ) an experimental study on use visual odometry for short-run self localization of field robot . in : 5th international conference on anonymous information and automation for sustainability , 2010 ieee , piscataway , p 150–155 rizos c , satirapod c ( 2010 ) contribution of gnss cors infrastructure to the mission of modern geodesy and status of gnss cors in thailand . eng j 15 ( 1 ) :25–42 rone w , ben-tzvi p ( 2013 ) mapping , localization and motion planning in mobile multi-robotic system . robotica 31 ( 1 ) :1–23 royer e , lhuillier m , dhome m et al ( 2007 ) monocular vision for mobile robot localization and autonomous navigation . int j comput vis 74 ( 3 ) :237–260 sanchez a , de castro a , elvira s et al ( 2012 ) autonomous indoor ultrasonic positioning system base on a low-cost conditioning circuit . measurement 45 ( 3 ) :276–283 scaramuzza d , fraundorfer f ( 2011 ) tutorial : visual odometry . ieee robot autom mag 18 ( 4 ) :80–92scaramuzza d , siegwart r ( 2008a ) appearance-guided monocular omnidirectional visual odometry for outdoor ground vehicle . ieee trans robot 24 ( 5 ) :1015–1026 scaramuzza d , siegwart r ( 2008b ) monocular omnidirectional visual odometry for outdoor ground vehicle . springer , berlin scaramuzza d , fraundorfer f , pollefeys m et al ( 2009 ) absolute scale in structure from motion from a single vehicle mount camera by exploit nonholonomic constraint . in : ieee 12th international conference on anonymous computer vision , 2009 . ieee , piscataway , p 1413–1419 scaramuzza d , fraundorfer f , pollefeys m ( 2010 ) closing the loop in appearance-guided omnidirectional visual odometry by use vocabulary tree . robot auton syst 58 ( 6 ) :820–827 siddiqui r , khatibi s ( 2014 ) robust visual odometry estimation of road vehicle from dominant surface for large-scale mapping . iet intell transp syst 9 ( 3 ) :314–322 soltani h , taghirad h , ravari an ( 2012 ) stereo-based visual navigation of mobile robot in unknown environment . in : 20th iranian conference on anonymous electrical engineering ( icee ) , 2012 . ieee , piscataway , p 946–951page 26 of 26 aqel et al . springerplus ( 2016 ) 5:1897 souici a , courdesses m , ouldali a ( 2013 ) full-observability analysis and implementation of the general slam model . int j syst sci 44 ( 3 ) :568–581 steinbrücker f , sturm j , cremers d ( 2011 ) real-time visual odometry from dense rgb-d image . in : ieee international conference on anonymous computer vision workshop , 2011 . ieee , piscataway , p 719–722 stewenius h , engels c , nistér d ( 2006 ) recent development on direct relative orientation . isprs j photogramm remote sens 60 ( 4 ) :284–294 sünderhauf n , protzel p ( 2007 ) stereo odometry—a review of approach . chemnitz university of technology , chemnitz ( technical report ) takahashi t ( 2007 ) 2d localization of outdoor mobile robot use 3d laser range data . doctoral dissertation , carnegie mellon university tardif j , pavlidis y , daniilidis k ( 2008 ) monocular visual odometry in urban environment use an omnidirectional cam- era . in : ieee/rsj international conference on anonymous intelligent robot and system , 2008 . ieee , piscataway , p 2531–2538 valiente garcía d , fernández rojo l , gil aparicio a et al ( 2012 ) visual odometry through appearance-and feature-based method with omnidirectional image . j robot 2012:1-13. doi:10.1155/2012/797063 van hamme d , goeman w , veelaert p et al ( 2015 ) robust monocular visual odometry for road vehicle use uncertain perspective projection . eurasip j image video process 1:1–21 villanueva-escudero c , villegas-cortez j , zúñiga-lópez a et al ( 2014 ) monocular visual odometry base navigation for a differential mobile robot with android os . in : human-inspired computing and it application . proceedings of the 13th mexican international conference on artificial intelligence , micai 2014 , tuxtla gutiérrez , mexico , november 16–22 , 2014 . part i. springer international publishing , p 281–292 wang c , zhao c , yang j ( 2011 ) monocular odometry in country road base on phase-derived optical flow and 4-dof ego-motion model . ind robot 38 ( 5 ) :509–520 wang d , liang h , zhu h et al ( 2014 ) a bionic camera-based polarization navigation sensor . sensors 14 ( 7 ) :13006–13023whelan t , johannsson h , kaess m et al ( 2013 ) robust real-time visual odometry for dense rgb-d mapping . in : ieee inter - national conference on anonymous robotics and automation , 2013 . ieee , piscataway , p 5724–5731 woodman oj ( 2007 ) an introduction to inertial navigation . university of cambridge , computer laboratory , technical report ( ucamcl-tr-696 ) , issn 1476-2986 wu fc , hu zy , duan fq ( 2005 ) 8-point algorithm revisit : factorized 8-point algorithm . in tenth ieee international conference on computer vision . vol 1 , ieee , piscataway , 488–494.‏ yoo j , hwang ss , kim sd et al ( 2014 ) scale-invariant template match use histogram of dominant gradient . pattern recognit 47 ( 9 ) :3006–3018 yu y , pradalier c , zong g ( 2011 ) appearance-based monocular visual odometry for ground vehicle . in : ieee/asme inter - national conference on anonymous advanced intelligent mechatronics , 2011 . ieee , piscataway , p 862–867 zhang am , kleeman l ( 2009 ) robust appearance base visual route follow for navigation in large-scale outdoor environment . int j robot res 28 ( 3 ) :331–356 zhang j , singh s , kantor g ( 2014 ) robust monocular visual odometry for a ground vehicle in undulate terrain . in : yoshida k , tadokoro s ( ed ) field and service robotics . springer tracts in advanced robotics , vol . 92 . springer , berlin , p 311–326 zhao f , huang q , gao w ( 2006a ) image matching by multiscale oriented corner correlation . in : narayanan pj , nayar sk , shum h-y ( ed ) computer vision – accv 2006 . proceedings of the 7th asian conference on computer vision , hyderabad , india , january 13–16 , 2006 . part i. lecture notes in computer science , vol . 3851 . springer , berlin , p 928–937 zhao f , huang q , gao w ( 2006b ) image matching by normalized cross-correlation . in : proceedings of 2006 ieee interna- tional conference on anonymous acoustic , speech and signal processing , 2006. vol 2 , ieee , piscataway , p 2 zienkiewicz j , davison a ( 2014 ) extrinsics autocalibration for dense planar visual odometry . j field robot 32:803–825","['review of visual odometry : type , approach , challenge , and application mohammad o . a. aqel1* , mohammad h. marhaban2 , m. iqbal saripan3 and napsiah bt . ismail4 background accurate localization of a vehicle be a fundamental challenge in mobile robot applica - tions . a robot must maintain knowledge of it position over time to achieve autono - mous navigation . therefore , various sensor , technique , and system for mobile robot positioning , such a wheel odometry , laser/ultrasonic odometry , global position system ( gps ) , global navigation satellite system ( gnss ) , inertial navigation system ( ins ) , and visual odometry ( vo ) , have be develop by researcher and engineer . however , each technique have it own weakness . although wheel odometry be the simple tech - nique available for position estimation , it suffer from position drift due to wheel slip - page ( fernandez and price 2004 ) . ins be highly prone to accumulate drift , and a highly precise ins be expensive and an unviable solution for commercial purpose . although gps be the most common solution to localization a it can provide absolute position without error accumulation , it be only effective in place with a clear view of the sky . moreover , it can not be use indoors and in confined space ( gonzalez et al . 2012 ) . the commercial gps estimate position with error in the order of meter . this error be con - sidered too large for precise application that require accuracy in centimeter , such a autonomous parking . differential gps and real time kinematic gps can provide position with centimeter accuracy , but these technique be expensive.abstract accurate localization of a vehicle be a fundamental challenge and one of the most important task of mobile robot . for autonomous navigation , motion tracking , and obstacle detection and avoidance , a robot must maintain knowledge of it position over time . vision-based odometry be a robust technique utilize for this purpose . it allow a vehicle to localize itself robustly by use only a stream of', 'robot must maintain knowledge of it position over time . vision-based odometry be a robust technique utilize for this purpose . it allow a vehicle to localize itself robustly by use only a stream of image capture by a camera attach to the vehicle . this paper present a review of state-of-the-art visual odometry ( vo ) and it type , approach , application , and challenge . vo be compare with the most common localization sensor and technique , such a inertial navigation system , global positioning system , and laser sensor . several area for future research be also highlight . keywords : visual odometry , localization sensor , image stream , global positioning system , inertial navigation systemopen access © the author ( s ) 2016 . this article be distribute under the term of the creative commons attribution 4.0 international license ( http : //creativecommons.org/licenses/by/4.0/ ) , which permit unrestricted use , distribution , and reproduction in any medium , provide you give appropriate credit to the original author ( s ) and the source , provide a link to the creative commons license , and indicate if change be made.reviewaqel et al . springerplus ( 2016 ) 5:1897 doi 10.1186/s40064-016-3573-7 *correspondence : aqel2001 @ hotmail.com 1 department of engineering , faculty of engineering and information technology , al-azhar university-gaza , gaza , palestine full list of author information be available at the end of the articlepage 2 of 26 aqel et al . springerplus ( 2016 ) 5:1897 the term “ odometry ” originate from the two greek word hodos ( mean “ journey ” or “ travel ” ) and metron ( mean “ measure ” ) ( fernandez and price 2004 ) . this derivation be relate to the estimation of the change in a robot ’ s pose ( translation and orientation ) over time . mobile robot use data from motion sensor to estimate their position rela - tive to their initial location ; this process be call odometry . vo be a technique ( show in fig . 1 ) use to localize a robot by use only a stream of image acquire from a single or', 'position rela - tive to their initial location ; this process be call odometry . vo be a technique ( show in fig . 1 ) use to localize a robot by use only a stream of image acquire from a single or multiple camera attach to the robot ( scaramuzza and fraundorfer 2011 ) . the image contain a sufficient amount of meaningful information ( color , texture , shape , etc . ) to estimate the movement of a camera in a static environment ( rone and ben-tzvi 2013 ) . the article be organize a follow . the next section present the six most common sensor and technology utilize for localization in robotic application and compare their advantage and disadvantage . “ vo ” section provide a detailed discussion on vo and it type , approach , application , and challenge . prior related work be present and discuss in “ prior vo work ” section . finally , the conclusion for this article be pre - sented in “ conclusions ” section . localization sensor and technique wheel odometry the simple and most widely utilized method to estimate the position of mobile robot be wheel odometry . it be use to estimate wheeled vehicle position by count the num - ber of revolution of the wheel that be in contact with the ground . wheel revolution can be translate accurately into linear displacement relative to the ground ( borenstein et al . 1996 ) . encoders be use to measure wheel rotation , a show in fig . 2 . fig . 1 visual odometry [ aqel et al . 2016 ] fig . 2 wheel odometry with an optical encoder [ pololu corporation 2016 ] page 3 of 26 aqel et al . springerplus ( 2016 ) 5:1897 wheel odometry be a relative positioning technique . it suffer from position drift and inaccuracy because of wheel slippage , which lead to error accumulation over time ( fer - nandez and price 2004 ; nourani-vatani et al . 2009 ) . translation and orientation error in wheel odometry increase proportionally with the total travel distance . wheel odometry be simple and inexpensive , allow for high sampling rate , and exhibit good short-term accuracy ( borenstein et al', 'odometry increase proportionally with the total travel distance . wheel odometry be simple and inexpensive , allow for high sampling rate , and exhibit good short-term accuracy ( borenstein et al . 1997 ; aboelmagd et al . 2013 ) . ins ins be a relative positioning technique that provide the position and orientation of an object relative to a know starting point , orientation , and velocity . as show in fig . 3 , it be a navigation aid that use a computer , motion sensor ( accelerometer ) , and rota - tion sensor ( rate gyroscope ) to continuously calculate the position , orientation , and velocity of a move vehicle , which could be a ground vehicle , an airplane , a spaceship , a rocket , a surface ship , or a submarine . the advantage of ins be that it be self-contained , that be , it do not require external reference ( wang et al . 2014 ; woodman 2007 ) . however , ins be highly prone to drift accumulation because calculation of the change in velocity and position be implement by perform successive mathematical inte - grations of acceleration with respect to time . accelerometer data need to be integrate twice to yield the position , whereas rate-gyro data be only integrated once to track the orientation . therefore , any small error in the measurement of acceleration and angu - lar velocity be integrate into large error in velocity , which be compound into still large error in position ( rone and ben-tzvi 2013 ; wang et al . 2014 ; woodman 2007 ) . the error be cumulative and increase with time . thus , the position need to be peri - odically correct with the input of another navigation system . consequently , inertial sensor be inaccurate and unsuitable for position applica - tions over an extended period of time and be usually utilized to supplement other navi - gation system , such a gps , to provide a high degree of accuracy than be possible with the use of any single system ( maklouf and adwaib 2014 ) . moreover , accurate inertial navigation require high-cost equipment . thus , the high cost of a', 'degree of accuracy than be possible with the use of any single system ( maklouf and adwaib 2014 ) . moreover , accurate inertial navigation require high-cost equipment . thus , the high cost of a highly precise ins fig . 3 inertial navigation system . a block diagram of ins . b miniature ins [ sbg systems 2016 ] page 4 of 26 aqel et al . springerplus ( 2016 ) 5:1897 make the method an unviable solution for commercial purpose ( borenstein et al . 1996 , 1997 ) . gps/gnss gnss be use a an umbrella term for all current and future global radio-navigation sys - tems include the u.s. gps , the russian global navigation satellite system ( glonass ) , and the european georgia library learning online system ( galileo ) . at present , there be two navigation satellite system in orbit which be gps and glonass . gali - leo be plan to be deploy and operational by 2013 ( rizos et al . 2010 ) . before gps be invent in the early 1970s by the u.s. department of defense ( dod ) , the primary method of navigation revolve around the map and compass . gps be a sat - ellite-based navigation system that allow user to accurately determine their location anywhere on or slightly above the surface of the earth ( el-rabbany 2002 ; cook 2011 ) . gps be utilize for more than simple outdoor navigational exercise ; it be use in geol - ogy , agriculture , landscaping , construction , and public transportation . gps provide accurate position , navigation , and time information free of charge to anyone who have a gps receiver . gps consist of a nominal constellation of 24 operational satellite orbit the earth and transmitting encode radio frequency ( rf ) signal . they be arrange so that four satellite be place in each of six orbital plane to ensure continuous world - wide coverage , a show in fig . 4a ( el-rabbany 2002 ; aboelmagd et al . 2013 ) . only four satellite be need to provide positioning or location information . through trilateration , ground receiver can calculate their position by use the travel time of the satellite ’ s signal and', 'four satellite be need to provide positioning or location information . through trilateration , ground receiver can calculate their position by use the travel time of the satellite ’ s signal and information about their current location that be include in the transmitted signal . each satellite be equip with a radio transmitter and receiver and atomic clock . the receiver clock be not as precise a the atomic clock and normally exhibit bias . this bias generate error in the travel time of the signal and lead to error in the calculation of the distance to the satellite . theoretically , by use the principle of trilateration/triangulation , a gps receiver require the range to three satellite only to calculate the 3d position ( latitude , longitude , and altitude ) , but a fourth satellite be require to estimate the offset of the receiver ’ s clock from the system clock and to cor - rect clock bias in the receiver . figure 4b show the concept of gps positioning by trilat - eration use three satellite ( aboelmagd et al . 2013 ; cook 2011 ) . fig . 4 global positioning system [ aboelmagd et al . 2013 ] . a gps satellite constellation . b concept of posi- tioning by trilateration ( red dot represent user ’ s position ) page 5 of 26 aqel et al . springerplus ( 2016 ) 5:1897 gps provide the absolute position with a know ratio of error . its main advantage be it immunity to error accumulation over time and it long-term stability . gps be a revolutionary technology for outdoor navigation ; it be effective in area with a clear view of the sky but be unusable for indoor , confine , underground , and underwater space . the limitation of gps include outage cause by satellite signal blockage , occasional high noise content , multipath effect , low bandwidth , and interference or jamming . gps outages occur in urban canyon , tunnel , and other gps-denied environment and con - fin place ( gonzalez et al . 2012 ; maklouf and adwaib 2014 ; cook 2011 ; wang et al . 2014 ) . common standalone gps be use for positioning and have an', ', and other gps-denied environment and con - fin place ( gonzalez et al . 2012 ; maklouf and adwaib 2014 ; cook 2011 ; wang et al . 2014 ) . common standalone gps be use for positioning and have an accuracy within 10 m. differential gps ( dgps ) and real-time kinematic gps ( rtk-gps ) be invent to improve gps accuracy and allow for localization in outdoor open-field environment within a sub-meter or centimeter order . they be relative position technique that employ two or more receiver simultaneously to track the same satellite . dgps mainly consist of three element : one gps receiver ( base station ) locate at a known location , one gps receiver ( user receiver ) call a rover , and a radio communication medium between these two receiver ( fig . 5 ) . dgps can correct bias error of the user receiver by use measure bias error at the base station ( aboelmagd et al . 2013 ; morales and tsubouchi 2007 ) . rtk-gps provide real-time measurement in centimeter accuracy . it provide two solution , namely , float and fix . the first solution require a minimum of four common satellite and provide an accuracy range of approximately 20 cm to 1 m. the second rtk-gps solution require at least five common satellite and provide accuracy within 2 cm ( aboelmagd et al . 2013 ; cook 2011 ) . sonar/ultrasonic sensor sonar/ultrasonic sensor utilize acoustic energy to detect object and measure distance from the sensor to the target object . they have two main part , namely , transmitter and receiver . the transmitter send a short ultrasonic pulse , and the receiver receive what come back of the signal after it have reflect off nearby object . the sensor measure the time-of-flight ( tof ) , which be the time from signal transmission to reception . given that the transmission rate of an ultrasonic signal be know , the distance to the target that reflect the signal can be compute . sonar sensor can be utilize to localize mobile fig . 5 real-time differential global positioning systempage 6 of 26 aqel et al . springerplus ( 2016 ) 5:1897', 'that reflect the signal can be compute . sonar sensor can be utilize to localize mobile fig . 5 real-time differential global positioning systempage 6 of 26 aqel et al . springerplus ( 2016 ) 5:1897 robot through model matching or triangulation by compute the pose change between every two sensor input acquire at two different pose . by combine many sonar sen - sors , a sonar array can obtain a detailed picture of the environment and exhibit high positioning accuracy ( jiménez and seco 2005 ; kreczmer 2010 ) . the major drawback of these sensor be the reflection of signal wave that be highly dependent on the material and the orientation of the object surface . moreover , they be sensitive to noise from the environment and other robot use ultrasound with the same frequency . many object in the environment be assume to be specular reflec - tor for ultrasonic wave , which cause a sonar sensor to receive a multi-reflected echo instead of the first one ( kreczmer 2010 ; rone and ben-tzvi 2013 ; sanchez et al . 2012 ) . laser sensor laser sensor can be utilize in several application relate to position . it be a remote sensing technology for distance measurement that involve transmit a laser toward the target and then analyze the reflected light . laser-based range measurement depend on either tof or phase-shift technique . similar to the sonar sensor , in a tof system , a short laser pulse be send out , and the time until it return be measure . this type of sensor be often refer to a laser radar or light detection and range sensor ( lidar ) . however , in phase-shift system , a continuous signal be transmit . the phase of the returned signal be compare with a reference signal generate by the same source . the velocity of the target and the distance to it be measure with the doppler shift ( horn and schmidt 1995 ; takahashi 2007 ) . lidar be mostly use in obstacle detection and avoidance , mapping , and 3d motion capture . lidar can be integrate with gps and ins to enhance the accuracy of outdoor positioning application', '2007 ) . lidar be mostly use in obstacle detection and avoidance , mapping , and 3d motion capture . lidar can be integrate with gps and ins to enhance the accuracy of outdoor positioning application . although sonar sensor have a large beam width that allow for increase coverage , the angular resolution with a laser scanner be much good than that with an ultrasonic one ( aboelmagd et al . 2013 ; lingemann et al . 2005 ) . a drawback of lidar when compare with sonar sensor be that it entail a highly expensive solution . moreover , analysis of lidar data have a high computational cost and may affect the response of real-time application . the iterative manner of calculate the optimal match between two laser scan increase the computational cost . furthermore , scan can fail when the material appear a transparent for the laser , such a glass , because the reflection on these surface lead to suspicious data ( takahashi 2007 ; horn and schmidt 1995 ; lingemann et al . 2005 ) . optical camera cameras and vision system can be employ in mobile robotic application for locali - zation and to perform various task . recently , many researcher have be show interest in visual-based localization system because these system be more robust and reliable than other sensor-based localization system . camera image can be utilize for indoor and outdoor vehicle navigation , such a to detect road edge , lane , and their transition as well a road intersection . the image capture by a camera can provide a large amount of information to be use for several purpose , include localization . compared with proximity sensor , optical camera be low-cost sensor that provide a large amount of meaningful information . moreover , they be passive ; that be , visual page 7 of 26 aqel et al . springerplus ( 2016 ) 5:1897 localization system do not suffer from the interference often encounter when active ultrasonic or laser proximity sensor be use ( frontoni 2012 ; rone and ben-tzvi 2013 ) . vision-based navigation of mobile robot be one of the main', 'from the interference often encounter when active ultrasonic or laser proximity sensor be use ( frontoni 2012 ; rone and ben-tzvi 2013 ) . vision-based navigation of mobile robot be one of the main goal of computer vision and robotics research ( campbell et al . 2005 ) . this approach be a non-contact method for the effective positioning of mobile robot , particularly in outdoor application ( naga - tani et al . 2010 ) . for autonomous navigation , a robot need to track it own position and motion . vo provide an incremental online estimation of the vehicle position by analyze the image sequence capture by a camera ( campbell et al . 2005 ; gonzalez et al . 2012 ) . vision-based odometry be an inexpensive alternative technique that be rela - tively more accurate than conventional technique , such a gps , ins , and wheel odome - try ( howard 2008 ) . vo have a good trade-off among cost , reliability , and implementation complexity ( nistér et al . 2004 ) . it can estimate robot location inexpensively by use a consumer-grade camera instead of expensive sensor or system , such a gps and ins ( nistér et al . 2006 ; nourani-vatani et al . 2009 ) . however , image analysis be typically computationally expensive . in visual localization , the computation involve several step , namely , ( 1 ) acquisition of camera image , ( 2 ) extraction of several image feature ( edge , corner , line , etc . ) , ( 3 ) matching between image frame , and ( 4 ) calculation of the position by calculate the pixel displacement between frame . moreover , vision algorithm be highly sensitive to operate and envi - ronmental condition , such a lightning , texture , illumination change throughout the day , presence of blur in image , presence of shadow , and presence of water or snow on the ground . therefore , these algorithm may perform well under several condition , but in other environmental condition , it will not work well and thus become unreliable ( aboelmagd et al . 2013 ) . table 1 show a summary of the feature and drawback of the six', 'several condition , but in other environmental condition , it will not work well and thus become unreliable ( aboelmagd et al . 2013 ) . table 1 show a summary of the feature and drawback of the six most commonly use localization technology . the process of estimate ego-motion ( translation and orientation of an agent ( e.g. , vehicle , human , and robot ) ) by use only the input of a single or multiple camera attach to it be call vo ( scaramuzza and fraundorfer 2011 ) . vo localization be the main task for autonomous vehicle to be able to track their path and properly detect and avoid obstacle . vision-based odometry be one of the robust tech - niques use for vehicle localization . this section comprehensively discuss vo and it type , approach , application , and challenge . what be vo ? vo be the pose estimation process of an agent ( e.g. , vehicle , human , and robot ) that involve the use of only a stream of image acquire from a single or from multiple cam - era attach to it ( scaramuzza and fraundorfer 2011 ) . the core of vo be camera pose estimation ( ni and dellaert 2006 ) . it be an ego-motion online estimation process from a video input ( munguia and gra 2007 ) . this approach be a non-contact method for the effective positioning of mobile robot ( nagatani et al . 2010 ) . vo provide an incremen - tal online estimation of a vehicle ’ s position by analyze the image sequence capture by a camera ( campbell et al . 2005 ; gonzalez et al . 2012 ) .page 8 of 26 aqel et al . springerplus ( 2016 ) 5:1897 the idea of estimate a vehicle ’ s pose from visual input alone be introduce and describe by moravec in the early 1980s ( nistér et al . 2004 ; scaramuzza and fraundorfer 2011 ) . from 1980 to 2000 , vo research be dominate by nasa in preparation for the 2004 mars mission . the term “ visual odometry ” be coin by nistér et al . ( 2004 ) . the term be select because vision-based localization be similar to wheel odometry in that it incrementally estimate the motion of a vehicle by integrate the number of turn', 'nistér et al . ( 2004 ) . the term be select because vision-based localization be similar to wheel odometry in that it incrementally estimate the motion of a vehicle by integrate the number of turn of it wheel over time ( scaramuzza and fraundorfer 2011 ) . in the same manner , vo inte - grate pixel displacement between image frame over time . why vo ? vo be an inexpensive and alternative odometry technique that be more accurate than conventional technique , such a gps , ins , wheel odometry , and sonar localization system , with a relative position error range from 0.1 to 2 % ( scaramuzza and fraun - dorfer 2011 ) . this method be characterize by good balance among cost , reliability , and implementation complexity ( nistér et al . 2004 ) . the use of a consumer-grade camera instead of expensive sensor or system , such a gps , ins , and laser-based localization system , be a straightforward and inexpensive method to estimate robot location ( nis - tér et al . 2006 ; gonzalez et al . 2012 ; nourani-vatani et al . 2009 ) . although gps can be utilize for outdoor localization , lose gps information cause significant error ( taka - hashi 2007 ) . images store large amount of meaningful information , which be sufficient to esti - mate the movement of a camera ( rone and ben-tzvi 2013 ) . vo be unaffected by wheel slippage in uneven terrain or other unfavorable condition . furthermore , vo work table 1 comparison of commonly use localization sensor sensor/technology advantages disadvantages wheel odometry simple to determine position/orientation short term accuracy , and allow high sample rate low cost solutionposition drift due to wheel slippageerror accumulation over timevelocity estimation require numerical differentiation that produce additional noise ins provides both position and orientation use 3-axis accelerometer and gyro - scope not subject to interference outagesposition drift ( position estimation require second-order integral ) have long-term drift error gps/gnss provides absolute position with known', 'and gyro - scope not subject to interference outagesposition drift ( position estimation require second-order integral ) have long-term drift error gps/gnss provides absolute position with known value of error no error accumulation over timeunavailable in indoor , underwater , and close area affected by rf interference ultrasonic sensor provides a scalar distance measurement from sensor to object inexpensive solutionreflection of signal wave be dependent on material or orientation of obstacle surface suffer from interference if multiple sensor be use low angular resolution and scan rate laser sensor similar to sonar sensor but have high accuracy and scan rate return the distance to a single point ( rangefinder ) or an array of distance ( scanner ) reflection of signal wave be dependent on material or orientation of obstacle surface expensive solution optical camera images store a huge meaningful informa- tion provide high localization accuracyinexpensive solutionrequires image-processing and data- extraction technique high computational-cost to process imagespage 9 of 26 aqel et al . springerplus ( 2016 ) 5:1897 effectively in gps-denied environment ( scaramuzza and fraundorfer 2011 ) . the rate of local drift under vo be small than the drift rate of wheel encoders and low-precision ins ( howard 2008 ) . vo can be integrate with gps and ins for maximum accuracy . different from laser and sonar localization system , vo do not emit any detecta - ble energy into the environment . moreover , compare with gps , vo do not require the existence of other signal ( ni and dellaert 2006 ) . compared with the use of other sensor , the use of camera for robot localization have the advantage of cost reduction , allow for a simple integration of ego-motion data into other vision-based algorithm , such a obstacle , pedestrian and lane detection , and without the need for calibration between sensor ( wang et al . 2011 ) . cameras be small , cheap , lightweight , low pow - ered , and versatile . thus , they can also be employ in', 'detection , and without the need for calibration between sensor ( wang et al . 2011 ) . cameras be small , cheap , lightweight , low pow - ered , and versatile . thus , they can also be employ in any vehicle ( land , underwater , air ) and for other robotic task ( e.g. , object detection and recognition ) . vo challenge although indoor robot localization have be implement successfully , robot locali - zation in outdoor environment remain a challenging problem . many factor , ( e.g. , terrain be usually not flat , direct sunlight , shadow , and dynamic change in the environment cause by wind and sunlight ) make localization difficult in outdoor envi - ronments ( takahashi 2007 ) . the main challenge in vo system be mainly relate to computational cost and light and imaging condition ( gonzalez et al . 2013 ; nagatani et al . 2010 ; nourani-vatani and borges 2011 ; yu et al . 2011 ) . for vo to work efficiently , sufficient illumination and a static scene with enough tex - ture should be present in the environment to allow apparent motion to be extract ( scaramuzza and fraundorfer 2011 ) . in area that have a smooth and low-textured sur - face floor , directional sunlight and lighting condition be highly consider , lead to non-uniform scene lighting . moreover , shadow from static or dynamic object or from the vehicle itself can disturb the calculation of pixel displacement and thus result in erro - neous displacement estimation ( gonzalez et al . 2012 ; nourani-vatani and borges 2011 ) . monocular vision system suffer from scale uncertainty ( kitt et al . 2011 ; cumani 2011 ; zhang et al . 2014 ) . if the surface be uneven , the image scale will fluctuate , and the image scale factor will be difficult to estimate . according to kitt et al . ( 2011 ) , estimation of the scaling factor may become erroneous when a large change in the road slope occur , which may lead to incorrect estimation of the result trajectory . vo application vo have a wide range of application and have be effectively apply in several field .', 'change in the road slope occur , which may lead to incorrect estimation of the result trajectory . vo application vo have a wide range of application and have be effectively apply in several field . its application domain include robotics , automotive , and wearable computing ( scara - muzza and fraundorfer 2011 ; fraundorfer and scaramuzza 2012 ) . vo be apply in many type of mobile robotic system , such a ground , underwater , aerial , and space robot . in space exploration , for example , vo be use to estimate the ego-motion of the nasa mars rover ( maimone et al . 2007 ) . nasa utilizes vo to track the motion of the rover a a supplement to dead reckon . vo be mainly use for navigation and to reach target efficiently as well a to avoid obstacle while drive . it be also apply in unmanned aerial vehicle ( uavs ) to perform autonomous take-off and landing and point-to-point navigation . moreover , vo play a page 10 of 26 aqel et al . springerplus ( 2016 ) 5:1897 significant role in autonomous underwater vehicle and coral-reef inspection system ( dunbabin et al . 2005 ) . given that the gps signal degrades or becomes unavailable in underwater environment , underwater vehicle can not rely on gps for pose estimation ; therefore , vo be consider a cost-effective solution for underwater localization system . in the automotive industry , vo also play a big role . it be apply in numerous driver assistance system , such a vision-based assisted brake system . vo be consider a cost-effective solution compare with lidar system ( fraundorfer and scaramuzza 2012 ) . in ground vehicle robotics , the effective use of visual sensor for navigation and obstacle detection be the main goal ( nistér et al . 2006 ) . vo be employ in case where the gps signal be unavailable ( in planetary environment ) , too heavy to carry ( on a small air vehicle ) , or insufficiently accurate at a low cost ( in agricultural application ) ( zhang et al . 2014 ; jiang et al . 2014a ) . it be also use in agricultural field robot to estimate the robot ’', 'vehicle ) , or insufficiently accurate at a low cost ( in agricultural application ) ( zhang et al . 2014 ; jiang et al . 2014a ) . it be also use in agricultural field robot to estimate the robot ’ s position relative to the crop ( ericson and astrand 2008 ; jiang et al . 2014a ) . types of camera use in vo vo can be classify accord to the type of camera/data sensor utilize to estimate the robot trajectory ( valiente garcía et al . 2012 ) . various type of camera , such a stereo , monocular , stereo or monocular omnidirectional , and rgb-d camera ( fig . 6 ) , can be use for vo purpose . most vo method that have be propose in exist literature use either stereo or monocular camera and can be roughly classify a stereo or monocular vo sys - tems . the system that utilize a binocular camera be consider stereo vo system , a implement by nistér et al . ( 2006 ) , howard ( 2008 ) , azartash et al . ( 2014 ) , golban et al . ( 2012 ) , soltani et al . ( 2012 ) , siddiqui and khatibi ( 2014 ) , mouats et al . ( 2014 ) , alonso et al . ( 2012 ) , mcmanus et al . ( 2013 ) , jiang et al . ( 2013 ) , garcía-garcía et al . ( 2008 ) , mar - tinez ( 2015 ) ; those that use a monocular camera be consider monocular vo system , fig . 6 different type of camera use in vo system . a stereo camera [ courtesy of voltrium ] . b stereo omnidirectional [ courtesy of occam ] . c monocular camera [ courtesy of microsoft ] . d monocular omnidirec- tional [ courtesy of occam ] page 11 of 26 aqel et al . springerplus ( 2016 ) 5:1897 a apply by yu et al . ( 2011 ) , gonzalez et al . ( 2012 , 2013 ) , lovegrove et al . ( 2011 ) , mar - tinez ( 2013 ) , nagatani et al . ( 2010 ) , nourani-vatani et al . ( 2009 ) , royer et al . ( 2007 ) , jiang et al . ( 2014b ) . a binocular camera have two lens , with a separate image sensor for each lens . it have be use on mars to estimate robot motion since early 2004 ( nistér et al . 2006 ) . given that information on the third dimension ( i.e. , depth ) can be extract from a sin - gle frame , the image', 'be use on mars to estimate robot motion since early 2004 ( nistér et al . 2006 ) . given that information on the third dimension ( i.e. , depth ) can be extract from a sin - gle frame , the image scale can be immediately and instantaneously retrieve because the size of the stereo baseline be fix and know , thereby result in an efficient and accurate triangulation process . moreover , the various feature present in both type of camera increase the track ability in subsequent frame ( gonzalez et al . 2012 ; nistér et al . 2006 ) . however , stereo camera be more expensive than conventional camera . in addition , binocular camera require more calibration effort than monocular camera , and error in calibration directly affect the motion estimation process ( kitt et al . 2011 ) . furthermore , it be very important for stereo vo that the two image of the stereo pair to be acquire at exactly the same time interval . that can be achieve by synchronize the shutter speed of the two camera of stereo vision or by synchronize the two camera by an external trigger signal provide by the controlling pc through serial or parallel port ( krešo et al . 2013 ; cumani and guiducci 2008 ) . much more effort be require to maintain a calibrated constant baseline between the pair of camera than to maintain a single calibrate camera . stereo vo can be degrade to the monocular case when the stereo baseline be much small than the distance to the scene from the camera . ste - reo vision become ineffective in this case , and monocular method be recommend ( scaramuzza and fraundorfer 2011 ; sünderhauf and protzel 2007 ) . using a monocular camera mitigate the effect of calibration error in motion estima - tion . low cost and easy deployment be the main motivation for use the monocular camera in many common application , such a cellular phone and laptop . however , monocular vision system suffer from scale uncertainty ( kitt et al . 2011 ; cumani 2011 ) . as discuss by nagatani et al . ( 2010 ) , kitt et al . ( 2011 ) , nourani-vatani et al .', 'laptop . however , monocular vision system suffer from scale uncertainty ( kitt et al . 2011 ; cumani 2011 ) . as discuss by nagatani et al . ( 2010 ) , kitt et al . ( 2011 ) , nourani-vatani et al . ( 2009 ) , gonzalez et al . ( 2012 ) , cumani ( 2011 ) , if the surface be uneven , the image scale will fluc - tuate , and the image scale factor will become difficult to estimate . according to ( kitt et al . 2011 ) , estimation of the scaling factor may become erroneous if a large change in the road slope occur , which may lead to incorrect estimation of the result trajec - tory . monocular vo system , compare with stereo vo system , be essentially good for small robotics because they conserve the space of the baseline between the pair of stereo camera . moreover , interfacing and synchronization be more difficult with stereo cam - era than with monocular camera . several vo system utilize omni-directional camera , a present by scaramuzza and siegwart ( 2008a ) , valiente garcía et al . ( 2012 ) , bunschoten and krose ( 2003 ) , scara - muzza and siegwart ( 2008b ) , scaramuzza et al . ( 2010 ) , tardif et al . ( 2008 ) , and several others employ rgb-d camera that provide both color and dense depth image , a pre - sented by fabian and clayton ( 2014a ) , steinbrücker et al . ( 2011 ) , huang et al . ( 2011 ) , fang and zhang ( 2015 ) , fabian and clayton ( 2014b ) , dryanovski et al . ( 2013 ) , kerl et al . ( 2013 ) , whelan et al . ( 2013 ) . omni-directional camera can represent a scene with a very wide field of vision ( fov ) ( up to 360° fov ) . given that omni-directional camera can provide more information than common camera and their feature stay in the camera page 12 of 26 aqel et al . springerplus ( 2016 ) 5:1897 fov for a long period of time , a well refine 3d model of the world structure can be generate ( valiente garcía et al . 2012 ) . table 2 show a summary of the feature and drawback of the three most commonly use camera for vo . each type of camera have advantage and disadvantage , so no sin -', '( valiente garcía et al . 2012 ) . table 2 show a summary of the feature and drawback of the three most commonly use camera for vo . each type of camera have advantage and disadvantage , so no sin - gle type can provide a 100 % perfect solution . approaches of vo estimating the position of a mobile robot with vision-based odometry can generally be approach in three way : through a feature-based approach , an appearance-based approach , or a hybrid of feature- and appearance-based approach ( scaramuzza and fraundorfer 2011 ; valiente garcía et al . 2012 ) . feature‑based approach the feature-based approach , a use by nistér et al . ( 2006 ) , howard ( 2008 ) , cumani ( 2011 ) , benseddik et al . ( 2014 ) , naroditsky et al . ( 2012 ) , jiang et al . ( 2013 ) , villanueva- escudero et al . ( 2014 ) , parra et al . ( 2010 ) , involve extract image feature ( such a corner , line , and curve ) between sequential image frame , match or track the distinctive one among the extracted feature , and finally estimate the motion . in this approach , match an image with a previous one be accomplish by compare each feature in both image and calculate the euclidean distance of feature vector to find the candidate matching feature . afterward , the displacement be obtain by calculat - ing the velocity vector between the identified pair of point ( lowe 2004 ; nistér et al . 2004 , 2006 ) . if stereo vo be implement , the extracted feature from the first frame be match with the corresponding point in the second frame , thus provide the 3d position of the point in space . the camera motion be estimate base on feature displacement where relative pose of camera can be estimate by find the geomet - ric transformation between two image acquire by the camera use a set of corre - sponding feature point . to compute the matching between the feature point of two image , near neighbour pair among their feature descriptor have to be determine . an 8-point algorithm be propose by longuet-higgins to compute the pose via the table 2 comparison', 'point of two image , near neighbour pair among their feature descriptor have to be determine . an 8-point algorithm be propose by longuet-higgins to compute the pose via the table 2 comparison between type of camera use for vo type of vo camera pros cons monocular low cost and easy deployment light weight : good for small roboticssimple calibrationsuffer from image scale uncertainty stereo image scale and depth information be easy to be retrieve provide 3d visionmore expensive and need more calibration effort than monocular camera it be degrade to the monocular case when the stereo baseline be much small than the distance to the scene from the camera difficult interfacing and synchronization . omnidirectional provides very wide field of vision ( fov ) ( up to 360° fov ) can generate well refine 3d model of the world structure rotational invariancecomplex systemmultiple camera calibrate and synchro - nizing needs high bandwidthexpensivepage 13 of 26 aqel et al . springerplus ( 2016 ) 5:1897 essential matrix ( longuet-higgins 1981 ) . this method be similar to the structure from motion ( sfm ) method ( kicman and narkiewicz 2013 ) . many work have be imple - mented to improve the robustness of longuet-higgins approach ( hartley 1997 , wu et al . 2005 ) or to solve it efficiently in a closed-form algorithm with the minimal set of five point ( nistér 2004 ) . in ( nistér 2004 ) , the relative camera pose be estimate from five matching feature point . however , several algorithm use 6 , 7 , and 8 feature pair for relative motion estimation ( stewenius et al . 2006 ) . feature-based vo have be success - fully utilize a the navigation system of mars exploration rover ( maimone et al . 2007 ) as well a in the mission of the mars science laboratory ( johnson et al . 2008 ) . kalman filter be one of the important bayesian filter use to improve the accuracy and refine the vo estimation result ( van hamme et al . 2015 ) . it use a prior vehicle state estimate to predict current feature location and then compare this', 'filter use to improve the accuracy and refine the vo estimation result ( van hamme et al . 2015 ) . it use a prior vehicle state estimate to predict current feature location and then compare this prediction to current observation to calculate an updated vehicle state . the state estimate deliv - ered by the kalman filter utilize any available information to minimize the mean of the squared error of the estimate with regard to the available information ( lin et al . 2013 ) . in helmick et al . ( 2004 ) , a kalman filter pose estimator have be implement with vo system for autonomous rover in high slip environment . in this helmick work , salient feature in stereo image be track and a maximum likelihood motion estima - tion algorithm be use to estimate rover motion between successively acquire stereo image pair . the kalman filter merges data from an inertial measurement unit ( imu ) and vo . this merged estimate be then compare to the kinematic estimate to determine if and how much slippage have occur . if slippage have occur then a slip vector be cal - culated by differencing the current kalman filter estimate from the kinematic estimate to be use for calculate the necessary wheel velocity and steer angle to compen - sate for slip and follow the desired path . appearance‑based approach the appearance-based approach , a implement by gonzalez et al . ( 2012 , 2013 ) , lovegrove et al . ( 2011 ) , yu et al . ( 2011 ) , nourani-vatani et al . ( 2009 ) , nourani-vatani and borges ( 2011 ) , mcmanus et al . ( 2013 ) , zhang and kleeman ( 2009 ) , bellotto et al . ( 2008 ) , monitor the change in the appearance of acquire image and the intensity of pixel information therein instead of extract and track feature . it focus on the information extract from the pixel intensity ( valiente garcía et al . 2012 ) . the camera motion and vehicle speed can be estimate use optical flow ( of ) . of algorithm use the intensity value of the neighboring pixel to compute the displacement of brightness pattern from one image frame to', 'and vehicle speed can be estimate use optical flow ( of ) . of algorithm use the intensity value of the neighboring pixel to compute the displacement of brightness pattern from one image frame to another ( campbell et al . 2004 ; barron et al . 1994 ) . algorithms that estimate the displacement for all the image pixel be know a dense of algorithm such a the horn-schunck algorithm which calculate the displacement at each pixel by use global constraint ( horn and schunck 1982 ) . however , algorithms that calculate the displacement for a selected number of pixel in the image be call sparse optical flow algorithm such a the lucas-kanade method ( lucas and kanade 1981 ) . dense algorithms avoid feature extraction but be less robust to noise compare to sparse of algorithm . therefore , sparse of algorithm be desirable over dense of algorithm for many vo application ( campbell et al . 2005 , corke et al . 2004 , nourani- vatani and pradalier 2010 ) . in sparse algorithm , the feature should be choose carefully , page 14 of 26 aqel et al . springerplus ( 2016 ) 5:1897 consider that pixel in region with more variance between the neighbor will pro - duce more reliable displacement estimation . the commonly use method in appearance-based approach be the template matching method . the template match method select a patch or a template from the current image frame and attempt to match this patch in the next frame . vehicle displacement and rotation angle be retrieve by match a template between two consecutive image frame . template matching be a main task in various computer vision application . it be extensively apply in various area , such a object detection , video compression , and automatic inspection ( yoo et al . 2014 ; brunelli 2009 ) . template matching be the process of determine the existence and position of a sub-image or an object inside a large scene image ( choi and kim 2002 ; goshtasby et al . 1984 ) . the sub-image be call the template , and the large image be call the search area . template match decides', 'or an object inside a large scene image ( choi and kim 2002 ; goshtasby et al . 1984 ) . the sub-image be call the template , and the large image be call the search area . template match decides whether the template exist in the search area and determine it location if it do . it compute the degree of similarity between the template and search area by shift the template over the search area and calculate the degree of similarity in each location base on various similarity measure . the shift position that have the large similarity degree be the likely position of the template find in the search area ( yoo et al . 2014 ; jurie and dhome 2002 ; goshtasby et al . 1984 ; choi and kim 2002 ) . the main similarity measure that be widely use in template matching be sum of square or absolute difference ( ssd/sad ) and normalize cross correlation ( ncc ) ( yoo et al . 2014 ; goshtasby et al . 1984 ) . ncc a a measure be more accurate than ssd/ sad . however , ncc algorithm be computationally slow than ssd/sad algorithm ( goshtasby et al . 1984 ; choi and kim 2002 ; yoo et al . 2014 ) . given that ncc-based tem - plate matching compute the normalized cross correlation of intensity value between two window , it be consider one of the most common template matching method that be invariant to linear brightness and contrast variation ( mahmood and khan 2012 ; zhao et al . 2006a , b ) . figure 7 present the flowchart of the required step of the vo system algorithm use correlation-based template matching ( aqel et al . 2016 ) . the algorithm begin by fig . 7 flowchart of visual odometry system algorithmpage 15 of 26 aqel et al . springerplus ( 2016 ) 5:1897 acquire a pair of consecutive image frame . thereafter , the template be select from the first frame and then match with the next frame through normalize cross correla - tion . then , the pixel displacement between the template and the maximum correlation point be calculate . once the horizontal and vertical pixel displacement ( δu and δv ) be measure , these pixel', '- tion . then , the pixel displacement between the template and the maximum correlation point be calculate . once the horizontal and vertical pixel displacement ( δu and δv ) be measure , these pixel displacement be convert to the physical horizontal and verti - cal camera displacement ( in meter ) by use the intrinsic and extrinsic camera calibra - tion parameter through the following equation : the 2d image coordinate frame be convert to the camera coordinate frame by reverse the direction of the x and y ax because image and camera coordinate be opposite each other . for the camera coordinate plane ( x c , yc , zc ) to be convert to the vehicle coordinate plane ( xv , yv , zv ) with the application of euler angle , rotation matrix rc be calculate by rotate the camera coordinate plane 180° around the z-axis and then by 180° around the new y-axis , a depict in eqs . ( 2 ) and ( 3 ) . as the motion model be assume to be a an ackerman-steered model , the physical vehicle displacement ( translation δx and rotation δθ ) in the vehicle coordinate plane be then calculate use eq . ( 4 ) . where δx v and δyv be the vehicle displacement in the vehicle coordinate frame , and lcam be the distance between the camera center and the vehicle ’ s center of rotation . finally , the new position ( pnew ) of the vehicle in the world coordinate plane be calculate use eq . ( 5 ) which be equal to the previous position ( pprevious ) plus the incremental trans - lation ( tincremental ) in the x-axis direction and , use the rotation matrix rz-axis , rotate around the z-axis by a head angle equal to θi + 1 . hybrid of feature‑ and appearance‑based approach the feature-based approach be suitable for textured scenario , such a rough and urban environment ( johnson et al . 2008 ; gonzalez et al . 2012 ) . however , this approach fail to ( 1 ) /delta1xc=−/delta1u/parenleftbiggzc fx/parenrightbigg /delta1yc=−/delta1v/parenleftbiggzc fy/parenrightbigg ( 2 ) rc=rz×ry=\\uf8ee \\uf8f0cos ( θ z ) −sin ( θz ) 0 sin ( θz ) co ( θ z ) 0 0 01\\uf8f9 \\uf8fb×\\uf8ee \\uf8f0cos ( θ y ) 0', 'fx/parenrightbigg /delta1yc=−/delta1v/parenleftbiggzc fy/parenrightbigg ( 2 ) rc=rz×ry=\\uf8ee \\uf8f0cos ( θ z ) −sin ( θz ) 0 sin ( θz ) co ( θ z ) 0 0 01\\uf8f9 \\uf8fb×\\uf8ee \\uf8f0cos ( θ y ) 0 sin ( θy ) 0 10 −sin ( θy ) 0 co ( θ y ) \\uf8f9 \\uf8fb , ( 3 ) \\uf8ee \\uf8f0/delta1xvi /delta1yvi /delta1zvi\\uf8f9 \\uf8fb=rc×\\uf8ee \\uf8f0/delta1xci /delta1yci /delta1zci\\uf8f9 \\uf8fb . ( 4 ) �xi=�xvi , �θi=tan−1/parenleftbigg�yvi lcam/parenrightbigg , ( 5 ) pnew=pprevious+rz−axis×tincremental , page 16 of 26 aqel et al . springerplus ( 2016 ) 5:1897 deal with texture-less or low-textured environment of a single pattern ( e.g. , sandy soil , asphalt , and concrete ) . the few salient feature that can be detect and track in these low-textured environment make the feature-based approach inefficient in such envi - ronments ( nourani-vatani et al . 2009 ; nourani-vatani and borges 2011 ; gonzalez et al . 2012 ; johnson et al . 2008 ) . by contrast , the appearance-based approach be more robust and superior to feature track method in low-textured ( kicman and narkiewicz 2013 ; nourani-vatani and borges 2011 ) . given that a large template can be employ in the matching process with this method , the probability of successful match between two consecutive image frame be high . in some scenario , hybrid approach be the best solution which be a combination of fea - ture- and appearance- base approach . they combine between track salient fea - tures over the frame and use the pixel intensity information of the whole or batch of image . for example , in scaramuzza and siegwart ( 2008a ) , the hybrid approach be implement because the appearance-based approach alone be not very robust to image occlusion . therefore , in their work , image feature from the ground plane be use to estimate the vehicle translation while the image appearance be use to estimate the rotation of the vehicle . prior vo work vision-based odometry can estimate robot location inexpensively by use a consumer-grade camera instead of expensive sensor or system , such a gps and ins ( nistér et al . 2006 ; gonzalez et al . 2012 ; nourani-vatani', 'can estimate robot location inexpensively by use a consumer-grade camera instead of expensive sensor or system , such a gps and ins ( nistér et al . 2006 ; gonzalez et al . 2012 ; nourani-vatani et al . 2009 ) . it provide an incremental online estimation of the vehicle position by analyze the image sequence capture by a cam - era ( campbell et al . 2005 ; gonzalez et al . 2012 ) . vo a an effective non-contact position - ing method , particularly in outdoor application , be one of the main goal in computer vision and robotics research ( campbell et al . 2005 ; nagatani et al . 2010 ) . it be character - ized by good trade-off among cost , reliability , and implementation complexity ( nistér et al . 2004 ) . camera attachment to vehicle in exist literature , most vo system have cameras mount and attach to the vehi - cle , either orient toward the ground or face forward . a downward-facing camera be utilize by nourani-vatani et al . ( 2009 ) , yu et al . ( 2011 ) , nourani-vatani and borges ( 2011 ) , lovegrove et al . ( 2011 ) , nagatani et al . ( 2010 ) , kadir et al . ( 2015 ) , zienkiewicz and davison ( 2014 ) for vehicle position estimation with an appearance-based template matching approach . two monocular camera be use by gonzalez et al . ( 2012 ) : a downward-facing monocular camera for displacement and a front-facing camera a a visual compass to estimate the vehicle orientation . although the forward-facing cam - era provide more information than the downward-facing camera , template matching or feature track with the forward-facing camera can be disturb by shadow and dynamic change in the environment cause by wind and sunlight ( piyathilaka and munasinghe 2010 ; dille et al . 2010 ) . moreover , a forward-facing vo system under low- light condition require the surround environment to be illuminate and possibly require more power than the vehicle can provide.page 17 of 26 aqel et al . springerplus ( 2016 ) 5:1897 stereo vo estimating a vehicle ’ s ego-motion by use only visual input be introduce in the', 'require more power than the vehicle can provide.page 17 of 26 aqel et al . springerplus ( 2016 ) 5:1897 stereo vo estimating a vehicle ’ s ego-motion by use only visual input be introduce in the early 1980s by moravec ( 1980 ) . most of the early vo research be drive by nasa to develop a vo system for planetary rover with the capability to estimate motion in mars , which have uneven and rough terrain . moravec use a planetary rover equip with a single camera slide on a rail , which be call a slider stereo . the rover move in a stop-and-go manner . in each stop location , the camera slide horizontally and captured nine image at equidistant interval . by use his propose corner detector , the corner be detect in an image and match through ncc . finally , motion be estimate by triangulation of the 3d point see at two consecutive robot position . although moravec utilize a single sliding camera , his work be relate to the class of stereo vo algorithm . matthies and shafer ( 1987 ) utilize a stereo system and moravec ’ s approach to detect and track corner ; he obtain good result with 2 % relative error on a 5.5 m trajec - tory for a planetary rover . nistér et al . ( 2004 ) coin the term “ visual odometry ” and demonstrate the first real-time long-run implementation with a robust outlier rejection scheme . they do not use moravec ’ s approach to track feature among stereo frame , but they detect feature independently in all frame and only allow match between feature . this scenario avoid feature drift during cross correlation-based tracking . in cheng et al . ( 2005 ) , the importance of stereo vo during nasa ’ s mission with the rov - er spirit and opportunity be present . other recent work on stereo vo for different type of robot in different environment be present by nistér et al . ( 2006 ) , howard ( 2008 ) , azartash et al . ( 2014 ) , golban et al . ( 2012 ) , soltani et al . ( 2012 ) , li et al . ( 2013 ) . a real-time stereo vo system be implement by howard ( 2008 ) for ground vehicle through feature match', 'et al . ( 2014 ) , golban et al . ( 2012 ) , soltani et al . ( 2012 ) , li et al . ( 2013 ) . a real-time stereo vo system be implement by howard ( 2008 ) for ground vehicle through feature match rather than track and employ stereo range data for inlier detection . stereo vo be implement by helmick et al . ( 2004 ) to allow a mars rover to accu - rately follow path in high-slip environment and to estimate it travel motion . it depend on track distinctive scene feature in stereo imagery and estimate the change in the position and altitude of two or more pair of stereo image by use maxi - mum likelihood motion estimation . a correlation-based search and track base on an affine template be implement to precisely determine the 2d position of select feature in the second image pair and to eliminate the track error cause by a large roll and scale change between image . stereo matching be then perform on these track feature in the second pair to determine their new 3d position . the slippage rate be compute with the kalman filter , which merge the estimate from vo and imu and compare the estimate with the motion estimate from vehicle kinematics . monocular vo when the distance to the scene from the stereo camera be much large than the stereo baseline , stereo vo can be degrade to the monocular case , and stereo vision become ineffectual ( scaramuzza and fraundorfer 2011 , sünderhauf and protzel 2007 ) . in monocular vo , both the relative motion and 3d structure be compute from 2d bear - ing data ( scaramuzza and fraundorfer 2011 ) . successful work that employ vo with a single camera have be conduct in the last decade by use both monocular ( yu et al . 2011 ; gonzalez et al . 2012 , 2013 ; lovegrove et al . 2011 ; martinez 2013 ; nagatani page 18 of 26 aqel et al . springerplus ( 2016 ) 5:1897 et al . 2010 , nourani-vatani et al . 2009 ; van hamme et al . 2015 ; lee et al . 2015 ; forster et al . 2014 ; villanueva-escudero et al . 2014 ) and omnidirectional camera ( yu et al . 2011 ; gonzalez et al . 2012 ; 2013 ;', 'et al . 2009 ; van hamme et al . 2015 ; lee et al . 2015 ; forster et al . 2014 ; villanueva-escudero et al . 2014 ) and omnidirectional camera ( yu et al . 2011 ; gonzalez et al . 2012 ; 2013 ; lovegrove et al . 2011 ; martinez 2013 ; nagatani et al . 2010 ; nourani-vatani et al . 2009 ; scaramuzza and siegwart 2008a ; corke et al . 2004 ; bunscho - ten and krose 2003 ; valiente garcía et al . 2012 ) . in nistér et al . ( 2004 ) , a real-time vo that can estimate motion from a monocular or stereo camera have be develop . furthermore , the first real-time large-scale vo with a monocular camera be present . it use feature track approach and random sample consensus ( ransac ) for outlier rejection . the new upcoming camera pose be compute through 3d to 2d camera-pose estimation . the developed algorithm , which consist of three phase ( feature detection , feature tracking , and motion estima - tion ) , can be apply to either monocular or stereo vision system , with a slight change in the motion estimation phase . the algorithm begin by extract corner from each image frame and then track the detected feature between frame . a match cri - terion be implement to successfully track feature from one image to the next . finally , the motion estimation phase be execute . in the case of a monocular vision system , the motion estimation phase calculate the pose for each tracked feature by use a five- point pose algorithm . afterward , the 3d position of each detected feature be calculate with the first and last acquired image . next , 3d point information be use for the esti - mation of the 3d pose of the camera . in a stereo vision system , the 3d position of each extracted feature be calculate through stereo matching of the feature between the two image obtain by each of the camera . van hamme et al . propose a monocular vo algorithm which use planar tracking of feature point on the world ground plane surround the vehicle rather than traditional 3d pose estimation ( van hamme et al . 2015 ) . for easy consistency of', 'vo algorithm which use planar tracking of feature point on the world ground plane surround the vehicle rather than traditional 3d pose estimation ( van hamme et al . 2015 ) . for easy consistency of motion among fea - tures , feature tracking be apply not in the image coordinate of the perspective cam - era but in the ground plane coordinate . an online self-learning approach of monocular vo and ground classification for ground vehicle be present by lee et al . ( 2015 ) . a constrained kinematic model be utilize to solve the motion and structure problem and to estimate the ground surface . a probabilistic appearance-based ground classi - fier that be learn online be use for effective sampling in the geometric search for the ground point . thus , a combination of geometric estimate with appearance-based classification be perform to achieve an online self-learning scheme from monocular vision . forster et al . present a semi-direct monocular vo algorithm that be apply to a micro aerial vehicle ( forster et al . 2014 ) . this algorithm operate directly on pixel inten - sities and eliminate the need for feature extraction and match technique in motion estimation . it use a probabilistic mapping method that explicitly model outlier meas - urements to estimate 3d point . a monocular omnidirectional vo use a hybrid combination of feature- and appear - ance-based approach be develop by scaramuzza and siegwart ( 2008a ) . the fea - tures from the ground plane be use by track scale-invariant feature transform ( sift ) point to estimate the translation and absolute scale . an image appearance visual compass be use to estimate the rotation of the vehicle . the feature-based approach be also utilized to detect failure of the appearance-based method because it be not robust to obstructions.page 19 of 26 aqel et al . springerplus ( 2016 ) 5:1897 in piyathilaka and munasinghe ( 2010 ) , an experimental study on the use of vo for short-run self-localization of field robot be present . fast fourier transform ( fft ) base on image', '2016 ) 5:1897 in piyathilaka and munasinghe ( 2010 ) , an experimental study on the use of vo for short-run self-localization of field robot be present . fast fourier transform ( fft ) base on image registration technique be apply to calculate the relative translation and orientation between consecutive frame capture from a ground surface by a down - ward-facing monocular camera . the result of this study show that fft fail when the ground surface be low-textured and have repeat feature , such a cut grass , gravel , and sand . simultaneous localization and mapping ( slam ) be a technique allow robot to oper - ate in an environment without a priori knowledge of a map ( souici et al . 2013 ) . by slam , robot can localize itself in an unknown environment and incrementally gener - eat a map of this environment while at the same time use this map to estimate it new pose relative to this map . visual slam use camera sensor to acquire observation data to be use in build the map . in features-based slam , slam use environment to update the position of the robot by extract feature from the environment and re-observing when the robot move around . for example , lsd-slam and orb-slam be real-time algorithm for simultaneous localization and mapping with a monocular freely- move camera ( engel et al . 2014 ; mur-artal et al . 2015 ) . orb-slam be a feature-based approach robust to severe motion clutter , allow wide baseline loop closing and re-local - ization , and include full automatic initialization . orb feature have enough recognition power to enable place recognition from severe viewpoint change and very fast to extract and match ( without the need of multithreading acceleration ) that enable real-time accu - rate tracking and mapping . however , lsd-slam us direct approach which do not need feature extraction and thus avoid the corresponding artefact . it be able to generate semi-dense reconstruction of the environment , while the camera be localize by opti - mizing directly over image pixel intensity . moreover , it', 'the corresponding artefact . it be able to generate semi-dense reconstruction of the environment , while the camera be localize by opti - mizing directly over image pixel intensity . moreover , it be robust to blur , low-texture environment like asphalt . vo limitation according to gonzalez et al . ( 2013 ) , nagatani et al . ( 2010 ) , nourani-vatani and borges ( 2011 ) , yu et al . ( 2011 ) , the main limitation of vo system be relate to the computa - tional cost and light and imaging condition ( i.e. , direct sunlight , shadow , image blur , and image scale variance ) . in area that have a smooth and low-textured surface floor , the directional sunlight and lighting condition be highly consider , which lead to non-uniform scene lighting . moreover , the shadow from static or dynamic object and from the vehicle itself can disturb the calculation of pixel displacement , which cause error in displacement estimation ( gonzalez et al . 2012 , nourani-vatani and borges 2011 ) . in gonzalez et al . ( 2012 , 2013 ) , yu et al . ( 2011 ) , nourani-vatani et al . ( 2009 ) , nou - rani-vatani and borges ( 2011 ) , siddiqui and khatibi ( 2014 ) , a monocular vo be imple - mented through ncc template match for ground car-like vehicle . in these study , the best positioning accuracy be achieve with less than 3 % error of the total travelling distance . the limitation of these system be relate to the negative effect of shadow , image blur , and deficiency in deal with scale variance at uneven surface . these limi - tations lead to false matching , which increase the estimation errors.page 20 of 26 aqel et al . springerplus ( 2016 ) 5:1897 scale uncertainty according to kitt et al . ( 2011 ) , cumani ( 2011 ) , choi et al . ( 2015 ) , monocular vision system be negatively affected and may fail because of scale uncertainty . in stereo vo system , the scale of motion can be recover by use the baseline between the two camera a a reference . however , in monocular vo system , scale ambiguity be unsolva - ble when camera motion', 'stereo vo system , the scale of motion can be recover by use the baseline between the two camera a a reference . however , in monocular vo system , scale ambiguity be unsolva - ble when camera motion be unconstrained ( zhang et al . 2014 ) . as discuss by nagatani et al . ( 2010 ) , kitt et al . ( 2011 ) , nourani-vatani et al . ( 2009 ) , gonzalez et al . ( 2012 ) , cum - ani ( 2011 ) , estimate the fluctuate image scale factor on an uneven terrain be difficult . according to kitt et al . ( 2011 ) , when a large change in road slope occur , estimation of the scaling factor may become erroneous , which may lead to incorrect estimation of the result trajectory . the relative scale with respect to the previous frame be determine use either knowledge of the 3d structure or the trifocal tensor because the absolute image scale be unknown . therefore , the absolute scale can be determine from direct measurement ( e.g. , measure the size of an object in the scene ) , motion constraint , or integration with other sensor , such a inertial measurement unit ( imu ) and range sen - sors ( scaramuzza and fraundorfer 2011 ; hartley and zisserman 2004 ) . scale ambiguity can be overcome by use independent information on the observed scene , such a the actual size of know object ( cumani 2011 ) . as discuss by ( nagatani et al . 2010 , nou - rani-vatani et al . 2009 ) , image scale variance occur when a robot move on non-smooth or loose soil floor that make the wheel go up or down ; then , the distance between the camera and the ground change , and the image zoom in and out . this image scale fluc - tuation affect the image ( make them short and wide than the actual scene ) , pre - vent correct match for visual tracking , and result in poor and unreliable motion estimation . several sensor , such a laser range finder , acceleration , and imu sensor , can be utilize to measure the camera height fluctuation ( gonzalez et al . 2012 ) . recovering the image scale be possible when the camera motion be constrain to a surface . for', ', and imu sensor , can be utilize to measure the camera height fluctuation ( gonzalez et al . 2012 ) . recovering the image scale be possible when the camera motion be constrain to a surface . for example , in kitt et al . ( 2011 ) , image scale ambiguity be solve by use the ackermann steer model and assume that the vehicle drive on a planar road sur - face . in nourani-vatani and borges ( 2011 ) , the planar motion of a vehicle be estimate by use a downward-facing camera and the ackermann steer model for estimation . moreover , an ins system be use to obtain vehicle pitch and roll angle . to resolve the image scale variation problem , ( nourani-vatani and borges 2011 ; gonzalez et al . 2012 ) regard the distance between the downward-facing camera and the ground a almost constant because the difference in camera height be cancel throughout the experi - ment a zero mean . in scaramuzza et al . ( 2009 ) , a monocular camera position with an offset to the vehicle rotation center be use to recover scale a the vehicle turn . however , the formulation degenerate in straight driving , and the scale be no longer recoverable . in zhang et al . ( 2014 ) , a method that do not require the imaged terrain to be flat be demonstrate . the method can simultaneously recover the inclination angle of the ground and estimate the motion . wheel odometry deal with case in which the detect terrain be not flat . recently , a new approach be design and apply by naga - tani et al . ( 2010 ) . the author develop a telecentric camera by use a ccd camera and telecentric lens that maintain the same fov regardless of the variation in camera height from the ground.page 21 of 26 aqel et al . springerplus ( 2016 ) 5:1897 table 3 summary table of some vo work in literature reference camera type approach vo estimation accuracy limitations gonzalez et al . ( 2013 ) two monocular camera : downward- facing camera for displacement and front-facing camera for orientation estimationappearance-based approach ( ncc tem- plate matching ) error < 3 % of the', '. ( 2013 ) two monocular camera : downward- facing camera for displacement and front-facing camera for orientation estimationappearance-based approach ( ncc tem- plate matching ) error < 3 % of the total travelling distance and < 8° average orientation errorfalse match due to shadow and blur at velocity > 1.5 m/s can ’ t deal with scale variance on non- smooth surface van hamme et al . ( 2015 ) monocular camera feature-based approach ( inverse per - spective projection and kalman filter for tracking of feature in the ground plane ) > 8.5 % translation error ( for 800 m ) significant rotational bias on some estimate trajectory segment due to non-planarity of the road environment in those segment scaramuzza and siegwart ( 2008a ) omnidirectional camera hybrid approach ( track sift feature point from ground plane to estimate translation . image appearance similarity measure ( ncc , manhattan and euclid- ean distance ) be use to estimate the rotation of the car ) error be < 2 % of the distance traverse 5° average orientation errorunavoidable visual odometry drift and deviation due to road hump that violate the planar motion assumption nistér et al . ( 2004 ) stereo camera feature-based approach ( detection of feature independently in all frame and only allow match between salient feature ) 1.63 % error over 380 m of the distance traversedno mention howard ( 2008 ) stereo camera feature-based approach ( feature match- ing and employ stereo range data for inlier detection ) 0.25 % error over 400 m of the distance traversedself-shadow lead to false-matchesit do not work effectively on vegetated environment nourani-vatani and borges ( 2011 ) monocular camera appearance-based approach ( ncc multi- template matching which select best template base on entropy ) error < 5 % of total travel distance5° average head errordeficiency in deal with scale variance at uneven surface system can ’ t deal with sunny/shadow region yu et al . ( 2011 ) monocular camera appearance-based approach ( ncc rotate template matching ) 1.38 %', 'in deal with scale variance at uneven surface system can ’ t deal with sunny/shadow region yu et al . ( 2011 ) monocular camera appearance-based approach ( ncc rotate template matching ) 1.38 % distance error and 2.8° head errorcannot deal with image scale variance , shadow and blur nagatani et al . ( 2010 ) telecentric camera ( which maintain the same field of ground area view , regardless of variation in camera height from groundappearance-based approach ( cross cor - relation template matching ) < 3 % error indoor experiment1.5 % ( for 100 m trajectory ) at 0.4 m/s speedcannot estimate the camera height from ground variation zhang et al . ( 2014 ) monocular camera feature-based approach [ tracking of fea- tures use lucas kanade tomasi ( lkt ) ] error be < 1 % of the distance traverse image scale uncertainty at complicate ground condition for example loose soil floorspage 22 of 26 aqel et al . springerplus ( 2016 ) 5:1897 in guo and meng ( 2012 ) , a system for vo and obstacle detection that involve the use of only a single camera be propose . the kanade–lucas–tomasi ( klt ) feature tracker be utilize for feature extraction , and the ransac algorithm be use for outlier rejection . the relative pose between two consecutive frame be extract from the essential matrix through svd decomposition . given that the absolute scale of the translation can not be derive from monocular motion estimation , the scale ambiguity problem be solve by use the constraint of camera mounting and ground planar assumption . to detect obstacle and separate the ground and obstacle area from each other , the image be segment into region . each region be classify a either ground or off-ground accord to three criterion : homography constraint , feature point distribu - tion , and boundary point reconstruction . table 3 be a summary table of some vo prior work which illustrate in “ prior vo work ” section . conclusions vo and it type , approach , and challenge be present and discuss . the most common positioning sensor and technique be', 'some vo prior work which illustrate in “ prior vo work ” section . conclusions vo and it type , approach , and challenge be present and discuss . the most common positioning sensor and technique be present , and their feature and limitation be discuss and compare . different sensor and technique , such a wheel odometry , gps , ins , sonar and laser sensor , and visual sensor , can be utilize for localization task . each technique have it own drawback . vo be the localization of a robot use only a stream of image acquire from a camera attach to the robot . vo be a highly accurate solution to estimate the ego-motion of robot ; it can avoid most of the drawback of other sensor . vo be an inexpensive solution and be unaffected by wheel slippage in uneven terrain . although gps be the most common solution to localization because it can determine the absolute position without error accumulation , it be only effective in area with a clear view of the sky . it can not be use indoors and in confined space . the commercial gps estimate position with error in the order of meter . these error be consider too large for precise application that require accuracy in centimeter , such a autonomous parking . differential gps and real-time kinematic gps can determine the position with centimeter accuracy , but these technique be expensive . meanwhile , vo work effec - tively in gps-denied environment . ins be highly prone to accumulate drift , and a highly precise ins be an expensive and unviable solution for commercial purpose . the rate of local drift under vo be small than the drift rate of wheel encoders and low-precision ins . generally , estimate the position of a mobile robot use the vision-based odome - try technique can be approach in three way : through a feature-based approach , an appearance-based approach , or a hybrid of feature- and appearance-based approach . the feature-based approach be suitable for textured scenario . template match method be highly appropriate for low-textured scenario and be superior to', 'of feature- and appearance-based approach . the feature-based approach be suitable for textured scenario . template match method be highly appropriate for low-textured scenario and be superior to feature track - ing method because it work robustly on almost texture-less surface . the main challenge in vo system be relate to computational cost and light and imaging condition ( i.e. , directional sunlight , shadow , image blur , and image scale/rota - tion variance ) . most of the vo system propose in exist literature fail or can not work effectively in outdoor environment with shadow and directional sunlight . shadows and page 23 of 26 aqel et al . springerplus ( 2016 ) 5:1897 directional sunlight have negative effect that disturb the estimation of pixel displace - ment between image frame and lead to error in vehicle position estimation . authors ’ contribution all author contribute equally to this work . all author read and approve the final manuscript . author detail 1 department of engineering , faculty of engineering and information technology , al-azhar university-gaza , gaza , pales- tine . 2 department of electrical and electronic engineering , faculty of engineering , universiti putra malaysia , 43400 ser - dang , selangor , malaysia . 3 department of computer and communication engineering , faculty of engineering , universiti putra malaysia , 43400 serdang , selangor , malaysia . 4 department of mechanical and manufacturing engineering , faculty of engineering , universiti putra malaysia , 43400 serdang , selangor , malaysia . competing interest the author declare that they have no compete interest . received : 7 april 2016 accepted : 18 october 2016 references aboelmagd n , karmat tb , georgy j ( 2013 ) fundamentals of inertial navigation , satellite-based positioning and their integration . springer , berlin alonso ip , llorca df , gavilán m et al ( 2012 ) accurate global localization use visual odometry and digital map on urban environment . ieee trans intell transp syst 13 ( 4 ) :1535–1545 aqel m ,', 'alonso ip , llorca df , gavilán m et al ( 2012 ) accurate global localization use visual odometry and digital map on urban environment . ieee trans intell transp syst 13 ( 4 ) :1535–1545 aqel m , marhaban m , iqbal m et al ( 2016 ) adaptive-search template matching technique base on vehicle acceleration for monocular visual odometry system . ieej trans electr electr eng 11 ( 6 ) :739–752 azartash h , banai n , nguyen tq ( 2014 ) an integrate stereo visual odometry for robotic navigation . robot auton syst 62 ( 4 ) :414–421 barron jl , fleet dj , beauchemin ss ( 1994 ) performance of optical flow technique . int j comput vis 12:43–77bellotto n , burn k , fletcher e et al ( 2008 ) appearance-based localization for mobile robot use digital zoom and visual compass . robot auton syst 56 ( 2 ) :143–156 benseddik he , djekoune o , belhocine m ( 2014 ) sift and surf performance evaluation for mobile robot-monocular visual odometry . j image gr 2 ( 1 ) :7 borenstein j , everett h , feng l ( 1996 ) where be i ? sensors and method for mobile robot positioning . univ mich 119 ( 120 ) :27 borenstein j , everett hr , feng l et al ( 1997 ) mobile robot positioning-sensors and technique . naval command , control and ocean surveillance center rdt and e division , san diego brunelli r ( ed ) ( 2009 ) template match technique in computer vision : theory and practice , 1st edn . wiley , new yorkbunschoten r , krose b ( 2003 ) visual odometry from an omnidirectional vision system . in : proceedings ieee international conference on anonymous robotics and automation , 2003. vol 1 , ieee , piscataway , p 577–583 campbell j , sukthankar r , nourbakhsh i ( 2004 ) techniques for evaluate optical flow for visual odometry in extreme terrain . int conf intell robot syst 4:3704–3711 campbell j , sukthankar r , nourbakhsh i et al ( 2005 ) a robust visual odometry and precipice detection system use consumer-grade monocular vision . in : proceedings of the 2005 ieee international conference on anonymous robotics and automation 2005 . ieee ,', 'visual odometry and precipice detection system use consumer-grade monocular vision . in : proceedings of the 2005 ieee international conference on anonymous robotics and automation 2005 . ieee , piscataway , p 3421–3427 cheng y , maimone m , matthies l ( 2005 ) visual odometry on the mars exploration rover . in : ieee international conference on anonymous system , man and cybernetics , 2005. vol 1 , ieee , piscataway , p 903–910 choi m , kim w ( 2002 ) a novel two stage template matching method for rotation and illumination invariance . pattern recognit 35 ( 1 ) :119–129 choi s , park j , yu w ( 2015 ) simplified epipolar geometry for real-time monocular visual odometry on road . int j control autom syst 13 ( 6 ) :1454–1464 cook g ( 2011 ) mobile robot : navigation , control and remote sensing . wiley , new yorkcorke p , strelow d , singh s ( 2004 ) omnidirectional visual odometry for a planetary rover . in : proceedings of the ieee/rsj international conference on anonymous intelligent robot and system , 2004. vol 4 , ieee , piscataway , p 4007–4012 cumani a ( 2011 ) feature localization refinement for improved visual odometry accuracy . int j circuits syst signal process 5 ( 2 ) :151–158 cumani a , guiducci a ( 2008 ) fast stereo-based visual odometry for rover navigation . wseas trans circuits syst 7 ( 7 ) :648–657 dille m , grocholsky b , singh s ( 2010 ) outdoor downward-facing optical flow odometry with commodity sensor . in : howard a , iagnemma k , kelly a ( ed ) field and service robotics . springer tracts in advanced robotics , vol 62 . springer , berlin , p 183–193 dryanovski i , valenti rg , xiao j ( 2013 ) fast visual odometry and mapping from rgb-d data . in : international conference on ieee anonymous robotics and automation 2013 . ieee , piscataway , p 2305–2310 dunbabin m , roberts j , usher k et al ( 2005 ) a hybrid auv design for shallow water reef navigation . in : proceedings of the 2005 ieee international conference on anonymous robotics and automation , 2005 . ieee , piscataway , p', 'k et al ( 2005 ) a hybrid auv design for shallow water reef navigation . in : proceedings of the 2005 ieee international conference on anonymous robotics and automation , 2005 . ieee , piscataway , p 2105–2110 el-rabbany a ( 2002 ) introduction to gps : the global positioning system . artech house , londonengel j , schops t , cremers d ( 2014 ) lsd-slam : large-scale direct monocular slam . in : european conference on com- puter vision , zurich , p 834–849page 24 of 26 aqel et al . springerplus ( 2016 ) 5:1897 ericson e , astrand b ( 2008 ) visual odometry system for agricultural field robot . in : anonymous proceedings of the world congress on engineering and computer science fabian jr , clayton gm ( 2014a ) adaptive visual odometry use rgb-d camera . in : international conference on anony- mous advanced intelligent mechatronics , 2014 . ieee , piscataway , p 1533–1538 fabian j , clayton gm ( 2014b ) error analysis for visual odometry on indoor , wheel mobile robot with 3-d sensor . ieee/ asme trans mechatron 19 ( 6 ) :1896–1906 fang z , zhang y ( 2015 ) experimental evaluation of rgb-d visual odometry method . int j adv robot syst 12 ( 3 ) :26 fernandez d , price a ( 2004 ) visual odometry for an outdoor mobile robot . in : anonymous 2004 ieee conference on robotics , automation and mechatronics . ieee , piscataway , p 816–821 forster c , pizzoli m , scaramuzza d ( 2014 ) svo : fast semi-direct monocular visual odometry . in : 2014 ieee international conference on anonymous robotics and automation . ieee , piscataway , p 15–22 fraundorfer f , scaramuzza d ( 2012 ) visual odometry : part ii : matching , robustness , optimization , and application . ieee robot autom mag 19 ( 2 ) :78–90 frontoni e ( 2012 ) vision base mobile robotics : mobile robot localization use vision sensor and active probabilistic approach . lulu . com . isbn-13:978-1471069772 garcía-garcía r , sotelo ma , parra i et al ( 2008 ) 3d visual odometry for road vehicle . j intell robot syst 51 ( 1 ) :113–134golban c , istvan s , nedevschi s (', 'lulu . com . isbn-13:978-1471069772 garcía-garcía r , sotelo ma , parra i et al ( 2008 ) 3d visual odometry for road vehicle . j intell robot syst 51 ( 1 ) :113–134golban c , istvan s , nedevschi s ( 2012 ) stereo base visual odometry in difficult traffic scene . in : anonymous intelligent vehicle symposium ( iv ) , 2012 ieee . ieee , piscataway , p 736–741 gonzalez r , rodriguez f , guzman jl et al ( 2012 ) combined visual odometry and visual compass for off-road mobile robot localization . robotica 30 ( 6 ) :865–878 gonzalez r , rodriguez f , guzman jl et al ( 2013 ) control of off-road mobile robot use visual odometry and slip com- pensation . adv robot 27 ( 11 ) :893–906 goshtasby a , gage sh , bartholic jf ( 1984 ) a two-stage cross correlation approach to template matching . ieee trans pat - tern anal mach intell 3:374–378 guo s , meng c ( 2012 ) monocular visual odometry and obstacle detection system base on ground constraint . in : ge ss , khatib o , cabibihan j-j , simmons r , williams m-a ( ed ) social robotics . springer , berlin , p 516–525 hartley r ( 1997 ) in defense of the eight-point algorithm . ieee trans pattern anal mach intell 19 ( 6 ) :580–593hartley r , zisserman a ( 2004 ) multiple view geometry in computer vision , 2nd edn . cambridge university press , cambridge helmick dm , cheng y , clouse ds , et al ( 2004 ) path follow use visual odometry for a mar rover in high-slip environ- ments . in : proceedings 2004 anonymous aerospace conference on ieee , 2004. vol 2 , ieee , piscataway , p 772–789 horn j , schmidt g ( 1995 ) continuous localization of a mobile robot base on 3d-laser-range-data , predict sensor image , and dead-reckoning . robot auton syst 14 ( 2 ) :99–118 horn bk , schunck bg ( 1982 ) determining optical flow . artif intell 17:185–203howard a ( 2008 ) real-time stereo visual odometry for autonomous ground vehicle . in : anonymous 2008 ieee/rsj international conference on intelligent robot and system . p 3946–3952 huang as , bachrach a , henry p et al ( 2011 ) visual odometry', 'for autonomous ground vehicle . in : anonymous 2008 ieee/rsj international conference on intelligent robot and system . p 3946–3952 huang as , bachrach a , henry p et al ( 2011 ) visual odometry and mapping for autonomous flight use an rgb-d camera . in : anonymous international symposium on robotics research . p 1–16 jiang y , xu y , liu y ( 2013 ) performance evaluation of feature detection and matching in stereo visual odometry . neuro - compute 120:380–390 jiang d , yang l , li d et al ( 2014a ) development of a 3d ego-motion estimation system for an autonomous agricultural vehicle . biosyst eng 121:150–159 jiang y , xiong g , chen h et al ( 2014b ) incorporating a wheeled vehicle model in a new monocular visual odometry algorithm for dynamic outdoor environment . sensors 14 ( 9 ) :16159–16180 jiménez a , seco f ( 2005 ) ultrasonic localization method for accurate positioning . instituto de automatica industrial , madrid johnson ae , goldberg sb , cheng y et al ( 2008 ) robust and efficient stereo feature track for visual odometry . in : ieee international conference on anonymous robotics and automation , 2008 . ieee , piscataway , p 39–46 jurie f , dhome m ( 2002 ) real time robust template matching . in : proceedings of the 13th british machine vision confer - ence ( bmvc 2002 ) , the british machine vision association , cardiff , uk , p 123–132 kadir ha , arshad m , aghdam hh , et al ( 2015 ) monocular visual odometry for in-pipe inspection robot . jurnal teknologi 74 ( 9 ) :35–40 kerl c , sturm j , cremers d ( 2013 ) robust odometry estimation for rgb-d camera . in : 2013 ieee international conference on anonymous robotics and automation ( icra ) , ieee , piscataway , p 3748–3754 kicman p , narkiewicz j ( 2013 ) concept of integrate ins/visual system for autonomous mobile robot operation . marine navigation and safety of sea transportation : navigational problem , crc press , p 35–40 . isbn : 978-1-138-00107-7 kitt bm , rehder j , chambers ad et al ( 2011 ) monocular visual odometry use a planar road', 'and safety of sea transportation : navigational problem , crc press , p 35–40 . isbn : 978-1-138-00107-7 kitt bm , rehder j , chambers ad et al ( 2011 ) monocular visual odometry use a planar road model to solve scale ambi- guity . in : proceedings of the european conference on mobile robot , örebro university , sweden , p 43–48 kreczmer b ( 2010 ) objects localization and differentiation use ultrasonic sensor . intech open access publisher , west palm beach krešo i , ševrović m , šegvić s ( 2013 ) a novel georeferenced dataset for stereo visual odometry . in : \\u200f proceedings of the croa- tian computer vision workshop , ccvw 2013 , university of zagreb , croatia , p 43–48\\u200f lee b , daniilidis k , lee dd ( 2015 ) online self-supervised monocular visual odometry for ground vehicle . in : ieee interna- tional conference on anonymous robotics and automation , 2015 . ieee , piscataway , p 5232–5238 li l , lian j , guo l et al ( 2013 ) visual odometry for planetary exploration rover in sandy terrains . int j adv robot syst 10:234. doi:10.5772/56342 lin lh , lawrence pd , hall r ( 2013 ) robust outdoor stereo vision slam for heavy machine rotation sense . mach vis appl 24 ( 1 ) :205–226 lingemann k , nüchter a , hertzberg j et al ( 2005 ) high-speed laser localization for mobile robot . robot auton syst 51 ( 4 ) :275–296 longuet-higgins hc ( 1981 ) a computer algorithm for reconstruct a scene from two projection . nature 293:133–135page 25 of 26 aqel et al . springerplus ( 2016 ) 5:1897 lovegrove s , davison aj , ibañez-guzmán j ( 2011 ) accurate visual odometry from a rear parking camera . in : proceedings of anonymous ieee intelligent vehicle symposium . p 788–793 lowe dg ( 2004 ) distinctive image feature from scale-invariant keypoints . int j comput vis 60 ( 2 ) :91–110 lucas bd , kanade t ( 1981 ) an iterative image registration technique with an application to stereo vision . int jt conf artif intell 3:674–679 mahmood a , khan s ( 2012 ) correlation-coefficient-based fast template match through partial elimination', 'technique with an application to stereo vision . int jt conf artif intell 3:674–679 mahmood a , khan s ( 2012 ) correlation-coefficient-based fast template match through partial elimination . ieee trans image process 21 ( 4 ) :2099–2108 maimone m , cheng y , matthies l ( 2007 ) two year of visual odometry on the mar exploration rover . j field robot 24 ( 3 ) :169–186 maklouf o , adwaib a ( 2014 ) performance evaluation of gps ins main integration approach . world acad sci eng technol int j mech aerosp ind mechatron eng 8 ( 2 ) :476–484 martinez g ( 2013 ) monocular visual odometry from frame to frame intensity difference for planetary exploration mobile robot . in : ieee workshop on robot vision ( worv ) , p 54–59 martinez g ( 2015 ) intensity-difference base monocular visual odometry for planetary rover . in : sun y , behal a , ronald chung c-k ( ed ) new development in robot vision , springer , berlin , p 181–198 matthies l , shafer s ( 1987 ) error model in stereo navigation . ieee j robot autom 3 ( 3 ) :239–248mcmanus c , furgale p , barfoot td ( 2013 ) towards lighting-invariant visual navigation : an appearance-based approach use scan laser-rangefinders . robot auton syst 61 ( 8 ) :836–852 morales y , tsubouchi t ( 2007 ) dgps , rtk-gps and starfire dgps performance under tree shade environment . in : ieee international conference on anonymous integration technology , 2007 . ieee , piscataway , p 519–524 moravec h ( 1980 ) obstacle avoidance and navigation in the real world by a see robot rover . stanford univ. , stanford ( ph.d. dissertation ) mouats t , aouf n , sappa ad et al ( 2014 ) multispectral stereo odometry . ieee trans transp syst 16:1210–1224munguia r , gra a ( 2007 ) monocular slam for visual odometry . in : ieee international symposium on anonymous intel- ligent signal processing , 2007 . ieee , piscataway , p 1–6 mur-artal r , montiel jm , tardós jd et al ( 2015 ) orb-slam : a versatile and accurate monocular slam system . ieee trans robot 31 ( 5 ) :1147–1163 nagatani k , ikeda a , ishigami', 'piscataway , p 1–6 mur-artal r , montiel jm , tardós jd et al ( 2015 ) orb-slam : a versatile and accurate monocular slam system . ieee trans robot 31 ( 5 ) :1147–1163 nagatani k , ikeda a , ishigami g et al ( 2010 ) development of a visual odometry system for a wheeled robot on loose soil use a telecentric camera . adv robot 24 ( 8–9 ) :1149–1167 naroditsky o , zhou xs , gallier j et al ( 2012 ) two efficient solution for visual odometry use directional correspondence . ieee trans pattern anal mach intell 34 ( 4 ) :818–824 ni k , dellaert f ( 2006 ) stereo track and three-point/one-point algorithms-a robust approach in visual odometry . in : ieee international conference on anonymous image processing , 2006 . ieee , piscataway , p 2777–2780 nistér d ( 2004 ) an efficient solution to the five-point relative pose problem . ieee trans pattern anal mach intell 26 ( 6 ) :756–770 nistér d , naroditsky o , bergen j ( 2004 ) visual odometry . in : anonymous proceedings of the ieee computer society confer - ence on computer vision and pattern recognition , vol 1 , ieee , piscataway , p i652–i659 nistér d , naroditsky o , bergen j ( 2006 ) visual odometry for ground vehicle application . j field robot 23 ( 1 ) :3–20nourani-vatani n , borges pvk ( 2011 ) correlation-based visual odometry for ground vehicle . j field robot 28 ( 5 ) :742–768nourani-vatani n , pradalier c ( 2010 ) scene change detection for vision-based topological mapping and localization . in : ieee/rsj international conference on robot and system . ieee , piscataway , p 3792–3797 nourani-vatani n , roberts j , srinivasan mv ( 2009 ) practical visual odometry for car-like vehicle . in : ieee international conference on anonymous robotics and automation , 2009 . ieee , piscataway , p 3551–3557 parra i , sotelo m , llorca df et al ( 2010 ) robust visual odometry for vehicle localization in urban environment . robotica 28 ( 03 ) :441–452 piyathilaka l , munasinghe r ( 2010 ) an experimental study on use visual odometry for short-run self localization of field', 'for vehicle localization in urban environment . robotica 28 ( 03 ) :441–452 piyathilaka l , munasinghe r ( 2010 ) an experimental study on use visual odometry for short-run self localization of field robot . in : 5th international conference on anonymous information and automation for sustainability , 2010 ieee , piscataway , p 150–155 rizos c , satirapod c ( 2010 ) contribution of gnss cors infrastructure to the mission of modern geodesy and status of gnss cors in thailand . eng j 15 ( 1 ) :25–42 rone w , ben-tzvi p ( 2013 ) mapping , localization and motion planning in mobile multi-robotic system . robotica 31 ( 1 ) :1–23 royer e , lhuillier m , dhome m et al ( 2007 ) monocular vision for mobile robot localization and autonomous navigation . int j comput vis 74 ( 3 ) :237–260 sanchez a , de castro a , elvira s et al ( 2012 ) autonomous indoor ultrasonic positioning system base on a low-cost conditioning circuit . measurement 45 ( 3 ) :276–283 scaramuzza d , fraundorfer f ( 2011 ) tutorial : visual odometry . ieee robot autom mag 18 ( 4 ) :80–92scaramuzza d , siegwart r ( 2008a ) appearance-guided monocular omnidirectional visual odometry for outdoor ground vehicle . ieee trans robot 24 ( 5 ) :1015–1026 scaramuzza d , siegwart r ( 2008b ) monocular omnidirectional visual odometry for outdoor ground vehicle . springer , berlin scaramuzza d , fraundorfer f , pollefeys m et al ( 2009 ) absolute scale in structure from motion from a single vehicle mount camera by exploit nonholonomic constraint . in : ieee 12th international conference on anonymous computer vision , 2009 . ieee , piscataway , p 1413–1419 scaramuzza d , fraundorfer f , pollefeys m ( 2010 ) closing the loop in appearance-guided omnidirectional visual odometry by use vocabulary tree . robot auton syst 58 ( 6 ) :820–827 siddiqui r , khatibi s ( 2014 ) robust visual odometry estimation of road vehicle from dominant surface for large-scale mapping . iet intell transp syst 9 ( 3 ) :314–322 soltani h , taghirad h , ravari an ( 2012 ) stereo-based visual', 'visual odometry estimation of road vehicle from dominant surface for large-scale mapping . iet intell transp syst 9 ( 3 ) :314–322 soltani h , taghirad h , ravari an ( 2012 ) stereo-based visual navigation of mobile robot in unknown environment . in : 20th iranian conference on anonymous electrical engineering ( icee ) , 2012 . ieee , piscataway , p 946–951page 26 of 26 aqel et al . springerplus ( 2016 ) 5:1897 souici a , courdesses m , ouldali a ( 2013 ) full-observability analysis and implementation of the general slam model . int j syst sci 44 ( 3 ) :568–581 steinbrücker f , sturm j , cremers d ( 2011 ) real-time visual odometry from dense rgb-d image . in : ieee international conference on anonymous computer vision workshop , 2011 . ieee , piscataway , p 719–722 stewenius h , engels c , nistér d ( 2006 ) recent development on direct relative orientation . isprs j photogramm remote sens 60 ( 4 ) :284–294 sünderhauf n , protzel p ( 2007 ) stereo odometry—a review of approach . chemnitz university of technology , chemnitz ( technical report ) takahashi t ( 2007 ) 2d localization of outdoor mobile robot use 3d laser range data . doctoral dissertation , carnegie mellon university tardif j , pavlidis y , daniilidis k ( 2008 ) monocular visual odometry in urban environment use an omnidirectional cam- era . in : ieee/rsj international conference on anonymous intelligent robot and system , 2008 . ieee , piscataway , p 2531–2538 valiente garcía d , fernández rojo l , gil aparicio a et al ( 2012 ) visual odometry through appearance-and feature-based method with omnidirectional image . j robot 2012:1-13. doi:10.1155/2012/797063 van hamme d , goeman w , veelaert p et al ( 2015 ) robust monocular visual odometry for road vehicle use uncertain perspective projection . eurasip j image video process 1:1–21 villanueva-escudero c , villegas-cortez j , zúñiga-lópez a et al ( 2014 ) monocular visual odometry base navigation for a differential mobile robot with android os . in : human-inspired computing and it application .', 'c , villegas-cortez j , zúñiga-lópez a et al ( 2014 ) monocular visual odometry base navigation for a differential mobile robot with android os . in : human-inspired computing and it application . proceedings of the 13th mexican international conference on artificial intelligence , micai 2014 , tuxtla gutiérrez , mexico , november 16–22 , 2014 . part i. springer international publishing , p 281–292 wang c , zhao c , yang j ( 2011 ) monocular odometry in country road base on phase-derived optical flow and 4-dof ego-motion model . ind robot 38 ( 5 ) :509–520 wang d , liang h , zhu h et al ( 2014 ) a bionic camera-based polarization navigation sensor . sensors 14 ( 7 ) :13006–13023whelan t , johannsson h , kaess m et al ( 2013 ) robust real-time visual odometry for dense rgb-d mapping . in : ieee inter - national conference on anonymous robotics and automation , 2013 . ieee , piscataway , p 5724–5731 woodman oj ( 2007 ) an introduction to inertial navigation . university of cambridge , computer laboratory , technical report ( ucamcl-tr-696 ) , issn 1476-2986 wu fc , hu zy , duan fq ( 2005 ) 8-point algorithm revisit : factorized 8-point algorithm . in tenth ieee international conference on computer vision . vol 1 , ieee , piscataway , 488–494.\\u200f yoo j , hwang ss , kim sd et al ( 2014 ) scale-invariant template match use histogram of dominant gradient . pattern recognit 47 ( 9 ) :3006–3018 yu y , pradalier c , zong g ( 2011 ) appearance-based monocular visual odometry for ground vehicle . in : ieee/asme inter - national conference on anonymous advanced intelligent mechatronics , 2011 . ieee , piscataway , p 862–867 zhang am , kleeman l ( 2009 ) robust appearance base visual route follow for navigation in large-scale outdoor environment . int j robot res 28 ( 3 ) :331–356 zhang j , singh s , kantor g ( 2014 ) robust monocular visual odometry for a ground vehicle in undulate terrain . in : yoshida k , tadokoro s ( ed ) field and service robotics . springer tracts in advanced robotics , vol . 92 . springer , berlin , p', 'visual odometry for a ground vehicle in undulate terrain . in : yoshida k , tadokoro s ( ed ) field and service robotics . springer tracts in advanced robotics , vol . 92 . springer , berlin , p 311–326 zhao f , huang q , gao w ( 2006a ) image matching by multiscale oriented corner correlation . in : narayanan pj , nayar sk , shum h-y ( ed ) computer vision – accv 2006 . proceedings of the 7th asian conference on computer vision , hyderabad , india , january 13–16 , 2006 . part i. lecture notes in computer science , vol . 3851 . springer , berlin , p 928–937 zhao f , huang q , gao w ( 2006b ) image matching by normalized cross-correlation . in : proceedings of 2006 ieee interna- tional conference on anonymous acoustic , speech and signal processing , 2006. vol 2 , ieee , piscataway , p 2 zienkiewicz j , davison a ( 2014 ) extrinsics autocalibration for dense planar visual odometry . j field robot 32:803–825']",https://www.mdpi.com/1424-8220/22/12/4582
14.pdf," abstract — with rapid advancement in the area of mobile robotics and industrial automation , a grow need have arise towards accurate navigation and localization of move object . camera base motion estimation be one such technique which be gain huge popularity ow ing to it simplicity and use of limited resource in generate motion path . in this paper , an attempt be make to introduce this topic for beginner cover different aspect of vision base motion estimation task . the evolution of vo scheme over last f ew decade be discuss under two broad category , that be , geometric and non -geometric approach . the geometric approach be far detail under three different class , that be , feature -based , appearance -based , and a hybrid of feature and appearan ce base scheme . the non -geometric approach be one of the recent paradigm shift from conventional pose estimation technique and be thus discuss in a separate section . towards the end , a list of different datasets for visual odometry and ally research area be provide for a ready reference . keywords : motion estimation , visua l odometry , direct vo , rgbd vo , learning base vo , visual odometry datasets i . introduction with rise automation in different engineering field , mobile robotics be gain huge popularity . the unmanned vehicle be one such proliferate example that be expand it fleet in different appli cation range from commercial to strategic use . one of the simple mechanism to estimate the motion of a terrestrial vehicle be to use wheel encoders . however , these have limit usage in ground vehicle and suffer from inaccuracy that occur due to wh eel slippage during movement in muddy , slippery , sandy or loose terrain . the error arise at each instant get accumulate over time and the estimate pose drift in proportion to the distance travel [ 1 ] . traditio nal navigation approach such a inertial navigation system ( ins ) , the global positioning system ( gps ) , sonar , radar , and lidar be currently in use for different application [ 2 ] . unavailability of gps signal in an indoor and under -surface environment , unacceptable high drift use inertial sensor during extend gps outage , issue of possible confusion with nearby robot for sonar & radar , and the line of sight requirement for laser -based system be some of t he limitations associate with these navigation system . one of the promising solution lie in the art of visual odometry that help in estimate motion information with the help of camera mount over the vehicle . the onboard vision system track visu al landmark to estimate rotation and translation between two -time instant . the art of vision -based navigation be inspire by the behavior of a bird which rely heavily on it vision for guidance and control [ 3 ] . the initial work on estimate motion from a camera by moravec have help in establish the current shashi poddar be with csir - central scientific instruments organisation , chandigarh , india ( email : shashipoddar @ csio.res.in ) rahul kottath be with academy of scientific & innovative research , csir - csio campus , chandigarh india , ( email : rahulkottath @ gmail.com ) visual odometry ( vo ) pipeline [ 4 ] . simultaneous localization and mapping ( slam ) , a superset of vo , localize and build a map of it environment along with the trajectory of a move object [ 5 ] . however , our discussion in this paper be limit to visual odometry , which incrementally estimate the camera pose and refines it use optimization technique . a visual odometry system consist of a specific camera arrangement , the software architecture and the hardware platform to yield camera pose at every time instant . the camera pose estimation can be either appearance or fea ture base . the appearance - base technique operate on intensity value directly and match template of sub -images over two frame or the optical flow value to estimate motion [ 6 ] . the feature -based technique e xtract distinct interest point that can be track with the help of vector describe the local region around the key-points . these technique be dependent on the image texture and be generally not applicable in texture -less or low texture environment such a sandy soil , asphalt , etc . [ 7 ] . the vo technique can also be classify a geometric and learn base . the geometric vo technique be the one that explore camera geometry for estimate motion whe reas the learning base vo scheme train regression model to estimate motion parameter when fed with label data [ 8 ] . the learning -based vo technique do not require the camera parameter to be know initia lly and can estimate trajectory with correct scale even for monocular case [ 9 ] . the vo scheme can be implement either with a monocular , stereo , or rgb -d camera depend on the system design . stereo vo mimic the human vision system and can estimate the image scale immediately unlike monocular vo . however , stereo camera system requ ire more calibration effort and stringent camera synchronization without which the error propagate over time . the monocular camera be prefer for inexpensive and small form factor application such a phone , laptop , etc . where the mounting of two camera s with a specified baseline be not always feasible . some of the approach that aim to recover scale information for monocular vo be the usage of imu information [ 10 ] , optimization approach during loop closure [ 11 ] , and incorporate know dimensional information from wall , building , etc . [ 12 ] . an rgb -d camera provide color and depth information for each pixel in an image . the rgb -d vo start with the 3d position of feature point which be then use to obtain transformation through iterative closest point algorithm [ 13 ] . the vo scheme have find it major application in the automobile indus try in driver assistance and autonomous navigation [ 14 ] . one of the application of visual vinod karar be with csir - central scientific instruments organisation , chandigarh , india ( email : vinodkarar @ csio.res.in ) evolution of visual odometry techniques shashi poddar , rahul kottath , vinod karar odometry have be to estimate vehicle motion from the rear - parking camera and use this information with gps to provide accurate localization [ 15 ] . the task of visual servoing ( moving the camera to a desired orientation ) be very similar to the visual odometry problem require pose estimation for a different purpose [ 16 ] . these scheme be not only useful for navigation of rover on surface of other planet such a mars [ 17 ] but be also useful for track of satellite that need to be repair use a ser vicer [ 18 ] . although these vo technique have show promising result for variety of these application , they be sensitive to environmental change such a light condition , surround texture , the presence of water , snow , etc . some of the other condition that lead to poor tracking data be motion blur , the presence of shadow , visual similarity , degenerate configuration , and occlusion . along with these , some man - make error also creep into the data during image ac quisition and process step such a lens distortion and calibration , feature matching , triangulation , trajectory drift due to dead - reckon which lead to outlier . therefore , the vo scheme need to be robust and have the ability to manage these issue efficiently . in order to handle the environmental condition , different technique have be propose in the literature such a the usage of nir camera for dark environment [ 19 ] or usage of rank transform to handle light condition [ 20 ] . kaess et al . handle data degeneration by divide the image into two cluster base on disparity and compute rotation and translation with distant and nearby object , respectively [ 21 ] . several outlier rejection scheme have be propose in the literature of which ransac and it different variant be very commonly use [ 22 ] . the drift in trajectory over image fra me be compensate use different strategy such a loop closure and bundle adjustment ( ba ) . slam be an extended kalman filter ( ekf ) estimator that aim at obtain accurate motion vector give all the past feature position and their tracking informati on [ 23 ] . unlike slam that reduce drift by loop closure detection while visit same scene location , bundle adjustment ( ba ) optimize camera pose over image frame [ 24 ] . the ba framework minimize the re -projection error over the observed 3d image point and the predicted image point obtain use camera pose , intrinsic camera parameter and distortion parameter [ 25 ] . sliding ba use a fix ed window of previous image frame for optimization and be more popularly use for real-time application . alternately , the fusion of visual odometry with other motion estimation modality such a imu , gps [ 26 ] , abso lute sensor , compass [ 6 ] also exist in literature and be use to improve position accuracy . the rest of this paper be divide into different section . section 2 detail the evolution of visual odometry scheme un der different sub - category , that be , feature -based , appearance -based and learn -based along with some discussion on rgb -d base vo scheme . section 3 provide a list of different datasets specific to visual odometry and their allied area and finally section 4 conclude the paper . ii . evolution of visual odometry visual odometry ( vo ) be define a those set of algorithm that help in estimate motion by take cue from the image . these sensor can be either monocular , stereo or rgb -d in nature and have different algorithm framework , respectively . vo have a wide range of application vary from game & virtual reality , wearable computing , industrial manufacturing , healthcare , underwater , aerial , space robotics , driver assistance system , agriculture field robot , automobile , pedestrian & indoor navigation , and c ontrol & guidance of unmanned vehicle . in recent year , several vo technique have be publish in the literature and be a non -trivial task to have holistic view over the full breadth of these scheme . however , few judicious attempt have be make by so me of the researcher in review specific aspect of these approach . one of the popular review in the area of motion from image sequence be present by aggarwal and nandhakumar in 1988 by classify them into feature -based and optical flow base [ 27 ] . later , sedouza and kak survey the work carry out in last two decade and classified these technique into map -based , map -building -based , and map -less navigation scheme [ 28 ] . the map -based navigation approach require the robot to be feed with a model of the environment and a sequence of expect landmark whereas the map - building scheme create a representation of outer environment see by the camera . unlike the se , the maple approach do not require any map to be create for navigation and estimate motion by observe external object [ 28 ] . scaramuzza and fraundorfer publish two landmark article on feature base visual odometry pipeline which be very helpful to a newbie in this research area . it segregate the feature base pose estimation framework , into 2d -to-2d , 3d -to-3d , and 3d -to- 2d in concise step , provide detail of their origin and implementation . in 201 1 , weiss et al . classify the vo technique base on the camera location , one in which it be place in the environment and the other in which it be place on the uav , respectively [ 29 ] . the former technique be good for accurate and robust motion estimation of a robot move only in a know environment while the late track a known pattern or unknown landmark in the environment [ 30 ] . aqel et al . segregate several topic of i nterest to vo research community such a use of different sensor , vo application , approach and their current limitation [ 1 ] . yousif et al . attempt to provide an overview on structure from motion scheme which inc luded visual odometry and other localization and mapping method [ 31 ] . recently , janai et al . have put up a detailed review on different computer vision methodology use for autonomous driving . it dedicate a sub-section to ego -motion estimation with brief overview of recent article publish in the area of stereo and monocular visual odometry [ 32 ] . several article have be report in the literature that use hybrid of t wo different approach , sense modality etc . for example , scaramuzza & siegwart propose a hybrid of appearance and feature base approach [ 7 ] , a combination of visual camera and lidar sensor [ 33 ] , vision and imu [ 26 ] , vision and compass [ 6 ] , etc . the information provide in this paper be derive from the above mention review work alon g with the other allied literature in the area of visual odometry . however , the discussion here be limit to the evolution of vo approach from it original form to it current scenario . in order to provide brevity , the evolution of vo have be cover und er two broad sub - section , that be , geometric and non -geometric -based approach . these geometric approach be the one that exploit information from the projective geometry and the non - geometric approach a the one that be base on learning . the no n-analytical approach have gain recent popularity with the evolution of machine learning . the geometric approach have be far classify a feature -based , appearance -based and a hybrid of both the feature and appearance base . a. geometric approaches : feature -based the research on visual odometry find it ’ s origin back in 1960 's where a lunar rover be build by stanford university for control it from the earth . this cart be far explore by moravec to demonstrate correspondence base stereo navigation approach in 1980 [ 34 ] . this scheme match distinct feature between stereo image and triangulate them to the 3 -d world frame . once the robot move , these feature point be match in the next frame to obtain corresponding 3d point and generate motion parameter . mattheis and shafer improve upon this technique by model the triangulation error a 3 -d gaussian distribution rather than scalar weight [ 35 ] . arun et al . propose a least square base approach for determine a transformation between 3 -d point cloud [ 36 ] . weng et al . propose a matrix -weighted least square solution which perform remarkably well than unweighted or scalar - weighted solution [ 37 ] . it also propose an iterative optimal scheme to obtain motion parameter and 3 -d point . some of the researcher employ kalman filter to estimate motion parameter by m odeling the noise in image data by gaussian distribution [ 38 ] [ 39 ] . olson et al . solve the motion estimation problem through a maximum -likelihood formulation and mentio ns the use of an absolute orientation sensor to reduce the error growth rate for long distance navigation [ 40 ] . different formulation to solve feature -based vo have be discuss in the literature before 2000 and i s also present in the literature in review article . hence , not much of it will be discuss here and the main emphasis will be to present the improvement in feature -based vo scheme chronologically post 2000 era . in 2002 , se et al . propose to use scale invariant feature , sift for track interest point across image frame and estimation ego -motion [ 41 ] . improper image calibration , feature mismatch , image noise , and triangulation error be some of the contribute fac tor toward outlier during pose estimation . several outlier rejection scheme for r obust estimation such a ransac , mlesac and their variant have be propose in the literature . some of the researcher aim at estimate motion parameter without havin g a priori knowledge of camera calibration parameter [ 42 ] . however , a mention by nister , know the intrinsic parameter in advance help in obtain more accurate and robust motion estimate especially for planar or near planar scene [ 43 ] . nister also provide the step -by-step motion estimation framework for both the monocular & stereo case and coin the popular term 'visual odometry ' in this paper [ 44 ] . later , engels et al . estimate pose use this five -point algorithm follow by a bundle adjustment base refinement strategy [ 45 ] . se et al . apply hough transform and ransac approa ch for obtain a consistent set of match over which the least square minimization be apply to obtain pose estimation . it have also contribute towards the use of small feature descriptor of size 16 with sufficient discriminative power to match featu re , motivate researcher for more efficient feature descriptor [ 46 ] . tardif et al . estimate rotation use epipolar geometry and translation use 3d map while optimize the current location alone , rather than all previous location a do in bundle adjustment [ 47 ] . extensive work have also be report towards simultaneous localization and mapping ( a superset of pose estimation ) during this time and later , but be not disc ussed in this paper [ 48 , 5 ] . until this time , extensive work have be report on different motion estimation sub -routines such a estimate motion from different camera type [ 49 , 50 ] , estimate calibration & essential matrix [ 51 ] , and pose refinement use bundle adjustment [ 24 ] , [ 52 ] which contribute towards accuracy improvement . with this grow confidence on feature -based vo technique and it demonstration on ground vehicle navigation , it be use in navigate mars exploration rover [ 17 ] . it then receive renew interest among the researcher and several improvement be propose in the literature . kaess et al . use flow information for segregate distant and close point and estimate rotation and translation from them separately [ 21 ] . kalantari et al . propose a pose estimation algorithm use three point and the knowledge of vertical direction obtain from imu or the vanishing point [ 53 ] . however , this scheme be unable to provide closed -form solution and have singularity issue . naroditsky et al . present a closed form solution use similar three -plus-one algorithm by use vanish point or gravitational vector a the reference [ 54 ] . later , in 2011 scaramuzza et al . propose the 1-point algorithm for motion estimation by utilize the non - holonomic constraint of wheeled vehicle which switch to the standard 5 -point algorithm on detection of less inliers [ 55 ] . lee et al . extend this work for multi -camera set -up by model it a a generalized camera and propose a 2 -point algorithm for obtain the scale metric [ 56 ] . song et al . propose d a multi -thread monocular visual odometry technique that do not require any assumption regard the environment for scale estimation . epipolar search in all these thread with insertion of persistent 3d point at key -frames help in improve it accu racy and speed [ 57 ] . persson et al . extend this approach by generalize it for the stereo case . the 3d correspondence and the pose estimate by motion model be use to predict track position and in refinement process [ 58 ] . badino et al . improve the positional accuracy of feature point by average it position over all it previous occurrence and use these integrate feature for improve ego-motion accuracy [ 4 ] . kreso & segvic ( 2015 ) pay significance to the camera calibration parameter and corrects them by match feature point from one frame to the other with available ground truth motion [ 59 ] . cvisic & petrovic use a combination of stereo and monocular vo for estimate rotation use five -point algorithm and translation by minimize re -projection error a do for the stereo case . rotation estimate through monocular case help i n overcome error arise due to imperfect calibration while translation estimation through stereo case increase the accuracy [ 60 ] . bellavia et al . propose a key -frame selection strategy base on the existence o f image point with sufficient displacement [ 61 ] . liu et al . propose an improvement over the ransac scheme by generate the hypothesis preferentially and use three best hypothesis to estimate motion [ 62 ] . two different sub -categories of these feature -based vo , that be , usage of different feature descriptor and selection of a feature subset be cover below . 1 ) use of different features feature -based visual odometry involve keypoint detection , description , and a matching process to establish correspond image point which be then use for motion estimation . the traditional edge and corner feature detection strategy such a moravec and harris corner detector be very p opular initially and provide fast image correspondence . with the evolution of scale and transformation invariant feature extraction methodology such a sift , surf , orb , brisk , etc . these be more widely use a compare to simple corner detector . diff erent visual odometry research article have use different feature detection -description technique , that be , harris corner detector in [ 63 ] , sift in [ 47 ] , surf in [ 56 ] , censure in [ 64 ] , brief in [ 58 ] , surf – syba in [ 65 ] , and orb in [ 66 ] . given the wide gamut of feature detection technique , it be non-trivial to select a technique that suit one ’ s speed and accuracy requirement . some of the research work evaluate different feature detector and descriptor for visual odome try task and act a a beacon for judicious feature selection base on available hardware resource and design criterion . schmidt et al . compare the performance of different detector - descriptor pair and highlight the speedup achieve by a pairing of sing le scale feature detector with a reduced version of surf descriptor [ 67 ] . jiang et al . far extend this work by experiment on a large set of detector - descriptor pair and datasets [ 68 ] . it be show that brisk detector - descriptor be robust against image change and take less time a compare to sift and surf for visual odometry pipeline . additionally , it propose the use of multiple scale detector only fo r extreme motion while use single scale detector such a corner -based feature to expedite processing while maintain similar accuracy . further , chien et al . compare the performance of sift , surf , orb and a - kaze feature for the task of visual odom etry and find the surf base technique to yield maximum accuracy while the orb feature to be computationally cheap at the cost of low accuracy [ 69 ] . although most of the feature -based vo technique use point -featu re , very less work have be do with line feature owe to it computational complexity . in 2013 , witt and weltin propose an iterative closest multiple line algorithm to use line feature for pose estimation [ 70 ] . however , this scheme could not be apply for image with high texture and need complement with the point -based feature . ojeda and jimenez combine the point and line feature in a probabilistic manner [ 66 ] rather than combine them directly a attempt by koletschka et al . [ 71 ] . the probabilistic combination lead to an efficient solution with reduced effect of noisy measurement and an easy integration in the probabilisti c mobile robotics [ 66 ] . 2 ) features s election not only the selection of appropriate feature detection technique , researcher have devise mechanism by which only a portion of the detected feature be use for improved pose estimation . kitt et al . incorporate a bucketing approach for feature selection wherein the image be divide into grid such that each grid contribute only a specified number of match for further processing [ 63 ] . this approach reduce computational complexity and improve ego -motion accuracy with uniform feature distribution [ 72 ] . cvisic & petrovic classify feature from each bucket into four different class and sele cted strong feature from each class for motion estimation [ 60 ] . maeztu et al . carry out the complete feature detection , description , and match in correspond grid obtain by bucket . it not only help in improve estimate motion by reduce outlier but act a a framework for parallel implementation in multi -core architectures [ 73 ] . kitt et al . extend this technique by classify feature into move and non -moving feature with the help of randomized decision tree follow with the bucketing technique to select feature for motion estimation [ 74 ] . zhou et al . use random fern classifier to segregate matchable from non -matchable point and compute essential matrix only from the matchable one \\cite [ 75 ] . the main disadvantage of these classifier base technique be that they require train in advance and thus an online le arning approach be need to adapt in different situation [ 74 ] . escalera et al . propose a stereo vo technique that use static feature belong to the ground surface only , thus reduce the total number of feature s be use for pose estimation [ 76 ] . recently , kottath et al . propose an inertia constrain vo approach which select only those feature that follow the predicted motion model [ 77 ] . it be a simplified implementation of the technique propose by kostavelis on non-iterative outlier removal for stereo vo [ 78 ] and wu et al . wherein smoothness motion constraint be use to reject outlier [ 79 ] . the pose estimate from these feature -based vo scheme be generally pass through a filtering or an optimization framework for improved motion estimate . feature -based ekf -slam have be a popular technique fo r localization and mapping in the computer vision community which use sparse interest point [ 48 ] . a dedicated set of researcher have also employ kalman filter for motion estimation , account for noise r obustness . webb et al . employ tracked image feature point along with their epipolar constraint a measurement model for estimate state in the ekf framework . not much have be far detail on these filtering or optimization base technique and th e discussion will remain limited to pose estimation work alone . the following sub -section give a brief of appearance -based vo scheme that have evolve in parallel to the feature -based ego -motion estimation scheme . b . appearance base visual odometry appeara nce-based visual odometry be another class of geometric approach that do not rely on sparse feature and estimate motion by optimize the photometric error . generally , the feature -based technique be say to be noisy and the feature need not necess arily be distinguishable from their surroundings in smoothly vary landscape such a foggy environment or a sandy area [ 80 ] . the feature -matching step at time lead to wrong association which need to be remove d and can even be expensive if implement use neural network . instead , these appearance -based technique ( also refer a direct method ) utilize information from the complete image , lead to robust ego -motion estimate even in low -textured environme nt [ 81 ] . using whole image rather than few landmark reduces aliasing issue associate with similar look place , work even with smooth vary landscape and be fast to implement [ 80 ] . the appearance - base technique be generally of two type : region -based matching and optical flow base . the region -based match either can be achieve through correlation ( template matching ) or with the help of global appearance base descriptor and image alignment approach . correlation base technique ( also refer a template matching ) for align image have be a widely research area in the past use global invariant image representation or similarity measure . these scheme have several limitation which be overcome with the use of locally invariant similarity measure and global constraint [ 82 ] . the image alignment technique propose by irani and anandan be able to estimate parametric 2d motion model for image acquire by sensor from different modality [ 82 ] . mandelbaum et al . extend this scheme for estimate 3d ego-motion , which be iteratively refine over the multi - resolution framework . it estimate pose for a batch of image that be bootstrapped with a priori motion estimate for speed up estimation proce s . the a priori information can be either obtain from the previous batch or an external sensor or through a kalman filter prediction [ 83 ] . vatani et al . propose a simple and practical approach to ego -motion estimation use constrain correlation base approach . some of the modification carry out over simple correlation approach be correlation mask size base on image height , mask location a per vehicle motion and feeding of a small predictor area in which the mask be match [ 84 ] . yu et al . extend this work by use a rotated template that help in estimate both the translation and rotation between two frame [ 85 ] . vatani et al . select an appropriate template image from multiple - template and use linear forward prediction filter to select window location for faster and accurate matching process [ 86 ] . frederic lab rosse propose a visual compass technique base on template -matching to estimate pixel displacement in the image capture by an omni -directional camera look at the environment [ 80 ] . scaramuzza incorporate the v isual compass scheme to estimate rotation a this scheme be robust to systematic error from camera calibration and error accumulation due to integration over time [ 87 ] . gonzalez et al . incorporate labr osse ’ s visual compass technique for rotation estimation along with traditional template match approach to estimate translation use two different camera point at the environment and the ground , respectively [ 6 ] . recently , aqel et al . propose an adaptive template -matching scheme with reduced mask size and change template position base on vehicle acceleration [ 88 ] . several recent work be report towards robust t emplate match technique for other application and can be extend for visual odometry problem as well . some of the effort be also make towards the usage of global image appearance for registring image that can then be use for estimate ego -motion . goecke et al . make use of fourier -mellin transformation [ 89 ] while menegatti et al . use phase information from image ’ s fourier signature for estimate vehicle motion from these global descriptor [ 90 ] . the use of image registration technique for motion analysis find it mention in the article publish by lucas and kanade in the early 80 [ 91 ] . a set of technique use pre -stored image sequence for comparison with the current image and yield an indoor navigation estimate [ 92 ] , [ 93 ] . zhou et al . use histogram to describe the appearance of pre -stored im age frame a template which be then compare with the histogram of current image for recognize vehicle ’ s current location [ 94 ] . however , these scheme be not able to detect the orientation accurately which be de picted in the experiment carry out by pajda & hlavac [ 95 ] . jogan and leonardis correlate image use a combination of zero phase representation ( zpr ) and eigen -space of oriented image to yield rotation invariance but be sensitive to noise and occlusion [ 96 ] . comport et al . use reference stereo image pair to yield dense correspondence for estimate 6dof pose . this scheme be base on the quadrifocal relationship between image intensity and be robust to occlusion , inter -frame displacement , and illumination change [ 97 ] . comport et al . later extend his own work by design a scheme that minimize the intensity error between t he entire image while overcome the inter -frame overlap issue associate with region -based approach [ 98 ] . lovegrove et al . propose an image alignment approach to estimate vehicle motion by take the advantage of texture present on planar road surface [ 15 ] . some of the other region base match scheme use motion paralla x to compute 3d translation and parametric transformation between two frame [ 99 ] , [ 100 ] . irani et al . mention the advantage of decompose the camera motion into parametri c motion and parallax displacement rather than into translation and rotation . this plane -plus-parallax scheme be say to be more robust , stable and simple than the optical flow base approach a it require solve small set of linear equation [ 101 ] . these region -based scheme require a specific interest area to be define in the image that need to be match through sufficient overlap with the other image . further , the image registration process require a n optimization technique to minimize an objective function , which be generally subject to local minimum and divergence issue [ 98 ] . an inappropriate choice of argument minimization criterion and existence of independ ently move object be some of the major concern that can be avoid with optical flow base vo scheme [ 102 ] . optical flow be one of the fundamental principle that define ego -motion of an observer see in an image a per gibson ’ s ecological optic [ 103 ] . the use of optical flow for estimate motion information be inspire by the biological cue use by insect for navigation purpose [ 104 ] . some early attempt toward estimate ego -motion from optical flow be take up by clocksin ( 1978 ) [ 105 ] , ullman ( 1979 ) [ 106 ] , and prazdny ( 1980 ) [ 107 ] . however , most of the technique consider the scene to contain a single object or restrict the motion to be translatory with an assumption of only planar surface in the environment . the basic formulati on of optical flow propose by horn and schunck [ 108 ] get violate in the presence of motion discontinuity and vary illumination [ 109 ] . gilad adiv solve the motion disc ontinuity issue by compute motion for each of the connected partition of flow vector . these segment be later group together to formulate a motion hypothesis that be compatible with all the segment in a group [ 102 ] . black and anandan propose to use statistical framework that help in estimate the motion of the majority of the pixel while eliminate outlier [ 110 ] . several work have be report towards estimat ing illumination by mode lling it with a multiplicative/ additive factor or in a recursive framework . kim et al . address both the illumination and motion discontinuity issue by integrate black and anandan ’ s approach for handle motion discontinuity [ 110 ] with that of gennert and negahdaripour ’ s illumination variation model [ 111 ] specifically design for motion estimation task [ 109 ] . these optical flow base motion estimation method also refer to a direct method , use complete image information that can be apply to recover global 2d or 3d motion model [ 112 ] . giachetti et al . propose a correl ation base dense optical flow technique for estimate ego -motion of a car move in usual street with the help of a tv camera mount parallel to the ground [ 113 ] . the correlation base optical flow with large mask help in overcome instability associate with compute derivative and temporal filtering help in reduce the disturbance due to shock and vibration by reject horizontal component of optical flow . however , this scheme be not reliable in the presence of independently move object , and movement through hilly , dense vegetation , and clutter area . k. j. hanna describe an iterative approach to estimate camera motion directly through brightness derivative while use ego - motion and brightness constraint for refinement [ 114 ] . corke et al . mention the significance of use omnidirectional camera for estimate motion a it can retain feature for long dur ation and incorporate more information [ 104 ] . hyslop and humbert use the wide -field motion information from optical flow for estimate 6 -dof motion parameter and provide reliable information for navigation i n an unknown environment [ 115 ] . in 2005 , campbell et al . propose an optical flow base ego-motion estimation technique wherein the rotation be estimate use feature that be far from the camera and translatio n use near -by feature [ 116 ] . some of the research work aim at estimate motion in a controlled environment that can not be generalize for outdoor condition . for example , the technique propose by srinivasan obtain ego - motion for the set -up require camera to track change in ceiling light pattern have limit scope [ 117 ] . grabe et al . ( 2012a ) demonstrate an optical flow base close loop control uav operat ion use onboard hardware alone . this scheme aim at continuous motion recovery rather than estimate frame -to-frame motion alone [ 118 ] . grabe et al . ( 2012b ) far extend their work by employ feature that bel ong to a dominant plane alone to obtain improved velocity estimate [ 119 ] . tykkala and comport present a direct stereo base slam method wherein the motion be estimate by direct image alignment [ 120 ] . the lsd -slam estimate depth at pixel with large intensity gradient and also estimate rigid body motion by align image base on the depth map . however , this scheme use cue from both the stereo and monocular set -up and ha ndles brightness change in the image frame to yield good estimate [ 121 ] . recently , engel et al . propose a direct sparse odometry scheme , which optimize the photometric error in a framework similar to sparse bundl e adjustment . it avoid the use of geometric prior use in feature -based approach and use all image point to achieve robustness [ 81 ] . several work have also be report in the literature recently that estimate ego-motion use a combination of feature and optical flow [ 122 ] . optical flow have not only be use for estimate motion but also to help uavs navigate by provide cue relate to the presence of an obstacle in the vehicle path [ 123 ] . however , optical flow base scheme have their own limitation such a match in texture -less surface ( concrete , sand , etc . ) and computational complexity . the rgb -d camera be one of the framework that have low computational complexity a it provide depth value for image point directly through a depth sensor embed in the color camera . one set of technique formulate the vo task a energy minimization problem [ 124 ] while the other one estimate trajectory by classical registration technique [ 125 ] . the photometric error formulation of direct method be combine with the error in dense map obtai ned from rgb -d sensor to formulate a cost function which can be solve use numerical optimization algorithm [ 126 ] . the registration base scheme can achieve alignment base on shape , feature , or surface normal projection [ 127 ] . dryanovski et al . propose a scheme for align 3d point against a global model use iterative close point algorithm [ 13 ] . li and lee propose a fast visual odometry scheme by select few salient point on the source frame for icp and integrate intensity value in the correspondence estimation for improved icp [ 127 ] . a brief review of related work can be see in the article publish by kerl et al . which estimate motion by register the two rgb -d image directly on the basis of photometric error [ 128 ] . whelan et al . propose a robust rgb -d base visual odometry scheme which help in colored volumetric reconstruction of different scene and be one of the late work in this area [ 129 ] . c. geometric ap proaches : hybrid of feature and appearance base the hybrid algorithm for visual odometry take advantage of both the direct ( feature -based ) and indirect method ( appearance -based ) . feature -based scheme provide reliable data at the cost of certain loss o f available information while appearance -based method provide dense reconstruction exploit total available data but have error associate with few area . oliensis and werman propose an algorithm that combine direct and indirect scheme in one framewo rk with the main aim to incorporate all the available data and improve motion estimate [ 130 ] . morency and gupta propose a hybrid registration scheme that incorporate feature track information and optical flow constraint in one framework [ 131 ] . scaramuzza et al . use appearance - base approach to estimate rotation while translation be estimate by feature extract from the ground plane [ 132 ] . forster et al . propose a semi -direct vo technique that obtain feature correspondence from direct motion estimation which be then incorporate a matched point for feature -based pose estimation [ 133 ] . the direct motion estimation scheme here use sparse model -based image alignment to obtain feature correspondence and be term semi -direct owe to it fusion with feature base vo approach . silva et al . propose a dense ego-motion estimation tech nique complement with the feature base vo to obtain translation scale factor accurately and later refine with the kalman filter [ 134 ] . silva et al . extend it further by employ probabilistic corresponden ce for fully dense probabilistic stereo ego -motion and have mention it to be robust against difficult image scenario [ 135 ] . d. non-geometric approaches with the evolution of good computing resource , machine learning technique be be use for several real -time application . alvinn be one the initial attempt towards the usage of machine learn technique to improve the performance of navlab , the carnegie mellon autonomous navigation test vehicle use a three layer neural network in 1989 [ 136 ] . it be far improve by speed it by 5 -times use “ on the fly ” training approach in which the system imita tes the human driver under actual driving condition [ 137 ] . learning can be use in any stage of the odometry pipeline such a feature learn for good pose estimation [ 138 ] , estimate feature correspondence [ 139 ] , homography estimation [ 140 ] , etc . visual odometry base on machine learning be one of the emerge technique for motion estimation , a it do not require the camera calibration parameter to be know explicitly . the labelled data be use to train a regression/ classification model that c an estimate the ego -motion once an input image sequence be provide . these non -geometric learn -based approach can estimate translation to the correct scale and be robust against similar kind of noise with which it be train . this switch from geo metry -based to learn -based approach be one of the recent paradigm shift in the area of visual navigation . one of the initial work towards learn -based vo by roberts et al . aim at learn the platform velocity and turn rate from optical flow [ 141 ] . guizilini and ramos eliminate the use of geometric model by learn the effect of camera motion on image structure and vehicle dynamic . it use a coupled gaussian process for supervised learning of ego -motion fr om optical flow information [ 9 ] , [ 142 ] . they far extend their work for estimate linear and angular velocity by use optical flow information from a single camera along with multiple -output gaussian process framework ( mogp ) [ 143 ] . konda and memisevic make use of convolutional neural network ( cnn ) base synchrony auto encoder for joint estimation of depth and motion parameter s from single / multiple camera [ 144 ] . this work be far extend for visual odometry application by estimate local change in velocity and direction through the cnn architecture [ 138 ] . mohanty et al . use deep cnn to extract high level feature for estimate transformation between two time instant [ 145 ] . xu et al . use large scale crowd dataset to predict vehicle ego -motio n from it previous state estimate and instantaneous camera observation [ 146 ] . an improved cnn , that be , recurrent cnn , be use for achieve end -to-end pose estimation by learn geometrical feature in a sequential man ner [ 147 ] . the cnn structure have also be use in estimate scale for monocular visual odometry with the help of street mask use for ground plane estimation [ 148 ] . peretr oukhin et al . incorporate a cnn variant , that be , bayesian cnn to track sun direction and incorporate it into the vo pipeline for improved ego -motion estimate [ 149 ] . recently , zhan et al . propose a learni ng scheme for vo framework which use train data of single -view depth information along with two - view odometry data [ 150 ] . different model estimation scheme such a support vector machine , gaussian process , fuzzy logic , etc . be also report in the literature and hold great potential to be extend in the future [ 151 ] [ 152 ] . with this brief analysis , it can be conclude that the machine learning base vo technique hold huge potential for further improvement in estimate accurate motion estimate . the above sub -sections provide a brief overview of the evolution of visual odometry scheme vary from geometric to non -geometri c approach . along with these , vo scheme have also be develop for infrared camera but this paper do not provide much detail on them [ 153 ] , [ 154 ] . with progress in different subroutine of vo scheme , good and faster vo scheme be bind to evolve . some of the recent work have show the new direction in the motion estimation task such a pose estimation use event -based camera [ 155 ] , direct sparse odometry [ 81 ] , large scale direct slam [ 121 ] , robust real time vo use dense rgb -d camera [ 129 ] , etc . this e volution of vo scheme will still continue and this overview be a very small attempt to present different dimension in which the vo scheme currently deal . iii . visual odometry datasets with grow research in robotics and computer vision algorithm , it beca me very important to generate benchmarking datasets with ground -truth value that help in compare one algorithm over the other . in this attempt , several datasets have be put up by the researcher publicly for compare ego -motion estimation and it all ied technique . among the visual odometry stereo datasets , malaga [ 156 ] , [ 157 ] and new college [ 158 ] datasets be some of the early dataset for mobile robot localization . the karlsruhe dataset [ 159 ] could not be very popular a it have some acquisition issue . the kitti vision benchmark [ 160 ] suite be one of the most popular datasets in the computer vision research , especially for visual odometry task and be use by several researcher to compare their pose estimation scheme . some of the datasets aim to provide additional sensor data for be tter comparison with ground truth and target a relatively large research community that work on vision - aided navigation . the wean hall dataset [ 161 ] , kagaru airborne stereo dataset [ 162 ] , euroc mav dataset [ 163 ] , oxford robotcar dataset [ 164 ] be some of the datasets that provide stereo camera frame a along with the information from lidar , im u , and gps . of these , oxford robotcar dataset be suit to deep learning -based scheme that require huge datasets for training and estimate motion through image directly . the tum - monocular visual odometry dataset [ 165 ] , [ 166 ] and lsd -slam [ 167 ] be dedicate to the development of pose estimation and localization through monocular camera . one of the recent datasets aim at h igh- speed robotics be provide in zurich – event -camera dataset [ 155 ] for design new class of pose estimation algorithm with very high frame rate event -based camera . ford campus vision dataset [ 168 ] , eth - vision \\ & laser datasets from a heterogeneous uav fleet [ 169 ] , zurich urban micro aerial vehicle dataset [ 170 ] , and tum visual -inerti al dataset [ 171 ] be some of the datasets design specifically for slam , collaborative 3d reconstruction , appearance base localization , and visual odometry base application , respectively . rgb -d base motion estima tion be one of the other research area that be gain importance and thus dedicate benchmark datasets have also be put up for the same . the tum -rgb -d slam dataset [ 172 ] publish in 2012 be one of the early attempt towards provide rgb -d data for evaluation of visual odometry and visual slam scheme . the mit sata center dataset [ 173 ] and icl - nuim rgb -d dataset [ 174 ] map the i ndoor environment with a rgb -d camera and focus mainly on floor planning and surface reconstruction . very recently , the eth – rgb -d dataset [ 175 ] have also be publish which use laser scanner to generate ground truth information for structure from motion kind of application . some of the ego -motion estimation scheme have also be develop and test over synthetic datasets of which the new tsukuba dataset [ 176 ] be very famous and use by several researcher . these datasets be generate entirely on computer use different photo edit software . the multi -fov synthetic dataset [ 177 ] be one of the late attempt towards synthetic dataset that simulate fly robot hover in a room and vehicle move in a city . iv . conclusion in this work , an attempt be make to provide a holistic picture of the visual odometry technique encompass different branch of this tree . the paper start with an introduction to the motion estimation scheme and their wide application in different engineering field . the vo scheme have be discuss under two broad category , that be , geometric and non -geometric approach . the gamut of geometric approach be very wide and be thus sub -divided into three different sub -class , that be , feature -based , appearance - base , and hybrid scheme . towards the end , a list of different vo datasets be provide for ready reference which have be segregate into dif ferent class depend on the sensing modality . on the basis of recent research article , it be see that a huge impetus be give towards machine learn base vo , rgb -d base vo and other hybrid scheme that take the advantage of both direct and indi rect/ sparse and dense approach in one coherent framework . it be also pertinent to mention here that this work be not an exhaustive survey of visual odometry research article a it be a grow research area and a huge amount of related work have happene d in the past . acknowledgment this research have be support by drdo - aeronautical research & development board through grant -in-aid project on ” design and development of visual odometry system . references [ 1 ] m. o . a. aqel , m. h. marhaban , m. i. sar ipan and n. b. ismail , '' review of visual odometry : type , approach , challenge , and application , '' springerplus , vol . 5 , p. 1897 , 2016 . [ 2 ] r. madison , g. andrews , p. debitetto , s. rasmussen and m. bottkol , `` vision -aided navigation for small uavs in gps - challenge environment , '' in aiaa infotech @ aerospace 2007 conference and exhibit , 2007 . [ 3 ] s. m. ettinger , `` design and implementation of autonomous vision - guide micro air vehicle , '' 2001 . [ 4 ] h. badino , a. yamamoto and t. kanade , `` visual odometry by multi -frame feature integration , '' in computer vision workshops ( iccvw ) , 2013 ieee international conference on , 2013 . [ 5 ] h. durrant -whyte and t. bailey , `` simultaneous localization and mapping : part i , '' ieee robotics \\ & automation magazine , vol . 13 , pp . 99 -110 , 2006 . [ 6 ] r. gonzalez , f. rodriguez , j. l. guzman , c. pradalier and r. siegwart , `` combined visual odometry and vi sual compass for off - road mobile robot localization , '' robotica , vol . 30 , pp . 865 -878 , 2012 . [ 7 ] d. scaramuzza and r. siegwart , `` appearance -guided monocular omnidirectional visual odometry for outdoor ground vehicle , '' ieee transaction on robotics , vol . 24 , pp . 1015 -1026 , 2008 . [ 8 ] t. a. ciarfuglia , g. costante , p. valigi and e. ricci , `` evaluation of non-geometric method for visual odometry , '' robotics and autonomous systems , vol . 62 , pp . 1717 -1730 , 2014 . [ 9 ] v. guizilini and f. ramos , `` visual odometry learn for unmanned aerial vehicle , '' in robotics and automation ( icra ) , 2011 ieee international conference on , 2011 . [ 10 ] g. n { \\ '' u } tzi , s. weiss , d. scaramuzza and r. siegwart , `` fusion of imu and vision for absolute scale estimation in monocular slam , '' journal of intelligent \\ & robotic system , vol . 61 , pp . 287 -299 , 2011 . [ 11 ] h. strasdat , j. m. m. montiel and a. j. davison , `` scale drift -aware large scale monocular slam , '' robotics : science and systems vi , vol . 2 , 2010 . [ 12 ] s. hilsenbeck , a. m { \\ '' o } ller , r. huitl , g. schroth , m. kranz and e. steinbach , `` scale -preserving long -term visual odometry for indoor navigation , '' in indoor positioning and indoor navigation ( ipin ) , 2012 international conference on , 2012 . [ 13 ] i. dryanovski , r. g. valenti and j. xiao , `` fast visual odometry and mapping from rgb -d data , '' in robotics and automation ( icra ) , 2013 ieee international conference on , 2013 . [ 14 ] m. bertozzi , a. broggi and a. fascioli , `` vision -based intelligent vehicle : state of the art and perspective , '' robotics and autonomous system , vol . 32 , pp . 1 -16 , 2000 . [ 15 ] s. lovegrove , a. j. davison and j. ibanez -guzm { \\'a } n , `` accurate visual odometr y from a rear parking camera , '' in intelligent vehicles symposium ( iv ) , 2011 ieee , 2011 . [ 16 ] a. i. comport , e. marchand , m. pressigout and f. chaumette , `` real - time markerless track for augmented reality : the virtual visual servoing framework , '' ieee transactions on visualization and computer graphic , vol . 12 , pp . 615 -628 , 2006 . [ 17 ] m. maimone , y. cheng and l. matthies , `` two year of visual odometry on the mar exploration rover , '' journal of field robotics , vol . 24 , pp . 169 -186 , 2007 . [ 18 ] n. w. oumer and g. panin , `` 3d point tracking and pose estimation of a space object use stereo image , '' in pattern recognition ( icpr ) , 2012 21st international conference on , 2012 . [ 19 ] j. ruppelt and g. f. trommer , `` stereo -camera visual odometry for outdoor area and in dark indoor environment , '' ieee aerospace and electronic systems magazine , vol . 31 , pp . 4 -12 , 2016 . [ 20 ] c. golban , s. istvan and s. nedevschi , `` stereo base visual odometry in difficult traffic scene , '' in intelligent vehicles symp osium ( iv ) , 2012 ieee , 2012 . [ 21 ] m. kaess , k. ni and f. dellaert , `` flow separation for fast and robust stereo odometry , '' in robotics and automation , 2009 . icra'09 . ieee international conference on , 2009 . [ 22 ] r. raguram , o . chum , m. pollefeys , j. matas and j . -m. frahm , '' usac : a universal framework for random sample consensus , '' ieee transaction on pattern analysis and machine intelligence , vol . 35 , pp . 2022 -2038 , 2013 . [ 23 ] a. j. davison , `` real -time simultan eous localisation and mapping with a single camera , '' in null , 2003 . [ 24 ] b. triggs , p. f. mclauchlan , r. i. hartley and a. w. fitzgibbon , '' bundle adjustment —a modern synthesis , '' in international workshop on vision algorithm , 1999 . [ 25 ] m. i . a. lour akis and a . a. argyros , `` sba : a software package for generic sparse bundle adjustment , '' acm transactions on mathematical software ( toms ) , vol . 36 , p. 2 , 2009 . [ 26 ] m. agrawal and k. konolige , `` real -time localization in outdoor environment use stereo vision and inexpensive gps , '' in pattern recognition , 2006 . icpr 2006 . 18th international conference on , 2006 . [ 27 ] j. k. aggarwal and n. nandhakumar , `` on the computation of motion from sequence of image -a review , '' proceedings of the ieee , vol . 76 , pp . 917 -935 , 1988 . [ 28 ] g. n. desouza and a. c. kak , `` vision for mobile robot navigation : a survey , '' ieee transaction on pattern analysis and machine intelligence , vol . 24 , pp . 237 -267 , 2002 . [ 29 ] s. weiss , d. scaramuzza and r. siegwart , `` monocular -slam -- base navigation for autonomous micro helicopter in gps -denied environment , '' journal of field robotics , vol . 28 , pp . 854 -874 , 2011 . [ 30 ] d. eynard , p. vasseur , c. demonceaux and v. fr { \\'e } mont , `` real time uav altitude , attitude and motion estimation from hybrid stereovision , '' autonomous robots , vol . 33 , pp . 157 -172 , 2012 . [ 31 ] k. yousif , a. bab -hadiashar and r. hoseinnezhad , `` an overview to visual odometry and visual slam : applications to mob ile robotics , '' intelligent industrial systems , vol . 1 , pp . 289 -311 , 2015 . [ 32 ] j. janai , f. g { \\ '' u } ney , a. behl and a. geiger , `` computer vision for autonomous vehicle : problems , datasets and state -of-the-art , '' arxiv preprint arxiv:1704.05519 , 2017 . [ 33 ] j. zhang and s. singh , `` visual -lidar odometry and mapping : low - drift , robust , and fast , '' in robotics and automation ( icra ) , 2015 ieee international conference on , 2015 . [ 34 ] h. p. moravec , `` obstacle avoidance and navigation in the real world by a see robot rover. , '' 1980 . [ 35 ] l. matthies and s. t. e. v. e. n. a. shafer , `` error modeling in stereo navigation , '' ieee journal on robotics and automation , vol . 3 , pp . 239-248 , 1987 . [ 36 ] k. s. arun , t. s. huang and s. d. blostein , `` least -squares fi tting of two 3 -d point set , '' ieee transactions on pattern analysis and machine intelligence , pp . 698 -700 , 1987 . [ 37 ] j. weng , p. cohen and n. rebibo , `` motion and structure estimation from stereo image sequence , '' ieee transactions on robotics and autom ation , vol . 8 , pp . 362 -382 , 1992 . [ 38 ] t. j. broida and r. chellappa , `` estimation of object motion parameter from noisy image , '' ieee transaction on pattern analysis and machine intelligence , pp . 90 -99 , 1986 . [ 39 ] j. hallam , `` resolving observer mot ion by object tracking , '' in proceedings of the eighth international joint conference on artificial intelligence -volume 2 , 1983 . [ 40 ] c. f. olson , l. h. matthies , m. schoppers and m. w. maimone , '' stereo ego -motion improvement for robust rover navigation , '' in robotics and automation , 2001 . proceedings 2001 icra . ieee international conference on , 2001 . [ 41 ] s. se , d. lowe and j . little , `` mobile robot localization and mapping with uncertainty use scale -invariant visual landmark , '' the international journal of robotics research , vol . 21 , pp . 735 -758 , 2002 . [ 42 ] r. i. hartley , `` estimation of relative camera position for uncalibrated camera , '' in european conference on computer vision , 1992 . [ 43 ] d. nist { \\'e } r , `` an efficient solution to the five -point relative pose problem , '' ieee transaction on pattern analysis and machine intelligence , vol . 26 , pp . 756 -770 , 2004 . [ 44 ] d. nist { \\'e } r , o. naroditsky and j. bergen , `` visual odometry , '' in computer vision and pattern recognition , 2004 . cvpr 2004 . proceedings of the 2004 ieee computer society conference on , 2004 . [ 45 ] c. engels , h. stew { \\'e } nius and d. nist { \\'e } r , `` bundle adjustment rule , '' photogrammetric computer vision , vol . 2 , 2006 . [ 46 ] s. se , d. g. lowe and j. j . little , `` vision -based global localization and mapping for mobile robot , '' ieee transactions on robotics , vol . 21 , pp . 364 -375 , 2005 . [ 47 ] j.-p. tardif , y. pavlidis and k. daniilidis , `` monocular visual odometry in urban environment use an omnidirectional camera , '' in intelligent robots and systems , 2008 . iros 2008 . ieee/rsj international conference on , 2008 . [ 48 ] a. j. davison and d. w. murray , `` simultaneous localization and map-building use active vision , '' ieee transaction on pattern analysis and machine intelligence , vol . 24 , pp . 865 -880 , 2002 . [ 49 ] r. hartley and a. zisserman , multiple view geometry in computer vision , cambridge university press , 2003 . [ 50 ] p. chang and m. hebert , `` omni -directional structure from motion , '' in omnidirectional vision , 2000 . proceedings . ieee workshop on , 2000 . [ 51 ] b. mi { \\v { c } } u { \\v { s } } { \\'\\i } k and t. pajdla , `` omnidirectional camera model and epi polar geometry estimation by ransac with bucketing ? , '' in scandinavian conference on image analysis , 2003 . [ 52 ] m. lhuillier , `` automatic structure and motion use a catadioptric camera , '' in proceedings of the 6th workshop on omnidirectional vision , camera networks and non -classical cameras , 2005 . [ 53 ] m. kalantari , a. hashemi , f. jung and j . -p. gu { \\'e } don , `` a new solution to the relative orientation problem use only 3 point and the vertical direction , '' journal of mathematical imaging and vision , vol . 39 , pp . 259 -268 , 2011 . [ 54 ] o. naroditsky , x. s. zhou , j. gallier , s. i. roumeliotis and k. daniilidis , `` two efficient solution for visual odometry use directional correspondence , '' ieee transaction on pattern analysis and machine intelligence , vol . 34 , pp . 818 -824 , 2012 . [ 55 ] d. scaramuzza , `` 1 -point -ransac structure from motion for vehicle - mount camera by exploit non -holonomic constraint , '' international journal of computer vision , vol . 95 , pp . 74 -85 , 2011 . [ 56 ] g. h. lee , f. faund orfer and m. pollefeys , `` motion estimation for self-driving car with a generalized camera , '' in computer vision and pattern recognition ( cvpr ) , 2013 ieee conference on , 2013 . [ 57 ] s. song , m. chandraker and c. c. guest , `` parallel , real -time monocular vi sual odometry , '' in robotics and automation ( icra ) , 2013 ieee international conference on , 2013 . [ 58 ] m. persson , t. piccini , m. felsberg and r. mester , `` robust stereo visual odometry from monocular technique , '' in intelligent vehicles symposium ( iv ) , 20 15 ieee , 2015 . [ 59 ] i. kre { \\v { s } } o and s. { \\v { s } } egvic , `` improving the egomotion estimation by correct the calibration bias , '' in 10th international conference on computer vision theory and applications , 2015 . [ 60 ] i. cvi { \\v { s } } i { \\'c } and i. petrovi { \\'c } , `` stereo odometry base on careful feature selection and tracking , '' in mobile robots ( ecmr ) , 2015 european conference on , 2015 . [ 61 ] f. bellavia , m. fanfani and c. colombo , `` selective visual odometry for accurate auv localization , '' autonomous robots , vol . 41 , pp . 133-143 , 2017 . [ 62 ] y. liu , y. gu , j. li and x. zhang , `` robust stereo visual odometry using improved ransac -based methods for mobile robot localization , '' sensors , vol . 17 , p. 2339 , 2017 . [ 63 ] b. kitt , a. geiger and h. lategahn , `` visual odometry base on stereo image sequence with ransac -based outlier rejection scheme , '' in intelligent vehicles symposium ( iv ) , 2010 ieee , 2010 . [ 64 ] k. konolige , m. agrawal and j. sola , `` large -scale visual odom etry for rough terrain , '' in robotics research , springer , 2010 , pp . 201 - 212 . [ 65 ] a. desai and d. -j. lee , `` visual odometry drift reduction use syba descriptor and feature transformation , '' ieee transactions on intelligent transportation systems , vol . 17 , pp . 1839 -1851 , 2016 . [ 66 ] r. gomez -ojeda and j. gonzalez -jimenez , `` robust stereo visual odometry through a probabilistic combination of point and line segment , '' in robotics and automation ( icra ) , 2016 ieee international conference on , 2016 . [ 67 ] a. schmidt , m. kraft and a. kasi { \\'n } ski , `` an evaluation of image feature detector and descriptor for robot navigation , '' in international conference on computer vision and graphics , 2010 . [ 68 ] y. jiang , y. xu and y. liu , `` performance evaluation of feature detection and matching in stereo visual odometry , '' neurocomputing , vol . 120 , pp . 380 -390 , 2013 . [ 69 ] h.-j . chien , c. -c. chuang , c. -y. chen and r. klette , `` when to use what feature ? sift , surf , orb , or a -kaze feature for monocular visual odo metry , '' in image and vision computing new zealand ( ivcnz ) , 2016 international conference on , 2016 . [ 70 ] j. witt and u. weltin , `` robust stereo visual odometry use iterative closest multiple line , '' in intelligent robots and systems ( iros ) , 2013 ieee/rs j international conference on , 2013 . [ 71 ] t. koletschka , l. puig and k. daniilidis , `` mevo : multi - environment stereo visual odometry , '' in intelligent robots and systems ( iros 2014 ) , 2014 ieee/rsj international conference on , 2014 . [ 72 ] z. zhang , r. deriche , o. faugeras and q . -t. luong , `` a robust technique for match two uncalibrated image through the recovery of the unknown epipolar geometry , '' artificial intelligence , vol . 78 , pp . 87 -119 , 1995 . [ 73 ] l. de -maeztu , u. elordi , m. niet o , j. barandiaran and o. otaegui , '' a temporally consistent grid -based visual odometry framework for multi -core architecture , '' journal of real -time image processing , vol . 10 , pp . 759 -769 , 2015 . [ 74 ] b. kitt , f. moosmann and c. stiller , `` moving on to dyn amic environment : visual odometry use feature classification , '' in intelligent robots and systems ( iros ) , 2010 ieee/rsj international conference on , 2010 . [ 75 ] w. zhou , h. fu and x . an , `` a classification -based visual odometry approach , '' in intelligent human -machine systems and cybernetics ( ihmsc ) , 2016 8th international conference on , 2016 . [ 76 ] a. escalera , e. izquierdo , d. mart { \\'\\i } n , b. musleh , f. garc { \\'\\i } a and j. m. armingol , `` stereo visual odometry in urban environment base on detect gr ound feature , '' robotics and autonomous systems , vol . 80 , pp . 1 -10 , 2016 . [ 77 ] r. kottath , d. p. yalamandala , s. poddar , a. p. bhondekar and v. karar , `` inertia constrain visual odometry for navigational application , '' in image information processing ( iciip ) , 2017 fourth international conference on , 2017 . [ 78 ] i. kostavelis , e. boukas , l. nalpantidis and a. gasteratos , `` stereo - base visual odometry for autonomous robot navigation , '' international journal of advanced robotic systems , vol . 13 , p. 21 , 2016 . [ 79 ] m. wu , s. -k. lam and t. srikanthan , `` a framework for fast and robust visual odometry , '' ieee transactions on intelligent transportation systems , vol . 18 , pp . 3433 -3448 , 2017 . [ 80 ] f. labrosse , `` the visual compass : performanc e and limitation of an appearance -based method , '' journal of field robotics , vol . 23 , pp . 913-941 , 2006 . [ 81 ] j. engel , v. koltun and d. cremers , `` direct sparse odometry , '' ieee transaction on pattern analysis and machine intelligence , vol . 40 , pp . 611 -625 , 2018 . [ 82 ] m. irani and p. anandan , `` robust multi -sensor image alignment , '' in computer vision , 1998 . sixth international conference on , 1998 . [ 83 ] r. mandelbaum , g. salgian and h. sawhney , `` correlation -based estimation of ego -motion and structur e from motion and stereo , '' in computer vision , 1999 . the proceedings of the seventh ieee international conference on , 1999 . [ 84 ] n. nourani -vatani , j. roberts and m. v. srinivasan , `` practical visual odometry for car -like vehicle , '' in robotics and automation , 2009 . icra'09 . ieee international conference on , 2009 . [ 85 ] y. yu , c. pradalier and g. zong , `` appearance -based monocular visual odometry for ground vehicle , '' in advanced intelligent mechatronics ( aim ) , 2011 ieee/asme international conferenc e on , 2011 . [ 86 ] n. nourani -vatani and p. v. k. borges , `` correlation -based visual odometry for ground vehicle , '' journal of field robotics , vol . 28 , pp . 742 -768 , 2011 . [ 87 ] d. scaramuzza , `` omnidirectional vision , '' 2007 . [ 88 ] m. o . a. aqel , m. h. marhaban , m. i. saripan and n. b. ismail , '' adaptive -search template matching technique base on vehicle acceleration for monocular visual odometry system , '' ieej transactions on electrical and electronic engineering , vol . 11 , pp . 739-752 , 2016 . [ 89 ] r. goecke , a. asthana , n. pettersson and l. petersson , `` visual vehicle egomotion estimation use the fourier -mellin transform , '' in intelligent vehicles symposium , 2007 ieee , 2007 . [ 90 ] e. menegatti , t. maeda and h. ishiguro , `` image -based memory for robot navigation use property of omnidirectional image , '' robotics and autonomous systems , vol . 47 , pp . 251 -267 , 2004 . [ 91 ] b. lucas and t. kanade , `` b. ( 1981 ) . an iterative image registr ation technique with an application to stereo vision , '' in proc . darpa image understanding workshop , 1981 . [ 92 ] y. matsumoto , m. inaba and h. inoue , `` visual navigation use view -sequenced route representation , '' in robotics and automation , 1996 . proceedi ngs. , 1996 ieee international conference on , 1996 . [ 93 ] t. ohno , a. ohya and s. yuta , `` autonomous navigation for mobile robot refer pre -recorded image sequence , '' in intelligent robots and systems ' 96 , iros 96 , proceedings of the 1996 ieee/rsj inter national conference on , 1996 . [ 94 ] c. zhou , y. wei and t. tan , `` mobile robot self -localization base on global visual appearance feature , '' in robotics and automation , 2003 . proceedings . icra'03 . ieee international conference on , 2003 . [ 95 ] t. pajdla and v. hlav { \\'a } { \\v { c } } , `` zero phase representation of panoramic image for image base localization , '' in international conference on computer analysis of images and patterns , 1999 . [ 96 ] m. jogan and a. leonardis , `` robust localization use eigenspace of spin -images , '' in omnidirectional vision , 2000 . proceedings . ieee workshop on , 2000 . [ 97 ] a. i. comport , e. malis and p. rives , `` accurate quadrifocal tracking for robust 3d visual odometry , '' in robotics and automation , 2007 ieee international conference on , 2007 . [ 98 ] a. i. comport , e. malis and p. rives , `` real -time quadrifocal visual odometry , '' the international journal of robotics research , vol . 29 , pp . 245 -266 , 2010 . [ 99 ] m. irani , b. rousso and s. peleg , `` computing occluding and transparent motion , '' international journal of computer vision , vol . 12 , pp . 5 -16 , 1994 . [ 100 ] r. cipolla , y. okamoto and y. kuno , `` robust structure from motion use motion parallax , '' in comput er vision , 1993 . proceedings. , fourth international conference on , 1993 . [ 101 ] m. irani , b. rousso and s. peleg , `` recovery of ego -motion use region alignment , '' ieee transactions on pattern analysis and machine intelligence , vol . 19 , pp . 268 -272 , 1997 . [ 102 ] g. adiv , `` determining three -dimensional motion and structure from optical flow generate by several move object , '' ieee transaction on pattern analysis and machine intelligence , pp . 384 - 401 , 1985 . [ 103 ] j. j. gibson , `` visually control locomotion and visual orientation in animal , '' british journal of psychology , vol . 49 , pp . 182 -194 , 1958 . [ 104 ] m. v. srinivasan , m. lehrer , w. h. kirchner and s. w. zhang , '' range perception through apparent image spee d in freely fly honeybee , '' visual neuroscience , vol . 6 , pp . 519 -535 , 1991 . [ 105 ] w. f. clocksin , `` determining the orientation of surface from optical flow , '' in proceedings of the 1978 aisb/gi conference on artificial intelligence , 1978 . [ 106 ] s. ullman , `` the interpretation of structure from motion , '' proc . r. soc . lond . b , vol . 203 , pp . 405 -426 , 1979 . [ 107 ] k. prazdny , `` egomotion and relative depth map from optical flow , '' biological cybernetics , vol . 36 , pp . 87 -102 , 1980 . [ 108 ] b. k. p. horn and b. g. schunck , `` determining optical flow , '' artificial intelligence , vol . 17 , pp . 185 -203 , 1981 . [ 109 ] y.-h. kim , a. m. mart { \\'\\i } nez and a. c. kak , `` robust motion estimation under vary illumination , '' image and vision computing , vol . 23 , pp . 365 -375 , 2005 . [ 110 ] m. j . black and p. anandan , `` the robust estimation of multiple motion : parametric and piecewise -smooth flow field , '' computer vision and image understanding , vol . 63 , pp . 75 -104 , 1996 . [ 111 ] m. a. gennert and s. negahdaripour , `` relaxing the brightness constancy assumption in compute optical flow , '' 1987 . [ 112 ] m. irani and p. anandan , `` about direct method , '' in international workshop on vision algorithms , 1999 . [ 113 ] a. giachetti , m. campani and v. torre , `` t he use of optical flow for road navigation , '' ieee transaction on robotics and automation , vol . 14 , pp . 34 -48 , 1998 . [ 114 ] k. j. hanna , `` direct multi -resolution estimation of ego -motion and structure from motion , '' in visual motion , 1991. , proceedings of the ieee workshop on , 1991 . [ 115 ] a. m. hyslop and j. s. humbert , `` autonomous navigation in three - dimensional urban environment use wide -field integration of optic flow , '' journal of guidance , control , and dynamic , vol . 33 , pp . 147-159 , 2010 . [ 116 ] j. campbell , r. sukthankar , i. nourbakhsh and a. pahwa , `` a robust visual odometry and precipice detection system use consumer - grade monocular vision , '' in robotics and automation , 2005 . icra 2005 . proceedings of the 2005 ieee international conferen ce on , 2005 . [ 117 ] m. v. srinivasan , `` an image -interpolation technique for the computation of optic flow and egomotion , '' biological cybernetics , vol . 71 , pp . 401 -415 , 1994 . [ 118 ] v. grabe , h. h. b { \\ '' u } lthoff and p. r. giordano , `` on -board velocity estimation and close -loop control of a quadrotor uav base on optical flow , '' in robotics and automation ( icra ) , 2012 ieee international conference on , 2012 . [ 119 ] v. grabe , h. h. b { \\ '' u } lthoff and p. r. giordano , `` robust optical - flow base self -motion esti mation for a quadrotor uav , '' in intelligent robots and systems ( iros ) , 2012 ieee/rsj international conference on , 2012 . [ 120 ] t. tykk { \\ '' a } l { \\ '' a } and a. i. comport , `` a dense structure model for image base stereo slam , '' in robotics and automation ( icra ) , 2011 ieee international conference on , 2011 . [ 121 ] j. engel , j. st { \\ '' u } ckler and d. cremers , `` large -scale direct slam with stereo camera , '' in intelligent robots and systems ( iros ) , 2015 ieee/rsj international conference on , 2015 . [ 122 ] r. ranftl , v. vineet , q. chen and v. koltun , `` dense monocular depth estimation in complex dynamic scene , '' in proceedings of the ieee conference on computer vision and pattern recognition , 2016 . [ 123 ] s. temizer , `` optical flow base local navigation , '' 2 001 . [ 124 ] c. kerl , j. sturm and d. cremers , `` dense visual slam for rgb -d camera , '' in intelligent robots and systems ( iros ) , 2013 ieee/rsj international conference on , 2013 . [ 125 ] i. dryanovski , c. jaramillo and j. xiao , `` incremental registration of rgb -d image , '' in robotics and automation ( icra ) , 2012 ieee international conference on , 2012 . [ 126 ] t. tykk { \\ '' a } l { \\ '' a } , c. audras and a. i. comport , `` direct iterative closest point for real -time visual odometry , '' in computer vision workshops ( iccv work shop ) , 2011 ieee international conference on , 2011 . [ 127 ] s. li and d. lee , `` fast visual odometry use intensity -assisted iterative closest point , '' ieee robotics and automation letters , vol . 1 , pp . 992 -999 , 2016 . [ 128 ] c. kerl , j. sturm and d. cremers , `` robust odometry estimation for rgb -d camera , '' in robotics and automation ( icra ) , 2013 ieee international conference on , 2013 . [ 129 ] t. whelan , h. johannsson , m. kaess , j. j. leonard and j. mcdonald , '' robust real -time visual odometry for dense rgb -d mapping , '' in robotics and automation ( icra ) , 2013 ieee international conference on , 2013 . [ 130 ] j. oliensis and m. werman , `` structure from motion use point , line , and intensity , '' in computer vision and pattern recog nition , 2000 . proceedings . ieee conference on , 2000 . [ 131 ] l.-p. morency and r. gupta , `` robust real -time egomotion from stereo image , '' in image processing , 2003 . icip 2003 . proceedings . 2003 international conference on , 2003 . [ 132 ] d. scaramuzza , f. fraundorfer , m. pollefeys and r. siegwart , '' closing the loop in appearance -guided structure -from -motion for omnidirectional camera , '' in the 8th workshop on omnidirectional vision , camera networks and non -classical cameras -omnivis , 2008 . [ 133 ] c. forst er , m. pizzoli and d. scaramuzza , `` svo : fast semi -direct monocular visual odometry , '' in robotics and automation ( icra ) , 2014 ieee international conference on , 2014 . [ 134 ] h. silva , a. bernardino and e. silva , `` probabilistic egomotion for stereo visual odometry , '' journal of intelligent \\ & robotic systems , vol . 77 , pp . 265 -280 , 2015 . [ 135 ] h. silva , a. bernardino and e. silva , `` a voting method for stereo egomotion estimation , '' international journal of advanced robotic systems , vol . 14 , p. 1729881417710795 , 2017 . [ 136 ] d. a. pomerleau , `` alvinn : an autonomous land vehicle in a neural networ k , '' in advances in neural information process system , 1989 . [ 137 ] d. a. pomerleau , `` efficient training of artificial neural network for autonomous navigation , '' neural computation , vol . 3 , pp . 88 -97 , 1991 . [ 138 ] k. r. konda and r. memisevic , `` lea rning visual odometry with a convolutional network. , '' in visapp ( 1 ) , 2015 . [ 139 ] d. detone , t. malisiewicz and a. rabinovich , `` deep image homography estimation , '' arxiv preprint arxiv:1606.03798 , 2016 . [ 140 ] r. memisevic , `` learning to relate image , '' ieee transaction on pattern analysis and machine intelligence , vol . 35 , pp . 1829 -1846 , 2013 . [ 141 ] r. roberts , h. nguyen , n. krishnamurthi and t. balch , `` memory - base learning for visual odometry , '' in robotics and automation , 2008 . icra 2008 . ieee international conference on , 2008 . [ 142 ] v. guizilini and f. ramos , `` semi -parametric model for visual odometry , '' in robotics and automation ( icra ) , 2012 ieee international conference on , 2012 . [ 143 ] v. guizilini and f. ramos , `` semi -parametric learni ng for visual odometry , '' the international journal of robotics research , vol . 32 , pp . 526 -546 , 2013 . [ 144 ] k. konda and r. memisevic , `` unsupervised learning of depth and motion , '' arxiv preprint arxiv:1312.3429 , 2013 . [ 145 ] v. mohanty , s. agrawal , s. datta , a. ghosh , v. d. sharma and d. chakravarty , `` deepvo : a deep learning approach for monocular visual odometry , '' arxiv preprint arxiv:1611.06069 , 2016 . [ 146 ] h. xu , y. gao , f. yu and t. darrell , `` end -to-end learning of driv ing model from large -scale video datasets , '' arxiv preprint , 2016 . [ 147 ] s. wang , r. clark , h. wen and n. trigoni , `` deepvo : towards end - to-end visual odometry with deep recurrent convolutional neural network , '' in robotics and automation ( icra ) , 2017 ieee international conference on , 2017 . [ 148 ] n. fanani , a. st { \\ '' u } rck , m. ochs , h. bradler and r. mester , '' predictive monocular odometry ( pmo ) : what be possible without ransac and multiframe bundle adjustment ? , '' image and vision computing , vol . 68 , pp . 3-13 , 2017 . [ 149 ] v. peretroukhin , l. clement and j. kelly , `` inferring sun direction to improve visual odometry : a deep learning approach , '' the international journal of robotics research , p. 0278364917749732 , 2018 . [ 150 ] h. zhan , r. garg , c. s. weer asekera , k. li , h. agarwal and i. reid , '' unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction , '' arxiv preprint arxiv:1803.03893 , 2018 . [ 151 ] m. bojarski , d. del testa , d. dworakowski , b. firner , b. fle pp , p. goyal , l. d. jackel , m. monfort , u. muller , j. zhang and others , '' end to end learn for self -driving car , '' arxiv preprint arxiv:1604.07316 , 2016 . [ 152 ] r. mahajan , p. v. shanmuganathan , v. karar and s. poddar , '' flexible threshold visual odomet ry algorithm using fuzzy logics , '' in proceedings of 2nd international conference on computer vision \\ & image processing , 2018 . [ 153 ] t. mouats , n. aouf , l. chermak and m. a. richardson , `` thermal stereo odometry for uavs , '' ieee sensors journal , vol . 15 , pp . 6335 -6347 , 2015 . [ 154 ] t. mouats , n. aouf , a. d. sappa , c. aguilera and r. toledo , '' multispectral stereo odometry , '' ieee transactions on intelligent transportation systems , vol . 16 , pp . 1210 -1224 , 2015 . [ 155 ] e. mueggler , h. rebecq , g. gallego , t. delbruck and d. scaramuzza , `` the event -camera dataset and simulator : event -based data for pose estimation , visual odometry , and slam , '' the international journal of robotics research , vol . 36 , pp . 142 -149 , 2017 . [ 156 ] j.-l. blanco , f. -a. moreno and j. gonzalez , `` a collection of outdoor robotic datasets with centimeter -accuracy ground truth , '' autonomous robots , vol . 27 , p. 327 , 2009 . [ 157 ] j.-l. blanco -claraco , f. - { . moreno -due { \\~n } a and j. gonz { \\'a } lez -jim { \\'e } nez , `` the málaga urban dataset : high -rate stereo and lidar in a realistic urban scenario , '' the international journal of robotics research , vol . 33 , pp . 207 -214 , 2014 . [ 158 ] m. smi th , i. baldwin , w. churchill , r. paul and p. newman , `` the new college vision and laser data set , '' the international journal of robotics research , vol . 28 , pp . 595 -599 , 2009 . [ 159 ] a. geiger , j. ziegler and c. stiller , `` stereoscan : dense 3d reconstructio n in real -time , '' in intelligent vehicles symposium ( iv ) , 2011 ieee , 2011 . [ 160 ] a. geiger , p. lenz , c. stiller and r. urtasun , `` vision meet robotics : the kitti dataset , '' the international journal of robotics research , vol . 32 , pp . 1231 -1237 , 2013 . [ 161 ] h. alismail , b. browning , m. b. dias , b. argall , b. browning , y. gu , m. veloso , b. argall , b. browning , m. veloso and others , '' evaluating pose estimation methods for stereo visual odometry on robots , '' in proceeding of the 11th international confere nce on intelligent autonomous systems ( ias -11 ) , 2008 . [ 162 ] m. warren , d. mckinnon , h. he , a. glover , m. shiel and b. upcroft , `` large scale monocular vision -only map from a fix - wing suas , '' in field and service robotics , 2014 . [ 163 ] m. burri , j. nikolic , p. gohl , t. schneider , j. rehder , s. omari , m. w. achtelik and r. siegwart , `` the euroc micro aerial vehicle datasets , '' the international journal of robotics research , vol . 35 , pp . 1157 -1163 , 2016 . [ 164 ] w. maddern , g. pascoe , c. linegar and p. newman , `` 1 year , 1000 km : the oxford robotcar dataset , '' the international journal of robotics research , vol . 36 , pp . 3 -15 , 2017 . [ 165 ] p. bergmann , r. wang and d. cremers , `` online photometric calibration of auto exposure video for realtime visual odome try and slam , '' ieee robotics and automation letters , vol . 3 , pp . 627 - 634 , 2018 . [ 166 ] j. engel , v. usenko and d. cremers , `` a photometrically calibrate benchmark for monocular visual odometry , '' arxiv preprint arxiv:1607.02555 , 2016 . [ 167 ] d. caruso , j. engel and d. cremers , `` large -scale direct slam for omnidirectional camera , '' in intelligent robots and systems ( iros ) , 2015 ieee/rsj international conference on , 2015 . [ 168 ] g. pandey , j. r. mcbride and r. m. eustice , `` ford campus vision and lidar data set , '' the international journal of robotics research , vol . 30 , pp . 1543 -1552 , 2011 . [ 169 ] t. hinzmann , t. stastny , g. conte , p. doherty , p. rudol , m. wzorek , e. galceran , r. siegwart and i. gilitschenski , '' collaborative 3d reconstruction using heterogeneous uavs : system and experiments , '' in international symposium on experimental robotics , 2016 . [ 170 ] a. l. majdik , c. till and d. scaramuzza , `` the zurich urban micro aerial v ehicle dataset , '' the international journal of robotics research , vol . 36 , pp . 269 -273 , 2017 . [ 171 ] d. schubert , t. goll , n. demmel , v. usenko , j. st { \\ '' u } ckler and d. cremers , `` the tum vi benchmark for evaluating visual -inertial odometry , '' arxiv preprint arxiv:1804.06120 , 2018 . [ 172 ] j. sturm , n. engelhard , f. endres , w. burgard and d. cremers , `` a benchmark for the evaluation of rgb -d slam system , '' in intelligent robots and systems ( iros ) , 2012 ieee/rsj international conference on , 2012 . [ 173 ] m. fallon , h. johannsson , m. kaess and j. j. leonard , `` the mit stata center dataset , '' the international journal of robotics research , vol . 32 , pp . 1695 -1699 , 2013 . [ 174 ] a. handa , t. whelan , j. mcdonald and a. j. davison , `` a benchmark for rgb -d visual o dometry , 3d reconstruction and slam , '' in robotics and automation ( icra ) , 2014 ieee international conference on , 2014 . [ 175 ] h. oleynikova , z. taylor , m. fehr , j. nieto and r. siegwart , '' voxblox : building 3d sign distance field for planning , '' arxiv , pp . arxiv -- 1611 , 2016 . [ 176 ] m. peris , s. martull , a. maki , y. ohkawa and k. fukui , `` towards a simulation drive stereo vision system , '' in pattern recognition ( icpr ) , 2012 21st international conference on , 2012 . [ 177 ] z. zhang , h. rebecq , c. forster a nd d. scaramuzza , `` benefit of large field -of-view camera for visual odometry , '' in robotics and automation ( icra ) , 2016 ieee international conference on , 2016 .","['\\uf020 abstract — with rapid advancement in the area of mobile robotics and industrial automation , a grow need have arise towards accurate navigation and localization of move object . camera base motion estimation be one such technique which be gain huge popularity ow ing to it simplicity and use of limited resource in generate motion path . in this paper , an attempt be make to introduce this topic for beginner cover different aspect of vision base motion estimation task . the evolution of vo scheme over last f ew decade be discuss under two broad category , that be , geometric and non -geometric approach . the geometric approach be far detail under three different class , that be , feature -based , appearance -based , and a hybrid of feature and appearan ce base scheme . the non -geometric approach be one of the recent paradigm shift from conventional pose estimation technique and be thus discuss in a separate section . towards the end , a list of different datasets for visual odometry and ally research area be provide for a ready reference . keywords : motion estimation , visua l odometry , direct vo , rgbd vo , learning base vo , visual odometry datasets i . introduction with rise automation in different engineering field , mobile robotics be gain huge popularity . the unmanned vehicle be one such proliferate example that be expand it fleet in different appli cation range from commercial to strategic use . one of the simple mechanism to estimate the motion of a terrestrial vehicle be to use wheel encoders . however , these have limit usage in ground vehicle and suffer from inaccuracy that occur due to wh eel slippage during movement in muddy , slippery , sandy or loose terrain . the error arise at each instant get accumulate over time and the estimate pose drift in proportion to the distance travel [ 1 ] . traditio nal navigation approach such a inertial navigation system ( ins ) , the global positioning system ( gps ) , sonar , radar , and lidar be currently in use for different application [ 2 ] .', '. traditio nal navigation approach such a inertial navigation system ( ins ) , the global positioning system ( gps ) , sonar , radar , and lidar be currently in use for different application [ 2 ] . unavailability of gps signal in an indoor and under -surface environment , unacceptable high drift use inertial sensor during extend gps outage , issue of possible confusion with nearby robot for sonar & radar , and the line of sight requirement for laser -based system be some of t he limitations associate with these navigation system . one of the promising solution lie in the art of visual odometry that help in estimate motion information with the help of camera mount over the vehicle . the onboard vision system track visu al landmark to estimate rotation and translation between two -time instant . the art of vision -based navigation be inspire by the behavior of a bird which rely heavily on it vision for guidance and control [ 3 ] . the initial work on estimate motion from a camera by moravec have help in establish the current shashi poddar be with csir - central scientific instruments organisation , chandigarh , india ( email : shashipoddar @ csio.res.in ) rahul kottath be with academy of scientific & innovative research , csir - csio campus , chandigarh india , ( email : rahulkottath @ gmail.com ) visual odometry ( vo ) pipeline [ 4 ] . simultaneous localization and mapping ( slam ) , a superset of vo , localize and build a map of it environment along with the trajectory of a move object [ 5 ] . however , our discussion in this paper be limit to visual odometry , which incrementally estimate the camera pose and refines it use optimization technique . a visual odometry system consist of a specific camera arrangement , the software architecture and the hardware platform to yield camera pose at every time instant . the camera pose estimation can be either appearance or fea ture base . the appearance - base technique operate on intensity value directly and match template of sub -images over two frame or the optical', 'camera pose estimation can be either appearance or fea ture base . the appearance - base technique operate on intensity value directly and match template of sub -images over two frame or the optical flow value to estimate motion [ 6 ] . the feature -based technique e xtract distinct interest point that can be track with the help of vector describe the local region around the key-points . these technique be dependent on the image texture and be generally not applicable in texture -less or low texture environment such a sandy soil , asphalt , etc . [ 7 ] . the vo technique can also be classify a geometric and learn base . the geometric vo technique be the one that explore camera geometry for estimate motion whe reas the learning base vo scheme train regression model to estimate motion parameter when fed with label data [ 8 ] . the learning -based vo technique do not require the camera parameter to be know initia lly and can estimate trajectory with correct scale even for monocular case [ 9 ] . the vo scheme can be implement either with a monocular , stereo , or rgb -d camera depend on the system design . stereo vo mimic the human vision system and can estimate the image scale immediately unlike monocular vo . however , stereo camera system requ ire more calibration effort and stringent camera synchronization without which the error propagate over time . the monocular camera be prefer for inexpensive and small form factor application such a phone , laptop , etc . where the mounting of two camera s with a specified baseline be not always feasible . some of the approach that aim to recover scale information for monocular vo be the usage of imu information [ 10 ] , optimization approach during loop closure [ 11 ] , and incorporate know dimensional information from wall , building , etc . [ 12 ] . an rgb -d camera provide color and depth information for each pixel in an image . the rgb -d vo start with the 3d position of feature point which be then use to obtain transformation through iterative closest point algorithm', 'color and depth information for each pixel in an image . the rgb -d vo start with the 3d position of feature point which be then use to obtain transformation through iterative closest point algorithm [ 13 ] . the vo scheme have find it major application in the automobile indus try in driver assistance and autonomous navigation [ 14 ] . one of the application of visual vinod karar be with csir - central scientific instruments organisation , chandigarh , india ( email : vinodkarar @ csio.res.in ) evolution of visual odometry techniques shashi poddar , rahul kottath , vinod karar odometry have be to estimate vehicle motion from the rear - parking camera and use this information with gps to provide accurate localization [ 15 ] . the task of visual servoing ( moving the camera to a desired orientation ) be very similar to the visual odometry problem require pose estimation for a different purpose [ 16 ] . these scheme be not only useful for navigation of rover on surface of other planet such a mars [ 17 ] but be also useful for track of satellite that need to be repair use a ser vicer [ 18 ] . although these vo technique have show promising result for variety of these application , they be sensitive to environmental change such a light condition , surround texture , the presence of water , snow , etc . some of the other condition that lead to poor tracking data be motion blur , the presence of shadow , visual similarity , degenerate configuration , and occlusion . along with these , some man - make error also creep into the data during image ac quisition and process step such a lens distortion and calibration , feature matching , triangulation , trajectory drift due to dead - reckon which lead to outlier . therefore , the vo scheme need to be robust and have the ability to manage these issue efficiently . in order to handle the environmental condition , different technique have be propose in the literature such a the usage of nir camera for dark environment [ 19 ] or usage of rank transform to handle light condition', 'the environmental condition , different technique have be propose in the literature such a the usage of nir camera for dark environment [ 19 ] or usage of rank transform to handle light condition [ 20 ] . kaess et al . handle data degeneration by divide the image into two cluster base on disparity and compute rotation and translation with distant and nearby object , respectively [ 21 ] . several outlier rejection scheme have be propose in the literature of which ransac and it different variant be very commonly use [ 22 ] . the drift in trajectory over image fra me be compensate use different strategy such a loop closure and bundle adjustment ( ba ) . slam be an extended kalman filter ( ekf ) estimator that aim at obtain accurate motion vector give all the past feature position and their tracking informati on [ 23 ] . unlike slam that reduce drift by loop closure detection while visit same scene location , bundle adjustment ( ba ) optimize camera pose over image frame [ 24 ] . the ba framework minimize the re -projection error over the observed 3d image point and the predicted image point obtain use camera pose , intrinsic camera parameter and distortion parameter [ 25 ] . sliding ba use a fix ed window of previous image frame for optimization and be more popularly use for real-time application . alternately , the fusion of visual odometry with other motion estimation modality such a imu , gps [ 26 ] , abso lute sensor , compass [ 6 ] also exist in literature and be use to improve position accuracy . the rest of this paper be divide into different section . section 2 detail the evolution of visual odometry scheme un der different sub - category , that be , feature -based , appearance -based and learn -based along with some discussion on rgb -d base vo scheme . section 3 provide a list of different datasets specific to visual odometry and their allied area and finally section 4 conclude the paper . ii . evolution of visual odometry visual odometry ( vo ) be define a those set of algorithm that help in estimate', 'to visual odometry and their allied area and finally section 4 conclude the paper . ii . evolution of visual odometry visual odometry ( vo ) be define a those set of algorithm that help in estimate motion by take cue from the image . these sensor can be either monocular , stereo or rgb -d in nature and have different algorithm framework , respectively . vo have a wide range of application vary from game & virtual reality , wearable computing , industrial manufacturing , healthcare , underwater , aerial , space robotics , driver assistance system , agriculture field robot , automobile , pedestrian & indoor navigation , and c ontrol & guidance of unmanned vehicle . in recent year , several vo technique have be publish in the literature and be a non -trivial task to have holistic view over the full breadth of these scheme . however , few judicious attempt have be make by so me of the researcher in review specific aspect of these approach . one of the popular review in the area of motion from image sequence be present by aggarwal and nandhakumar in 1988 by classify them into feature -based and optical flow base [ 27 ] . later , sedouza and kak survey the work carry out in last two decade and classified these technique into map -based , map -building -based , and map -less navigation scheme [ 28 ] . the map -based navigation approach require the robot to be feed with a model of the environment and a sequence of expect landmark whereas the map - building scheme create a representation of outer environment see by the camera . unlike the se , the maple approach do not require any map to be create for navigation and estimate motion by observe external object [ 28 ] . scaramuzza and fraundorfer publish two landmark article on feature base visual odometry pipeline which be very helpful to a newbie in this research area . it segregate the feature base pose estimation framework , into 2d -to-2d , 3d -to-3d , and 3d -to- 2d in concise step , provide detail of their origin and implementation . in 201 1 , weiss et al . classify', 'the feature base pose estimation framework , into 2d -to-2d , 3d -to-3d , and 3d -to- 2d in concise step , provide detail of their origin and implementation . in 201 1 , weiss et al . classify the vo technique base on the camera location , one in which it be place in the environment and the other in which it be place on the uav , respectively [ 29 ] . the former technique be good for accurate and robust motion estimation of a robot move only in a know environment while the late track a known pattern or unknown landmark in the environment [ 30 ] . aqel et al . segregate several topic of i nterest to vo research community such a use of different sensor , vo application , approach and their current limitation [ 1 ] . yousif et al . attempt to provide an overview on structure from motion scheme which inc luded visual odometry and other localization and mapping method [ 31 ] . recently , janai et al . have put up a detailed review on different computer vision methodology use for autonomous driving . it dedicate a sub-section to ego -motion estimation with brief overview of recent article publish in the area of stereo and monocular visual odometry [ 32 ] . several article have be report in the literature that use hybrid of t wo different approach , sense modality etc . for example , scaramuzza & siegwart propose a hybrid of appearance and feature base approach [ 7 ] , a combination of visual camera and lidar sensor [ 33 ] , vision and imu [ 26 ] , vision and compass [ 6 ] , etc . the information provide in this paper be derive from the above mention review work alon g with the other allied literature in the area of visual odometry . however , the discussion here be limit to the evolution of vo approach from it original form to it current scenario . in order to provide brevity , the evolution of vo have be cover und er two broad sub - section , that be , geometric and non -geometric -based approach . these geometric approach be the one that exploit information from the projective geometry and the non - geometric', ""er two broad sub - section , that be , geometric and non -geometric -based approach . these geometric approach be the one that exploit information from the projective geometry and the non - geometric approach a the one that be base on learning . the no n-analytical approach have gain recent popularity with the evolution of machine learning . the geometric approach have be far classify a feature -based , appearance -based and a hybrid of both the feature and appearance base . a. geometric approaches : feature -based the research on visual odometry find it ’ s origin back in 1960 's where a lunar rover be build by stanford university for control it from the earth . this cart be far explore by moravec to demonstrate correspondence base stereo navigation approach in 1980 [ 34 ] . this scheme match distinct feature between stereo image and triangulate them to the 3 -d world frame . once the robot move , these feature point be match in the next frame to obtain corresponding 3d point and generate motion parameter . mattheis and shafer improve upon this technique by model the triangulation error a 3 -d gaussian distribution rather than scalar weight [ 35 ] . arun et al . propose a least square base approach for determine a transformation between 3 -d point cloud [ 36 ] . weng et al . propose a matrix -weighted least square solution which perform remarkably well than unweighted or scalar - weighted solution [ 37 ] . it also propose an iterative optimal scheme to obtain motion parameter and 3 -d point . some of the researcher employ kalman filter to estimate motion parameter by m odeling the noise in image data by gaussian distribution [ 38 ] [ 39 ] . olson et al . solve the motion estimation problem through a maximum -likelihood formulation and mentio ns the use of an absolute orientation sensor to reduce the error growth rate for long distance navigation [ 40 ] . different formulation to solve feature -based vo have be discuss in the literature before 2000 and i s also present in the literature in review article . hence"", ""for long distance navigation [ 40 ] . different formulation to solve feature -based vo have be discuss in the literature before 2000 and i s also present in the literature in review article . hence , not much of it will be discuss here and the main emphasis will be to present the improvement in feature -based vo scheme chronologically post 2000 era . in 2002 , se et al . propose to use scale invariant feature , sift for track interest point across image frame and estimation ego -motion [ 41 ] . improper image calibration , feature mismatch , image noise , and triangulation error be some of the contribute fac tor toward outlier during pose estimation . several outlier rejection scheme for r obust estimation such a ransac , mlesac and their variant have be propose in the literature . some of the researcher aim at estimate motion parameter without havin g a priori knowledge of camera calibration parameter [ 42 ] . however , a mention by nister , know the intrinsic parameter in advance help in obtain more accurate and robust motion estimate especially for planar or near planar scene [ 43 ] . nister also provide the step -by-step motion estimation framework for both the monocular & stereo case and coin the popular term 'visual odometry ' in this paper [ 44 ] . later , engels et al . estimate pose use this five -point algorithm follow by a bundle adjustment base refinement strategy [ 45 ] . se et al . apply hough transform and ransac approa ch for obtain a consistent set of match over which the least square minimization be apply to obtain pose estimation . it have also contribute towards the use of small feature descriptor of size 16 with sufficient discriminative power to match featu re , motivate researcher for more efficient feature descriptor [ 46 ] . tardif et al . estimate rotation use epipolar geometry and translation use 3d map while optimize the current location alone , rather than all previous location a do in bundle adjustment [ 47 ] . extensive work have also be report towards simultaneous localization and"", 'use 3d map while optimize the current location alone , rather than all previous location a do in bundle adjustment [ 47 ] . extensive work have also be report towards simultaneous localization and mapping ( a superset of pose estimation ) during this time and later , but be not disc ussed in this paper [ 48 , 5 ] . until this time , extensive work have be report on different motion estimation sub -routines such a estimate motion from different camera type [ 49 , 50 ] , estimate calibration & essential matrix [ 51 ] , and pose refinement use bundle adjustment [ 24 ] , [ 52 ] which contribute towards accuracy improvement . with this grow confidence on feature -based vo technique and it demonstration on ground vehicle navigation , it be use in navigate mars exploration rover [ 17 ] . it then receive renew interest among the researcher and several improvement be propose in the literature . kaess et al . use flow information for segregate distant and close point and estimate rotation and translation from them separately [ 21 ] . kalantari et al . propose a pose estimation algorithm use three point and the knowledge of vertical direction obtain from imu or the vanishing point [ 53 ] . however , this scheme be unable to provide closed -form solution and have singularity issue . naroditsky et al . present a closed form solution use similar three -plus-one algorithm by use vanish point or gravitational vector a the reference [ 54 ] . later , in 2011 scaramuzza et al . propose the 1-point algorithm for motion estimation by utilize the non - holonomic constraint of wheeled vehicle which switch to the standard 5 -point algorithm on detection of less inliers [ 55 ] . lee et al . extend this work for multi -camera set -up by model it a a generalized camera and propose a 2 -point algorithm for obtain the scale metric [ 56 ] . song et al . propose d a multi -thread monocular visual odometry technique that do not require any assumption regard the environment for scale estimation . epipolar search in all these thread with', '. song et al . propose d a multi -thread monocular visual odometry technique that do not require any assumption regard the environment for scale estimation . epipolar search in all these thread with insertion of persistent 3d point at key -frames help in improve it accu racy and speed [ 57 ] . persson et al . extend this approach by generalize it for the stereo case . the 3d correspondence and the pose estimate by motion model be use to predict track position and in refinement process [ 58 ] . badino et al . improve the positional accuracy of feature point by average it position over all it previous occurrence and use these integrate feature for improve ego-motion accuracy [ 4 ] . kreso & segvic ( 2015 ) pay significance to the camera calibration parameter and corrects them by match feature point from one frame to the other with available ground truth motion [ 59 ] . cvisic & petrovic use a combination of stereo and monocular vo for estimate rotation use five -point algorithm and translation by minimize re -projection error a do for the stereo case . rotation estimate through monocular case help i n overcome error arise due to imperfect calibration while translation estimation through stereo case increase the accuracy [ 60 ] . bellavia et al . propose a key -frame selection strategy base on the existence o f image point with sufficient displacement [ 61 ] . liu et al . propose an improvement over the ransac scheme by generate the hypothesis preferentially and use three best hypothesis to estimate motion [ 62 ] . two different sub -categories of these feature -based vo , that be , usage of different feature descriptor and selection of a feature subset be cover below . 1 ) use of different features feature -based visual odometry involve keypoint detection , description , and a matching process to establish correspond image point which be then use for motion estimation . the traditional edge and corner feature detection strategy such a moravec and harris corner detector be very p opular initially and provide fast', 'image point which be then use for motion estimation . the traditional edge and corner feature detection strategy such a moravec and harris corner detector be very p opular initially and provide fast image correspondence . with the evolution of scale and transformation invariant feature extraction methodology such a sift , surf , orb , brisk , etc . these be more widely use a compare to simple corner detector . diff erent visual odometry research article have use different feature detection -description technique , that be , harris corner detector in [ 63 ] , sift in [ 47 ] , surf in [ 56 ] , censure in [ 64 ] , brief in [ 58 ] , surf – syba in [ 65 ] , and orb in [ 66 ] . given the wide gamut of feature detection technique , it be non-trivial to select a technique that suit one ’ s speed and accuracy requirement . some of the research work evaluate different feature detector and descriptor for visual odome try task and act a a beacon for judicious feature selection base on available hardware resource and design criterion . schmidt et al . compare the performance of different detector - descriptor pair and highlight the speedup achieve by a pairing of sing le scale feature detector with a reduced version of surf descriptor [ 67 ] . jiang et al . far extend this work by experiment on a large set of detector - descriptor pair and datasets [ 68 ] . it be show that brisk detector - descriptor be robust against image change and take less time a compare to sift and surf for visual odometry pipeline . additionally , it propose the use of multiple scale detector only fo r extreme motion while use single scale detector such a corner -based feature to expedite processing while maintain similar accuracy . further , chien et al . compare the performance of sift , surf , orb and a - kaze feature for the task of visual odom etry and find the surf base technique to yield maximum accuracy while the orb feature to be computationally cheap at the cost of low accuracy [ 69 ] . although most of the feature -based vo technique use', 'and find the surf base technique to yield maximum accuracy while the orb feature to be computationally cheap at the cost of low accuracy [ 69 ] . although most of the feature -based vo technique use point -featu re , very less work have be do with line feature owe to it computational complexity . in 2013 , witt and weltin propose an iterative closest multiple line algorithm to use line feature for pose estimation [ 70 ] . however , this scheme could not be apply for image with high texture and need complement with the point -based feature . ojeda and jimenez combine the point and line feature in a probabilistic manner [ 66 ] rather than combine them directly a attempt by koletschka et al . [ 71 ] . the probabilistic combination lead to an efficient solution with reduced effect of noisy measurement and an easy integration in the probabilisti c mobile robotics [ 66 ] . 2 ) features s election not only the selection of appropriate feature detection technique , researcher have devise mechanism by which only a portion of the detected feature be use for improved pose estimation . kitt et al . incorporate a bucketing approach for feature selection wherein the image be divide into grid such that each grid contribute only a specified number of match for further processing [ 63 ] . this approach reduce computational complexity and improve ego -motion accuracy with uniform feature distribution [ 72 ] . cvisic & petrovic classify feature from each bucket into four different class and sele cted strong feature from each class for motion estimation [ 60 ] . maeztu et al . carry out the complete feature detection , description , and match in correspond grid obtain by bucket . it not only help in improve estimate motion by reduce outlier but act a a framework for parallel implementation in multi -core architectures [ 73 ] . kitt et al . extend this technique by classify feature into move and non -moving feature with the help of randomized decision tree follow with the bucketing technique to select feature for motion estimation [', '. extend this technique by classify feature into move and non -moving feature with the help of randomized decision tree follow with the bucketing technique to select feature for motion estimation [ 74 ] . zhou et al . use random fern classifier to segregate matchable from non -matchable point and compute essential matrix only from the matchable one \\\\cite [ 75 ] . the main disadvantage of these classifier base technique be that they require train in advance and thus an online le arning approach be need to adapt in different situation [ 74 ] . escalera et al . propose a stereo vo technique that use static feature belong to the ground surface only , thus reduce the total number of feature s be use for pose estimation [ 76 ] . recently , kottath et al . propose an inertia constrain vo approach which select only those feature that follow the predicted motion model [ 77 ] . it be a simplified implementation of the technique propose by kostavelis on non-iterative outlier removal for stereo vo [ 78 ] and wu et al . wherein smoothness motion constraint be use to reject outlier [ 79 ] . the pose estimate from these feature -based vo scheme be generally pass through a filtering or an optimization framework for improved motion estimate . feature -based ekf -slam have be a popular technique fo r localization and mapping in the computer vision community which use sparse interest point [ 48 ] . a dedicated set of researcher have also employ kalman filter for motion estimation , account for noise r obustness . webb et al . employ tracked image feature point along with their epipolar constraint a measurement model for estimate state in the ekf framework . not much have be far detail on these filtering or optimization base technique and th e discussion will remain limited to pose estimation work alone . the following sub -section give a brief of appearance -based vo scheme that have evolve in parallel to the feature -based ego -motion estimation scheme . b . appearance base visual odometry appeara nce-based visual odometry be', 'give a brief of appearance -based vo scheme that have evolve in parallel to the feature -based ego -motion estimation scheme . b . appearance base visual odometry appeara nce-based visual odometry be another class of geometric approach that do not rely on sparse feature and estimate motion by optimize the photometric error . generally , the feature -based technique be say to be noisy and the feature need not necess arily be distinguishable from their surroundings in smoothly vary landscape such a foggy environment or a sandy area [ 80 ] . the feature -matching step at time lead to wrong association which need to be remove d and can even be expensive if implement use neural network . instead , these appearance -based technique ( also refer a direct method ) utilize information from the complete image , lead to robust ego -motion estimate even in low -textured environme nt [ 81 ] . using whole image rather than few landmark reduces aliasing issue associate with similar look place , work even with smooth vary landscape and be fast to implement [ 80 ] . the appearance - base technique be generally of two type : region -based matching and optical flow base . the region -based match either can be achieve through correlation ( template matching ) or with the help of global appearance base descriptor and image alignment approach . correlation base technique ( also refer a template matching ) for align image have be a widely research area in the past use global invariant image representation or similarity measure . these scheme have several limitation which be overcome with the use of locally invariant similarity measure and global constraint [ 82 ] . the image alignment technique propose by irani and anandan be able to estimate parametric 2d motion model for image acquire by sensor from different modality [ 82 ] . mandelbaum et al . extend this scheme for estimate 3d ego-motion , which be iteratively refine over the multi - resolution framework . it estimate pose for a batch of image that be bootstrapped with a priori', 'et al . extend this scheme for estimate 3d ego-motion , which be iteratively refine over the multi - resolution framework . it estimate pose for a batch of image that be bootstrapped with a priori motion estimate for speed up estimation proce s . the a priori information can be either obtain from the previous batch or an external sensor or through a kalman filter prediction [ 83 ] . vatani et al . propose a simple and practical approach to ego -motion estimation use constrain correlation base approach . some of the modification carry out over simple correlation approach be correlation mask size base on image height , mask location a per vehicle motion and feeding of a small predictor area in which the mask be match [ 84 ] . yu et al . extend this work by use a rotated template that help in estimate both the translation and rotation between two frame [ 85 ] . vatani et al . select an appropriate template image from multiple - template and use linear forward prediction filter to select window location for faster and accurate matching process [ 86 ] . frederic lab rosse propose a visual compass technique base on template -matching to estimate pixel displacement in the image capture by an omni -directional camera look at the environment [ 80 ] . scaramuzza incorporate the v isual compass scheme to estimate rotation a this scheme be robust to systematic error from camera calibration and error accumulation due to integration over time [ 87 ] . gonzalez et al . incorporate labr osse ’ s visual compass technique for rotation estimation along with traditional template match approach to estimate translation use two different camera point at the environment and the ground , respectively [ 6 ] . recently , aqel et al . propose an adaptive template -matching scheme with reduced mask size and change template position base on vehicle acceleration [ 88 ] . several recent work be report towards robust t emplate match technique for other application and can be extend for visual odometry problem as well . some of the effort be', 'acceleration [ 88 ] . several recent work be report towards robust t emplate match technique for other application and can be extend for visual odometry problem as well . some of the effort be also make towards the usage of global image appearance for registring image that can then be use for estimate ego -motion . goecke et al . make use of fourier -mellin transformation [ 89 ] while menegatti et al . use phase information from image ’ s fourier signature for estimate vehicle motion from these global descriptor [ 90 ] . the use of image registration technique for motion analysis find it mention in the article publish by lucas and kanade in the early 80 [ 91 ] . a set of technique use pre -stored image sequence for comparison with the current image and yield an indoor navigation estimate [ 92 ] , [ 93 ] . zhou et al . use histogram to describe the appearance of pre -stored im age frame a template which be then compare with the histogram of current image for recognize vehicle ’ s current location [ 94 ] . however , these scheme be not able to detect the orientation accurately which be de picted in the experiment carry out by pajda & hlavac [ 95 ] . jogan and leonardis correlate image use a combination of zero phase representation ( zpr ) and eigen -space of oriented image to yield rotation invariance but be sensitive to noise and occlusion [ 96 ] . comport et al . use reference stereo image pair to yield dense correspondence for estimate 6dof pose . this scheme be base on the quadrifocal relationship between image intensity and be robust to occlusion , inter -frame displacement , and illumination change [ 97 ] . comport et al . later extend his own work by design a scheme that minimize the intensity error between t he entire image while overcome the inter -frame overlap issue associate with region -based approach [ 98 ] . lovegrove et al . propose an image alignment approach to estimate vehicle motion by take the advantage of texture present on planar road surface [ 15 ] . some of the other region base match', '[ 98 ] . lovegrove et al . propose an image alignment approach to estimate vehicle motion by take the advantage of texture present on planar road surface [ 15 ] . some of the other region base match scheme use motion paralla x to compute 3d translation and parametric transformation between two frame [ 99 ] , [ 100 ] . irani et al . mention the advantage of decompose the camera motion into parametri c motion and parallax displacement rather than into translation and rotation . this plane -plus-parallax scheme be say to be more robust , stable and simple than the optical flow base approach a it require solve small set of linear equation [ 101 ] . these region -based scheme require a specific interest area to be define in the image that need to be match through sufficient overlap with the other image . further , the image registration process require a n optimization technique to minimize an objective function , which be generally subject to local minimum and divergence issue [ 98 ] . an inappropriate choice of argument minimization criterion and existence of independ ently move object be some of the major concern that can be avoid with optical flow base vo scheme [ 102 ] . optical flow be one of the fundamental principle that define ego -motion of an observer see in an image a per gibson ’ s ecological optic [ 103 ] . the use of optical flow for estimate motion information be inspire by the biological cue use by insect for navigation purpose [ 104 ] . some early attempt toward estimate ego -motion from optical flow be take up by clocksin ( 1978 ) [ 105 ] , ullman ( 1979 ) [ 106 ] , and prazdny ( 1980 ) [ 107 ] . however , most of the technique consider the scene to contain a single object or restrict the motion to be translatory with an assumption of only planar surface in the environment . the basic formulati on of optical flow propose by horn and schunck [ 108 ] get violate in the presence of motion discontinuity and vary illumination [ 109 ] . gilad adiv solve the motion disc ontinuity issue by compute motion', 'optical flow propose by horn and schunck [ 108 ] get violate in the presence of motion discontinuity and vary illumination [ 109 ] . gilad adiv solve the motion disc ontinuity issue by compute motion for each of the connected partition of flow vector . these segment be later group together to formulate a motion hypothesis that be compatible with all the segment in a group [ 102 ] . black and anandan propose to use statistical framework that help in estimate the motion of the majority of the pixel while eliminate outlier [ 110 ] . several work have be report towards estimat ing illumination by mode lling it with a multiplicative/ additive factor or in a recursive framework . kim et al . address both the illumination and motion discontinuity issue by integrate black and anandan ’ s approach for handle motion discontinuity [ 110 ] with that of gennert and negahdaripour ’ s illumination variation model [ 111 ] specifically design for motion estimation task [ 109 ] . these optical flow base motion estimation method also refer to a direct method , use complete image information that can be apply to recover global 2d or 3d motion model [ 112 ] . giachetti et al . propose a correl ation base dense optical flow technique for estimate ego -motion of a car move in usual street with the help of a tv camera mount parallel to the ground [ 113 ] . the correlation base optical flow with large mask help in overcome instability associate with compute derivative and temporal filtering help in reduce the disturbance due to shock and vibration by reject horizontal component of optical flow . however , this scheme be not reliable in the presence of independently move object , and movement through hilly , dense vegetation , and clutter area . k. j. hanna describe an iterative approach to estimate camera motion directly through brightness derivative while use ego - motion and brightness constraint for refinement [ 114 ] . corke et al . mention the significance of use omnidirectional camera for estimate motion a it can retain feature', 'derivative while use ego - motion and brightness constraint for refinement [ 114 ] . corke et al . mention the significance of use omnidirectional camera for estimate motion a it can retain feature for long dur ation and incorporate more information [ 104 ] . hyslop and humbert use the wide -field motion information from optical flow for estimate 6 -dof motion parameter and provide reliable information for navigation i n an unknown environment [ 115 ] . in 2005 , campbell et al . propose an optical flow base ego-motion estimation technique wherein the rotation be estimate use feature that be far from the camera and translatio n use near -by feature [ 116 ] . some of the research work aim at estimate motion in a controlled environment that can not be generalize for outdoor condition . for example , the technique propose by srinivasan obtain ego - motion for the set -up require camera to track change in ceiling light pattern have limit scope [ 117 ] . grabe et al . ( 2012a ) demonstrate an optical flow base close loop control uav operat ion use onboard hardware alone . this scheme aim at continuous motion recovery rather than estimate frame -to-frame motion alone [ 118 ] . grabe et al . ( 2012b ) far extend their work by employ feature that bel ong to a dominant plane alone to obtain improved velocity estimate [ 119 ] . tykkala and comport present a direct stereo base slam method wherein the motion be estimate by direct image alignment [ 120 ] . the lsd -slam estimate depth at pixel with large intensity gradient and also estimate rigid body motion by align image base on the depth map . however , this scheme use cue from both the stereo and monocular set -up and ha ndles brightness change in the image frame to yield good estimate [ 121 ] . recently , engel et al . propose a direct sparse odometry scheme , which optimize the photometric error in a framework similar to sparse bundl e adjustment . it avoid the use of geometric prior use in feature -based approach and use all image point to achieve robustness [ 81 ] .', 'the photometric error in a framework similar to sparse bundl e adjustment . it avoid the use of geometric prior use in feature -based approach and use all image point to achieve robustness [ 81 ] . several work have also be report in the literature recently that estimate ego-motion use a combination of feature and optical flow [ 122 ] . optical flow have not only be use for estimate motion but also to help uavs navigate by provide cue relate to the presence of an obstacle in the vehicle path [ 123 ] . however , optical flow base scheme have their own limitation such a match in texture -less surface ( concrete , sand , etc . ) and computational complexity . the rgb -d camera be one of the framework that have low computational complexity a it provide depth value for image point directly through a depth sensor embed in the color camera . one set of technique formulate the vo task a energy minimization problem [ 124 ] while the other one estimate trajectory by classical registration technique [ 125 ] . the photometric error formulation of direct method be combine with the error in dense map obtai ned from rgb -d sensor to formulate a cost function which can be solve use numerical optimization algorithm [ 126 ] . the registration base scheme can achieve alignment base on shape , feature , or surface normal projection [ 127 ] . dryanovski et al . propose a scheme for align 3d point against a global model use iterative close point algorithm [ 13 ] . li and lee propose a fast visual odometry scheme by select few salient point on the source frame for icp and integrate intensity value in the correspondence estimation for improved icp [ 127 ] . a brief review of related work can be see in the article publish by kerl et al . which estimate motion by register the two rgb -d image directly on the basis of photometric error [ 128 ] . whelan et al . propose a robust rgb -d base visual odometry scheme which help in colored volumetric reconstruction of different scene and be one of the late work in this area [ 129 ] . c.', '[ 128 ] . whelan et al . propose a robust rgb -d base visual odometry scheme which help in colored volumetric reconstruction of different scene and be one of the late work in this area [ 129 ] . c. geometric ap proaches : hybrid of feature and appearance base the hybrid algorithm for visual odometry take advantage of both the direct ( feature -based ) and indirect method ( appearance -based ) . feature -based scheme provide reliable data at the cost of certain loss o f available information while appearance -based method provide dense reconstruction exploit total available data but have error associate with few area . oliensis and werman propose an algorithm that combine direct and indirect scheme in one framewo rk with the main aim to incorporate all the available data and improve motion estimate [ 130 ] . morency and gupta propose a hybrid registration scheme that incorporate feature track information and optical flow constraint in one framework [ 131 ] . scaramuzza et al . use appearance - base approach to estimate rotation while translation be estimate by feature extract from the ground plane [ 132 ] . forster et al . propose a semi -direct vo technique that obtain feature correspondence from direct motion estimation which be then incorporate a matched point for feature -based pose estimation [ 133 ] . the direct motion estimation scheme here use sparse model -based image alignment to obtain feature correspondence and be term semi -direct owe to it fusion with feature base vo approach . silva et al . propose a dense ego-motion estimation tech nique complement with the feature base vo to obtain translation scale factor accurately and later refine with the kalman filter [ 134 ] . silva et al . extend it further by employ probabilistic corresponden ce for fully dense probabilistic stereo ego -motion and have mention it to be robust against difficult image scenario [ 135 ] . d. non-geometric approaches with the evolution of good computing resource , machine learning technique be be use for several real -time', 'it to be robust against difficult image scenario [ 135 ] . d. non-geometric approaches with the evolution of good computing resource , machine learning technique be be use for several real -time application . alvinn be one the initial attempt towards the usage of machine learn technique to improve the performance of navlab , the carnegie mellon autonomous navigation test vehicle use a three layer neural network in 1989 [ 136 ] . it be far improve by speed it by 5 -times use “ on the fly ” training approach in which the system imita tes the human driver under actual driving condition [ 137 ] . learning can be use in any stage of the odometry pipeline such a feature learn for good pose estimation [ 138 ] , estimate feature correspondence [ 139 ] , homography estimation [ 140 ] , etc . visual odometry base on machine learning be one of the emerge technique for motion estimation , a it do not require the camera calibration parameter to be know explicitly . the labelled data be use to train a regression/ classification model that c an estimate the ego -motion once an input image sequence be provide . these non -geometric learn -based approach can estimate translation to the correct scale and be robust against similar kind of noise with which it be train . this switch from geo metry -based to learn -based approach be one of the recent paradigm shift in the area of visual navigation . one of the initial work towards learn -based vo by roberts et al . aim at learn the platform velocity and turn rate from optical flow [ 141 ] . guizilini and ramos eliminate the use of geometric model by learn the effect of camera motion on image structure and vehicle dynamic . it use a coupled gaussian process for supervised learning of ego -motion fr om optical flow information [ 9 ] , [ 142 ] . they far extend their work for estimate linear and angular velocity by use optical flow information from a single camera along with multiple -output gaussian process framework ( mogp ) [ 143 ] . konda and memisevic make use of convolutional', 'and angular velocity by use optical flow information from a single camera along with multiple -output gaussian process framework ( mogp ) [ 143 ] . konda and memisevic make use of convolutional neural network ( cnn ) base synchrony auto encoder for joint estimation of depth and motion parameter s from single / multiple camera [ 144 ] . this work be far extend for visual odometry application by estimate local change in velocity and direction through the cnn architecture [ 138 ] . mohanty et al . use deep cnn to extract high level feature for estimate transformation between two time instant [ 145 ] . xu et al . use large scale crowd dataset to predict vehicle ego -motio n from it previous state estimate and instantaneous camera observation [ 146 ] . an improved cnn , that be , recurrent cnn , be use for achieve end -to-end pose estimation by learn geometrical feature in a sequential man ner [ 147 ] . the cnn structure have also be use in estimate scale for monocular visual odometry with the help of street mask use for ground plane estimation [ 148 ] . peretr oukhin et al . incorporate a cnn variant , that be , bayesian cnn to track sun direction and incorporate it into the vo pipeline for improved ego -motion estimate [ 149 ] . recently , zhan et al . propose a learni ng scheme for vo framework which use train data of single -view depth information along with two - view odometry data [ 150 ] . different model estimation scheme such a support vector machine , gaussian process , fuzzy logic , etc . be also report in the literature and hold great potential to be extend in the future [ 151 ] [ 152 ] . with this brief analysis , it can be conclude that the machine learning base vo technique hold huge potential for further improvement in estimate accurate motion estimate . the above sub -sections provide a brief overview of the evolution of visual odometry scheme vary from geometric to non -geometri c approach . along with these , vo scheme have also be develop for infrared camera but this paper do not provide much', 'of the evolution of visual odometry scheme vary from geometric to non -geometri c approach . along with these , vo scheme have also be develop for infrared camera but this paper do not provide much detail on them [ 153 ] , [ 154 ] . with progress in different subroutine of vo scheme , good and faster vo scheme be bind to evolve . some of the recent work have show the new direction in the motion estimation task such a pose estimation use event -based camera [ 155 ] , direct sparse odometry [ 81 ] , large scale direct slam [ 121 ] , robust real time vo use dense rgb -d camera [ 129 ] , etc . this e volution of vo scheme will still continue and this overview be a very small attempt to present different dimension in which the vo scheme currently deal . iii . visual odometry datasets with grow research in robotics and computer vision algorithm , it beca me very important to generate benchmarking datasets with ground -truth value that help in compare one algorithm over the other . in this attempt , several datasets have be put up by the researcher publicly for compare ego -motion estimation and it all ied technique . among the visual odometry stereo datasets , malaga [ 156 ] , [ 157 ] and new college [ 158 ] datasets be some of the early dataset for mobile robot localization . the karlsruhe dataset [ 159 ] could not be very popular a it have some acquisition issue . the kitti vision benchmark [ 160 ] suite be one of the most popular datasets in the computer vision research , especially for visual odometry task and be use by several researcher to compare their pose estimation scheme . some of the datasets aim to provide additional sensor data for be tter comparison with ground truth and target a relatively large research community that work on vision - aided navigation . the wean hall dataset [ 161 ] , kagaru airborne stereo dataset [ 162 ] , euroc mav dataset [ 163 ] , oxford robotcar dataset [ 164 ] be some of the datasets that provide stereo camera frame a along with the information from lidar , im u , and gps . of', 'dataset [ 162 ] , euroc mav dataset [ 163 ] , oxford robotcar dataset [ 164 ] be some of the datasets that provide stereo camera frame a along with the information from lidar , im u , and gps . of these , oxford robotcar dataset be suit to deep learning -based scheme that require huge datasets for training and estimate motion through image directly . the tum - monocular visual odometry dataset [ 165 ] , [ 166 ] and lsd -slam [ 167 ] be dedicate to the development of pose estimation and localization through monocular camera . one of the recent datasets aim at h igh- speed robotics be provide in zurich – event -camera dataset [ 155 ] for design new class of pose estimation algorithm with very high frame rate event -based camera . ford campus vision dataset [ 168 ] , eth - vision \\\\ & laser datasets from a heterogeneous uav fleet [ 169 ] , zurich urban micro aerial vehicle dataset [ 170 ] , and tum visual -inerti al dataset [ 171 ] be some of the datasets design specifically for slam , collaborative 3d reconstruction , appearance base localization , and visual odometry base application , respectively . rgb -d base motion estima tion be one of the other research area that be gain importance and thus dedicate benchmark datasets have also be put up for the same . the tum -rgb -d slam dataset [ 172 ] publish in 2012 be one of the early attempt towards provide rgb -d data for evaluation of visual odometry and visual slam scheme . the mit sata center dataset [ 173 ] and icl - nuim rgb -d dataset [ 174 ] map the i ndoor environment with a rgb -d camera and focus mainly on floor planning and surface reconstruction . very recently , the eth – rgb -d dataset [ 175 ] have also be publish which use laser scanner to generate ground truth information for structure from motion kind of application . some of the ego -motion estimation scheme have also be develop and test over synthetic datasets of which the new tsukuba dataset [ 176 ] be very famous and use by several researcher . these datasets be generate entirely on computer use', ""have also be develop and test over synthetic datasets of which the new tsukuba dataset [ 176 ] be very famous and use by several researcher . these datasets be generate entirely on computer use different photo edit software . the multi -fov synthetic dataset [ 177 ] be one of the late attempt towards synthetic dataset that simulate fly robot hover in a room and vehicle move in a city . iv . conclusion in this work , an attempt be make to provide a holistic picture of the visual odometry technique encompass different branch of this tree . the paper start with an introduction to the motion estimation scheme and their wide application in different engineering field . the vo scheme have be discuss under two broad category , that be , geometric and non -geometric approach . the gamut of geometric approach be very wide and be thus sub -divided into three different sub -class , that be , feature -based , appearance - base , and hybrid scheme . towards the end , a list of different vo datasets be provide for ready reference which have be segregate into dif ferent class depend on the sensing modality . on the basis of recent research article , it be see that a huge impetus be give towards machine learn base vo , rgb -d base vo and other hybrid scheme that take the advantage of both direct and indi rect/ sparse and dense approach in one coherent framework . it be also pertinent to mention here that this work be not an exhaustive survey of visual odometry research article a it be a grow research area and a huge amount of related work have happene d in the past . acknowledgment this research have be support by drdo - aeronautical research & development board through grant -in-aid project on ” design and development of visual odometry system . references [ 1 ] m. o . a. aqel , m. h. marhaban , m. i. sar ipan and n. b. ismail , '' review of visual odometry : type , approach , challenge , and application , '' springerplus , vol . 5 , p. 1897 , 2016 . [ 2 ] r. madison , g. andrews , p. debitetto , s. rasmussen and m. bottkol ,"", ""review of visual odometry : type , approach , challenge , and application , '' springerplus , vol . 5 , p. 1897 , 2016 . [ 2 ] r. madison , g. andrews , p. debitetto , s. rasmussen and m. bottkol , `` vision -aided navigation for small uavs in gps - challenge environment , '' in aiaa infotech @ aerospace 2007 conference and exhibit , 2007 . [ 3 ] s. m. ettinger , `` design and implementation of autonomous vision - guide micro air vehicle , '' 2001 . [ 4 ] h. badino , a. yamamoto and t. kanade , `` visual odometry by multi -frame feature integration , '' in computer vision workshops ( iccvw ) , 2013 ieee international conference on , 2013 . [ 5 ] h. durrant -whyte and t. bailey , `` simultaneous localization and mapping : part i , '' ieee robotics \\\\ & automation magazine , vol . 13 , pp . 99 -110 , 2006 . [ 6 ] r. gonzalez , f. rodriguez , j. l. guzman , c. pradalier and r. siegwart , `` combined visual odometry and vi sual compass for off - road mobile robot localization , '' robotica , vol . 30 , pp . 865 -878 , 2012 . [ 7 ] d. scaramuzza and r. siegwart , `` appearance -guided monocular omnidirectional visual odometry for outdoor ground vehicle , '' ieee transaction on robotics , vol . 24 , pp . 1015 -1026 , 2008 . [ 8 ] t. a. ciarfuglia , g. costante , p. valigi and e. ricci , `` evaluation of non-geometric method for visual odometry , '' robotics and autonomous systems , vol . 62 , pp . 1717 -1730 , 2014 . [ 9 ] v. guizilini and f. ramos , `` visual odometry learn for unmanned aerial vehicle , '' in robotics and automation ( icra ) , 2011 ieee international conference on , 2011 . [ 10 ] g. n { \\\\ '' u } tzi , s. weiss , d. scaramuzza and r. siegwart , `` fusion of imu and vision for absolute scale estimation in monocular slam , '' journal of intelligent \\\\ & robotic system , vol . 61 , pp . 287 -299 , 2011 . [ 11 ] h. strasdat , j. m. m. montiel and a. j. davison , `` scale drift -aware large scale monocular slam , '' robotics : science and systems vi , vol . 2 , 2010 . [ 12 ] s. hilsenbeck , a. m { \\\\ '' o }"", ""] h. strasdat , j. m. m. montiel and a. j. davison , `` scale drift -aware large scale monocular slam , '' robotics : science and systems vi , vol . 2 , 2010 . [ 12 ] s. hilsenbeck , a. m { \\\\ '' o } ller , r. huitl , g. schroth , m. kranz and e. steinbach , `` scale -preserving long -term visual odometry for indoor navigation , '' in indoor positioning and indoor navigation ( ipin ) , 2012 international conference on , 2012 . [ 13 ] i. dryanovski , r. g. valenti and j. xiao , `` fast visual odometry and mapping from rgb -d data , '' in robotics and automation ( icra ) , 2013 ieee international conference on , 2013 . [ 14 ] m. bertozzi , a. broggi and a. fascioli , `` vision -based intelligent vehicle : state of the art and perspective , '' robotics and autonomous system , vol . 32 , pp . 1 -16 , 2000 . [ 15 ] s. lovegrove , a. j. davison and j. ibanez -guzm { \\\\'a } n , `` accurate visual odometr y from a rear parking camera , '' in intelligent vehicles symposium ( iv ) , 2011 ieee , 2011 . [ 16 ] a. i. comport , e. marchand , m. pressigout and f. chaumette , `` real - time markerless track for augmented reality : the virtual visual servoing framework , '' ieee transactions on visualization and computer graphic , vol . 12 , pp . 615 -628 , 2006 . [ 17 ] m. maimone , y. cheng and l. matthies , `` two year of visual odometry on the mar exploration rover , '' journal of field robotics , vol . 24 , pp . 169 -186 , 2007 . [ 18 ] n. w. oumer and g. panin , `` 3d point tracking and pose estimation of a space object use stereo image , '' in pattern recognition ( icpr ) , 2012 21st international conference on , 2012 . [ 19 ] j. ruppelt and g. f. trommer , `` stereo -camera visual odometry for outdoor area and in dark indoor environment , '' ieee aerospace and electronic systems magazine , vol . 31 , pp . 4 -12 , 2016 . [ 20 ] c. golban , s. istvan and s. nedevschi , `` stereo base visual odometry in difficult traffic scene , '' in intelligent vehicles symp osium ( iv ) , 2012 ieee , 2012 . [ 21 ] m. kaess , k. ni and f."", ""] c. golban , s. istvan and s. nedevschi , `` stereo base visual odometry in difficult traffic scene , '' in intelligent vehicles symp osium ( iv ) , 2012 ieee , 2012 . [ 21 ] m. kaess , k. ni and f. dellaert , `` flow separation for fast and robust stereo odometry , '' in robotics and automation , 2009 . icra'09 . ieee international conference on , 2009 . [ 22 ] r. raguram , o . chum , m. pollefeys , j. matas and j . -m. frahm , '' usac : a universal framework for random sample consensus , '' ieee transaction on pattern analysis and machine intelligence , vol . 35 , pp . 2022 -2038 , 2013 . [ 23 ] a. j. davison , `` real -time simultan eous localisation and mapping with a single camera , '' in null , 2003 . [ 24 ] b. triggs , p. f. mclauchlan , r. i. hartley and a. w. fitzgibbon , '' bundle adjustment —a modern synthesis , '' in international workshop on vision algorithm , 1999 . [ 25 ] m. i . a. lour akis and a . a. argyros , `` sba : a software package for generic sparse bundle adjustment , '' acm transactions on mathematical software ( toms ) , vol . 36 , p. 2 , 2009 . [ 26 ] m. agrawal and k. konolige , `` real -time localization in outdoor environment use stereo vision and inexpensive gps , '' in pattern recognition , 2006 . icpr 2006 . 18th international conference on , 2006 . [ 27 ] j. k. aggarwal and n. nandhakumar , `` on the computation of motion from sequence of image -a review , '' proceedings of the ieee , vol . 76 , pp . 917 -935 , 1988 . [ 28 ] g. n. desouza and a. c. kak , `` vision for mobile robot navigation : a survey , '' ieee transaction on pattern analysis and machine intelligence , vol . 24 , pp . 237 -267 , 2002 . [ 29 ] s. weiss , d. scaramuzza and r. siegwart , `` monocular -slam -- base navigation for autonomous micro helicopter in gps -denied environment , '' journal of field robotics , vol . 28 , pp . 854 -874 , 2011 . [ 30 ] d. eynard , p. vasseur , c. demonceaux and v. fr { \\\\'e } mont , `` real time uav altitude , attitude and motion estimation from hybrid stereovision , ''"", "", vol . 28 , pp . 854 -874 , 2011 . [ 30 ] d. eynard , p. vasseur , c. demonceaux and v. fr { \\\\'e } mont , `` real time uav altitude , attitude and motion estimation from hybrid stereovision , '' autonomous robots , vol . 33 , pp . 157 -172 , 2012 . [ 31 ] k. yousif , a. bab -hadiashar and r. hoseinnezhad , `` an overview to visual odometry and visual slam : applications to mob ile robotics , '' intelligent industrial systems , vol . 1 , pp . 289 -311 , 2015 . [ 32 ] j. janai , f. g { \\\\ '' u } ney , a. behl and a. geiger , `` computer vision for autonomous vehicle : problems , datasets and state -of-the-art , '' arxiv preprint arxiv:1704.05519 , 2017 . [ 33 ] j. zhang and s. singh , `` visual -lidar odometry and mapping : low - drift , robust , and fast , '' in robotics and automation ( icra ) , 2015 ieee international conference on , 2015 . [ 34 ] h. p. moravec , `` obstacle avoidance and navigation in the real world by a see robot rover. , '' 1980 . [ 35 ] l. matthies and s. t. e. v. e. n. a. shafer , `` error modeling in stereo navigation , '' ieee journal on robotics and automation , vol . 3 , pp . 239-248 , 1987 . [ 36 ] k. s. arun , t. s. huang and s. d. blostein , `` least -squares fi tting of two 3 -d point set , '' ieee transactions on pattern analysis and machine intelligence , pp . 698 -700 , 1987 . [ 37 ] j. weng , p. cohen and n. rebibo , `` motion and structure estimation from stereo image sequence , '' ieee transactions on robotics and autom ation , vol . 8 , pp . 362 -382 , 1992 . [ 38 ] t. j. broida and r. chellappa , `` estimation of object motion parameter from noisy image , '' ieee transaction on pattern analysis and machine intelligence , pp . 90 -99 , 1986 . [ 39 ] j. hallam , `` resolving observer mot ion by object tracking , '' in proceedings of the eighth international joint conference on artificial intelligence -volume 2 , 1983 . [ 40 ] c. f. olson , l. h. matthies , m. schoppers and m. w. maimone , '' stereo ego -motion improvement for robust rover navigation , '' in robotics and"", ""on artificial intelligence -volume 2 , 1983 . [ 40 ] c. f. olson , l. h. matthies , m. schoppers and m. w. maimone , '' stereo ego -motion improvement for robust rover navigation , '' in robotics and automation , 2001 . proceedings 2001 icra . ieee international conference on , 2001 . [ 41 ] s. se , d. lowe and j . little , `` mobile robot localization and mapping with uncertainty use scale -invariant visual landmark , '' the international journal of robotics research , vol . 21 , pp . 735 -758 , 2002 . [ 42 ] r. i. hartley , `` estimation of relative camera position for uncalibrated camera , '' in european conference on computer vision , 1992 . [ 43 ] d. nist { \\\\'e } r , `` an efficient solution to the five -point relative pose problem , '' ieee transaction on pattern analysis and machine intelligence , vol . 26 , pp . 756 -770 , 2004 . [ 44 ] d. nist { \\\\'e } r , o. naroditsky and j. bergen , `` visual odometry , '' in computer vision and pattern recognition , 2004 . cvpr 2004 . proceedings of the 2004 ieee computer society conference on , 2004 . [ 45 ] c. engels , h. stew { \\\\'e } nius and d. nist { \\\\'e } r , `` bundle adjustment rule , '' photogrammetric computer vision , vol . 2 , 2006 . [ 46 ] s. se , d. g. lowe and j. j . little , `` vision -based global localization and mapping for mobile robot , '' ieee transactions on robotics , vol . 21 , pp . 364 -375 , 2005 . [ 47 ] j.-p. tardif , y. pavlidis and k. daniilidis , `` monocular visual odometry in urban environment use an omnidirectional camera , '' in intelligent robots and systems , 2008 . iros 2008 . ieee/rsj international conference on , 2008 . [ 48 ] a. j. davison and d. w. murray , `` simultaneous localization and map-building use active vision , '' ieee transaction on pattern analysis and machine intelligence , vol . 24 , pp . 865 -880 , 2002 . [ 49 ] r. hartley and a. zisserman , multiple view geometry in computer vision , cambridge university press , 2003 . [ 50 ] p. chang and m. hebert , `` omni -directional structure from motion , '' in"", ""[ 49 ] r. hartley and a. zisserman , multiple view geometry in computer vision , cambridge university press , 2003 . [ 50 ] p. chang and m. hebert , `` omni -directional structure from motion , '' in omnidirectional vision , 2000 . proceedings . ieee workshop on , 2000 . [ 51 ] b. mi { \\\\v { c } } u { \\\\v { s } } { \\\\'\\\\i } k and t. pajdla , `` omnidirectional camera model and epi polar geometry estimation by ransac with bucketing ? , '' in scandinavian conference on image analysis , 2003 . [ 52 ] m. lhuillier , `` automatic structure and motion use a catadioptric camera , '' in proceedings of the 6th workshop on omnidirectional vision , camera networks and non -classical cameras , 2005 . [ 53 ] m. kalantari , a. hashemi , f. jung and j . -p. gu { \\\\'e } don , `` a new solution to the relative orientation problem use only 3 point and the vertical direction , '' journal of mathematical imaging and vision , vol . 39 , pp . 259 -268 , 2011 . [ 54 ] o. naroditsky , x. s. zhou , j. gallier , s. i. roumeliotis and k. daniilidis , `` two efficient solution for visual odometry use directional correspondence , '' ieee transaction on pattern analysis and machine intelligence , vol . 34 , pp . 818 -824 , 2012 . [ 55 ] d. scaramuzza , `` 1 -point -ransac structure from motion for vehicle - mount camera by exploit non -holonomic constraint , '' international journal of computer vision , vol . 95 , pp . 74 -85 , 2011 . [ 56 ] g. h. lee , f. faund orfer and m. pollefeys , `` motion estimation for self-driving car with a generalized camera , '' in computer vision and pattern recognition ( cvpr ) , 2013 ieee conference on , 2013 . [ 57 ] s. song , m. chandraker and c. c. guest , `` parallel , real -time monocular vi sual odometry , '' in robotics and automation ( icra ) , 2013 ieee international conference on , 2013 . [ 58 ] m. persson , t. piccini , m. felsberg and r. mester , `` robust stereo visual odometry from monocular technique , '' in intelligent vehicles symposium ( iv ) , 20 15 ieee , 2015 . [ 59 ] i. kre { \\\\v { s } } o and"", ""t. piccini , m. felsberg and r. mester , `` robust stereo visual odometry from monocular technique , '' in intelligent vehicles symposium ( iv ) , 20 15 ieee , 2015 . [ 59 ] i. kre { \\\\v { s } } o and s. { \\\\v { s } } egvic , `` improving the egomotion estimation by correct the calibration bias , '' in 10th international conference on computer vision theory and applications , 2015 . [ 60 ] i. cvi { \\\\v { s } } i { \\\\'c } and i. petrovi { \\\\'c } , `` stereo odometry base on careful feature selection and tracking , '' in mobile robots ( ecmr ) , 2015 european conference on , 2015 . [ 61 ] f. bellavia , m. fanfani and c. colombo , `` selective visual odometry for accurate auv localization , '' autonomous robots , vol . 41 , pp . 133-143 , 2017 . [ 62 ] y. liu , y. gu , j. li and x. zhang , `` robust stereo visual odometry using improved ransac -based methods for mobile robot localization , '' sensors , vol . 17 , p. 2339 , 2017 . [ 63 ] b. kitt , a. geiger and h. lategahn , `` visual odometry base on stereo image sequence with ransac -based outlier rejection scheme , '' in intelligent vehicles symposium ( iv ) , 2010 ieee , 2010 . [ 64 ] k. konolige , m. agrawal and j. sola , `` large -scale visual odom etry for rough terrain , '' in robotics research , springer , 2010 , pp . 201 - 212 . [ 65 ] a. desai and d. -j. lee , `` visual odometry drift reduction use syba descriptor and feature transformation , '' ieee transactions on intelligent transportation systems , vol . 17 , pp . 1839 -1851 , 2016 . [ 66 ] r. gomez -ojeda and j. gonzalez -jimenez , `` robust stereo visual odometry through a probabilistic combination of point and line segment , '' in robotics and automation ( icra ) , 2016 ieee international conference on , 2016 . [ 67 ] a. schmidt , m. kraft and a. kasi { \\\\'n } ski , `` an evaluation of image feature detector and descriptor for robot navigation , '' in international conference on computer vision and graphics , 2010 . [ 68 ] y. jiang , y. xu and y. liu , `` performance evaluation of feature detection and"", ""and descriptor for robot navigation , '' in international conference on computer vision and graphics , 2010 . [ 68 ] y. jiang , y. xu and y. liu , `` performance evaluation of feature detection and matching in stereo visual odometry , '' neurocomputing , vol . 120 , pp . 380 -390 , 2013 . [ 69 ] h.-j . chien , c. -c. chuang , c. -y. chen and r. klette , `` when to use what feature ? sift , surf , orb , or a -kaze feature for monocular visual odo metry , '' in image and vision computing new zealand ( ivcnz ) , 2016 international conference on , 2016 . [ 70 ] j. witt and u. weltin , `` robust stereo visual odometry use iterative closest multiple line , '' in intelligent robots and systems ( iros ) , 2013 ieee/rs j international conference on , 2013 . [ 71 ] t. koletschka , l. puig and k. daniilidis , `` mevo : multi - environment stereo visual odometry , '' in intelligent robots and systems ( iros 2014 ) , 2014 ieee/rsj international conference on , 2014 . [ 72 ] z. zhang , r. deriche , o. faugeras and q . -t. luong , `` a robust technique for match two uncalibrated image through the recovery of the unknown epipolar geometry , '' artificial intelligence , vol . 78 , pp . 87 -119 , 1995 . [ 73 ] l. de -maeztu , u. elordi , m. niet o , j. barandiaran and o. otaegui , '' a temporally consistent grid -based visual odometry framework for multi -core architecture , '' journal of real -time image processing , vol . 10 , pp . 759 -769 , 2015 . [ 74 ] b. kitt , f. moosmann and c. stiller , `` moving on to dyn amic environment : visual odometry use feature classification , '' in intelligent robots and systems ( iros ) , 2010 ieee/rsj international conference on , 2010 . [ 75 ] w. zhou , h. fu and x . an , `` a classification -based visual odometry approach , '' in intelligent human -machine systems and cybernetics ( ihmsc ) , 2016 8th international conference on , 2016 . [ 76 ] a. escalera , e. izquierdo , d. mart { \\\\'\\\\i } n , b. musleh , f. garc { \\\\'\\\\i } a and j. m. armingol , `` stereo visual odometry in urban environment"", ""8th international conference on , 2016 . [ 76 ] a. escalera , e. izquierdo , d. mart { \\\\'\\\\i } n , b. musleh , f. garc { \\\\'\\\\i } a and j. m. armingol , `` stereo visual odometry in urban environment base on detect gr ound feature , '' robotics and autonomous systems , vol . 80 , pp . 1 -10 , 2016 . [ 77 ] r. kottath , d. p. yalamandala , s. poddar , a. p. bhondekar and v. karar , `` inertia constrain visual odometry for navigational application , '' in image information processing ( iciip ) , 2017 fourth international conference on , 2017 . [ 78 ] i. kostavelis , e. boukas , l. nalpantidis and a. gasteratos , `` stereo - base visual odometry for autonomous robot navigation , '' international journal of advanced robotic systems , vol . 13 , p. 21 , 2016 . [ 79 ] m. wu , s. -k. lam and t. srikanthan , `` a framework for fast and robust visual odometry , '' ieee transactions on intelligent transportation systems , vol . 18 , pp . 3433 -3448 , 2017 . [ 80 ] f. labrosse , `` the visual compass : performanc e and limitation of an appearance -based method , '' journal of field robotics , vol . 23 , pp . 913-941 , 2006 . [ 81 ] j. engel , v. koltun and d. cremers , `` direct sparse odometry , '' ieee transaction on pattern analysis and machine intelligence , vol . 40 , pp . 611 -625 , 2018 . [ 82 ] m. irani and p. anandan , `` robust multi -sensor image alignment , '' in computer vision , 1998 . sixth international conference on , 1998 . [ 83 ] r. mandelbaum , g. salgian and h. sawhney , `` correlation -based estimation of ego -motion and structur e from motion and stereo , '' in computer vision , 1999 . the proceedings of the seventh ieee international conference on , 1999 . [ 84 ] n. nourani -vatani , j. roberts and m. v. srinivasan , `` practical visual odometry for car -like vehicle , '' in robotics and automation , 2009 . icra'09 . ieee international conference on , 2009 . [ 85 ] y. yu , c. pradalier and g. zong , `` appearance -based monocular visual odometry for ground vehicle , '' in advanced intelligent"", "", 2009 . icra'09 . ieee international conference on , 2009 . [ 85 ] y. yu , c. pradalier and g. zong , `` appearance -based monocular visual odometry for ground vehicle , '' in advanced intelligent mechatronics ( aim ) , 2011 ieee/asme international conferenc e on , 2011 . [ 86 ] n. nourani -vatani and p. v. k. borges , `` correlation -based visual odometry for ground vehicle , '' journal of field robotics , vol . 28 , pp . 742 -768 , 2011 . [ 87 ] d. scaramuzza , `` omnidirectional vision , '' 2007 . [ 88 ] m. o . a. aqel , m. h. marhaban , m. i. saripan and n. b. ismail , '' adaptive -search template matching technique base on vehicle acceleration for monocular visual odometry system , '' ieej transactions on electrical and electronic engineering , vol . 11 , pp . 739-752 , 2016 . [ 89 ] r. goecke , a. asthana , n. pettersson and l. petersson , `` visual vehicle egomotion estimation use the fourier -mellin transform , '' in intelligent vehicles symposium , 2007 ieee , 2007 . [ 90 ] e. menegatti , t. maeda and h. ishiguro , `` image -based memory for robot navigation use property of omnidirectional image , '' robotics and autonomous systems , vol . 47 , pp . 251 -267 , 2004 . [ 91 ] b. lucas and t. kanade , `` b. ( 1981 ) . an iterative image registr ation technique with an application to stereo vision , '' in proc . darpa image understanding workshop , 1981 . [ 92 ] y. matsumoto , m. inaba and h. inoue , `` visual navigation use view -sequenced route representation , '' in robotics and automation , 1996 . proceedi ngs. , 1996 ieee international conference on , 1996 . [ 93 ] t. ohno , a. ohya and s. yuta , `` autonomous navigation for mobile robot refer pre -recorded image sequence , '' in intelligent robots and systems ' 96 , iros 96 , proceedings of the 1996 ieee/rsj inter national conference on , 1996 . [ 94 ] c. zhou , y. wei and t. tan , `` mobile robot self -localization base on global visual appearance feature , '' in robotics and automation , 2003 . proceedings . icra'03 . ieee international conference"", ""zhou , y. wei and t. tan , `` mobile robot self -localization base on global visual appearance feature , '' in robotics and automation , 2003 . proceedings . icra'03 . ieee international conference on , 2003 . [ 95 ] t. pajdla and v. hlav { \\\\'a } { \\\\v { c } } , `` zero phase representation of panoramic image for image base localization , '' in international conference on computer analysis of images and patterns , 1999 . [ 96 ] m. jogan and a. leonardis , `` robust localization use eigenspace of spin -images , '' in omnidirectional vision , 2000 . proceedings . ieee workshop on , 2000 . [ 97 ] a. i. comport , e. malis and p. rives , `` accurate quadrifocal tracking for robust 3d visual odometry , '' in robotics and automation , 2007 ieee international conference on , 2007 . [ 98 ] a. i. comport , e. malis and p. rives , `` real -time quadrifocal visual odometry , '' the international journal of robotics research , vol . 29 , pp . 245 -266 , 2010 . [ 99 ] m. irani , b. rousso and s. peleg , `` computing occluding and transparent motion , '' international journal of computer vision , vol . 12 , pp . 5 -16 , 1994 . [ 100 ] r. cipolla , y. okamoto and y. kuno , `` robust structure from motion use motion parallax , '' in comput er vision , 1993 . proceedings. , fourth international conference on , 1993 . [ 101 ] m. irani , b. rousso and s. peleg , `` recovery of ego -motion use region alignment , '' ieee transactions on pattern analysis and machine intelligence , vol . 19 , pp . 268 -272 , 1997 . [ 102 ] g. adiv , `` determining three -dimensional motion and structure from optical flow generate by several move object , '' ieee transaction on pattern analysis and machine intelligence , pp . 384 - 401 , 1985 . [ 103 ] j. j. gibson , `` visually control locomotion and visual orientation in animal , '' british journal of psychology , vol . 49 , pp . 182 -194 , 1958 . [ 104 ] m. v. srinivasan , m. lehrer , w. h. kirchner and s. w. zhang , '' range perception through apparent image spee d in freely fly honeybee , '' visual"", "", vol . 49 , pp . 182 -194 , 1958 . [ 104 ] m. v. srinivasan , m. lehrer , w. h. kirchner and s. w. zhang , '' range perception through apparent image spee d in freely fly honeybee , '' visual neuroscience , vol . 6 , pp . 519 -535 , 1991 . [ 105 ] w. f. clocksin , `` determining the orientation of surface from optical flow , '' in proceedings of the 1978 aisb/gi conference on artificial intelligence , 1978 . [ 106 ] s. ullman , `` the interpretation of structure from motion , '' proc . r. soc . lond . b , vol . 203 , pp . 405 -426 , 1979 . [ 107 ] k. prazdny , `` egomotion and relative depth map from optical flow , '' biological cybernetics , vol . 36 , pp . 87 -102 , 1980 . [ 108 ] b. k. p. horn and b. g. schunck , `` determining optical flow , '' artificial intelligence , vol . 17 , pp . 185 -203 , 1981 . [ 109 ] y.-h. kim , a. m. mart { \\\\'\\\\i } nez and a. c. kak , `` robust motion estimation under vary illumination , '' image and vision computing , vol . 23 , pp . 365 -375 , 2005 . [ 110 ] m. j . black and p. anandan , `` the robust estimation of multiple motion : parametric and piecewise -smooth flow field , '' computer vision and image understanding , vol . 63 , pp . 75 -104 , 1996 . [ 111 ] m. a. gennert and s. negahdaripour , `` relaxing the brightness constancy assumption in compute optical flow , '' 1987 . [ 112 ] m. irani and p. anandan , `` about direct method , '' in international workshop on vision algorithms , 1999 . [ 113 ] a. giachetti , m. campani and v. torre , `` t he use of optical flow for road navigation , '' ieee transaction on robotics and automation , vol . 14 , pp . 34 -48 , 1998 . [ 114 ] k. j. hanna , `` direct multi -resolution estimation of ego -motion and structure from motion , '' in visual motion , 1991. , proceedings of the ieee workshop on , 1991 . [ 115 ] a. m. hyslop and j. s. humbert , `` autonomous navigation in three - dimensional urban environment use wide -field integration of optic flow , '' journal of guidance , control , and dynamic , vol . 33 , pp . 147-159 , 2010 ."", "", `` autonomous navigation in three - dimensional urban environment use wide -field integration of optic flow , '' journal of guidance , control , and dynamic , vol . 33 , pp . 147-159 , 2010 . [ 116 ] j. campbell , r. sukthankar , i. nourbakhsh and a. pahwa , `` a robust visual odometry and precipice detection system use consumer - grade monocular vision , '' in robotics and automation , 2005 . icra 2005 . proceedings of the 2005 ieee international conferen ce on , 2005 . [ 117 ] m. v. srinivasan , `` an image -interpolation technique for the computation of optic flow and egomotion , '' biological cybernetics , vol . 71 , pp . 401 -415 , 1994 . [ 118 ] v. grabe , h. h. b { \\\\ '' u } lthoff and p. r. giordano , `` on -board velocity estimation and close -loop control of a quadrotor uav base on optical flow , '' in robotics and automation ( icra ) , 2012 ieee international conference on , 2012 . [ 119 ] v. grabe , h. h. b { \\\\ '' u } lthoff and p. r. giordano , `` robust optical - flow base self -motion esti mation for a quadrotor uav , '' in intelligent robots and systems ( iros ) , 2012 ieee/rsj international conference on , 2012 . [ 120 ] t. tykk { \\\\ '' a } l { \\\\ '' a } and a. i. comport , `` a dense structure model for image base stereo slam , '' in robotics and automation ( icra ) , 2011 ieee international conference on , 2011 . [ 121 ] j. engel , j. st { \\\\ '' u } ckler and d. cremers , `` large -scale direct slam with stereo camera , '' in intelligent robots and systems ( iros ) , 2015 ieee/rsj international conference on , 2015 . [ 122 ] r. ranftl , v. vineet , q. chen and v. koltun , `` dense monocular depth estimation in complex dynamic scene , '' in proceedings of the ieee conference on computer vision and pattern recognition , 2016 . [ 123 ] s. temizer , `` optical flow base local navigation , '' 2 001 . [ 124 ] c. kerl , j. sturm and d. cremers , `` dense visual slam for rgb -d camera , '' in intelligent robots and systems ( iros ) , 2013 ieee/rsj international conference on , 2013 . [ 125 ] i."", "". [ 124 ] c. kerl , j. sturm and d. cremers , `` dense visual slam for rgb -d camera , '' in intelligent robots and systems ( iros ) , 2013 ieee/rsj international conference on , 2013 . [ 125 ] i. dryanovski , c. jaramillo and j. xiao , `` incremental registration of rgb -d image , '' in robotics and automation ( icra ) , 2012 ieee international conference on , 2012 . [ 126 ] t. tykk { \\\\ '' a } l { \\\\ '' a } , c. audras and a. i. comport , `` direct iterative closest point for real -time visual odometry , '' in computer vision workshops ( iccv work shop ) , 2011 ieee international conference on , 2011 . [ 127 ] s. li and d. lee , `` fast visual odometry use intensity -assisted iterative closest point , '' ieee robotics and automation letters , vol . 1 , pp . 992 -999 , 2016 . [ 128 ] c. kerl , j. sturm and d. cremers , `` robust odometry estimation for rgb -d camera , '' in robotics and automation ( icra ) , 2013 ieee international conference on , 2013 . [ 129 ] t. whelan , h. johannsson , m. kaess , j. j. leonard and j. mcdonald , '' robust real -time visual odometry for dense rgb -d mapping , '' in robotics and automation ( icra ) , 2013 ieee international conference on , 2013 . [ 130 ] j. oliensis and m. werman , `` structure from motion use point , line , and intensity , '' in computer vision and pattern recog nition , 2000 . proceedings . ieee conference on , 2000 . [ 131 ] l.-p. morency and r. gupta , `` robust real -time egomotion from stereo image , '' in image processing , 2003 . icip 2003 . proceedings . 2003 international conference on , 2003 . [ 132 ] d. scaramuzza , f. fraundorfer , m. pollefeys and r. siegwart , '' closing the loop in appearance -guided structure -from -motion for omnidirectional camera , '' in the 8th workshop on omnidirectional vision , camera networks and non -classical cameras -omnivis , 2008 . [ 133 ] c. forst er , m. pizzoli and d. scaramuzza , `` svo : fast semi -direct monocular visual odometry , '' in robotics and automation ( icra ) , 2014 ieee international conference on"", "", 2008 . [ 133 ] c. forst er , m. pizzoli and d. scaramuzza , `` svo : fast semi -direct monocular visual odometry , '' in robotics and automation ( icra ) , 2014 ieee international conference on , 2014 . [ 134 ] h. silva , a. bernardino and e. silva , `` probabilistic egomotion for stereo visual odometry , '' journal of intelligent \\\\ & robotic systems , vol . 77 , pp . 265 -280 , 2015 . [ 135 ] h. silva , a. bernardino and e. silva , `` a voting method for stereo egomotion estimation , '' international journal of advanced robotic systems , vol . 14 , p. 1729881417710795 , 2017 . [ 136 ] d. a. pomerleau , `` alvinn : an autonomous land vehicle in a neural networ k , '' in advances in neural information process system , 1989 . [ 137 ] d. a. pomerleau , `` efficient training of artificial neural network for autonomous navigation , '' neural computation , vol . 3 , pp . 88 -97 , 1991 . [ 138 ] k. r. konda and r. memisevic , `` lea rning visual odometry with a convolutional network. , '' in visapp ( 1 ) , 2015 . [ 139 ] d. detone , t. malisiewicz and a. rabinovich , `` deep image homography estimation , '' arxiv preprint arxiv:1606.03798 , 2016 . [ 140 ] r. memisevic , `` learning to relate image , '' ieee transaction on pattern analysis and machine intelligence , vol . 35 , pp . 1829 -1846 , 2013 . [ 141 ] r. roberts , h. nguyen , n. krishnamurthi and t. balch , `` memory - base learning for visual odometry , '' in robotics and automation , 2008 . icra 2008 . ieee international conference on , 2008 . [ 142 ] v. guizilini and f. ramos , `` semi -parametric model for visual odometry , '' in robotics and automation ( icra ) , 2012 ieee international conference on , 2012 . [ 143 ] v. guizilini and f. ramos , `` semi -parametric learni ng for visual odometry , '' the international journal of robotics research , vol . 32 , pp . 526 -546 , 2013 . [ 144 ] k. konda and r. memisevic , `` unsupervised learning of depth and motion , '' arxiv preprint arxiv:1312.3429 , 2013 . [ 145 ] v. mohanty , s. agrawal , s. datta , a."", "", pp . 526 -546 , 2013 . [ 144 ] k. konda and r. memisevic , `` unsupervised learning of depth and motion , '' arxiv preprint arxiv:1312.3429 , 2013 . [ 145 ] v. mohanty , s. agrawal , s. datta , a. ghosh , v. d. sharma and d. chakravarty , `` deepvo : a deep learning approach for monocular visual odometry , '' arxiv preprint arxiv:1611.06069 , 2016 . [ 146 ] h. xu , y. gao , f. yu and t. darrell , `` end -to-end learning of driv ing model from large -scale video datasets , '' arxiv preprint , 2016 . [ 147 ] s. wang , r. clark , h. wen and n. trigoni , `` deepvo : towards end - to-end visual odometry with deep recurrent convolutional neural network , '' in robotics and automation ( icra ) , 2017 ieee international conference on , 2017 . [ 148 ] n. fanani , a. st { \\\\ '' u } rck , m. ochs , h. bradler and r. mester , '' predictive monocular odometry ( pmo ) : what be possible without ransac and multiframe bundle adjustment ? , '' image and vision computing , vol . 68 , pp . 3-13 , 2017 . [ 149 ] v. peretroukhin , l. clement and j. kelly , `` inferring sun direction to improve visual odometry : a deep learning approach , '' the international journal of robotics research , p. 0278364917749732 , 2018 . [ 150 ] h. zhan , r. garg , c. s. weer asekera , k. li , h. agarwal and i. reid , '' unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction , '' arxiv preprint arxiv:1803.03893 , 2018 . [ 151 ] m. bojarski , d. del testa , d. dworakowski , b. firner , b. fle pp , p. goyal , l. d. jackel , m. monfort , u. muller , j. zhang and others , '' end to end learn for self -driving car , '' arxiv preprint arxiv:1604.07316 , 2016 . [ 152 ] r. mahajan , p. v. shanmuganathan , v. karar and s. poddar , '' flexible threshold visual odomet ry algorithm using fuzzy logics , '' in proceedings of 2nd international conference on computer vision \\\\ & image processing , 2018 . [ 153 ] t. mouats , n. aouf , l. chermak and m. a. richardson , `` thermal stereo odometry for uavs , '' ieee sensors"", ""of 2nd international conference on computer vision \\\\ & image processing , 2018 . [ 153 ] t. mouats , n. aouf , l. chermak and m. a. richardson , `` thermal stereo odometry for uavs , '' ieee sensors journal , vol . 15 , pp . 6335 -6347 , 2015 . [ 154 ] t. mouats , n. aouf , a. d. sappa , c. aguilera and r. toledo , '' multispectral stereo odometry , '' ieee transactions on intelligent transportation systems , vol . 16 , pp . 1210 -1224 , 2015 . [ 155 ] e. mueggler , h. rebecq , g. gallego , t. delbruck and d. scaramuzza , `` the event -camera dataset and simulator : event -based data for pose estimation , visual odometry , and slam , '' the international journal of robotics research , vol . 36 , pp . 142 -149 , 2017 . [ 156 ] j.-l. blanco , f. -a. moreno and j. gonzalez , `` a collection of outdoor robotic datasets with centimeter -accuracy ground truth , '' autonomous robots , vol . 27 , p. 327 , 2009 . [ 157 ] j.-l. blanco -claraco , f. - { . moreno -due { \\\\~n } a and j. gonz { \\\\'a } lez -jim { \\\\'e } nez , `` the málaga urban dataset : high -rate stereo and lidar in a realistic urban scenario , '' the international journal of robotics research , vol . 33 , pp . 207 -214 , 2014 . [ 158 ] m. smi th , i. baldwin , w. churchill , r. paul and p. newman , `` the new college vision and laser data set , '' the international journal of robotics research , vol . 28 , pp . 595 -599 , 2009 . [ 159 ] a. geiger , j. ziegler and c. stiller , `` stereoscan : dense 3d reconstructio n in real -time , '' in intelligent vehicles symposium ( iv ) , 2011 ieee , 2011 . [ 160 ] a. geiger , p. lenz , c. stiller and r. urtasun , `` vision meet robotics : the kitti dataset , '' the international journal of robotics research , vol . 32 , pp . 1231 -1237 , 2013 . [ 161 ] h. alismail , b. browning , m. b. dias , b. argall , b. browning , y. gu , m. veloso , b. argall , b. browning , m. veloso and others , '' evaluating pose estimation methods for stereo visual odometry on robots , '' in proceeding of the 11th international confere nce on"", "", m. veloso , b. argall , b. browning , m. veloso and others , '' evaluating pose estimation methods for stereo visual odometry on robots , '' in proceeding of the 11th international confere nce on intelligent autonomous systems ( ias -11 ) , 2008 . [ 162 ] m. warren , d. mckinnon , h. he , a. glover , m. shiel and b. upcroft , `` large scale monocular vision -only map from a fix - wing suas , '' in field and service robotics , 2014 . [ 163 ] m. burri , j. nikolic , p. gohl , t. schneider , j. rehder , s. omari , m. w. achtelik and r. siegwart , `` the euroc micro aerial vehicle datasets , '' the international journal of robotics research , vol . 35 , pp . 1157 -1163 , 2016 . [ 164 ] w. maddern , g. pascoe , c. linegar and p. newman , `` 1 year , 1000 km : the oxford robotcar dataset , '' the international journal of robotics research , vol . 36 , pp . 3 -15 , 2017 . [ 165 ] p. bergmann , r. wang and d. cremers , `` online photometric calibration of auto exposure video for realtime visual odome try and slam , '' ieee robotics and automation letters , vol . 3 , pp . 627 - 634 , 2018 . [ 166 ] j. engel , v. usenko and d. cremers , `` a photometrically calibrate benchmark for monocular visual odometry , '' arxiv preprint arxiv:1607.02555 , 2016 . [ 167 ] d. caruso , j. engel and d. cremers , `` large -scale direct slam for omnidirectional camera , '' in intelligent robots and systems ( iros ) , 2015 ieee/rsj international conference on , 2015 . [ 168 ] g. pandey , j. r. mcbride and r. m. eustice , `` ford campus vision and lidar data set , '' the international journal of robotics research , vol . 30 , pp . 1543 -1552 , 2011 . [ 169 ] t. hinzmann , t. stastny , g. conte , p. doherty , p. rudol , m. wzorek , e. galceran , r. siegwart and i. gilitschenski , '' collaborative 3d reconstruction using heterogeneous uavs : system and experiments , '' in international symposium on experimental robotics , 2016 . [ 170 ] a. l. majdik , c. till and d. scaramuzza , `` the zurich urban micro aerial v ehicle dataset , '' the"", ""system and experiments , '' in international symposium on experimental robotics , 2016 . [ 170 ] a. l. majdik , c. till and d. scaramuzza , `` the zurich urban micro aerial v ehicle dataset , '' the international journal of robotics research , vol . 36 , pp . 269 -273 , 2017 . [ 171 ] d. schubert , t. goll , n. demmel , v. usenko , j. st { \\\\ '' u } ckler and d. cremers , `` the tum vi benchmark for evaluating visual -inertial odometry , '' arxiv preprint arxiv:1804.06120 , 2018 . [ 172 ] j. sturm , n. engelhard , f. endres , w. burgard and d. cremers , `` a benchmark for the evaluation of rgb -d slam system , '' in intelligent robots and systems ( iros ) , 2012 ieee/rsj international conference on , 2012 . [ 173 ] m. fallon , h. johannsson , m. kaess and j. j. leonard , `` the mit stata center dataset , '' the international journal of robotics research , vol . 32 , pp . 1695 -1699 , 2013 . [ 174 ] a. handa , t. whelan , j. mcdonald and a. j. davison , `` a benchmark for rgb -d visual o dometry , 3d reconstruction and slam , '' in robotics and automation ( icra ) , 2014 ieee international conference on , 2014 . [ 175 ] h. oleynikova , z. taylor , m. fehr , j. nieto and r. siegwart , '' voxblox : building 3d sign distance field for planning , '' arxiv , pp . arxiv -- 1611 , 2016 . [ 176 ] m. peris , s. martull , a. maki , y. ohkawa and k. fukui , `` towards a simulation drive stereo vision system , '' in pattern recognition ( icpr ) , 2012 21st international conference on , 2012 . [ 177 ] z. zhang , h. rebecq , c. forster a nd d. scaramuzza , `` benefit of large field -of-view camera for visual odometry , '' in robotics and automation ( icra ) , 2016 ieee international conference on , 2016 .""]",https://doi.org/10.48550/arXiv.1611.06069
12.pdf,"see discussion , st at , and author pr ofiles f or this public ation at : http : //www .researchgate.ne t/public ation/344488567 comparison of ros-based monocular visual slam methods : dso , ldso , orb-slam2 and dynaslam chapt er in lecture not es in comput er scienc e · oct ober 2020 doi : 10.1007/978-3-030-60337-3_22 citations 31reads 2,858 7 author s , include : roman lavr eno v kazan ( v olga region ) f eder al univ ersity 81 publica tions 671 citations see profile tatyana t soy kazan ( v olga region ) f eder al univ ersity 68 publica tions 343 citations see profile fumit oshi matsuno kyoto univ ersity 570 publica tions 7,517 citations see profile mikhail svinin ritsumeik an univ ersity 204 publica tions 1,832 citations see profile all c ontent f ollo wing this p age be uplo aded b y roman lavr eno v on 06 oct ober 2020 . the user have r equest ed enhanc ement of the do wnlo aded file.comparison of ros-based monocular visual slam methods : dso , ldso , orb-slam2 and dynaslam eldar mingachev1 ( b ) , roman lavrenov1 ( b ) , tatyana tsoy1 ( b ) , fumitoshi matsuno2 , mikhail svinin3 , jackrit suthakorn4 , and evgeni magid1 1laboratory of intelligent robotic systems ( lirs ) , intelligent robotics department , higher institute for information technology and intelligent systems , kazan federal university , kazan , russian federation ermingachev @ stud.kpfu.ru , { lavrenov , tt , magid } @ it.kfu.ru 2department of mechanical engineering and science , kyoto university , kyoto 615-8540 , japan matsuno @ me.kyoto-u.ac.jp 3information science and engineering department , ritsumeikan university , 1-1-1 noji-higashi , kusatsu , shiga 525-8577 , japan svinin @ fc.ritsumei.ac.jp 4biomedical engineering department , mahidol university , 4 , 999 phuttamonthon , salaya 73170 , thailand jackrit.sut @ mahidol.ac.th http : //robot.kpfu.ru/eng abstract . stable and robust path planning of a ground mobile robot require a combination of accuracy and low latency in it state estima-tion . yet , state estimation algorithm should provide these under com- putational and power constraint of a robot embed hardware . the presented study oﬀers a comparative analysis of four cut edge pub- licly available within robot operate system ( ros ) monocular simul- taneous localization and mapping method : dso , ldso , orb-slam2 , and dynaslam . the analysis considers pose estimation accuracy ( align- ment , absolute trajectory , and relative pose root mean square error ) and trajectory precision of the four method at tum-mono and eurocdatasets . keywords : simultaneous localization and map ·visual slam · monocular slam ·visual odometry ·state estimation ·path planning ·benchmark test ·robot sense system 1 introduction simultaneous localization and mapping ( slam , [ 8 ] ) be an ability of an autonomous vehicle to start in an unknown location of an unknown environ- ment and then , use only relative observation , to incrementally construct a c/circlecopyrtspringer nature switzerland ag 2020 a. ronzhin et al . ( eds . ) : icr 2020 , lnai 12336 , pp . 222–233 , 2020.https : //doi.org/10.1007/978-3-030-60337-3 _22comparison of ros-based monocular visual slam methods 223 map of the environment [ 25 ] while simultaneously use the map to compute a bounded estimate of the vehicle location [ 22 ] . nowadays , slam be apply to state and pose estimation problem in various domain , from virtual and aug- mented reality to autonomous vehicle and robotics [ 12,15 ] . the ﬁeld have reach a mature level [ 7 ] that cause proprietary slam algorithm utilize in many commercial product as well a public availability of a number of open-source slam software package [ 6 ] . yet , due to sensor price and robot weight concern , currently the prevail type of slam be a monocular approach [ 5 ] . one of the main feature of a monocular slam be a scale-ambiguity [ 10 ] , which state that a world scale could not be observe and drift over time , be one of the major error source . being both a challenge and a beneﬁt , it allowsswitching seamlessly between diﬀerently scale environment [ 14 ] , while stereo or depth camera do not allow such ﬂexibility , have a limited range where they can provide reliable measurement [ 21 ] . this paper oﬀers a comparative analysis in term of a pose estimation accuracy and a trajectory precision of the most recent and popular robot operate system ( ros ) base open-source monocularslam method consider power constraint of mobile ground robot [ 18 ] . the four select slam method be dso [ 9 ] , ldso [ 13 ] , orb-slam2 [ 19,20 ] a n d dynaslam [ 2 ] 2 related work 2.1 the selected slam methods direct methods can estimate a completely dense reconstruction by a direct minimization of a photometric error and optical ﬂow regularization . some direct method focus on high-gradient area estimate semi-dense map [ 2 ] . the pre- sented study compare : – dso , which be a state-of-the-art pure direct method [ 9 ] , – ldso , which be dso ’ s late revision with a loop closure ability and a global map optimization [ 13 ] . feature-based methods rely on match key point and can only estimate a sparse reconstruction [ 3 ] , mostly provide a good trade-oﬀ between an accuracy and a runtime . the current study present a comparison of : –o r b - s l a m 2 [ 20 ] state-of-the-art visual slam method that track orb feature in real-time . it have a same monocular core a the original orb- slam [ 19 ] but be feature with an improved and optimized workﬂow . – the recently propose dynaslam [ 2 ] method , which add a front-end stage to the orb-slam2 system to have a more accurate tracking and a reusable map of a scene . it outperform the accuracy of the standard visual slam baseline in highly dynamic scenarios.224 e. mingachev et al . 2.2 benchmarks there be several publicly available datasets for the slam benchmark purpose , however , some of the exist one be not suitable to benchmark monocular slam algorithm due to a low precision of groundtruth data [ 16 ] . the current study consider the two most suitable datasets , tum-mono and euroc . tum-mono . schubert et al . [ 24 ] , engel , usenko , and cremers [ 11 ] develop a dataset for evaluate a track accuracy of a monocular visual odometry [ 17 ] and slam method . the dataset include 50 indoor and outdoor sequence , which start and end in the same position and contain groundtruth only for thesestart and end trajectory segment . all dataset sequence be photometrically calibrate and provide exposure time for each frame a report by a sensor , a camera response function , and a dense lens attenuation factor . this allow evaluate a track accuracy via an accumulated drift and a reliably benchmark direct method . euroc . burri et al . [ 4 ] propose a visual-inertial dataset aim at evalua- tion of localization and 3d environment reconstruction algorithm . the dataset consist of 11 sequence , record with two monocular camera onboard a micro- aerial vehicle . the datasets range from slow ﬂights under good visual conditionsto dynamic ﬂights with motion blur and poor illumination . each sequence con- tains synchronize stereo image , extrinsic and intrinsic calibration , an inertial unit ( imu ) measurement , and an accurate groundtruth ( approximately 1 mm ) record use a laser tracker and a motion capture system . compared to the tum-mono benchmark , the sequence in euroc be short and have less vari-ety a they only contain recording inside a single machine hall and a single laboratory room . 2.3 metrics tum-mono . to evaluate the tum-mono benchmark result , we use pro- pose by engel , usenko , and cremers metric [ 11 ] , an alignment root mean square error ( rmse ) - a combined error measure , which equally take into account an error cause by scale , rotation and translation drift over an entiretrajectory . it be the rmse between track trajectory when align to start and end segment . euroc . the euroc include entire groundtruth camera trajectory , which allow use an absolute trajectory rmse ( ate ) , a measure of a global trajectory accuracy , and a relative pose rmse ( rpe ) , which be a measure of a local pose accuracy , propose by sturm , engelhard , endres , burgard , and cremers [ 26 ] . overall , the rpe metric provide an elegant way to combine rotational and translational error into a single measurement , while the ate only considers translational error . as a result , the rpe be always slightly large than the atecomparison of ros-based monocular visual slam methods 225 ( or equal if there be no rotational error ) . however , rotational error typically also manifest themselves in wrong translation and be thus indirectly also capturedby the ate . from a practical perspective , the ate have an intuitive visualization , which facilitate a visual inspection . nevertheless , a the author note , the two metric be strongly correlate . trajectory detail level . in contrast to the review metric , which mainly focus on measure a diﬀerence between correspond frame , a trajectory detail level measure a diﬀerence between a length of trajectory , be an estimate and a groundtruth trajectory length ratio . the metric can be use to bench-mark eﬀectiveness of hardware capability usage and even estimate limit of a detail level of a particular slam algorithm while run on various hardware conﬁgurations . in addition , it could be useful in determine a suitable trade-oﬀbetween an accuracy and output data detail level . 3 comparative analysis mur-artal and tard´ os [ 20 ] propose run each sequence ﬁve time and show median result to account for a non-deterministic nature of a system . bescos , f´acil , civera and neira [ 2 ] extend this approach by increase the number of run up to 10 time , a dynamic object be prone to increase a non-deterministic eﬀect . in light of the above , the current study also utilize the extended approach . 3.1 hardware setup this study focus on slam method usage with mobile ground robot that imply a restriction on energy consumption and absence of strict constraint ona mobile robot weight , which , for example , be critical for slam usage with a uav . the select hardware platform with balanced computational resource and power consumption be the hp omen 15-ce057ur laptop with the technical speciﬁcations brieﬂy describe in table 1 . table 1 . hardware speciﬁcations . cpu intel core i7-7700 hq , 2800 mhz ram 16gb , ddr4 , 2400mhz weight 2.56 kg battery 70 wh li-ion power consumption 80w ( avg . load ) 226 e. mingachev et al . fig . 1 . the experimental sequence - the loop start and end ( leave ) and the global tra- jectory overview ( right ) . 3.2 tum-mono to prove the eﬀectiveness of the propose approach [ 11 ] , we expand the tum- mono dataset sequence with a new real-world sequence ( fig . 1 ) collect with pal robotics tiago base ground mobile robot [ 1 ] with a single monocular camera onboard [ 23 ] . the sequence present 13 min of video and about a 100-m length trajectory in a gradually change environment - from a narrow indoor corridor to a wide indoor corridor , which move the robot from illuminate scene to dark scenes.the sequence start and end in the same place with slow loopy motion allow a correct initialization of the slam algorithm . the groundtruth for the entire trajectory be record with the orb-slam2 [ 20 ] algorithm , in contrast to the other sequence groundtruth , which be provide by lsd-slam [ 10 ] o n l yf o r the start and end segment . we have evaluate the metric over dso , ldso , orb-slam2 , and dynaslam method on the expand tum-mono dataset , run the dataset sequence forward and backward , with the loop closure feature be disable , follow the dataset author ’ recommendation . figure 2presents the cumulative error graph – accumulate translational , rotational , and scale drift along with the rmse when align the estimate trajectory start and end segment with the provide groundtruth trajectory . the ﬁgure depict the number of run in which the error be below the corresponding x-values - the closer to the top left , the good . it be important to note thediﬀerence in magnitude – the rmse within start and end segment be about 100 time less than the alignment rmse . due to the groundtruth nature and the similarity of the experimental result , engel et al . [ 11 ] conclude that almost all of the alignment error originate not from the noise in the groundtruth , but from the accumulated drift . our experiment conﬁrmed this conclusion , which mean that these metric couldbe use for any benchmark with a groundtruth of any accuracy a a reference , even the one collect with slam algorithm . figure 3shows the color-coded alignment rmse range from 0 ( blue ) to 10 m ( red ) for each dataset sequence.comparison of ros-based monocular visual slam methods 227 02468 1 0 et050100150200250300350400450500translation error dso ldso orb-slam2 dyna-slam 0 4 8 1 21 62 0 er050100150200250300350400450500rotation error dso ldso orb-slam2 dyna-slam 1 1.5 2 2.5 3 3.5 4 e's050100150200250300350400450500scale error dso ldso orb-slam2 dyna-slam 0 0.02 0.04 0.06 0.08 0.1 ealign050100150200250300350400450500start-segment error dso ldso orb-slam2 dyna-slam 0 0.02 0.04 0.06 0.08 0.1 ealign050100150200250300350400450500end-segment error dso ldso orb-slam2 dyna-slam fig . 2 . accumulated translational ( et , m ) , rotational ( er , m ) , and scale ( e/prime s , m ) d r i f t s along with the start and end segment rmse ( ealign , m ) . dso 11 02 03 04 05 0fwd bwdexperimentldso 11 02 03 04 05 0orb-slam2 11 02 03 04 05 0dyna-slam 11 02 03 04 05 0012345678910 ealign sequence fig . 3 . color-coded alignment rmse ( ealign , m ) for each tum-mono dataset sequence . ( color ﬁgure online ) the experiment demonstrate that direct method provide outstanding result compare to the feature-based one - the tum-mono dataset be design especially for direct method benchmark purpose , provide full photometric data for each frame , which greatly improve the accuracy of such method . how-ever , there be not that much of a diﬀerence if compare dso to ldso - a we can assume , the ldso global map optimization slightly improve the overall accuracy of the base method . the same behaviour be observe while compare feature-based method - the accuracy of dynaslam be slightly low compare to orb-slam2 . however , the dynaslam initialization be always quick than the orb-slam2 initializa-tion ; in highly dynamic sequence , the orb-slam2 initialization only occur when move object disappear from a scene while dynaslam succeed in boot- strap the system in such dynamic scenarios.228 e. mingachev et al . 3.3 euroc we evaluate the metric over dso , ldso , orb-slam2 , and dynaslam on the euroc dataset over all sequence for each of the two camera stream , whichwere interpret a separate sequence with the same groundtruth ( ‘ .0 ’ and ‘ .1 ’ notation correspond to the ﬁrst and the second camera dataset respectively and be label on x-axis in figs . 4and5 ) . figure 4shows the calculated absolute trajectory rmse ( ate , measure in metre ) and the relative pose rmse ( rpe , measure in metre per second ) metric range from 0 ( blue ) to 2 ( red ) for all method . dso 2 4 6 8 10experiment 0.511.52 eatemh01.0 mh01.1mh02.0 mh02.1 mh03.0mh03.1mh04.0mh04.1 mh05.0 mh05.1 v101.0v101.1v102.0v102.1v103.0v103.1v201.0v201.1v202.0v202.1v203.0v203.12 4 6 8 10experiment 0.20.40.60.81 erpeldso 2 4 6 8 100.511.52mh01.0 mh01.1 mh02.0 mh02.1mh03.0mh03.1mh04.0 mh04.1 mh05.0 mh05.1 v101.0 v101.1v102.0v102.1 v103.0 v103.1v201.0v201.1v202.0v202.1 v203.0 v203.12 4 6 8 100.20.40.60.81 orb-slam2 2 468 10experiment 0.20.40.60.811.2 eatemh01.0 mh01.1mh02.0mh02.1 mh03.0 mh03.1 mh04.0mh04.1 mh05.0 mh05.1 v101.0v101.1v102.0 v102.1v103.0v103.1 v201.0v201.1v202.0 v202.1v203.0 v203.12 4 6 8 10experiment 0.20.40.60.811.2 erpedyna-slam 2 468 10 0.20.40.60.811.2mh01.0 mh01.1 mh02.0 mh02.1mh03.0mh03.1 mh04.0 mh04.1mh05.0 mh05.1 v101.0 v101.1v102.0v102.1 v103.0 v103.1 v201.0v201.1v202.0v202.1 v203.0 v203.12 4 6 8 100.511.5 fig . 4 . color-coded evaluation result for each euroc dataset sequence : absolute tra- jectory rmse ( at e , m ) and relative pose rmse ( rp e , m/s ) . ( color ﬁgure online ) as the analysis demonstrate , that in term of the rpe , the measure of local accuracy , the direct method generally perform signiﬁcantly good than the feature-based one , but it be still diﬃcult for them to overcome a harsh envi- ronment with a lack of light and prevailing rotational movement ( over transla-tional movement ) , a show in mh.05 , v1.03 , and v2.03 sequence . in term of the ate , the feature-based method demonstrate a stable performance , even in the “ hard ” sequence . however , the accuracy of dynaslam be slightly low , comparison of ros-based monocular visual slam methods 229 compare to orb-slam2 , since dynaslam succeed in bootstrapping the sys- tem with a dynamic content and always initialize quick than orb-slam2and thus have more frame to process ( and more room for accumulate error ) . figure 5demonstrates the calculated trajectory detail level for dso , ldso , orb-slam2 , and dynaslam method for each euroc dataset sequence . dso 2 46 8 10experiment 00.020.040.06 detail level ldsomh01.0 mh01.1 mh02.0 mh02.1mh03.0 mh03.1 mh04.0 mh04.1 mh05.0mh05.1 v101.0 v101.1 v102.0v102.1 v103.0 v103.1 v201.0 v201.1v202.0 v202.1 v203.0 v203.12 4 6 8 10experiment 00.020.040.06 detail levelorb-slam2 2 46 8 10 00.0050.01 dynaslammh01.0 mh01.1 mh02.0 mh02.1 mh03.0 mh03.1 mh04.0 mh04.1 mh05.0 mh05.1 v101.0 v101.1 v102.0 v102.1 v103.0v103.1 v201.0 v201.1 v202.0 v202.1 v203.0 v203.12 4 6 8 1000.0050.01 fig . 5 . color-coded trajectory detail level for each euroc dataset sequence . ( color ﬁgure online ) table 2 . median absolute trajectory rmse ( at e , m ) , relative pose rmse ( rp e , m/s ) & trajectory detail level ( detail ) for each euroc dataset sequence . sequence dso ldso orb-slam2 dynaslam at e rp e detail at e rp e detail at e rp e detail at e rp e detail mh.01 0.054 0.132 0.018 0.044 0.131 0.018 0.041 0.491 0.006 0.042 0.494 0.006 mh.02 0.063 0.134 0.025 0.044 0.139 0.025 0.035 0.458 0.006 0.036 0.465 0.007 mh.03 0.209 0.711 0.028 0.090 0.706 0.028 0.041 1.095 0.006 0.043 1.102 0.007 mh.04 0.173 0.632 0.022 0.136 0.642 0.022 0.074 0.560 0.009 0.076 0.568 0.011 mh.05 0.169 0.199 0.023 0.127 0.198 0.023 0.054 0.589 0.009 0.056 0.592 0.010 v1.01 0.104 0.088 0.023 0.099 0.089 0.023 0.054 0.454 0.005 0.054 0.459 0.006 v1.02 1.047 0.137 0.044 1.013 0.111 0.044 0.054 0.528 0.009 0.055 0.534 0.011 v1.03 0.584 0.334 0.057 0.607 0.375 0.057 0.091 0.409 0.009 0.097 0.411 0.010 v2.01 0.064 0.081 0.018 0.058 0.081 0.019 0.047 0.225 0.007 0.047 0.227 0.008 v2.02 0.162 0.306 0.037 0.106 0.281 0.038 0.051 0.508 0.009 0.053 0.552 0.009 v2.03 1.439 0.087 0.036 1.266 0.086 0.040 0.096 0.477 0.010 0.097 0.479 0.012 the obtain result suggest that the direct method typically distinguish more keyframes and , thus , have a good trajectory detail level , show a good local ( pose ) accuracy , compare to feature-based method . it be important to note the diﬀerence in the color scale of the feature-based method plot , whichis the diﬀerence in the detail level magnitude . dynaslam operate a slightly large amount of frame than orb-slam2 and have a slightly good trajectory detail level ( due to the quicker initialization ) .230 e. mingachev et al . table 3 . median alignment rmse ( align , m ) , absolute trajectory rmse ( at e , m ) , relative pose rmse ( rp e , m/s ) & trajectory detail level ( detail ) . metrics dso ldso orb-slam2 dynaslam align 0.8496 0.7769 5.7571 6.1891 at e 0.1683 0.1062 0.0507 0.0525 rp e 0.1373 0.1111 0.4765 0.4787 detail 0.0355 0.0376 0.0086 0.0099 3.4 summary tables 2and3summarize the calculated metric a a single median value for each euroc dataset sequence ( table 2 ) and the entire tum-mono and euroc datasets ( table 3 ) . the alignment error , accumulate drift , be in average 7.35 time low for direct method than for indirect : – ldso outperforms dso by 8.56 % – dynaslam be 6.98 % behind orb-slam2 the absolute trajectory error , global accuracy , for direct method be in average 2.66 time high than for indirect : – ldso outperforms dso by 36.89 % – dynaslam be 3.42 % behind orb-slam2 the relative position error , local accuracy , for direct method be in average 3.85 time low than for indirect : – ldso outperforms dso by 19.08 % – dynaslam be 0.46 % behind orb-slam2 the level of trajectory detail for direct method be in average 3.95 time high than for indirect : – ldso outperforms dso by 5.59 % , – dynaslam outperform orb-slam2 by 13.13 % . while it be expect that ldso should outperform it original source algo- rithm ( dso ) and experiment demonstrate it good performance with regardto all measure criterion , dynaslam and orb-slam2 have vary beneﬁts with respect to particular criterion , and this variety should be consider when select a slam algorithm for a speciﬁc task.comparison of ros-based monocular visual slam methods 231 4 conclusions and future work this paper present a comparative analysis of four publicly available ros-based monocular slam algorithm in term of the state estimation accuracy and the trajectory detail level . the analysis demonstrate that the direct method dso and ldso have a good accuracy while have entire photometric data availableand mainly focus on the local accuracy , which be also indirectly prove by the fact that they save and operate a relatively large amount of trajectory keyframes . for these reason , dso and ldso be more suitable for task involve a short-rangeoperation and require high accuracy in a local pose estimation , e.g.. , a 3d- reconstruction of an environment . at the same time , the feature-based method orb-slam2 and dynaslam outperform the direct one in term of the globalaccuracy , which , combine with an average trajectory detail level , make them a universal solution for most slam task - especially the one that require a long-range operating with stable and reliable result throughout an entiretrajectory . in task that involve dynamic object the accuracy of dynaslam will be signiﬁcantly high than orb-slam2 . the current study use hp omen 15-ce057ur laptop hardware . however , any slam method performance strongly correlate with available computa- tional resource . our ongoing work deal with expand the obtain resultsand compare the four algorithm ’ performance use several diﬀerent robot of the laboratory of intelligent robotic system [ 1,18 ] . we strongly believe that such comparison could be useful to the research community in order to have abetter perspective of how each metric varies depend on availability of real robot ’ onboard computational resource . acknowledgements . the report study be fund by the russian foundation for basic research ( rfbr ) , accord to the research project no . 19-58-70002 . the second author acknowledge the support by the research grant of kazan federal university.the forth and the ﬁfth author acknowledge the support of the japan science and technology agency , the jst strategic international collaborative research program , project no . 18065977 . the sixth author acknowledge the support of the nationalscience and technology development agency ( nstda ) , thailand , project id fda- co-2562-10058-th . special thanks to pal robotics for their kind professional support with tiago base robot software and hardware related issue . references 1 . bereznikov , d. , zakiev , a. : network failure detection and autonomous return for pmb-2 mobile robot . in : international conference on artiﬁcial life and robotics ( icarob ) , pp . 444–447 ( 2020 ) 2 . bescos , b. , f´ acil , j.m. , civera , j. , neira , j. : dynaslam : tracking , mapping , and inpainting in dynamic scene . ieee robot . autom . lett . 3 ( 4 ) , 4076–4083 ( 2018 ) 3 . bokovoy , a. , yakovlev , k. : sparse 3d point-cloud map upsampling and noise removal a a vslam post-processing step : experimental evaluation . in : ronzhin , a. , rigoll , g. , meshcheryakov , r . ( ed . ) icr 2018 . lncs ( lnai ) , vol . 11097 , pp . 23–33 . springer , cham ( 2018 ) . http : //doi.org/10.1007/978-3-319-99582-3 3232 e. mingachev et al . 4 . burri , m. , et al . : the euroc micro aerial vehicle datasets . int . j . robot . res . 35 ( 10 ) , 1157–1163 ( 2016 ) 5 . buyval , a. , afanasyev , i. , magid , e. : comparative analysis of ros-based monoc- ular slam method for indoor navigation . ninth international conference on machine vision , icmv , vol . 10341 , p. 103411k . international society for opticsand photonics ( 2017 ) 6 . carballo , a. , takeuchi , e. , takeda , k. : high density ground map use low bound- ary height estimation for autonomous vehicle . in : 21st international conferenceon intelligent transportation systems ( itsc ) , pp . 3811–3818 . ieee ( 2018 ) 7 . delmerico , j. , scaramuzza , d. : a benchmark comparison of monocular visual- inertial odometry algorithm for ﬂying robot . in : ieee 2018 international con- ference on robotics and automation , icra , pp . 2502–2509 . ieee ( 2018 ) 8 . dissanayake , m.g. , newman , p. , clark , s. , durrant-whyte , h.f. , csorba , m. : a solution to the simultaneous localization and map building ( slam ) problem . ieee trans . robot . autom . 17 ( 3 ) , 229–241 ( 2001 ) 9 . engel , j. , koltun , v. , cremers , d. : direct sparse odometry . ieee trans . pattern anal . mach . intell . 40 ( 3 ) , 611–625 ( 2017 ) 10 . engel , j. , sch¨ ops , t. , cremers , d. : lsd-slam : large-scale direct monocular slam . in : fleet , d. , pajdla , t. , schiele , b. , tuytelaars , t . ( ed . ) eccv 2014 . lncs , vol . 8690 , pp . 834–849 . springer , cham ( 2014 ) . http : //doi.org/10.1007/ 978-3-319-10605-2 54 11 . engel , j. , usenko , v. , cremers , d. : a photometrically calibrate benchmark for monocular visual odometry . arxiv preprint arxiv:1607.02555 ( 2016 ) 12 . gabdullin , a. , shvedov , g. , ivanou , m. , afanasyev , i. : analysis of onboard sensor- base odometry for a quadrotor uav in outdoor environment . in : international conference on artiﬁcial life and robotics ( icarob ) ( 2018 ) 13 . gao , x. , wang , r. , demmel , n. , cremers , d. : ldso : direct sparse odometry with loop closure . in : 2018 ieee/rsj international conference on intelligent robotsand systems , iros , pp . 2198–2204 . ieee ( 2018 ) 14 . ibragimov , i.z. , afanasyev , i.m . : comparison of ros-based visual slam meth- od in homogeneous indoor environment . in : 2017 14th workshop on positioning , navigation and communications ( wpnc ) , pp . 1–6 . ieee ( 2017 ) 15 . lavrenov , r. , matsuno , f. , magid , e. : modiﬁed spline-based navigation : guaran- teed safety for obstacle avoidance . in : ronzhin , a. , rigoll , g. , meshcheryakov , r . ( ed . ) icr 2017 . lncs ( lnai ) , vol . 10459 , pp . 123–133 . springer , cham ( 2017 ) . http : //doi.org/10.1007/978-3-319-66471-2 14 16 . mart´ ınez-garc´ ıa , e.a. , rivero-ju´ arez , j. , torres-m´ endez , l.a. , rodas-osollo , j.e . : divergent trinocular vision observer design for extended kalman ﬁlter robot stateestimation . proc . inst . mech . eng . part i j. syst . control eng . 233 ( 5 ) , 524–547 ( 2019 ) 17 . martinez garcia , e.a . : 4wd robot posture estimation by radial multi-view visual odometry . instituto de ingenier´ ıa y tecnolog´ ıa ( 2018 ) 18 . moskvin , i. , lavrenov , r. , magid , e. , svinin , m. : modelling a crawler robot use wheel a pseudo-tracks : model complexity vs performance . in : 7th international conference on industrial engineering and applications ( iciea ) , pp . 235–239 . ieee ( 2020 ) 19 . mur-artal , r. , montiel , j.m.m. , tardos , j.d . : orb-slam : a versatile and accu- rate monocular slam system . ieee trans . robot . 31 ( 5 ) , 1147–1163 ( 2015 ) 20 . mur-artal , r. , tard´ o , j.d . : orb-slam2 : an open-source slam system for monocular , stereo , and rgb-d camera . ieee trans . robot . 33 ( 5 ) , 1255–1262 ( 2017 ) comparison of ros-based monocular visual slam methods 233 21 . nagahama , k. , nishino , t. , kojima , m. , yamazaki , k. , okada , k. , inaba , m. : end point track for a move object with several attention region by composite vision system . in : international conference on mechatronics and automation , pp.590–596 . ieee ( 2011 ) 22 . rodriguez-telles , f.g. , mendez , l.a.t. , martinez-garcia , e.a . : a fast ﬂoor seg- mentation algorithm for visual-based robot navigation . in : 2013 international con-ference on computer and robot vision , pp . 167–173 . ieee ( 2013 ) 23 . saﬁn , r. , lavrenov , r. , tsoy , t. , svinin , m. , magid , e. : real-time video server implementation for a mobile robot . in : 2018 11th international conference on developments in esystems engineering ( dese ) , pp . 180–185 . ieee ( 2018 ) 24 . schubert , d. , goll , t. , demmel , n. , usenko , v. , st¨ uckler , j. , cremers , d. : the tum-mono vi benchmark for evaluate visual-inertial odometry . in : 2018 ieee/rsj international conference on intelligent robots and systems , iros , pp . 1680–1687 . ieee ( 2018 ) 25 . simakov , n. , lavrenov , r. , zakiev , a. , saﬁn , r. , mart´ ınez-garc´ ıa , e.a . : modeling usar map for the collection of information on the state of the environment . in : 2019 12th international conference on developments in esystems engineering ( dese ) , pp . 918–923 . ieee ( 2019 ) 26 . sturm , j. , engelhard , n. , endres , f. , burgard , w. , cremers , d. : a benchmark for the evaluation of rgb-d slam system . in : 2012 ieee/rsj internationalconference on intelligent robots and systems , pp . 573–580 . ieee ( 2012 ) view publication stats","['see discussion , st at , and author pr ofiles f or this public ation at : http : //www .researchgate.ne t/public ation/344488567 comparison of ros-based monocular visual slam methods : dso , ldso , orb-slam2 and dynaslam chapt er in lecture not es in comput er scienc e · oct ober 2020 doi : 10.1007/978-3-030-60337-3_22 citations 31reads 2,858 7 author s , include : roman lavr eno v kazan ( v olga region ) f eder al univ ersity 81 publica tions 671 citations see profile tatyana t soy kazan ( v olga region ) f eder al univ ersity 68 publica tions 343 citations see profile fumit oshi matsuno kyoto univ ersity 570 publica tions 7,517 citations see profile mikhail svinin ritsumeik an univ ersity 204 publica tions 1,832 citations see profile all c ontent f ollo wing this p age be uplo aded b y roman lavr eno v on 06 oct ober 2020 . the user have r equest ed enhanc ement of the do wnlo aded file.comparison of ros-based monocular visual slam methods : dso , ldso , orb-slam2 and dynaslam eldar mingachev1 ( b ) , roman lavrenov1 ( b ) , tatyana tsoy1 ( b ) , fumitoshi matsuno2 , mikhail svinin3 , jackrit suthakorn4 , and evgeni magid1 1laboratory of intelligent robotic systems ( lirs ) , intelligent robotics department , higher institute for information technology and intelligent systems , kazan federal university , kazan , russian federation ermingachev @ stud.kpfu.ru , { lavrenov , tt , magid } @ it.kfu.ru 2department of mechanical engineering and science , kyoto university , kyoto 615-8540 , japan matsuno @ me.kyoto-u.ac.jp 3information science and engineering department , ritsumeikan university , 1-1-1 noji-higashi , kusatsu , shiga 525-8577 , japan svinin @ fc.ritsumei.ac.jp 4biomedical engineering department , mahidol university , 4 , 999 phuttamonthon , salaya 73170 , thailand jackrit.sut @ mahidol.ac.th http : //robot.kpfu.ru/eng abstract . stable and robust path planning of a ground mobile robot require a combination of accuracy and low latency in it state estima-tion . yet , state estimation algorithm should', 'abstract . stable and robust path planning of a ground mobile robot require a combination of accuracy and low latency in it state estima-tion . yet , state estimation algorithm should provide these under com- putational and power constraint of a robot embed hardware . the presented study oﬀers a comparative analysis of four cut edge pub- licly available within robot operate system ( ros ) monocular simul- taneous localization and mapping method : dso , ldso , orb-slam2 , and dynaslam . the analysis considers pose estimation accuracy ( align- ment , absolute trajectory , and relative pose root mean square error ) and trajectory precision of the four method at tum-mono and eurocdatasets . keywords : simultaneous localization and map ·visual slam · monocular slam ·visual odometry ·state estimation ·path planning ·benchmark test ·robot sense system 1 introduction simultaneous localization and mapping ( slam , [ 8 ] ) be an ability of an autonomous vehicle to start in an unknown location of an unknown environ- ment and then , use only relative observation , to incrementally construct a c/circlecopyrtspringer nature switzerland ag 2020 a. ronzhin et al . ( eds . ) : icr 2020 , lnai 12336 , pp . 222–233 , 2020.https : //doi.org/10.1007/978-3-030-60337-3 _22comparison of ros-based monocular visual slam methods 223 map of the environment [ 25 ] while simultaneously use the map to compute a bounded estimate of the vehicle location [ 22 ] . nowadays , slam be apply to state and pose estimation problem in various domain , from virtual and aug- mented reality to autonomous vehicle and robotics [ 12,15 ] . the ﬁeld have reach a mature level [ 7 ] that cause proprietary slam algorithm utilize in many commercial product as well a public availability of a number of open-source slam software package [ 6 ] . yet , due to sensor price and robot weight concern , currently the prevail type of slam be a monocular approach [ 5 ] . one of the main feature of a monocular slam be a scale-ambiguity [ 10 ] , which state that a world scale', 'and robot weight concern , currently the prevail type of slam be a monocular approach [ 5 ] . one of the main feature of a monocular slam be a scale-ambiguity [ 10 ] , which state that a world scale could not be observe and drift over time , be one of the major error source . being both a challenge and a beneﬁt , it allowsswitching seamlessly between diﬀerently scale environment [ 14 ] , while stereo or depth camera do not allow such ﬂexibility , have a limited range where they can provide reliable measurement [ 21 ] . this paper oﬀers a comparative analysis in term of a pose estimation accuracy and a trajectory precision of the most recent and popular robot operate system ( ros ) base open-source monocularslam method consider power constraint of mobile ground robot [ 18 ] . the four select slam method be dso [ 9 ] , ldso [ 13 ] , orb-slam2 [ 19,20 ] a n d dynaslam [ 2 ] 2 related work 2.1 the selected slam methods direct methods can estimate a completely dense reconstruction by a direct minimization of a photometric error and optical ﬂow regularization . some direct method focus on high-gradient area estimate semi-dense map [ 2 ] . the pre- sented study compare : – dso , which be a state-of-the-art pure direct method [ 9 ] , – ldso , which be dso ’ s late revision with a loop closure ability and a global map optimization [ 13 ] . feature-based methods rely on match key point and can only estimate a sparse reconstruction [ 3 ] , mostly provide a good trade-oﬀ between an accuracy and a runtime . the current study present a comparison of : –o r b - s l a m 2 [ 20 ] state-of-the-art visual slam method that track orb feature in real-time . it have a same monocular core a the original orb- slam [ 19 ] but be feature with an improved and optimized workﬂow . – the recently propose dynaslam [ 2 ] method , which add a front-end stage to the orb-slam2 system to have a more accurate tracking and a reusable map of a scene . it outperform the accuracy of the standard visual slam baseline in highly dynamic scenarios.224 e.', 'stage to the orb-slam2 system to have a more accurate tracking and a reusable map of a scene . it outperform the accuracy of the standard visual slam baseline in highly dynamic scenarios.224 e. mingachev et al . 2.2 benchmarks there be several publicly available datasets for the slam benchmark purpose , however , some of the exist one be not suitable to benchmark monocular slam algorithm due to a low precision of groundtruth data [ 16 ] . the current study consider the two most suitable datasets , tum-mono and euroc . tum-mono . schubert et al . [ 24 ] , engel , usenko , and cremers [ 11 ] develop a dataset for evaluate a track accuracy of a monocular visual odometry [ 17 ] and slam method . the dataset include 50 indoor and outdoor sequence , which start and end in the same position and contain groundtruth only for thesestart and end trajectory segment . all dataset sequence be photometrically calibrate and provide exposure time for each frame a report by a sensor , a camera response function , and a dense lens attenuation factor . this allow evaluate a track accuracy via an accumulated drift and a reliably benchmark direct method . euroc . burri et al . [ 4 ] propose a visual-inertial dataset aim at evalua- tion of localization and 3d environment reconstruction algorithm . the dataset consist of 11 sequence , record with two monocular camera onboard a micro- aerial vehicle . the datasets range from slow ﬂights under good visual conditionsto dynamic ﬂights with motion blur and poor illumination . each sequence con- tains synchronize stereo image , extrinsic and intrinsic calibration , an inertial unit ( imu ) measurement , and an accurate groundtruth ( approximately 1 mm ) record use a laser tracker and a motion capture system . compared to the tum-mono benchmark , the sequence in euroc be short and have less vari-ety a they only contain recording inside a single machine hall and a single laboratory room . 2.3 metrics tum-mono . to evaluate the tum-mono benchmark result , we use pro- pose by engel , usenko ,', 'a they only contain recording inside a single machine hall and a single laboratory room . 2.3 metrics tum-mono . to evaluate the tum-mono benchmark result , we use pro- pose by engel , usenko , and cremers metric [ 11 ] , an alignment root mean square error ( rmse ) - a combined error measure , which equally take into account an error cause by scale , rotation and translation drift over an entiretrajectory . it be the rmse between track trajectory when align to start and end segment . euroc . the euroc include entire groundtruth camera trajectory , which allow use an absolute trajectory rmse ( ate ) , a measure of a global trajectory accuracy , and a relative pose rmse ( rpe ) , which be a measure of a local pose accuracy , propose by sturm , engelhard , endres , burgard , and cremers [ 26 ] . overall , the rpe metric provide an elegant way to combine rotational and translational error into a single measurement , while the ate only considers translational error . as a result , the rpe be always slightly large than the atecomparison of ros-based monocular visual slam methods 225 ( or equal if there be no rotational error ) . however , rotational error typically also manifest themselves in wrong translation and be thus indirectly also capturedby the ate . from a practical perspective , the ate have an intuitive visualization , which facilitate a visual inspection . nevertheless , a the author note , the two metric be strongly correlate . trajectory detail level . in contrast to the review metric , which mainly focus on measure a diﬀerence between correspond frame , a trajectory detail level measure a diﬀerence between a length of trajectory , be an estimate and a groundtruth trajectory length ratio . the metric can be use to bench-mark eﬀectiveness of hardware capability usage and even estimate limit of a detail level of a particular slam algorithm while run on various hardware conﬁgurations . in addition , it could be useful in determine a suitable trade-oﬀbetween an accuracy and output data detail level . 3', 'of a particular slam algorithm while run on various hardware conﬁgurations . in addition , it could be useful in determine a suitable trade-oﬀbetween an accuracy and output data detail level . 3 comparative analysis mur-artal and tard´ os [ 20 ] propose run each sequence ﬁve time and show median result to account for a non-deterministic nature of a system . bescos , f´acil , civera and neira [ 2 ] extend this approach by increase the number of run up to 10 time , a dynamic object be prone to increase a non-deterministic eﬀect . in light of the above , the current study also utilize the extended approach . 3.1 hardware setup this study focus on slam method usage with mobile ground robot that imply a restriction on energy consumption and absence of strict constraint ona mobile robot weight , which , for example , be critical for slam usage with a uav . the select hardware platform with balanced computational resource and power consumption be the hp omen 15-ce057ur laptop with the technical speciﬁcations brieﬂy describe in table 1 . table 1 . hardware speciﬁcations . cpu intel core i7-7700 hq , 2800 mhz ram 16gb , ddr4 , 2400mhz weight 2.56 kg battery 70 wh li-ion power consumption 80w ( avg . load ) 226 e. mingachev et al . fig . 1 . the experimental sequence - the loop start and end ( leave ) and the global tra- jectory overview ( right ) . 3.2 tum-mono to prove the eﬀectiveness of the propose approach [ 11 ] , we expand the tum- mono dataset sequence with a new real-world sequence ( fig . 1 ) collect with pal robotics tiago base ground mobile robot [ 1 ] with a single monocular camera onboard [ 23 ] . the sequence present 13 min of video and about a 100-m length trajectory in a gradually change environment - from a narrow indoor corridor to a wide indoor corridor , which move the robot from illuminate scene to dark scenes.the sequence start and end in the same place with slow loopy motion allow a correct initialization of the slam algorithm . the groundtruth for the entire trajectory be record with the orb-slam2', ""sequence start and end in the same place with slow loopy motion allow a correct initialization of the slam algorithm . the groundtruth for the entire trajectory be record with the orb-slam2 [ 20 ] algorithm , in contrast to the other sequence groundtruth , which be provide by lsd-slam [ 10 ] o n l yf o r the start and end segment . we have evaluate the metric over dso , ldso , orb-slam2 , and dynaslam method on the expand tum-mono dataset , run the dataset sequence forward and backward , with the loop closure feature be disable , follow the dataset author ’ recommendation . figure 2presents the cumulative error graph – accumulate translational , rotational , and scale drift along with the rmse when align the estimate trajectory start and end segment with the provide groundtruth trajectory . the ﬁgure depict the number of run in which the error be below the corresponding x-values - the closer to the top left , the good . it be important to note thediﬀerence in magnitude – the rmse within start and end segment be about 100 time less than the alignment rmse . due to the groundtruth nature and the similarity of the experimental result , engel et al . [ 11 ] conclude that almost all of the alignment error originate not from the noise in the groundtruth , but from the accumulated drift . our experiment conﬁrmed this conclusion , which mean that these metric couldbe use for any benchmark with a groundtruth of any accuracy a a reference , even the one collect with slam algorithm . figure 3shows the color-coded alignment rmse range from 0 ( blue ) to 10 m ( red ) for each dataset sequence.comparison of ros-based monocular visual slam methods 227 02468 1 0 et050100150200250300350400450500translation error dso ldso orb-slam2 dyna-slam 0 4 8 1 21 62 0 er050100150200250300350400450500rotation error dso ldso orb-slam2 dyna-slam 1 1.5 2 2.5 3 3.5 4 e's050100150200250300350400450500scale error dso ldso orb-slam2 dyna-slam 0 0.02 0.04 0.06 0.08 0.1 ealign050100150200250300350400450500start-segment error dso ldso orb-slam2"", ""1 1.5 2 2.5 3 3.5 4 e's050100150200250300350400450500scale error dso ldso orb-slam2 dyna-slam 0 0.02 0.04 0.06 0.08 0.1 ealign050100150200250300350400450500start-segment error dso ldso orb-slam2 dyna-slam 0 0.02 0.04 0.06 0.08 0.1 ealign050100150200250300350400450500end-segment error dso ldso orb-slam2 dyna-slam fig . 2 . accumulated translational ( et , m ) , rotational ( er , m ) , and scale ( e/prime s , m ) d r i f t s along with the start and end segment rmse ( ealign , m ) . dso 11 02 03 04 05 0fwd bwdexperimentldso 11 02 03 04 05 0orb-slam2 11 02 03 04 05 0dyna-slam 11 02 03 04 05 0012345678910 ealign sequence fig . 3 . color-coded alignment rmse ( ealign , m ) for each tum-mono dataset sequence . ( color ﬁgure online ) the experiment demonstrate that direct method provide outstanding result compare to the feature-based one - the tum-mono dataset be design especially for direct method benchmark purpose , provide full photometric data for each frame , which greatly improve the accuracy of such method . how-ever , there be not that much of a diﬀerence if compare dso to ldso - a we can assume , the ldso global map optimization slightly improve the overall accuracy of the base method . the same behaviour be observe while compare feature-based method - the accuracy of dynaslam be slightly low compare to orb-slam2 . however , the dynaslam initialization be always quick than the orb-slam2 initializa-tion ; in highly dynamic sequence , the orb-slam2 initialization only occur when move object disappear from a scene while dynaslam succeed in boot- strap the system in such dynamic scenarios.228 e. mingachev et al . 3.3 euroc we evaluate the metric over dso , ldso , orb-slam2 , and dynaslam on the euroc dataset over all sequence for each of the two camera stream , whichwere interpret a separate sequence with the same groundtruth ( ‘ .0 ’ and ‘ .1 ’ notation correspond to the ﬁrst and the second camera dataset respectively and be label on x-axis in figs . 4and5 ) . figure 4shows the calculated absolute trajectory rmse"", '( ‘ .0 ’ and ‘ .1 ’ notation correspond to the ﬁrst and the second camera dataset respectively and be label on x-axis in figs . 4and5 ) . figure 4shows the calculated absolute trajectory rmse ( ate , measure in metre ) and the relative pose rmse ( rpe , measure in metre per second ) metric range from 0 ( blue ) to 2 ( red ) for all method . dso 2 4 6 8 10experiment 0.511.52 eatemh01.0 mh01.1mh02.0 mh02.1 mh03.0mh03.1mh04.0mh04.1 mh05.0 mh05.1 v101.0v101.1v102.0v102.1v103.0v103.1v201.0v201.1v202.0v202.1v203.0v203.12 4 6 8 10experiment 0.20.40.60.81 erpeldso 2 4 6 8 100.511.52mh01.0 mh01.1 mh02.0 mh02.1mh03.0mh03.1mh04.0 mh04.1 mh05.0 mh05.1 v101.0 v101.1v102.0v102.1 v103.0 v103.1v201.0v201.1v202.0v202.1 v203.0 v203.12 4 6 8 100.20.40.60.81 orb-slam2 2 468 10experiment 0.20.40.60.811.2 eatemh01.0 mh01.1mh02.0mh02.1 mh03.0 mh03.1 mh04.0mh04.1 mh05.0 mh05.1 v101.0v101.1v102.0 v102.1v103.0v103.1 v201.0v201.1v202.0 v202.1v203.0 v203.12 4 6 8 10experiment 0.20.40.60.811.2 erpedyna-slam 2 468 10 0.20.40.60.811.2mh01.0 mh01.1 mh02.0 mh02.1mh03.0mh03.1 mh04.0 mh04.1mh05.0 mh05.1 v101.0 v101.1v102.0v102.1 v103.0 v103.1 v201.0v201.1v202.0v202.1 v203.0 v203.12 4 6 8 100.511.5 fig . 4 . color-coded evaluation result for each euroc dataset sequence : absolute tra- jectory rmse ( at e , m ) and relative pose rmse ( rp e , m/s ) . ( color ﬁgure online ) as the analysis demonstrate , that in term of the rpe , the measure of local accuracy , the direct method generally perform signiﬁcantly good than the feature-based one , but it be still diﬃcult for them to overcome a harsh envi- ronment with a lack of light and prevailing rotational movement ( over transla-tional movement ) , a show in mh.05 , v1.03 , and v2.03 sequence . in term of the ate , the feature-based method demonstrate a stable performance , even in the “ hard ” sequence . however , the accuracy of dynaslam be slightly low , comparison of ros-based monocular visual slam methods 229 compare to orb-slam2 , since dynaslam succeed in bootstrapping the sys- tem with a', '. however , the accuracy of dynaslam be slightly low , comparison of ros-based monocular visual slam methods 229 compare to orb-slam2 , since dynaslam succeed in bootstrapping the sys- tem with a dynamic content and always initialize quick than orb-slam2and thus have more frame to process ( and more room for accumulate error ) . figure 5demonstrates the calculated trajectory detail level for dso , ldso , orb-slam2 , and dynaslam method for each euroc dataset sequence . dso 2 46 8 10experiment 00.020.040.06 detail level ldsomh01.0 mh01.1 mh02.0 mh02.1mh03.0 mh03.1 mh04.0 mh04.1 mh05.0mh05.1 v101.0 v101.1 v102.0v102.1 v103.0 v103.1 v201.0 v201.1v202.0 v202.1 v203.0 v203.12 4 6 8 10experiment 00.020.040.06 detail levelorb-slam2 2 46 8 10 00.0050.01 dynaslammh01.0 mh01.1 mh02.0 mh02.1 mh03.0 mh03.1 mh04.0 mh04.1 mh05.0 mh05.1 v101.0 v101.1 v102.0 v102.1 v103.0v103.1 v201.0 v201.1 v202.0 v202.1 v203.0 v203.12 4 6 8 1000.0050.01 fig . 5 . color-coded trajectory detail level for each euroc dataset sequence . ( color ﬁgure online ) table 2 . median absolute trajectory rmse ( at e , m ) , relative pose rmse ( rp e , m/s ) & trajectory detail level ( detail ) for each euroc dataset sequence . sequence dso ldso orb-slam2 dynaslam at e rp e detail at e rp e detail at e rp e detail at e rp e detail mh.01 0.054 0.132 0.018 0.044 0.131 0.018 0.041 0.491 0.006 0.042 0.494 0.006 mh.02 0.063 0.134 0.025 0.044 0.139 0.025 0.035 0.458 0.006 0.036 0.465 0.007 mh.03 0.209 0.711 0.028 0.090 0.706 0.028 0.041 1.095 0.006 0.043 1.102 0.007 mh.04 0.173 0.632 0.022 0.136 0.642 0.022 0.074 0.560 0.009 0.076 0.568 0.011 mh.05 0.169 0.199 0.023 0.127 0.198 0.023 0.054 0.589 0.009 0.056 0.592 0.010 v1.01 0.104 0.088 0.023 0.099 0.089 0.023 0.054 0.454 0.005 0.054 0.459 0.006 v1.02 1.047 0.137 0.044 1.013 0.111 0.044 0.054 0.528 0.009 0.055 0.534 0.011 v1.03 0.584 0.334 0.057 0.607 0.375 0.057 0.091 0.409 0.009 0.097 0.411 0.010 v2.01 0.064 0.081 0.018 0.058 0.081 0.019 0.047 0.225 0.007 0.047 0.227 0.008 v2.02 0.162 0.306 0.037 0.106 0.281', '0.011 v1.03 0.584 0.334 0.057 0.607 0.375 0.057 0.091 0.409 0.009 0.097 0.411 0.010 v2.01 0.064 0.081 0.018 0.058 0.081 0.019 0.047 0.225 0.007 0.047 0.227 0.008 v2.02 0.162 0.306 0.037 0.106 0.281 0.038 0.051 0.508 0.009 0.053 0.552 0.009 v2.03 1.439 0.087 0.036 1.266 0.086 0.040 0.096 0.477 0.010 0.097 0.479 0.012 the obtain result suggest that the direct method typically distinguish more keyframes and , thus , have a good trajectory detail level , show a good local ( pose ) accuracy , compare to feature-based method . it be important to note the diﬀerence in the color scale of the feature-based method plot , whichis the diﬀerence in the detail level magnitude . dynaslam operate a slightly large amount of frame than orb-slam2 and have a slightly good trajectory detail level ( due to the quicker initialization ) .230 e. mingachev et al . table 3 . median alignment rmse ( align , m ) , absolute trajectory rmse ( at e , m ) , relative pose rmse ( rp e , m/s ) & trajectory detail level ( detail ) . metrics dso ldso orb-slam2 dynaslam align 0.8496 0.7769 5.7571 6.1891 at e 0.1683 0.1062 0.0507 0.0525 rp e 0.1373 0.1111 0.4765 0.4787 detail 0.0355 0.0376 0.0086 0.0099 3.4 summary tables 2and3summarize the calculated metric a a single median value for each euroc dataset sequence ( table 2 ) and the entire tum-mono and euroc datasets ( table 3 ) . the alignment error , accumulate drift , be in average 7.35 time low for direct method than for indirect : – ldso outperforms dso by 8.56 % – dynaslam be 6.98 % behind orb-slam2 the absolute trajectory error , global accuracy , for direct method be in average 2.66 time high than for indirect : – ldso outperforms dso by 36.89 % – dynaslam be 3.42 % behind orb-slam2 the relative position error , local accuracy , for direct method be in average 3.85 time low than for indirect : – ldso outperforms dso by 19.08 % – dynaslam be 0.46 % behind orb-slam2 the level of trajectory detail for direct method be in average 3.95 time high than for indirect : – ldso outperforms dso by 5.59 %', 'outperforms dso by 19.08 % – dynaslam be 0.46 % behind orb-slam2 the level of trajectory detail for direct method be in average 3.95 time high than for indirect : – ldso outperforms dso by 5.59 % , – dynaslam outperform orb-slam2 by 13.13 % . while it be expect that ldso should outperform it original source algo- rithm ( dso ) and experiment demonstrate it good performance with regardto all measure criterion , dynaslam and orb-slam2 have vary beneﬁts with respect to particular criterion , and this variety should be consider when select a slam algorithm for a speciﬁc task.comparison of ros-based monocular visual slam methods 231 4 conclusions and future work this paper present a comparative analysis of four publicly available ros-based monocular slam algorithm in term of the state estimation accuracy and the trajectory detail level . the analysis demonstrate that the direct method dso and ldso have a good accuracy while have entire photometric data availableand mainly focus on the local accuracy , which be also indirectly prove by the fact that they save and operate a relatively large amount of trajectory keyframes . for these reason , dso and ldso be more suitable for task involve a short-rangeoperation and require high accuracy in a local pose estimation , e.g.. , a 3d- reconstruction of an environment . at the same time , the feature-based method orb-slam2 and dynaslam outperform the direct one in term of the globalaccuracy , which , combine with an average trajectory detail level , make them a universal solution for most slam task - especially the one that require a long-range operating with stable and reliable result throughout an entiretrajectory . in task that involve dynamic object the accuracy of dynaslam will be signiﬁcantly high than orb-slam2 . the current study use hp omen 15-ce057ur laptop hardware . however , any slam method performance strongly correlate with available computa- tional resource . our ongoing work deal with expand the obtain resultsand compare the four algorithm ’ performance use', 'however , any slam method performance strongly correlate with available computa- tional resource . our ongoing work deal with expand the obtain resultsand compare the four algorithm ’ performance use several diﬀerent robot of the laboratory of intelligent robotic system [ 1,18 ] . we strongly believe that such comparison could be useful to the research community in order to have abetter perspective of how each metric varies depend on availability of real robot ’ onboard computational resource . acknowledgements . the report study be fund by the russian foundation for basic research ( rfbr ) , accord to the research project no . 19-58-70002 . the second author acknowledge the support by the research grant of kazan federal university.the forth and the ﬁfth author acknowledge the support of the japan science and technology agency , the jst strategic international collaborative research program , project no . 18065977 . the sixth author acknowledge the support of the nationalscience and technology development agency ( nstda ) , thailand , project id fda- co-2562-10058-th . special thanks to pal robotics for their kind professional support with tiago base robot software and hardware related issue . references 1 . bereznikov , d. , zakiev , a. : network failure detection and autonomous return for pmb-2 mobile robot . in : international conference on artiﬁcial life and robotics ( icarob ) , pp . 444–447 ( 2020 ) 2 . bescos , b. , f´ acil , j.m. , civera , j. , neira , j. : dynaslam : tracking , mapping , and inpainting in dynamic scene . ieee robot . autom . lett . 3 ( 4 ) , 4076–4083 ( 2018 ) 3 . bokovoy , a. , yakovlev , k. : sparse 3d point-cloud map upsampling and noise removal a a vslam post-processing step : experimental evaluation . in : ronzhin , a. , rigoll , g. , meshcheryakov , r . ( ed . ) icr 2018 . lncs ( lnai ) , vol . 11097 , pp . 23–33 . springer , cham ( 2018 ) . http : //doi.org/10.1007/978-3-319-99582-3 3232 e. mingachev et al . 4 . burri , m. , et al . : the euroc micro aerial vehicle datasets .', ') , vol . 11097 , pp . 23–33 . springer , cham ( 2018 ) . http : //doi.org/10.1007/978-3-319-99582-3 3232 e. mingachev et al . 4 . burri , m. , et al . : the euroc micro aerial vehicle datasets . int . j . robot . res . 35 ( 10 ) , 1157–1163 ( 2016 ) 5 . buyval , a. , afanasyev , i. , magid , e. : comparative analysis of ros-based monoc- ular slam method for indoor navigation . ninth international conference on machine vision , icmv , vol . 10341 , p. 103411k . international society for opticsand photonics ( 2017 ) 6 . carballo , a. , takeuchi , e. , takeda , k. : high density ground map use low bound- ary height estimation for autonomous vehicle . in : 21st international conferenceon intelligent transportation systems ( itsc ) , pp . 3811–3818 . ieee ( 2018 ) 7 . delmerico , j. , scaramuzza , d. : a benchmark comparison of monocular visual- inertial odometry algorithm for ﬂying robot . in : ieee 2018 international con- ference on robotics and automation , icra , pp . 2502–2509 . ieee ( 2018 ) 8 . dissanayake , m.g. , newman , p. , clark , s. , durrant-whyte , h.f. , csorba , m. : a solution to the simultaneous localization and map building ( slam ) problem . ieee trans . robot . autom . 17 ( 3 ) , 229–241 ( 2001 ) 9 . engel , j. , koltun , v. , cremers , d. : direct sparse odometry . ieee trans . pattern anal . mach . intell . 40 ( 3 ) , 611–625 ( 2017 ) 10 . engel , j. , sch¨ ops , t. , cremers , d. : lsd-slam : large-scale direct monocular slam . in : fleet , d. , pajdla , t. , schiele , b. , tuytelaars , t . ( ed . ) eccv 2014 . lncs , vol . 8690 , pp . 834–849 . springer , cham ( 2014 ) . http : //doi.org/10.1007/ 978-3-319-10605-2 54 11 . engel , j. , usenko , v. , cremers , d. : a photometrically calibrate benchmark for monocular visual odometry . arxiv preprint arxiv:1607.02555 ( 2016 ) 12 . gabdullin , a. , shvedov , g. , ivanou , m. , afanasyev , i. : analysis of onboard sensor- base odometry for a quadrotor uav in outdoor environment . in : international conference on artiﬁcial life and robotics (', ', shvedov , g. , ivanou , m. , afanasyev , i. : analysis of onboard sensor- base odometry for a quadrotor uav in outdoor environment . in : international conference on artiﬁcial life and robotics ( icarob ) ( 2018 ) 13 . gao , x. , wang , r. , demmel , n. , cremers , d. : ldso : direct sparse odometry with loop closure . in : 2018 ieee/rsj international conference on intelligent robotsand systems , iros , pp . 2198–2204 . ieee ( 2018 ) 14 . ibragimov , i.z. , afanasyev , i.m . : comparison of ros-based visual slam meth- od in homogeneous indoor environment . in : 2017 14th workshop on positioning , navigation and communications ( wpnc ) , pp . 1–6 . ieee ( 2017 ) 15 . lavrenov , r. , matsuno , f. , magid , e. : modiﬁed spline-based navigation : guaran- teed safety for obstacle avoidance . in : ronzhin , a. , rigoll , g. , meshcheryakov , r . ( ed . ) icr 2017 . lncs ( lnai ) , vol . 10459 , pp . 123–133 . springer , cham ( 2017 ) . http : //doi.org/10.1007/978-3-319-66471-2 14 16 . mart´ ınez-garc´ ıa , e.a. , rivero-ju´ arez , j. , torres-m´ endez , l.a. , rodas-osollo , j.e . : divergent trinocular vision observer design for extended kalman ﬁlter robot stateestimation . proc . inst . mech . eng . part i j. syst . control eng . 233 ( 5 ) , 524–547 ( 2019 ) 17 . martinez garcia , e.a . : 4wd robot posture estimation by radial multi-view visual odometry . instituto de ingenier´ ıa y tecnolog´ ıa ( 2018 ) 18 . moskvin , i. , lavrenov , r. , magid , e. , svinin , m. : modelling a crawler robot use wheel a pseudo-tracks : model complexity vs performance . in : 7th international conference on industrial engineering and applications ( iciea ) , pp . 235–239 . ieee ( 2020 ) 19 . mur-artal , r. , montiel , j.m.m. , tardos , j.d . : orb-slam : a versatile and accu- rate monocular slam system . ieee trans . robot . 31 ( 5 ) , 1147–1163 ( 2015 ) 20 . mur-artal , r. , tard´ o , j.d . : orb-slam2 : an open-source slam system for monocular , stereo , and rgb-d camera . ieee trans . robot . 33 ( 5 ) , 1255–1262 ( 2017 )', '5 ) , 1147–1163 ( 2015 ) 20 . mur-artal , r. , tard´ o , j.d . : orb-slam2 : an open-source slam system for monocular , stereo , and rgb-d camera . ieee trans . robot . 33 ( 5 ) , 1255–1262 ( 2017 ) comparison of ros-based monocular visual slam methods 233 21 . nagahama , k. , nishino , t. , kojima , m. , yamazaki , k. , okada , k. , inaba , m. : end point track for a move object with several attention region by composite vision system . in : international conference on mechatronics and automation , pp.590–596 . ieee ( 2011 ) 22 . rodriguez-telles , f.g. , mendez , l.a.t. , martinez-garcia , e.a . : a fast ﬂoor seg- mentation algorithm for visual-based robot navigation . in : 2013 international con-ference on computer and robot vision , pp . 167–173 . ieee ( 2013 ) 23 . saﬁn , r. , lavrenov , r. , tsoy , t. , svinin , m. , magid , e. : real-time video server implementation for a mobile robot . in : 2018 11th international conference on developments in esystems engineering ( dese ) , pp . 180–185 . ieee ( 2018 ) 24 . schubert , d. , goll , t. , demmel , n. , usenko , v. , st¨ uckler , j. , cremers , d. : the tum-mono vi benchmark for evaluate visual-inertial odometry . in : 2018 ieee/rsj international conference on intelligent robots and systems , iros , pp . 1680–1687 . ieee ( 2018 ) 25 . simakov , n. , lavrenov , r. , zakiev , a. , saﬁn , r. , mart´ ınez-garc´ ıa , e.a . : modeling usar map for the collection of information on the state of the environment . in : 2019 12th international conference on developments in esystems engineering ( dese ) , pp . 918–923 . ieee ( 2019 ) 26 . sturm , j. , engelhard , n. , endres , f. , burgard , w. , cremers , d. : a benchmark for the evaluation of rgb-d slam system . in : 2012 ieee/rsj internationalconference on intelligent robots and systems , pp . 573–580 . ieee ( 2012 ) view publication stats']",https://doi.org/10.48550/arXiv.2210.08117
9.pdf,"direct sparse odometry jakob engel , vladlen koltun , and daniel cremers abstract— direct sparse odometry ( dso ) be a visual odometry method base on a novel , highly accurate sparse and direct structure and motion formulation . it combine a fully direct probabilistic model ( minimize a photometric error ) with consistent , joint optimization of all model parameter , include geometry-represented a inverse depth in a reference frame-and camera motion . this be achieve in real time by omit the smoothness prior use in other direct method and instead sample pixel evenly throughout the image . since our method do not depend on keypoint detector or descriptor , it can naturally sample pixel from across all image region that have intensity gradient , include edge or smooth intensity variation on essentially featureless wall . the propose model integrate a full photometric calibration , account for exposure time , lens vignetting , and non-linear response function . we thoroughly evaluate our method on three different datasets comprise several hour of video . the experiment show that the presented approach signiﬁcantly outperform state-of-the-art direct and indirect method in a variety of real-world setting , both in term of track accuracy and robustness . index terms— visual odometry , slam , 3d reconstruction , structure from motion ç 1i ntroduction simultaneous localization and mapping ( slam ) and visual odometry ( vo ) be fundamental building block for many emerge technologies-from autonomous car and uavs to virtual and augmented reality . realtime method for slam and vo have make signiﬁcant progress in recent year . while for a long time the ﬁeld be dominate by fea- ture-based ( indirect ) method , in recent year a number of different approach have gain in popularity , namely direct anddense formulation . direct versus indirect . underlying all formulation be a probabilistic model that take noisy measurement yas input and compute an estimator xfor the unknown , hidden model parameter ( 3d world model and camera motion ) . typically a maximum likelihood approach be use , which ﬁnds the model parameter that maximize the probability of obtain the actual measurement , i.e. , x/c3 : ¼argmaxxpðyjxþ . indirect method then proceed in two step . first , the raw sensor measurement be pre-processed to generate an intermediate representation , solve part of the overall problem , such a compute the image coordinate of corre- sponding point . second , the computed intermediate value be interpret a noisy measurement yin a probabilistic model to estimate geometry and camera motion . note that the ﬁrst step be typically approach by extract and match a sparse set of keypoints—however other optionsexist , like establish correspondence in the form of dense , regularize optical ﬂow . it can also include method that extract and match parametric representation of other geometric primitive , such a line- or curve-segments . direct method skip this pre-computation step and directly use the actual sensor values-light receive from a certain direction over a certain time period-as measure- ments yin a probabilistic model . in the case of passive vision , the direct approach thus opti- mizes a photometric error , since the sensor provide photo- metric measurement . indirect method on the other hand optimize a geometric error , since the pre-computed values- point-positions or ﬂow-vecors-are geometric quantity . note that for other sensor modality like depth camera or laser scanner ( which directly measure geometric quantity ) direct formulation may also optimize a geometric error . dense versus sparse . sparse method use and reconstruct only a select set of independent point ( traditionally cor- ners ) , whereas dense method attempt to use and recon- struct all pixel in the 2d image domain . intermediate approach ( semi-dense ) refrain from reconstruct the complete surface , but still aim at use and reconstruct a ( largely connect and well-constrained ) subset . apart from the extent of the used image region however , a more fundamental—and consequential—difference lie in the addition of a geometry prior . in the sparse formulation , there be no notion of neighborhood , and geometry parameter ( key- point position ) be conditionally independent give the cam- era pose & intrinsics.1dense ( or semi-dense ) approach on the other hand exploit the connectedness of the used image region to formulate a geometry prior , typically favour smoothness . in fact , such a prior be necessarily require to make a dense world model observable from passive vision/c15j . engel be with oculus research , redmond , wa 98052 . he be with the department of computer science , technical university munich , m €unchen 80335 , germany . e-mail : jajuengel @ gmail.com . /c15v . koltun be with intel labs , santa clara , ca 95054-1549 . e-mail : vkoltun @ gmail.com . /c15d . cremers be with the department of computer science , technical university munich , m €unchen 80333 , germany . e-mail : cremers @ tum.de . manuscript receive 21 aug. 2016 ; revise 25 dec. 2016 ; accept 11 jan. 2017 . date of publication 11 apr . 2017 ; date of current version 13 feb. 2018 . recommended for acceptance by i. reid . for information on obtain reprint of this article , please send e-mail to : reprint @ ieee.org , and reference the digital object identiﬁer below . digital object identiﬁer no . 10.1109/tpami.2017.26585771.note that even though early ﬁltering-based method [ 4 ] , [ 12 ] keep track of point-point-correlations , these originate from marginalize camera pose , not from the model itself.ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 2018 611 0162-8828 /c2232017 ieee . translations and content mining be permit for academic research only . personal use be also permit , but republication/redistribu tion require ieee permission . see http : //www.ieee.org/publications_standards/publications/rights/index.html for more information.alone . in general , this prior be formulate directly in the form of an additional log-likelihood energy term [ 21 ] , [ 22 ] , [ 26 ] . note that the distinction between dense and sparse be not synonymous to direct and indirect —in fact , all four combina- tions exist : /c15sparse + indirect : this be the most widely-used formu- lation , estimate 3d geometry from a set of key- point-matches , thereby use a geometric error without a geometry prior . examples include the work of jin et al . [ 12 ] , monoslam [ 4 ] , ptam [ 16 ] , and orb-slam [ 20 ] . /c15dense + indirect : this formulation estimate 3d geometry from-or in conjunction with-a dense , regu- larized optical ﬂow ﬁeld , thereby combine a geo- metric error ( deviation from the ﬂow ﬁeld ) with a geometry prior ( smoothness of the ﬂow ﬁeld ) , exam- ples include [ 23 ] , [ 27 ] . /c15dense + direct : this formulation employ a photometric error as well a a geometric prior to estimate dense or semi-dense geometry . examples include dtam [ 21 ] , it precursor [ 26 ] , remode [ 22 ] , and lsd-slam [ 5 ] . /c15sparse + direct : this be the formulation propose in this paper . it optimize a photometric error deﬁned directly on the image , without incorporate a geo- metric prior . while we be not aware of any recent work use this formulation , a sparse and direct for- mulation be already propose by jin et al . in 2003 [ 13 ] . in contrast to their work however , which be base on an extended kalman ﬁlter , our method use a non-linear optimization framework . the moti- vation for explore the combination of sparse and direct be lay out in the following section . furthermore there be hybrid approach such a svo [ 9 ] , which use a direct formulation for initial alignment and to obtain correspondence , before switch to an indirect for- mulation for joint model optimization . 1.1 motivation the direct and sparse formulation for monocular visual odometry propose in this paper be motivate by the fol- lowing consideration . ( 1 ) direct : one of the main beneﬁts of keypoints be their ability to provide robustness to photometric and geo- metric distortion present in image take with off- the-shelf commodity camera . examples be auto- matic exposure change , non-linear response func- tions ( gamma correction/white-balancing ) , lens attenuation ( vignetting ) , de-bayering artefact , or even strong geometric distortion cause by a rolling shutter . this robustness or even invariance to photo- metric variation however come at the cost of dis- card potentially valuable information contain in exactly these variation . at the same time , for all use-cases mention in the introduction , million of device will be ( and already be ) equip with camera solely mean to provide data for computer vision algorithm , instead of capture image for human consumption . these camera should and will be design to provide a complete sensor model , and to capture data in a waythat best serve the processing algorithm : auto- exposure and gamma correction for instance be not unknown noise source , but feature that provide good image data—and that can be incorporate into the model , make the obtain data more informa- tive . since the direct approach model the full image formation process down to pixel intensity , it greatly beneﬁts from a more precise sensor model . one of the main beneﬁts of a direct formulation be that it do not require a point to be recognizable by itself , thereby allow for a more ﬁnely grained geometry representation ( pixelwise inverse depth ) . furthermore , we can sample from across all available data—including edge and weak intensity variations- generate a more complete model and lending more robustness in sparsely textured environment . ( 2 ) sparse : the main drawback of add a geometry prior be the introduction of correlation between geometry parameter , which render a statistically consistent , joint optimization in real time infeasible ( see fig . 2 ) . this be why exist dense or semi-dense approach ( a ) neglect or coarsely approximate cor- relation between geometry parameter ( orange ) , and / or between geometry parameter and camera pose ( green ) , and ( b ) employ different optimization method for the dense geometry part , such a a pri- mal-dual formulation [ 21 ] , [ 22 ] , [ 26 ] . in addition , the expressive complexity of today ’ s prior be limit : while they make the 3d reconstruc- tion denser , locally more accurate and more visually appeal , we find that prior can introduce a bias , and thereby reduce rather than increase long-term , large-scale accuracy . note that in time this may well change with the introduction of more realistic , unbi- ased prior learn from real-world data . fig . 1 . direct sparse odometry ( dso ) . 3d reconstruction and track tra- jectory for a 1:40 min video cycle around a building ( monocular visual odometry only ) . the bottom-left inset show a close-up of the start and end point , visualize the drift accumulate over the course of the trajec- tory . the bottom row show some video frames.612 ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 20181.2 contribution in this paper we propose a sparse and direct approach to mon- ocular visual odometry , an example reconstruction be show in fig . 1 . to our knowledge , it be the only fully direct method that jointly optimize the full likelihood for all involve model parameter , include camera pose , camera intrinsics , and geometry parameter ( inverse depth value ) . this be in con- trast to hybrid approach such a svo [ 9 ] , which revert to an indirect formulation for joint model optimization . optimization be perform in a sliding window , where old camera pose as well a point that leave the ﬁeld of view of the camera be marginalize , in a manner inspire by [ 17 ] . in contrast to exist approach , our method fur- ther take full advantage of photometric camera calibration , include lens attenuation , gamma correction , and know exposure time . this integrated photometric calibration fur- ther increase accuracy and robustness . our cpu-based implementation run in real time on a laptop computer . we show in extensive evaluation on three different datasets comprise several hour of video that it outperform other state-of-the-art approach ( direct and indirect ) , both in term of robustness and accuracy . with reduced setting ( less point and active keyframes ) , it even run at 5/c2real-time speed while still outperform state-of-the-art indirect method . on high , non-real-time setting in turn ( more point and active keyframes ) , it cre- ates semi-dense model similar in density to those of lsd- slam , but much more accurate . 2d irect sparse model our direct sparse odometry be base on continuous optimi- zation of the photometric error over a window of recent frame , take into account a photometrically calibrate model for image formation . in contrast to exist direct method , we jointly optimize for all involved parameter ( camera intrinsics , camera extrinsics , and inverse depth val- ues ) , effectively perform the photometric equivalent of windowed sparse bundle adjustment . we keep the geome- try representation employ by other direct approach , i.e. , 3d point be represent a inverse depth in a refer- ence frame ( and thus have one degree of freedom ) . notation . throughout the paper , bold lower-case letter ( x ) represent vector and bold upper-case letter ( h ) represent matrix . scalars will be represent by light lower-case letter ( t ) , function ( include image ) by light upper-case letter ( i ) . camera pose be represent a transformation matrix ti2se ( 3 ) , transform a point from the world frame into the camera frame . linearized pose-increments will be express a lie-algebra element xxi2se ( 3 ) , which-with a slight abuse of notation-we directly write a vector xxi2r6 . we further deﬁne the commonly use operator ( : se ( 3 ) /c2se ( 3 ) ! se ( 3 ) use a left-multi- plicative formulation , i.e. , xxi ( ti : ¼ebxxi/c1ti : ( 1 ) 2.1 calibration the direct approach comprehensively model the image for- mation process . in addition to a geometric camera model- which comprise the function that project a 3d point onto the 2d image—it be hence beneﬁcial to also consider a photo- metric camera model , which comprise the function that map real-world energy receive by a pixel on the sensor ( irradiance ) to the respective intensity value . note that for indirect method this be of little beneﬁt and hence widely ignore , a common feature extractor and descriptor be invariant ( or highly robust ) to photometric variation . 2.1.1 geometric camera calibration for simplicity , we formulate our method for the well-known pinhole camera model—radial distortion be remove in a pre- processing step . while for wide-angle camera this do reduce the ﬁeld of view , it allow comparison across method that only implement a limited choice of camera model . throughout this paper , we will denote projection by pc : r3 ! vand back-projection with p/c01 c : v/c2r ! r3 , where c denote the intrinsic camera parameter ( for the pinhole model these be the focal length and the principal point ) . note that analogously to [ 2 ] , our approach can be extend to other ( invertible ) camera model , although this do increase computational demand . 2.1.2 photometric camera calibration we use the image formation model use in [ 8 ] , which account for a non-linear response function g : r ! ½0 ; 255/c138 , as well a lens attenuation ( vignetting ) v : v ! ½0 ; 1/c138 . fig . 3 fig . 2 . sparse versus dense hessian structure . left : hessian structure of sparse bundle adjustment : since the geometry-geometry block be diago- nal , it can be solve efﬁciently use the schur complement . right : a geometry prior add ( partially unstructured ) geometry-geometry correlations—the result system be hence not only much large , but also becomes much hard to solve . for simplicity , we do not show the global camera intrinsic parameter . fig . 3 . photometric calibration . top : inverse response function g/c01and lens attenuation vof the camera use for fig . 1 . bottom : exposure tin millisecond for a sequence contain an indoor and an outdoor part . note how it vary by a factor of more than 500 , from 0.018 to 10.5 m . instead of treat these quantity a unknown noise source , we explicitly account for them in the photometric error model.engel et al . : direct sparse odometry 613shows an example calibration from the tum monovo data- set . the combined model be then give by iiðxþ¼g/c0 tivðxþbiðxþ/c1 ; ( 2 ) where biand iiare the irradiance and the observed pixel intensity in frame i , and tiis the exposure time . the model be apply by photometrically correct each video frame a very ﬁrst step , by compute i0 iðxþ : ¼tibiðxþ¼g/c01ðiiðxþþ vðxþ : ( 3 ) in the remainder of this paper , iiwill always refer to the photometrically correct image i0 i , except where otherwise state . 2.2 model formulation we deﬁne the photometric error of a point p2viin refer- ence frame ii , observe in a target frame ij , a the weighted ssd over a small neighborhood of pixel . our experiment have show that eight pixel , arrange in a slightly spread pattern ( see fig . 4 ) give a good trade-off between computa- tions require for evaluation , robustness to motion blur , and provide sufﬁcient information . note that in term of the contain information , evaluate the ssd over such a small neighborhood of pixel be similar to add ﬁrst- and second-order irradiance derivative constancy term ( in addition to irradiance constancy ) for the central pixel . let epj : ¼x p2npwpðij½p0/c138/c0bjþ/c0tjeaj tieai/c0 ii½p/c138/c0bi/c1/c13/c13/c13/c13/c13/c13/c13/c13 g ; ( 4 ) wherenpis the set of pixel include in the ssd ; ti ; tjthe exposure time of the image ii ; ij ; andk/c1kgthe huber norm . further , p0stands for the project point position of pwith inverse depth dp , give by p0¼pc/c0 rp/c01 cðp ; dpþþt/c1 ; ( 5 ) with rt 01/c20/c21 : ¼tjt/c01 i : ( 6 ) in order to allow our method to operate on sequence without know exposure time , we include an additional afﬁne brightness transfer function give by e/c0aiðii/c0biþ.n o t et h a t in contrast to most previous formulation [ 6 ] , [ 13 ] , the scalar factor e/c0aiis parametrized logarithmically . this both prevents it from become negative and avoids numerical issue aris- ing from multiplicative ( i.e. , exponentially increase ) drift.in addition to use robust huber penalty , we apply a gradient-dependent weighting wpgiven by wp : ¼c2 c2þkr iiðpþk2 2 ; ( 7 ) which down-weights pixel with high gradient . this weight function can be probabilistically interpret a add small , independent geometric noise on the project point position p0 , and immediately marginalize it-approx- imating small geometric error . to summarize , the error epj depend on the following variable : ( 1 ) the point ’ s inverse depth dp , ( 2 ) the camera intrinsics c , ( 3 ) the pose of the involved frame ti ; tj , and ( 4 ) their brightness transfer function parameter ai ; bi ; aj ; bj . the full photometric error over all frame and point be give by ephoto : ¼x i2fx p2pix j2obsðpþepj ; ( 8 ) where iruns over all frame f , pover all pointspiin frame i , a n d jover all frame obsðpþin which the point p be visible . fig . 5 show the result factor graph : the only difference to the classical reprojection error be the additional dependency of each residual on the pose of the host frame , i.e. , each term depend on two frame instead of only one . while this add off-diagonal entry to the pose-pose block of the hessian , it do not affect the sparsity pattern after application of the schur comple- ment to marginalize point parameter . the result sys- tem can thus be solve analogously to the indirect formulation . note that the jacobians with respect to the two frame ’ pose be linearly relate by the adjoint of their relative pose . in practice , this factor can then be p u l l e do u to ft h es u mw h e nc o m put the hessian or it schur complement , greatly r educe the additional com- putations cause by more v ariable dependency . if exposure time be know , we further add a prior pull- ing the afﬁne brightness transfer function to zero eprior : ¼x i2f/c21aa2 iþ/c21bb2 i/c0/c1 : ( 9 ) fig . 4 . residual pattern . patternnpused for energy computation . the bottom-right pixel be omit to enable sse-optimized processing . note that since we have 1 unknown per point ( it inverse depth ) , and do not use a regularizer , we require jnpj > 1in order for all model parameter to be well-constrained when optimize over only two frame . fig . 19 show an evaluation of how this pattern affect track accuracy . fig . 5 . factor graph for the direct sparse model . example with four key- frame and four point ; one in kf1 , two in kf2 , and one in kf4 . each energy term ( deﬁned in eq . ( 4 ) ) depend on the point ’ s host frame ( blue ) , the frame the point be observe in ( red ) , and the point ’ s inverse depth ( black ) . further , all term depend on the global camera intrinsics vector c , which be not shown.614 ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 2018if no photometric calibration be available , we set ti¼1and /c21a¼/c21b¼0 , a in this case they need to model the ( unknown ) change exposure time of the camera . as a side-note it should be mention that the ml estimator for a multiplica- tive factor a/c3¼argmaxap iðaxi/c0yiþ2is bias if both xi and yicontain noisy measurement ( see [ 7 ] ) ; cause ato drift in the unconstrained case /c21a¼0 . while this generally have little effect on the estimate pose , it may introduce a bias if the scene contain only few , weak intensity variation . point dimensionality . in the propose direct model , a point be parametrized by only one parameter ( the inverse depth in the reference frame ) , in contrast to three unknown a in the indirect model . to understand the reason for this difference , we ﬁrst note that in both case a 3d point be in fact an arbitrarily locate discrete sample on a continuous , real-world 3d surface . the difference then lie in the way this 2d location on the surface be deﬁned . in the indirect approach , it be implicitly deﬁned a the point , which ( project into an image ) generate a maximum in the used corner response function . this entail that both the surface , as well a the point ’ s location on the surface be unknowns , and need to be estimate . in our direct formulation , a point be simply deﬁned a the point , where the source pixel ’ s ray hit the surface , thus only one unknown remain . in addition to a reduced number of parameter , this naturally enable an inverse depth parametrization , which—in a gaussian framework— be well suit to represent uncertainty from stereo-based depth estimation , in particular for far-away point [ 3 ] . consistency . strictly speak , the propose direct sparse model do allow to use some observation ( pixel value ) multiple time , while others be not use at all . this be because—even though our point selection strategy attempt to avoid this by equally distribute point in space ( see section 3.2 ) —we allow point observation to overlap , and thus depend on the same pixel value ( s ) . this particularly happen in scene with little texture , where all point have to be choose from a small subset of textured image region . we however argue that this have negligible effect in practice , and—if desired—can be avoid by remove ( or down- weighting ) observation that use the same pixel value . 2.3 windowed optimization we follow the approach by leutenegger et al . [ 17 ] and opti- mize the total error ( 8 ) in a sliding window use the gauss-newton algorithm , which give a good trade-off between speed and ﬂexibility . for ease of notation , we extend the ( operator a deﬁned in ( 1 ) to all optimize parameters—for parameter otherthan se ( 3 ) pose it denote conventional addition . we will use zz2se ( 3 ) n/c2rmto denote all optimized variable , include camera pose , afﬁne brightness parameter , inverse depth value , and camera intrinsics . as in [ 17 ] , mar- ginalizing a residual that depend on a parameter in zzwill ﬁx the tangent space in which any future information ( delta-updates ) on that parameter be accumulate . we will denote the evaluation point for this tangent space with zz0 , and the accumulate delta-updates by xx2se ( 3 ) n/c2rm . the current state estimate be hence give by zz¼xx ( zz0 . fig . 6 vis- ualizes the relation between the different variable . gauss-newton optimization . we compute the gauss-new- ton system a h¼jtwj and b¼/c0jtwr ; ( 10 ) where w2rn/c2nis the diagonal matrix contain the weight , r2rnis the stacked residual vector , and j2rn/c2d be the jacobian of r. note that each point contribute jnpj¼8residuals to the energy . for notational simplicity , we will in the following consider only a single residual rk , and the associated row of the jacobian jk . during optimization—as well a when mar- ginalizing—residuals be always evaluate at the current state estimate , i.e. , rk¼rkðxx ( zz0þ ¼/c0 ij½p0ðti ; tj ; d ; cþ/c138/c0bj/c1 /c0tjeaj tieai/c0 ii½p/c138/c0bi/c1 ; ( 11 ) whereðti ; tj ; d ; c ; ai ; aj ; bi ; bjþ : ¼xx ( zz0are the current state variable the residual depend on . the jacobian jkis evalu- ated with respect to an additive increment toxx , i.e. , jk¼ @ rkððddþxxþ ( zz0þ @ dd : ( 12 ) it can be decompose a jk¼/c20 @ ij @ p0 | { z } ji @ p0ððddþxxþ ( zz0þ @ ddgeo|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ { zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ } jgeo ; @ rkððddþxxþ ( zz0þ @ ddphoto|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ { zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ } jphoto/c21 ; ( 13 ) where ddgeodenotes the “ geometric ” parameter ðti ; tj ; d ; cþ , and ddphoto denote the “ photometric ” parameter ðai ; aj ; bi ; bjþ . we employ two approximation , describe below . first , both jphotoandjgeoare evaluate at xx¼0 . this tech- nique be call “ first estimate jacobians ” [ 11 ] , [ 17 ] , and be require to maintain consistency of the system and prevent the accumulation of spurious information . in particular , in the presence of non-linear null-spaces in the energy ( in our formulation absolute pose and scale ) , add linearizations around different evaluation point eliminate these and thus slowly corrupt the system . in practice , this approxi- mation be very good , since jphoto , jgeoare smooth compare to the size of the increment xx . in contrast , jiis much less smooth , but do not affect the null-spaces . thus , it be evalu- ated at the current value for xx , i.e. , at the same point a the residual rk . we use centred difference to compute the image derivative at integer position , which be then bili- nearly interpolated.fig . 6 . windowed optimization . the red curve denote the parameter space , compose of non-euclidean camera pose in se ( 3 ) , and the remain euclidean parameter . the blue line corresponds to the tan- gent-space around zz0 , in which we ( 1 ) accumulate the quadratic margin- alization-prior on xx , and ( 2 ) compute gauss-newton step dd . for each parameter , the tangent space be ﬁxed as soon a that parameter become part of the marginalization term . note that while we treat all parameter equally in our notation , for euclidean parameter tangent- space and parameter-space coincide.engel et al . : direct sparse odometry 615second , jgeois assume to be the same for all residual belong to the same point , and evaluate only for the cen- ter pixel . again , this approximation be very good in practice . while it signiﬁcantly reduce the required computation , we have not observe a notable effect on accuracy for any of the used datasets . from the result linear system , an increment be com- put a dd¼h/c01band add to the current state xxnew ddþxx : ( 14 ) note that due to the first estimate jacobian approximation , a multiplicative formulation ( replace ðddþxxþ ( zz0with dd ( ðxx ( zz0þin ( 12 ) ) result in the exact same jacobian , thus a multiplicative update step xxnew logðdd ( exxþis equally valid . after each update step , we update zz0for all variable that be not part of the marginalization term , use zznew 0 xx ( zz0 and xx 0 . in practice , this include all depth value , as well a the pose of the new keyframe . each time a new keyframe be add , we perform up to 6 gauss-newton iter- ations , break early if ddis sufﬁciently small . we find that-since we never start far-away from the minimum—a levenberg-marquardt dampening ( which slow down convergence ) be not require . marginalization . when the active set of variable becomes too large , old variable be remove by marginalization use the schur complement . similar to [ 17 ] , we drop any residual term that would affect the sparsity pattern of h : when marginalize frame i , we ﬁrst marginalize all point inpi , as well a point that have not be observe in the last two keyframes . remaining observation of active point in frame iare drop from the system . marginalization proceeds a follow : let e0denote the part of the energy contain all residual that depend on state variable to be marginalize . we ﬁrst compute a gauss-newton approximation of e0around the current state estimate zz¼xx ( zz0 . this give e0ðxx ( zz0þ /c252ðxx/c0xx0þtbþðxx/c0xx0þthðxx/c0xx0þþc ¼2xxtðb/c0hxx0þ|ﬄﬄﬄﬄﬄﬄﬄ { zﬄﬄﬄﬄﬄﬄﬄ } ¼ : b0þxxthxxþðcþxxt 0hxx0/c0xxt 0bþ|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ { zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ } ¼ : c0 ; ( 15 ) where xx0denotes the current value ( evaluation point for r ) o f xx . the constant c ; c0can be drop , and h ; bare deﬁned a in ( 10 ) , ( 11 ) , ( 12 ) , and ( 13 ) . this be a quadratic function on xx , and we can apply the schur complement to marginalize a sub- set of variable . written a a linear system , it become haahab hbahbb/c20/c21xxa xxb/c20/c21 ¼b0 a b0 b '' # ; ( 16 ) where bdenotes the block of variable we would like to mar- ginalize , and athe block of variable we would like to keep . applying the schur complement yield dhaaxxa¼cb0 a , with dhaa¼haa/c0habh/c01 bbhba ( 17 ) cb0 a¼b0 a/c0habh/c01 bbb0 b : ( 18 ) the residual energy on xxacan hence be write a e0/c0 xxa ( ðzz0þa/c1 ¼2xxt acb0 aþxxt adhaaxxa : ( 19 ) this be a quadratic function on xxand can be trivially add to the full photometric error ephoto during all subsequent optimization and marginalization operation , replace the corresponding non-linear term . note that this require the tangent space for zz0to remain the same for all variable that appear in e0during all subsequent optimization and mar- ginalization step . 3v isual odometry front -end the front end be the part of the algorithm that /c15determines the set f ; pi , and obsðpþthat make up the error term of ephoto . it decide which point and frame be use , and in which frame a point be visi- ble-in particular , this include outlier removal and occlusion detection . /c15provides initialization for new parameter , require for optimize the highly non-convex energy func- tion ephoto . as a rule of thumb , a linearization of the image iis only valid in a 1-2 pixel radius ; hence all parameter involve in compute p0should be ini- tialized sufﬁciently accurately for p0to be off by no more than 1-2 pixel . /c15decides when a point/frame should be marginalize . as such , the front-end need to replace many operation that in the indirect setting be accomplish by keypoint detector ( determine visibility , point selection ) and ini- tialization procedure such a ransac . note that many procedure describe here be speciﬁc to the monocular case . for instance , use a stereo camera make obtain initial depth value more straightforward , while integration of an imu can signiﬁcantly robustify—or even directly pro- vide—a pose initialization for new frame . 3.1 frame management our method always keep a window of up to nfactive keyframes ( we use nf¼7 ) . every new frame be initially track with respect to these reference frame ( step 1 ) . it be then either discard or use to create a new keyframe ( step 2 ) . once a new keyframe-and respective new points- be create , the total photometric error ( 8 ) be optimize . afterwards , we marginalize one or more frame ( step 3 ) . step 1 : initial frame tracking . when a new keyframe be cre- ated , all active point be project into it and slightly dilate , create a semi-dense depth map . new frame be track with respect to only this frame use conventional two-frame direct image alignment , a multi-scale image pyra- mid and a constant motion model to initialize . fig . 7 show some examples—we find that further increase the den- sity have little to no beneﬁt in term of accuracy or robustness , while signiﬁcantly increase runtime . note that when down-scaling the image , a pixel be assign a depth value if at least one of the source pixel have a depth value a in [ 24 ] , signiﬁcantly increase the density on coarser resolution . if the ﬁnal rmse for a frame be more than twice that of the frame before , we assume that direct image alignment616 ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 2018failed and attempt to recover by initialize with up to 27 different small rotation in different direction . this recov- ery-tracking be do on the coarse pyramid level only , and take approximately 0.5 m per try . note that this ran- sac-like procedure be only rarely invoke , such a when the camera move very quickly or shakily . tightly integrat- ing an imu would likely render this unnecessary . step 2 : keyframe creation . similar to orb-slam , our strategy be to initially take many keyframes ( around 5-10 keyframes per second ) , and sparsify them afterwards by early marginalizing redundant keyframes . we combine three criterion to determine if a new keyframe be require : 1 ) new keyframes need to be create a the ﬁeld of view change . we measure this by the mean square optical ﬂow ( from the last keyframe to the late frame ) f : ¼ð1 npn i¼1kp/c0p0k2þ1 2during initial coarse tracking . 2 ) camera translation cause occlusion and dis- occlusion , which require more keyframes to be take ( even though fmay be small ) . this be mea- sured by the mean ﬂow without rotation , i.e. , ft : ¼ð1 npn i¼1kp/c0p0 tk2þ1 2 , where ptis the warped point position with r¼i3/c23 . 3 ) if the camera exposure time change signiﬁcantly , a new keyframe should be take . this be measure by the relative brightness factor between two frame a : ¼jlogðeaj/c0aitjt/c01 iþj . these three quantity can be obtain easily a a by- product of initial alignmen t. finally , a new keyframe be take if wffþwftftþwaa > t kf , w h e r e wf ; wft ; wapro- vide a relative weighting of these three indicator , and tkf¼1by default . step 3 : keyframe marginalization . our marginalization strategy be a follow ( let i1 ... inbe the set of active key- frame , with i1being the new and inbeing the old ) : 1 ) we always keep the late two keyframes ( i1andi2 ) . 2 ) frames with less than 5 percent of their point visible ini1are marginalize . 3 ) if more than nfframes be active , we marginalize the one ( exclude i1and i2 ) which maximize a “ distance score ” sðiiþ , compute a sðiiþ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ dði ; 1þp x j2½3 ; n/c138nfigðdði ; jþþ/c15þ/c01 ; ( 20 ) where dði ; jþis the euclidean distance between key- frame iiandij , and /c15a small constant . this score function be heuristically design to keep active key- frame well-distributed in 3d space , with more key- frame close to the most recent one . a keyframe be marginalize by ﬁrst marginalize all point represent in it , and then the frame itself , use the mar- ginalization procedure from section 2.3 . to preserve the sparsity structure of the hessian , all observation of still exist point in the frame be drop from the system . while this be clearly suboptimal ( in practice about half of all residual be drop for this reason ) , it allow to efﬁciently optimize the energy function . fig . 8 show an example of a scene , highlight the active set of point and frame . 3.2 point management most exist direct method focus on utilize as much image data a possible . to achieve this in real time , they accumulate early , sub-optimal estimate ( linearizations / depth triangulation ) , and ignore-or approximate-correla- tions between different parameter . in this work , we follow a different approach , and instead heavily sub-sample data to allow process it in real time in a joint optimization frame- work . in fact , our experiment show that image data be highly redundant , and the beneﬁt of simply use more data point quickly ﬂattens off . note that in contrast to indirect method , our direct framework still allow to sample from across all available data , include weakly textured or repetitive region and edge , which do provide a real beneﬁt ( see section 4 ) . we aim at always keep a ﬁxed number npof active point ( we use np¼2 ; 000 ) , equally distribute across space and active frame , in the optimization . in a ﬁrst step , we identify npcandidate point in each new keyframe ( step 1 ) . candidate point be not immediately add into the opti- mization , but instead be track individually in subsequent frame , generate a coarse depth value which will serve a initialization ( step 2 ) . when new point need to be add to fig . 8 . keyframe management . bottom : the six old keyframes in the optimization window , overlay with the point host in them ( already marginalize point be show in black ) . the top image show the full point cloud , as well a the position of all keyframes ( black camera frus- tum ) —active point and keyframes be show in red and blue respec- tively . the inlay show the newly add keyframe , overlay with all forward-warped active point , which will be use for initial alignment of subsequent frame . fig . 7 . example depth map use for initial frame track . the top row show the original image , the bottom row the color-coded depth map . since we aim at a ﬁxed number of point in the active optimization , they become more sparse in densely textured scene ( leave ) , while become similar in density to those of lsd-slam in scene where only few infor- mative image region be available to sample from ( right ) .engel et al . : direct sparse odometry 617the optimization , we choose a number of candidate point ( from across all frame in the optimization window ) to be activate , i.e. , add into the optimization ( step 3 ) . note that we choose npcandidates in each frame , however only keep npactive point across all active frame combine . this assure that we always have sufﬁcient candidate to acti- vate , even though some may become invalid a they leave the ﬁeld of view or be identiﬁed a outlier . step 1 : candidate point selection . our point selection strat- egy aim at select point that be ( 1 ) well-distributed in the image and ( 2 ) have sufﬁciently high image gradient magnitude with respect to their immediate surroundings . we obtain a region-adaptive gradient threshold by split the image into 32/c232blocks . for each block , we then com- pute the threshold a /c22gþgth , where /c22gis the median absolute gradient over all pixel in that block , and gtha global con- stant ( we use gth¼7 ) . to obtain an equal distribution of point throughout the image , we split it into d/c2dblocks , and from each block select the pixel with large gradient if it surpass the region-adaptive threshold . otherwise , we do not select a pixel from that block . we find that it be often beneﬁcial to also include some point with weak gradient from region where no high-gradient point be present , capture infor- mation from weak intensity variation originate for exam- ple from smoothly change illumination across white wall . to achieve this , we repeat this procedure twice more , with decreased gradient threshold and block-size 2dand 4d , respectively . the block-size dis continuously adapt such that this procedure generate the desired amount of point ( if too many point be create it be increase for the next frame , otherwise it be decrease ) . fig . 9 show the select point candidate for some example scene . note that for for candidate point selection , we use the raw image prior to photometric correction . step 2 : candidate point tracking . point candidate be track in subsequent frame use a discrete search along the epipolar line , minimize the photometric error ( 4 ) . from the best match we compute a depth and associate variance , which be use to constrain the search interval for the subsequent frame . this track strategy be inspire by lsd-slam . note that the compute depth only serve a initialization once the point be activate . step 3 : candidate point activation . after a set of old point be marginalize , new point candidate be activate to replace them . again , we aim at maintain a uniformspatial distribution across the image . to this end , we ﬁrst project all active point onto the most recent keyframe . we then activate candidate point which—also project into this keyframe—maximize the distance to any exist point ( require large distance for candidate create during the second or third block-run ) . fig . 7 show the result distri- bution of point in a number of scene . outlier and occlusion detection . since the available image data generally contain much more information than can be use in real time , we attempt to identify and remove poten- tial outlier as early a possible . first , when search along the epipolar line during candidate tracking , point for which the minimum be not sufﬁciently distinct be perma- nently discard , greatly reduce the number of false match in repetitive area . second , point observation for which the photometric error ( 4 ) surpass a threshold be remove . the threshold be continuously adapt with respect to the median residual in the respective frame . for “ bad ” frame ( e.g. , frame that contain a lot of motion blur ) , the threshold will be high , such that not all observation be remove . for good frame , in turn , the threshold will be low , a we can afford to be more strict . 4r esults in this section we will extensively evaluate our direct sparse mono-v oalgorithm ( dso ) . we both compare it to other monocular slam/vo method , as well a evaluate the effect of important design and parameter choice . we use three datasets for evaluation : ( 1 ) the tum monovo dataset [ 8 ] , which provide 50 pho- tometrically calibrated sequence , comprise 105 minute of video record in dozen of different envi- ronments , indoors and outdoors ( see fig . 11 ) . since the dataset only provide loop-closure-ground-truth fig . 9 . candidate selection . the top row show the original image , the bottom row show the point choose a candidate to be add to the map ( 2,000 in each frame ) . points select on the ﬁrst pas be show in green , those select on the second and third pas in blue and red respectively . green candidate be evenly spread across gradient-rich area , while point add on the second and third pas also cover region with very weak intensity variation , but be much sparser . fig . 10 . results on euroc mav ( top ) and icl_nuim ( bottom ) datasets . translational rmse after sim ( 3 ) alignment . rt ( dash ) denote hard- enforced real-time execution . further , we evaluate dso with low set- ting at 5 time real-time speed , and orb-slam when restrict local loop-closures to point that have be observe at least once within the lasttmax=10 s.618 ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 2018 ( allow to evaluate track accuracy via the accu- mulated drift after a large loop ) , we evaluate use thealignment error ( ealign ) a deﬁned in the respective publication . ( 2 ) the euroc mav dataset [ 1 ] , which contain 11 ste- reo-inertial sequence comprise 19 minute of video , record in three different indoor environ- ments . for this dataset , no photometric calibration or exposure time be available , hence we omit photo- metric image correction and set ( /c21a¼/c21b¼0 ) . we evaluate in term of the absolute trajectory error ( eate ) , which be the translational rmse after simð3þ alignment . for this dataset we crop the beginning of each sequence since they contain very shaky motion meant to initialize the imu biases—we only use the part of the sequence where the mav be in the air . ( 3 ) the icl-nuim dataset [ 10 ] , which contain eight ray- trace sequence comprise 4.5 minute of video , from two indoor environment . for this dataset , pho- tometric image correction be not require , and all exposure time can be set to t¼1 . again , we evalu- ate in term of the absolute trajectory error ( eate ) . methodology . we aim at an evaluation as comprehensive a possible give the available data , and thus run all sequence both forward and backwards , 5 time each ( to account for non-deterministic behaviour ) . for the euroc mav dataset we further run both the left and the right video separately . in total , this give 500 run for the tum- monovo dataset , 220 run for the euroc mav dataset , and 80 run for the icl-nuim dataset , which we run on 20 dedi- cated workstation . we remove the dependency on the host machine ’ s cpu speed by not enforce real-time execution , except where state otherwise : for orb-slam we play the video at 20 percent speed , whereas dso be run in a sequen- tialized , single-threaded implementation that run approxi- mately four time slow than real time . note that even though we do not enforce real-time execution for most of the experiment , we use the exact same parameter setting a for the real-time comparison . the result be summarize in the form of cumulative error plot ( see , e.g. , fig . 10 ) , which visualize for how manytracked sequence the respective error value ( eate/ealign ) be below a certain threshold ; 2thereby show both accuracy on sequence where a method work well , as well a robust- ness , i.e. , on how many sequence the method do not fail . the raw tracking result for all runs-as well a script to compute the ﬁgures-are provide in the supplementary material.3additional interesting analysis use the tum- monovo dataset—e.g. , the inﬂuence of the camera ’ s ﬁeld of view , the image resolution or the camera ’ s motion direc- tion-can be find in [ 8 ] . evaluated methods and parameter settings . we compare our method to the open-source implementation of ( monoc- ular ) orb-slam [ 20 ] . we also attempt to evaluate against the open-source implementation of lsd-slam [ 5 ] and svo [ 9 ] , however both method consistently fail on most of the sequence . a major reason for this be that they assume brightness constancy ( ignore exposure change ) , while both real-world datasets use contain heavy expo- sure variation . to facilitate a fair comparison and allow application of the loop-closure metric from the tum-monovo dataset , we disable explicit loop-closu re detection and re-localiza- tion for orb-slam . note that everything else ( include local and global ba ) remain unchanged , still allow orb-slam to detect incremental loop-closures that can be find via the co-visibility representation alone . all parameter be set to the same value across all sequence and datasets . the only exception be the icl-nuim data- set : for this dataset we set gth¼3f o rd s o , a n dl o w e rt h e fast threshold for orb-slam to 2 , which we find to give best result . 4.1 quantitative comparison fig . 10 show the absolute trajectory rmse eateon the euroc mav dataset and the icl-nuim dataset for both method ( if an algorithm get lose within a sequence , we set fig . 11 . tum mono-vo dataset . a single image from each of the 50 tum mono-vo dataset sequence ( s_01 to s_50 ) use for evaluation and parameter study , overlay with the predicted depth map from dso . the full dataset contains over 105 minute of video ( 190 ’ 000 frame ) . note the wide range of environment cover , range from narrow indoor corridores to wide outdoor area , include forest . 2.on default setting , we run each method 10 time forward and 10 time backwards , result in 1,000/440/160 run . the respective error plot summarize all these run , and be scale to ﬁt the 500/ 220/80 scale . 3.http : //vision.in.tum.de/dsoengel et al . : direct sparse odometry 619eate¼1 ) . fig . 12 show the alignment error ealign , as well a the rotation-drift erand scale-drift esfor the tum-monovo dataset . the full set of result for each evaluated trajectory be visualize in fig . 13 and fig . 14 . in addition to the non-real-time evaluation ( bold line ) , we evaluate both algorithm in a hard-enforced real-time set- ting on an intel i7-4910mq cpu ( dash line ) . in this mode , we enforce real-time by allow both orb-slam and dso to skip frame if track can not keep up-increasing the drift or potentially leading to complete loss of track . the direct , sparse approach clearly outperform orb-slam in accu- racy and robustness both on the tum-monovo dataset , as well a the synthetic icl_nuim dataset . on the euroc mav dataset , orb-slam achieve a good accuracy ( but low robustness ) . this be due to two major reason : ( 1 ) there be no photometric calibration available , and ( 2 ) the sequence contain many small loop or segment where the quadrocop- ter “ back-tracks ” the way it come , allow orb-slam ’ s local mapping component to implicitly close many small and some large loop , whereas our visual odometry formula- tion permanently marginalize all point and frame that leave the ﬁeld of view . we can validate this by prohibit orb-slam from match against any keypoints that have not be observe for more than tmax¼10s ( line with circle marker in fig . 10 ) : in this case , orb-slam perform similar to dso in term of accuracy , but be less robust . the slight difference in robustness for dso come from the fact that for real-time execution , track new frame and keyframe- creation be parallelize , thus new frame be track on the second-latest keyframe , instead of the late . in some rare cases—in particular during strong exposure changes-this cause initial image alignment to fail . to show the ﬂexibility of dso , we include result when run at 5 time the speed they be record at,4with reduce setting ( np¼800points , nf¼6active frame , 424/c2320image resolution , /c204gauss-newton iteration after a keyframe be create ) : even with such extreme set- ting , dso achieve very good accuracy and robustness on all three datasets . note that dso be design a a pure visual adometry while orb-slam constitute a full slam system , includ- ing loop-closure detection & correction and re-localiza- tion—all these additional ability be neglect or switch off in this comparison.fig . 12 . results on tum-monovo dataset . accumulated rotational drift erand scale drift esafter a large loop , as well a the alignment error a deﬁned in [ 8 ] . since esis a multiplicative factor , we aggregate e0 s¼maxðes ; e/c01 sþ . the solid line corresponds to sequentialized , non- real-time execution , the dashed line to hard enforced real-time process- ing . for dso , we also show result obtain at low parameter setting , run at 5 time real-time speed . fig . 13 . full evaluation result . all error value for the euroc mav data- set ( leave ) and the icl_nuim dataset ( right ) : each square correspond to the ( color-coded ) absolute trajectory error eateover the full sequence . we run each of the 11 + 8 sequence ( horizontal axis ) forward ( “ fwd ” ) and backwards ( “ bwd ” ) , 10 time each ( vertical axis ) ; for the euroc mav dataset we far use the left and the right image stream . fig . 10 show these error value aggregate a cumulative error plot ( bold , con- tinuous line ) . fig . 14 . full evaluation result . all error value for the tum-monovo dataset ( also see fig . 11 ) . each square correspond to the ( color-coded ) alignment error ealign , a deﬁned in [ 8 ] . we run each of the 50 sequence ( horizontal axis ) forward ( “ fwd ” ) and backwards ( “ bwd ” ) , 10 time each ( vertical axis ) . fig . 12 show all these error value aggregate a cumu- lative error plot ( bold , continuous line ) . 4.all image be load , decode , and pinhole-rectiﬁed beforehand.620 ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 2018runtime . the required compute depend both on the number of tracked frame , as well a the number of key- frame create ( i.e. , on how far the camera move ) . on the tum monovo dataset , the average single-threaded runtime for initial frame alignment and candidate point tracking ( perform for each frame ) be 18 m ( 6.5 m on “ reduce ” setting ) . creating a new keyframe take 143 m ( 43 m on “ reduce ” setting ) , also in a single thread . 4.2 parameter studies this section aim at evaluate a number of different param- eter and algorithm design choice , use the tum-monovo dataset . photometric calibration . we analyze the inﬂuence of pho- tometric calibration , verify that it in fact increase accu- racy and robustness for direct method . to this end , we incrementally disable the different component : 1 ) exposure ( blue ) : set ti¼1and/c21a¼/c21b¼0 . 2 ) vignette ( green ) : set vðxþ¼1 ( and 1. ) . 3 ) response ( yellow ) : set g/c01¼identity ( and 1-2. ) . 4 ) brightness constancy ( black ) : set /c21a¼/c21b¼1 , i.e. , disable afﬁne brightness correction ( and 1-3. ) . fig . 15 show the result . as expect , dso perform signiﬁ- cantly well with full photometric calibration , in particular compare to a basic brightness constancy assumption ( a use in many other direct or semi-direct approach like lsd-slam or svo ) . in turn , orb-slam performs worse when use photometrically calibrate image . in fact , the photometric calibration only affect the keypoint selection ( fast thresholding ) : for this step , use the originalimages—in particular with enabled gamma correction—lead to a good point distribution . note that we observe the same behaviour for dso , and thus also perform point selec- tion on the original image ( see section 3.2 ) . in all remain experiment we use the original image for orb-slam , and ( if available ) a full photometric calibration for dso . amount of data . we analyze the effect of change the amount of data use , by vary the number of active point np , as well a the number of frame in the active win- dow nf . note that increase nfallows to keep more obser- vations per point : for any point we only ever keep observation in active frame ; thus the number of observa- tions when marginalize a point be limit to nf ( see section 2.3 ) . fig . 16 summarize the result . we can observe that the beneﬁt of simply use more data quickly ﬂattens off after np¼500points . at the same time , the number of active frame have little inﬂuence after nf¼7 , while increas- ing the runtime quadratically . we further evaluate a ﬁxed- lag marginalization strategy ( i.e. , always marginalize the old keyframe , instead of use the propose distance score ) a in [ 17 ] : this perform signiﬁcantly bad . selection of data . in addition to evaluate the effect of the number of residual use , it be interest to look at which data be used—in particular since one of the main beneﬁts of a direct approach be the ability to sample from all point , instead of only use corner . to this end , we vary the gradi- ent threshold for point selection , gth ; the result be summa- rized in fig . 17 . while there seem to be a sweet spot around gth¼7 ( ifgthis too large , for some scene not enough well- distributed point be available to sample from—if it be too low , too much weight will be give to data with a low signal- to-noise ratio ) , the overall impact be relatively low . more interestingly , we analyse the effect of only use cor- ners , by restrict point candidate to fast corner only . we can clearly see that only use corner signiﬁcantly decrease performance . note that for low fast threshold , many false “ corner ” will be detect along edge , which our method can still use , in contrast to indirect method for which such point will be outlier . in fact , orb-slam achieve best performance use the default threshold of 20 . number of keyframes . we analyze the number of key- frame take by vary tkf ( see section 3.1 ) . for each value oftkfwe give the resulting average number of keyframes per second ; the default set tkf¼1results in eight key- frame per second , which be easily achieve in real time.fig . 15 . photometric calibration . errors on the tum-monovo dataset for orb-slam and dso , when incrementally disable photometric calibra- tion . while dso a a direct method clearly beneﬁts from photometric cal- ibration , orb-slam a an indirect approach performs well on the original image . we therefore use the original image for all other orb- slam evaluation . fig . 16 . amount of data use . errors on the tum-monovo dataset , when change the size of the optimization window ( top ) and the number of point ( bottom ) . using more than np¼500points or nf¼7active frame have only marginal impact . note that a real-time default setting , we use np¼2 ; 000andnf¼7 , mainly to obtain dense reconstructions.fig . 17 . selection of data use . errors on the tum-monovo dataset , when change the type of data use . left : errors for different gradient threshold gth , which seem to have a limited impact on the algorithms accuracy . right : errors when only use fast corner , at different threshold . using only fast corner signiﬁcantly reduce accuracy and robustness , show that the ability to use data from edge and weakly textured surface do have a real beneﬁt.engel et al . : direct sparse odometry 621the result be summarize in fig . 18 . taking too few key- frame ( less than 4 per second ) reduce the robustness , mainly in situation with strong occlusion / dis-occlusions , e.g. , when walk through door . taking too many key- frame , on the other hand ( more than 15 per second ) , decrease accuracy . this be because take more keyframes cause them to be marginalize earlier ( since nfis ﬁxed ) , thereby accumulate linearizations around early ( and less accurate ) linearization point . residual pattern . we test different residual pattern for np , cover small or large area . the result be show in fig . 19 . 4.3 geometric versus photometric noise study the fundamental difference between the propose direct model and the indirect model be the noise assumption . the direct approach model photometric noise , i.e. , additive noise on pixel intensity . in contrast , the indirect approach model geometric noise , i . e . , a d d i t i v en o i s eo nt h e ðu ; vþ-position of a point in the image plane , assume that keypoint descriptor be robust to photometric noise . it therefore come at no surprise that the indirect approach be signiﬁcantly more robust to geometric noise in the data . in turn , the direct approach performs well in the presence of strong photometric noise—which key- point-descriptors ( operate on a purely local level ) fail to ﬁ l t e ro u t .w ev e r i f yt h i sb ya nalyzing track accuracy on the tum-monovo dataset , w hen artiﬁcially add ( a ) geometric noise , and ( b ) photometric noise to the image . geometric noise . for each frame , we separately generate a low-frequency random ﬂow-map ng : v ! r2by up- sample a 3/c23grid ﬁlled with uniformly distribute ran- dom value from ½/c0dg ; dg/c1382 ( use bicubic interpolation ) .we then perturb the original image by shift each pixel xbyngðxþ i0 gðxþ : ¼iðxþngðxþþ : ( 21 ) this procedure simulate noise originate from ( unmod- eled ) roll shutter or inaccurate geometric camera calibra- tion . fig . 20 visualizes an example of the result noise pattern , as well a the accuracy of orb-slam and dso for different value of dg . as expect , we can clearly observe how dso ’ s performance quickly deteriorate with add geometric noise , whereas orb-slam be much less affected . this be because the ﬁrst step in the indirect pipeline— keypoint detection and extraction—is not affect by low- frequency geometric noise , a it operate on a purely local level . the second step then optimize a geometric noise model-which not surprisingly deal well with geometric noise . in the direct approach , in turn , geometric noise be not model , and thus have a much more severe effect—in fact , fordg > 1:5there likely exist no state for which all resid- uals be within the validity radius of the linearization of i ; thus optimization fail entirely ( which can be alleviate by use a coarser pyramid level ) . note that this result also suggest that the propose direct model be more susceptible to inaccurate intrinsic camera calibration than the indirect approach—in turn , it may beneﬁt more from accurate , non- parametric intrinsic calibration . photometric noise . for each frame , we separately generate a high-frequency random blur-map np : v ! r2by up- sample a 300/c2300grid ﬁlled with uniformly distribute random value in ½/c0dp ; dp/c1382 . we then perturb the original image by add anisotropic blur with standard deviation npðxþto pixel x i0 pðxþ : ¼z r2fðdd ; npðxþ2þiðxþddþddd ; ( 22 ) where fð/c1 ; npðxþ2þdenotes a 2d gaussian kernel with stan- dard deviation npðxþ . fig . 21 show the result . we can observe that dso be slightly more robust to photometric noise than orb-slam—this be because ( purely local ) key- point matching fails for high photometric noise , whereas a fig . 20 . geometric noise . effect of apply low-frequency geometric noise to the image , simulate geometric distortion such a a rolling shutter ( eval- uated on the tum-monovo dataset ) . the top row show an example image with dg¼2 . while the effect be hardly visible to the human eye ( observe that the close-up be slightly shift ) , it have a severe impact on slam accuracy , in particular when use a direct model . note that the distortion cause by a standard rolling shutter camera easily surpass dg¼3 . fig . 19 . residual pattern . errors on the tum-monovo dataset for some of the evaluated pattern np . using only a 3/c23neighborhood seem to perform slightly worse—using more than the propose 8-pixel pattern however seem to have little beneﬁt—at the same time , use a large neighbourhood increase the computational demand . note that these result may vary with low-level property of the use camera and lens , such a the point spread function.fig . 18 . number of keyframes . errors on the tum-monovo dataset , when change the number of keyframes take via the threshold tkf.622 ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 2018joint optimization of the photometric error well overcome the introduced distortion . to summarize : while the direct approach outperform the indirect approach on well-calibrated data , it be ill-suited in the presence of strong geometric noise , e.g. , originate from a roll shutter or inaccurate intrinsic calibration . in practice , this make the indirect model superior for smart- phone or off-the-shelf webcam , since these be design to capture video for human consumption-prioritizing reso- lution and light-sensitivity over geometric precision . in turn , the direct approach offer superior performance on data cap- tured with dedicated camera for machine-vision , since these put more importance on geometric precision , rather than capture appeal image for human consumption . note that this can be resolve by tightly integrate the roll shutter into the model , a do , e.g. , in [ 15 ] , [ 18 ] , [ 19 ] . 4.4 qualitative results in addition to accurate camera tracking , dso compute 3d point on all gradient-rich area , include edges—resultingin point-cloud reconstruction similar to the semi-dense reconstruction of lsd-slam . the density then directly correspond to how many point we keep in the active win- dow np . fig . 23 show some example . fig . 22 show three more scene ( one from each dataset ) , together with some corresponding depth map . note that our approach be able to track through scene with very little texture , whereas indirect approach fail . all reconstruction show be sim- ply accumulate from the odometry , without integrate loop-closures . see the supplementary video for more quali- tative result , available online . failure modes . as for all monocular method , the pre- dominant cause of failure be degenerate motion in the form of almost-pure rotation , since no new point can be triangulate . this issue be be pliﬁed since dso be design a pure visual odometry , and do not reuse point once they leave the ﬁeld of view . in contrast , a full slam for- mulation ( a use by orb-slam ) allow to recycle previ- ously triangulated point once they re-enter the ﬁeld of fig . 21 . photometric noise . effect of apply high-frequent , non-isotropic blur to the image , simulate photometric noise ( evaluate on the tum- monovo dataset ) . the top row show an example image with dp¼6 , the effect be clearly visible . since the direct approach model a photometric error , it be more robust to this type of noise than indirect method . fig . 23 . point density . 3d point cloud and some coarse depth map , i.e. , the most recent keyframe with all npactive point project into it ) for np¼500 ( top ) , np¼2 ; 000 ( middle ) , and np¼10 ; 000 ( bottom ) . fig . 22 . qualitative example . one scene from each dataset ( leave to right : v2_01_easy [ 1 ] , seq_38 [ 8 ] and ofﬁce_1 [ 10 ] ) , compute in real time with default setting . the bottom show some corresponding ( sparse ) depth maps-some scene contain very little texture , make them very challenge for indirect approaches.engel et al . : direct sparse odometry 623view ( implicit loop-closures a s discuss in section 4.1 ) , and thereby bridge over some period of degenerate cam- era motion . note that this issue stem from the employ windowed optimization stra tegy ( section 2.3 ) and be not inherent to the propose direct sparse model formulation ( section 2.2 ) . we have not observe signiﬁcant issue with non-lam- bertian surface , likely due to the concentration on high-gra- dient point . however , the propose direct model will fail when the scene light change drastically , e.g. , in the pres- ence of move light source or when the time of day change . while this be typically unproblematic for short- term visual odometry , it need to be address to facilitate life-long mapping , for instance by change the energy ( 4 ) to penalize gradient direction or magnitude difference instead of absolute intensity . 5c onclusion we have present a novel direct and sparse formulation for structure from motion . it combine the beneﬁts of direct method ( seamless ability to use & reconstruct all point instead of only corner ) with the ﬂexibility of sparse approach ( efﬁcient , joint optimization of all model param- eters ) . this be possible in real time by omit the geometric prior use by other direct method , and instead evaluate the photometric error for each point over a small neighbor- hood of pixel , to well-constrain the overall problem . fur- thermore , we incorporate full photometric calibration , complete the intrinsic camera model that traditionally only reﬂects the geometric component of the image forma- tion process . we have implement our direct & sparse model in the form of a monocular visual odometry algorithm , incremen- tally marginalize / eliminate old state to maintain real-time performance . to this end we have develop a front-end that perform data-selection and provide accu- rate initialization for optimize the highly non-convex energy function . our comprehensive evaluation on several hour of video show the superiority of the present for- mulation relative to state-of-the-art indirect method . we furthermore present an exhaustive parameter study , indi- cating that ( 1 ) simply use more data do not increase track accuracy ( although it make the 3d model denser ) , ( 2 ) use all point instead of only corner do provide a real gain in accuracy and robustness , and ( 3 ) incorporate photometric calibration do increase perfor- mance , in particular compare to the basic “ brightness constancy ” assumption . we have also show experimentally that the indirect approach—modeling a geometric error—is much more robust to geometric noise , e.g. , originate from a poor intrin- sic camera calibration or a rolling shutter . the direct approach be in turn more robust to photometric noise , and achieve superior accuracy on well-calibrated data . we believe this to be one of the main explanation for the recent revival of direct formulation after a dominance of indirect approach for more than a decade : for a long time , the pre- dominant source of digital image data be camera , which originally be design to capture image for human view- ing ( such a off-the-shelf webcam or integrate smartphonecameras ) . in this setting , the strong geometric distortion cause by roll shutter and imprecise lens favor the indirect approach . in turn , with 3d computer vision becom- ing an integral part of mass-market product ( include autonomous car and drone , as well a mobile device for vr and ar ) , camera be be develop speciﬁcally for this purpose , feature global shutter , precise lens , and high frame-rates—which allows direct formulation to real- ize their full potential . since the structure of the propose direct sparse energy formulation be the same a that of indirect method , it can be integrate with other optimization framework like ( double-windowed ) bundle adjustment [ 25 ] or incremental smoothing and mapping [ 14 ] . the main challenge here be the greatly increased degree of non-convexity compare to the indirect model , which originate from the inclusion of the image in the error function-this be likely to restrict the use of our model to video processing . acknowledgments this work be support through the erc consolidator grant “ 3d reloaded ” and through a google faculty research award . references [ 1 ] m. burri , et al. , “ the euroc micro aerial vehicle datasets , ” int . j . robot . res . , vol . 35 , pp . 1157–1163 , 2016 . [ 2 ] d. caruso , j. engel , and d. cremers , “ large-scale direct slam for omnidirectional camera , ” in proc . int . conf . intell . robot syst . , 2015 , pp . 141–148 . [ 3 ] j. civera , a. davison , and j. montiel , “ inverse depth parametriza- tion for monocular slam , ” ieee trans . robot . , vol . 24 , no . 5 , pp . 932–945 , oct. 2008 . [ 4 ] a. davison , i. reid , n. molton , and o. stasse , “ monoslam : real- time single camera slam , ” ieee trans . pattern anal . mach . intell . , vol . 29 , no . 6 , pp . 1052–1067 , jun . 2007 . [ 5 ] j. engel , t. sch €ops , and d. cremers , “ lsd-slam : large-scale direct monocular slam , ” in proc . eur . conf . comput . vis . , 2014 , pp . 834–849 . [ 6 ] j. engel , j. stueckler , and d. cremers , “ large-scale direct slam with stereo camera , ” in proc . ieee/rsj int . conf . intell . robot syst . , 2015 , pp . 1935–1942 . [ 7 ] j. engel , j. sturm , and d. cremers , “ scale-aware navigation of a low-cost quadrocopter with a monocular camera , ” robot . auton . syst. , vol . 62 , no . 11 , pp . 1646–1656 , 2014 . [ 8 ] j. engel , v. usenko , and d. cremers , “ a photometrically calibrate benchmark for monocular visual odometry , ” arxiv:1607.02555 , 2016 . [ 9 ] c. forster , m. pizzoli , and d. scaramuzza , “ svo : fast semi-direct monocular visual odometry , ” in proc . ieee int . conf . robot . autom . , 2014 , pp . 15–22 . [ 10 ] a. handa , t. whelan , j. mcdonald , and a. davison , “ a bench- mark for rgb-d visual odometry , 3d reconstruction and slam , ” inproc . ieee int . conf . robot . autom . , 2014 , pp . 1524–1531 . [ 11 ] g. p. huang , a. i. mourikis , and s. i. roumeliotis , “ a ﬁrst-esti- mate jacobian ekf for improve slam consistency , ” in proc . ieee/asme int . symp . exp . robot . , 2008 , pp . 619–624 . [ 12 ] h. jin , p. favaro , and s. soatto , “ real-time 3-d motion and struc- ture of point feature : front-end system for vision-based control and interaction , ” in proc . ieee conf . comput . vis . pattern recognit . , 2000 , pp . 778–779 . [ 13 ] h. jin , p. favaro , and s. soatto , “ a semi-direct approach to struc- ture from motion , ” visual comput . , vol . 19 , no . 6 , pp . 377–394 , 2003 . [ 14 ] m. kaess , h. johannsson , r. roberts , v. ila , j. leonard , and f. dellaert , “ isam2 : incremental smoothing and mapping use the bayes tree , ” int . j . robot . res . , vol . 31 , no . 2 , pp . 217–236 , feb. 2012 . [ 15 ] c. kerl , j. stueckler , and d. cremers , “ dense continuous-time tracking and map with rolling shutter rgb-d camera , ” in proc . ieee int . conf . comput . vis . , 2015 , pp . 2264–2272.624 ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 2018 [ 16 ] g. klein and d. murray , “ parallel tracking and mapping for small ar workspace , ” in proc . 6th ieee acm int . symp . mixed aug- mented reality , 2007 , pp . 225–234 . [ 17 ] s. leutenegger , s. lynen , m. bosse , r. siegwart , and p. furgale , “ keyframe-based visual–inertial odometry use nonlinear opti- mization , ” int . j . robot . res . , vol . 34 , no . 3 , pp . 314–334 , 2015 . [ 18 ] m. li , b. kim , and a. mourikis , “ real-time motion estimation on a cellphone use inertial sensing and a rolling-shutter camera , ” in proc . ieee int . conf . robot . autom . , 2013 , pp . 4712–4719 . [ 19 ] s. lovegrove , a. patron-perez , and g. sibley , “ spline fusion : a continuous-time representation for visual-inertial fusion with application to roll shutter camera , ” in proc . british mach . vis . conf . , 2013 , pp . 93.1–93.12 . [ 20 ] r. mur-artal , j. montiel , and j. tardos , “ orb-slam : a versatile and accurate monocular slam system , ” ieee trans . robot . , vol . 31 , no . 5 , pp . 1147–1163 , oct. 2015 . [ 21 ] r. newcombe , s. lovegrove , and a. davison , “ dtam : dense tracking and mapping in real-time , ” in proc . ieee int . conf . com- put . vis . , 2011 , pp . 2320–2327 . [ 22 ] m. pizzoli , c. forster , and d. scaramuzza , “ remode : probabilis- tic , monocular dense reconstruction in real time , ” in proc . ieee int . conf . robot . autom . , 2014 , pp . 2609–2616 . [ 23 ] r. ranftl , v. vineet , q. chen , and v. koltun , “ dense monocular depth estimation in complex dynamic scene , ” in proc . ieee conf . comput . vis . pattern recognit . , 2016 , pp . 4058–4066 . [ 24 ] t. sch €ops , j. engel , and d. cremers , “ semi-dense visual odometry for ar on a smartphone , ” in proc . ieee int . symp . mixed augmented reality , 2014 , pp . 145–150 . [ 25 ] h. strasdat , a. j. davison , j. m. m. montiel , and k. konolige , “ double window optimisation for constant time visual slam , ” in proc . ieee int . conf . comput . vis . , 2011 , pp . 2352–2359 . [ 26 ] j. st €uhmer , s. gumhold , and d. cremers , “ real-time dense geom- etry from a handheld camera , ” in proc . 32nd dagm conf . pattern recognit . , 2010 , pp . 11–20 . [ 27 ] l. valgaerts , a. bruhn , m. mainberger , and j. weickert , “ dense versus sparse approach for estimate the fundamental matrix , ” int . j. comput . vis . , vol . 96 , no . 2 , pp . 212–234 , 2012 . jakob engel receive the master ’ s degree in computer science from the technical university of munich , in 2011 , where he also spend 2012 to 2016 a full-time phd degree at the chair for computer vision and pattern recognition . he be a senior research scientist with oculus . in 2015 , he become a google phd fellow . in 2015 , he spend half a year at the intel visual computing lab . he join oculus research in july 2016 . vladlen koltun receive the phd degree in 2002 for new result in theoretical computational geome- try , spend three year at uc berkeley a a postdoc in the theory group , and join the stanford com- puter science faculty , in 2005 a a theoretician . he be the director of the intel visual computing lab . he switch to research in visual computing in 2007 and join intel a a principal researcher in 2015 to establish the visual computing lab . daniel cremers receive the phd degree in com- puter science from the university of mannheim , germany . subsequently , he spend two year a a postdoctoral researcher with ucla and one year a a permanent researcher at siemens corporate research , princeton , new jersey . from 2005 until 2009 , he be associate professor with the univer- sity of bonn , germany . since 2009 he hold the chair for computer vision and pattern recognition with technical university of munich . in 2016 , he receive the gottfried-wilhelm leibniz award , the big award in german academia . `` for more information on this or any other computing topic , please visit our digital library at www.computer .org/publications/dlib.engel et al . : direct sparse odometry 625","['direct sparse odometry jakob engel , vladlen koltun , and daniel cremers abstract— direct sparse odometry ( dso ) be a visual odometry method base on a novel , highly accurate sparse and direct structure and motion formulation . it combine a fully direct probabilistic model ( minimize a photometric error ) with consistent , joint optimization of all model parameter , include geometry-represented a inverse depth in a reference frame-and camera motion . this be achieve in real time by omit the smoothness prior use in other direct method and instead sample pixel evenly throughout the image . since our method do not depend on keypoint detector or descriptor , it can naturally sample pixel from across all image region that have intensity gradient , include edge or smooth intensity variation on essentially featureless wall . the propose model integrate a full photometric calibration , account for exposure time , lens vignetting , and non-linear response function . we thoroughly evaluate our method on three different datasets comprise several hour of video . the experiment show that the presented approach signiﬁcantly outperform state-of-the-art direct and indirect method in a variety of real-world setting , both in term of track accuracy and robustness . index terms— visual odometry , slam , 3d reconstruction , structure from motion ç 1i ntroduction simultaneous localization and mapping ( slam ) and visual odometry ( vo ) be fundamental building block for many emerge technologies-from autonomous car and uavs to virtual and augmented reality . realtime method for slam and vo have make signiﬁcant progress in recent year . while for a long time the ﬁeld be dominate by fea- ture-based ( indirect ) method , in recent year a number of different approach have gain in popularity , namely direct anddense formulation . direct versus indirect . underlying all formulation be a probabilistic model that take noisy measurement yas input and compute an estimator xfor the unknown , hidden model parameter ( 3d world model and camera', 'indirect . underlying all formulation be a probabilistic model that take noisy measurement yas input and compute an estimator xfor the unknown , hidden model parameter ( 3d world model and camera motion ) . typically a maximum likelihood approach be use , which ﬁnds the model parameter that maximize the probability of obtain the actual measurement , i.e. , x/c3 : ¼argmaxxpðyjxþ . indirect method then proceed in two step . first , the raw sensor measurement be pre-processed to generate an intermediate representation , solve part of the overall problem , such a compute the image coordinate of corre- sponding point . second , the computed intermediate value be interpret a noisy measurement yin a probabilistic model to estimate geometry and camera motion . note that the ﬁrst step be typically approach by extract and match a sparse set of keypoints—however other optionsexist , like establish correspondence in the form of dense , regularize optical ﬂow . it can also include method that extract and match parametric representation of other geometric primitive , such a line- or curve-segments . direct method skip this pre-computation step and directly use the actual sensor values-light receive from a certain direction over a certain time period-as measure- ments yin a probabilistic model . in the case of passive vision , the direct approach thus opti- mizes a photometric error , since the sensor provide photo- metric measurement . indirect method on the other hand optimize a geometric error , since the pre-computed values- point-positions or ﬂow-vecors-are geometric quantity . note that for other sensor modality like depth camera or laser scanner ( which directly measure geometric quantity ) direct formulation may also optimize a geometric error . dense versus sparse . sparse method use and reconstruct only a select set of independent point ( traditionally cor- ners ) , whereas dense method attempt to use and recon- struct all pixel in the 2d image domain . intermediate approach ( semi-dense ) refrain from reconstruct', 'independent point ( traditionally cor- ners ) , whereas dense method attempt to use and recon- struct all pixel in the 2d image domain . intermediate approach ( semi-dense ) refrain from reconstruct the complete surface , but still aim at use and reconstruct a ( largely connect and well-constrained ) subset . apart from the extent of the used image region however , a more fundamental—and consequential—difference lie in the addition of a geometry prior . in the sparse formulation , there be no notion of neighborhood , and geometry parameter ( key- point position ) be conditionally independent give the cam- era pose & intrinsics.1dense ( or semi-dense ) approach on the other hand exploit the connectedness of the used image region to formulate a geometry prior , typically favour smoothness . in fact , such a prior be necessarily require to make a dense world model observable from passive vision/c15j . engel be with oculus research , redmond , wa 98052 . he be with the department of computer science , technical university munich , m €unchen 80335 , germany . e-mail : jajuengel @ gmail.com . /c15v . koltun be with intel labs , santa clara , ca 95054-1549 . e-mail : vkoltun @ gmail.com . /c15d . cremers be with the department of computer science , technical university munich , m €unchen 80333 , germany . e-mail : cremers @ tum.de . manuscript receive 21 aug. 2016 ; revise 25 dec. 2016 ; accept 11 jan. 2017 . date of publication 11 apr . 2017 ; date of current version 13 feb. 2018 . recommended for acceptance by i. reid . for information on obtain reprint of this article , please send e-mail to : reprint @ ieee.org , and reference the digital object identiﬁer below . digital object identiﬁer no . 10.1109/tpami.2017.26585771.note that even though early ﬁltering-based method [ 4 ] , [ 12 ] keep track of point-point-correlations , these originate from marginalize camera pose , not from the model itself.ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 2018 611 0162-8828 /c2232017', ', these originate from marginalize camera pose , not from the model itself.ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 2018 611 0162-8828 /c2232017 ieee . translations and content mining be permit for academic research only . personal use be also permit , but republication/redistribu tion require ieee permission . see http : //www.ieee.org/publications_standards/publications/rights/index.html for more information.alone . in general , this prior be formulate directly in the form of an additional log-likelihood energy term [ 21 ] , [ 22 ] , [ 26 ] . note that the distinction between dense and sparse be not synonymous to direct and indirect —in fact , all four combina- tions exist : /c15sparse + indirect : this be the most widely-used formu- lation , estimate 3d geometry from a set of key- point-matches , thereby use a geometric error without a geometry prior . examples include the work of jin et al . [ 12 ] , monoslam [ 4 ] , ptam [ 16 ] , and orb-slam [ 20 ] . /c15dense + indirect : this formulation estimate 3d geometry from-or in conjunction with-a dense , regu- larized optical ﬂow ﬁeld , thereby combine a geo- metric error ( deviation from the ﬂow ﬁeld ) with a geometry prior ( smoothness of the ﬂow ﬁeld ) , exam- ples include [ 23 ] , [ 27 ] . /c15dense + direct : this formulation employ a photometric error as well a a geometric prior to estimate dense or semi-dense geometry . examples include dtam [ 21 ] , it precursor [ 26 ] , remode [ 22 ] , and lsd-slam [ 5 ] . /c15sparse + direct : this be the formulation propose in this paper . it optimize a photometric error deﬁned directly on the image , without incorporate a geo- metric prior . while we be not aware of any recent work use this formulation , a sparse and direct for- mulation be already propose by jin et al . in 2003 [ 13 ] . in contrast to their work however , which be base on an extended kalman ﬁlter , our method use a non-linear optimization framework . the moti- vation for explore the combination of', '. in 2003 [ 13 ] . in contrast to their work however , which be base on an extended kalman ﬁlter , our method use a non-linear optimization framework . the moti- vation for explore the combination of sparse and direct be lay out in the following section . furthermore there be hybrid approach such a svo [ 9 ] , which use a direct formulation for initial alignment and to obtain correspondence , before switch to an indirect for- mulation for joint model optimization . 1.1 motivation the direct and sparse formulation for monocular visual odometry propose in this paper be motivate by the fol- lowing consideration . ( 1 ) direct : one of the main beneﬁts of keypoints be their ability to provide robustness to photometric and geo- metric distortion present in image take with off- the-shelf commodity camera . examples be auto- matic exposure change , non-linear response func- tions ( gamma correction/white-balancing ) , lens attenuation ( vignetting ) , de-bayering artefact , or even strong geometric distortion cause by a rolling shutter . this robustness or even invariance to photo- metric variation however come at the cost of dis- card potentially valuable information contain in exactly these variation . at the same time , for all use-cases mention in the introduction , million of device will be ( and already be ) equip with camera solely mean to provide data for computer vision algorithm , instead of capture image for human consumption . these camera should and will be design to provide a complete sensor model , and to capture data in a waythat best serve the processing algorithm : auto- exposure and gamma correction for instance be not unknown noise source , but feature that provide good image data—and that can be incorporate into the model , make the obtain data more informa- tive . since the direct approach model the full image formation process down to pixel intensity , it greatly beneﬁts from a more precise sensor model . one of the main beneﬁts of a direct formulation be that it do not require a point to be', 'full image formation process down to pixel intensity , it greatly beneﬁts from a more precise sensor model . one of the main beneﬁts of a direct formulation be that it do not require a point to be recognizable by itself , thereby allow for a more ﬁnely grained geometry representation ( pixelwise inverse depth ) . furthermore , we can sample from across all available data—including edge and weak intensity variations- generate a more complete model and lending more robustness in sparsely textured environment . ( 2 ) sparse : the main drawback of add a geometry prior be the introduction of correlation between geometry parameter , which render a statistically consistent , joint optimization in real time infeasible ( see fig . 2 ) . this be why exist dense or semi-dense approach ( a ) neglect or coarsely approximate cor- relation between geometry parameter ( orange ) , and / or between geometry parameter and camera pose ( green ) , and ( b ) employ different optimization method for the dense geometry part , such a a pri- mal-dual formulation [ 21 ] , [ 22 ] , [ 26 ] . in addition , the expressive complexity of today ’ s prior be limit : while they make the 3d reconstruc- tion denser , locally more accurate and more visually appeal , we find that prior can introduce a bias , and thereby reduce rather than increase long-term , large-scale accuracy . note that in time this may well change with the introduction of more realistic , unbi- ased prior learn from real-world data . fig . 1 . direct sparse odometry ( dso ) . 3d reconstruction and track tra- jectory for a 1:40 min video cycle around a building ( monocular visual odometry only ) . the bottom-left inset show a close-up of the start and end point , visualize the drift accumulate over the course of the trajec- tory . the bottom row show some video frames.612 ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 20181.2 contribution in this paper we propose a sparse and direct approach to mon- ocular visual odometry , an example', 'on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 20181.2 contribution in this paper we propose a sparse and direct approach to mon- ocular visual odometry , an example reconstruction be show in fig . 1 . to our knowledge , it be the only fully direct method that jointly optimize the full likelihood for all involve model parameter , include camera pose , camera intrinsics , and geometry parameter ( inverse depth value ) . this be in con- trast to hybrid approach such a svo [ 9 ] , which revert to an indirect formulation for joint model optimization . optimization be perform in a sliding window , where old camera pose as well a point that leave the ﬁeld of view of the camera be marginalize , in a manner inspire by [ 17 ] . in contrast to exist approach , our method fur- ther take full advantage of photometric camera calibration , include lens attenuation , gamma correction , and know exposure time . this integrated photometric calibration fur- ther increase accuracy and robustness . our cpu-based implementation run in real time on a laptop computer . we show in extensive evaluation on three different datasets comprise several hour of video that it outperform other state-of-the-art approach ( direct and indirect ) , both in term of robustness and accuracy . with reduced setting ( less point and active keyframes ) , it even run at 5/c2real-time speed while still outperform state-of-the-art indirect method . on high , non-real-time setting in turn ( more point and active keyframes ) , it cre- ates semi-dense model similar in density to those of lsd- slam , but much more accurate . 2d irect sparse model our direct sparse odometry be base on continuous optimi- zation of the photometric error over a window of recent frame , take into account a photometrically calibrate model for image formation . in contrast to exist direct method , we jointly optimize for all involved parameter ( camera intrinsics , camera extrinsics , and inverse depth val- ues ) , effectively perform the photometric equivalent', 'to exist direct method , we jointly optimize for all involved parameter ( camera intrinsics , camera extrinsics , and inverse depth val- ues ) , effectively perform the photometric equivalent of windowed sparse bundle adjustment . we keep the geome- try representation employ by other direct approach , i.e. , 3d point be represent a inverse depth in a refer- ence frame ( and thus have one degree of freedom ) . notation . throughout the paper , bold lower-case letter ( x ) represent vector and bold upper-case letter ( h ) represent matrix . scalars will be represent by light lower-case letter ( t ) , function ( include image ) by light upper-case letter ( i ) . camera pose be represent a transformation matrix ti2se ( 3 ) , transform a point from the world frame into the camera frame . linearized pose-increments will be express a lie-algebra element xxi2se ( 3 ) , which-with a slight abuse of notation-we directly write a vector xxi2r6 . we further deﬁne the commonly use operator ( : se ( 3 ) /c2se ( 3 ) ! se ( 3 ) use a left-multi- plicative formulation , i.e. , xxi ( ti : ¼ebxxi/c1ti : ( 1 ) 2.1 calibration the direct approach comprehensively model the image for- mation process . in addition to a geometric camera model- which comprise the function that project a 3d point onto the 2d image—it be hence beneﬁcial to also consider a photo- metric camera model , which comprise the function that map real-world energy receive by a pixel on the sensor ( irradiance ) to the respective intensity value . note that for indirect method this be of little beneﬁt and hence widely ignore , a common feature extractor and descriptor be invariant ( or highly robust ) to photometric variation . 2.1.1 geometric camera calibration for simplicity , we formulate our method for the well-known pinhole camera model—radial distortion be remove in a pre- processing step . while for wide-angle camera this do reduce the ﬁeld of view , it allow comparison across method that only implement a limited choice of camera model . throughout this paper ,', 'a pre- processing step . while for wide-angle camera this do reduce the ﬁeld of view , it allow comparison across method that only implement a limited choice of camera model . throughout this paper , we will denote projection by pc : r3 ! vand back-projection with p/c01 c : v/c2r ! r3 , where c denote the intrinsic camera parameter ( for the pinhole model these be the focal length and the principal point ) . note that analogously to [ 2 ] , our approach can be extend to other ( invertible ) camera model , although this do increase computational demand . 2.1.2 photometric camera calibration we use the image formation model use in [ 8 ] , which account for a non-linear response function g : r ! ½0 ; 255/c138 , as well a lens attenuation ( vignetting ) v : v ! ½0 ; 1/c138 . fig . 3 fig . 2 . sparse versus dense hessian structure . left : hessian structure of sparse bundle adjustment : since the geometry-geometry block be diago- nal , it can be solve efﬁciently use the schur complement . right : a geometry prior add ( partially unstructured ) geometry-geometry correlations—the result system be hence not only much large , but also becomes much hard to solve . for simplicity , we do not show the global camera intrinsic parameter . fig . 3 . photometric calibration . top : inverse response function g/c01and lens attenuation vof the camera use for fig . 1 . bottom : exposure tin millisecond for a sequence contain an indoor and an outdoor part . note how it vary by a factor of more than 500 , from 0.018 to 10.5 m . instead of treat these quantity a unknown noise source , we explicitly account for them in the photometric error model.engel et al . : direct sparse odometry 613shows an example calibration from the tum monovo data- set . the combined model be then give by iiðxþ¼g/c0 tivðxþbiðxþ/c1 ; ( 2 ) where biand iiare the irradiance and the observed pixel intensity in frame i , and tiis the exposure time . the model be apply by photometrically correct each video frame a very ﬁrst step , by compute i0 iðxþ :', 'iiare the irradiance and the observed pixel intensity in frame i , and tiis the exposure time . the model be apply by photometrically correct each video frame a very ﬁrst step , by compute i0 iðxþ : ¼tibiðxþ¼g/c01ðiiðxþþ vðxþ : ( 3 ) in the remainder of this paper , iiwill always refer to the photometrically correct image i0 i , except where otherwise state . 2.2 model formulation we deﬁne the photometric error of a point p2viin refer- ence frame ii , observe in a target frame ij , a the weighted ssd over a small neighborhood of pixel . our experiment have show that eight pixel , arrange in a slightly spread pattern ( see fig . 4 ) give a good trade-off between computa- tions require for evaluation , robustness to motion blur , and provide sufﬁcient information . note that in term of the contain information , evaluate the ssd over such a small neighborhood of pixel be similar to add ﬁrst- and second-order irradiance derivative constancy term ( in addition to irradiance constancy ) for the central pixel . let epj : ¼x p2npwpðij½p0/c138/c0bjþ/c0tjeaj tieai/c0 ii½p/c138/c0bi/c1/c13/c13/c13/c13/c13/c13/c13/c13 g ; ( 4 ) wherenpis the set of pixel include in the ssd ; ti ; tjthe exposure time of the image ii ; ij ; andk/c1kgthe huber norm . further , p0stands for the project point position of pwith inverse depth dp , give by p0¼pc/c0 rp/c01 cðp ; dpþþt/c1 ; ( 5 ) with rt 01/c20/c21 : ¼tjt/c01 i : ( 6 ) in order to allow our method to operate on sequence without know exposure time , we include an additional afﬁne brightness transfer function give by e/c0aiðii/c0biþ.n o t et h a t in contrast to most previous formulation [ 6 ] , [ 13 ] , the scalar factor e/c0aiis parametrized logarithmically . this both prevents it from become negative and avoids numerical issue aris- ing from multiplicative ( i.e. , exponentially increase ) drift.in addition to use robust huber penalty , we apply a gradient-dependent weighting wpgiven by wp : ¼c2 c2þkr iiðpþk2 2 ; ( 7 ) which down-weights pixel with high gradient . this weight', ') drift.in addition to use robust huber penalty , we apply a gradient-dependent weighting wpgiven by wp : ¼c2 c2þkr iiðpþk2 2 ; ( 7 ) which down-weights pixel with high gradient . this weight function can be probabilistically interpret a add small , independent geometric noise on the project point position p0 , and immediately marginalize it-approx- imating small geometric error . to summarize , the error epj depend on the following variable : ( 1 ) the point ’ s inverse depth dp , ( 2 ) the camera intrinsics c , ( 3 ) the pose of the involved frame ti ; tj , and ( 4 ) their brightness transfer function parameter ai ; bi ; aj ; bj . the full photometric error over all frame and point be give by ephoto : ¼x i2fx p2pix j2obsðpþepj ; ( 8 ) where iruns over all frame f , pover all pointspiin frame i , a n d jover all frame obsðpþin which the point p be visible . fig . 5 show the result factor graph : the only difference to the classical reprojection error be the additional dependency of each residual on the pose of the host frame , i.e. , each term depend on two frame instead of only one . while this add off-diagonal entry to the pose-pose block of the hessian , it do not affect the sparsity pattern after application of the schur comple- ment to marginalize point parameter . the result sys- tem can thus be solve analogously to the indirect formulation . note that the jacobians with respect to the two frame ’ pose be linearly relate by the adjoint of their relative pose . in practice , this factor can then be p u l l e do u to ft h es u mw h e nc o m put the hessian or it schur complement , greatly r educe the additional com- putations cause by more v ariable dependency . if exposure time be know , we further add a prior pull- ing the afﬁne brightness transfer function to zero eprior : ¼x i2f/c21aa2 iþ/c21bb2 i/c0/c1 : ( 9 ) fig . 4 . residual pattern . patternnpused for energy computation . the bottom-right pixel be omit to enable sse-optimized processing . note that since we have 1 unknown per point ( it inverse', '( 9 ) fig . 4 . residual pattern . patternnpused for energy computation . the bottom-right pixel be omit to enable sse-optimized processing . note that since we have 1 unknown per point ( it inverse depth ) , and do not use a regularizer , we require jnpj > 1in order for all model parameter to be well-constrained when optimize over only two frame . fig . 19 show an evaluation of how this pattern affect track accuracy . fig . 5 . factor graph for the direct sparse model . example with four key- frame and four point ; one in kf1 , two in kf2 , and one in kf4 . each energy term ( deﬁned in eq . ( 4 ) ) depend on the point ’ s host frame ( blue ) , the frame the point be observe in ( red ) , and the point ’ s inverse depth ( black ) . further , all term depend on the global camera intrinsics vector c , which be not shown.614 ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 2018if no photometric calibration be available , we set ti¼1and /c21a¼/c21b¼0 , a in this case they need to model the ( unknown ) change exposure time of the camera . as a side-note it should be mention that the ml estimator for a multiplica- tive factor a/c3¼argmaxap iðaxi/c0yiþ2is bias if both xi and yicontain noisy measurement ( see [ 7 ] ) ; cause ato drift in the unconstrained case /c21a¼0 . while this generally have little effect on the estimate pose , it may introduce a bias if the scene contain only few , weak intensity variation . point dimensionality . in the propose direct model , a point be parametrized by only one parameter ( the inverse depth in the reference frame ) , in contrast to three unknown a in the indirect model . to understand the reason for this difference , we ﬁrst note that in both case a 3d point be in fact an arbitrarily locate discrete sample on a continuous , real-world 3d surface . the difference then lie in the way this 2d location on the surface be deﬁned . in the indirect approach , it be implicitly deﬁned a the point , which ( project into an image ) generate a maximum', '. the difference then lie in the way this 2d location on the surface be deﬁned . in the indirect approach , it be implicitly deﬁned a the point , which ( project into an image ) generate a maximum in the used corner response function . this entail that both the surface , as well a the point ’ s location on the surface be unknowns , and need to be estimate . in our direct formulation , a point be simply deﬁned a the point , where the source pixel ’ s ray hit the surface , thus only one unknown remain . in addition to a reduced number of parameter , this naturally enable an inverse depth parametrization , which—in a gaussian framework— be well suit to represent uncertainty from stereo-based depth estimation , in particular for far-away point [ 3 ] . consistency . strictly speak , the propose direct sparse model do allow to use some observation ( pixel value ) multiple time , while others be not use at all . this be because—even though our point selection strategy attempt to avoid this by equally distribute point in space ( see section 3.2 ) —we allow point observation to overlap , and thus depend on the same pixel value ( s ) . this particularly happen in scene with little texture , where all point have to be choose from a small subset of textured image region . we however argue that this have negligible effect in practice , and—if desired—can be avoid by remove ( or down- weighting ) observation that use the same pixel value . 2.3 windowed optimization we follow the approach by leutenegger et al . [ 17 ] and opti- mize the total error ( 8 ) in a sliding window use the gauss-newton algorithm , which give a good trade-off between speed and ﬂexibility . for ease of notation , we extend the ( operator a deﬁned in ( 1 ) to all optimize parameters—for parameter otherthan se ( 3 ) pose it denote conventional addition . we will use zz2se ( 3 ) n/c2rmto denote all optimized variable , include camera pose , afﬁne brightness parameter , inverse depth value , and camera intrinsics . as in [ 17 ] , mar- ginalizing a residual', 'use zz2se ( 3 ) n/c2rmto denote all optimized variable , include camera pose , afﬁne brightness parameter , inverse depth value , and camera intrinsics . as in [ 17 ] , mar- ginalizing a residual that depend on a parameter in zzwill ﬁx the tangent space in which any future information ( delta-updates ) on that parameter be accumulate . we will denote the evaluation point for this tangent space with zz0 , and the accumulate delta-updates by xx2se ( 3 ) n/c2rm . the current state estimate be hence give by zz¼xx ( zz0 . fig . 6 vis- ualizes the relation between the different variable . gauss-newton optimization . we compute the gauss-new- ton system a h¼jtwj and b¼/c0jtwr ; ( 10 ) where w2rn/c2nis the diagonal matrix contain the weight , r2rnis the stacked residual vector , and j2rn/c2d be the jacobian of r. note that each point contribute jnpj¼8residuals to the energy . for notational simplicity , we will in the following consider only a single residual rk , and the associated row of the jacobian jk . during optimization—as well a when mar- ginalizing—residuals be always evaluate at the current state estimate , i.e. , rk¼rkðxx ( zz0þ ¼/c0 ij½p0ðti ; tj ; d ; cþ/c138/c0bj/c1 /c0tjeaj tieai/c0 ii½p/c138/c0bi/c1 ; ( 11 ) whereðti ; tj ; d ; c ; ai ; aj ; bi ; bjþ : ¼xx ( zz0are the current state variable the residual depend on . the jacobian jkis evalu- ated with respect to an additive increment toxx , i.e. , jk¼ @ rkððddþxxþ ( zz0þ @ dd : ( 12 ) it can be decompose a jk¼/c20 @ ij @ p0 | { z } ji @ p0ððddþxxþ ( zz0þ @ ddgeo|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ { zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ } jgeo ; @ rkððddþxxþ ( zz0þ @ ddphoto|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ { zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ } jphoto/c21 ; ( 13 ) where ddgeodenotes the “ geometric ” parameter ðti ; tj ; d ; cþ , and ddphoto denote the “ photometric ” parameter ðai ; aj ; bi ; bjþ . we employ two approximation , describe below . first , both jphotoandjgeoare evaluate at xx¼0 . this tech- nique be call “ first estimate jacobians ” [ 11 ] , [ 17 ] , and be require to maintain consistency of the system and prevent the', 'below . first , both jphotoandjgeoare evaluate at xx¼0 . this tech- nique be call “ first estimate jacobians ” [ 11 ] , [ 17 ] , and be require to maintain consistency of the system and prevent the accumulation of spurious information . in particular , in the presence of non-linear null-spaces in the energy ( in our formulation absolute pose and scale ) , add linearizations around different evaluation point eliminate these and thus slowly corrupt the system . in practice , this approxi- mation be very good , since jphoto , jgeoare smooth compare to the size of the increment xx . in contrast , jiis much less smooth , but do not affect the null-spaces . thus , it be evalu- ated at the current value for xx , i.e. , at the same point a the residual rk . we use centred difference to compute the image derivative at integer position , which be then bili- nearly interpolated.fig . 6 . windowed optimization . the red curve denote the parameter space , compose of non-euclidean camera pose in se ( 3 ) , and the remain euclidean parameter . the blue line corresponds to the tan- gent-space around zz0 , in which we ( 1 ) accumulate the quadratic margin- alization-prior on xx , and ( 2 ) compute gauss-newton step dd . for each parameter , the tangent space be ﬁxed as soon a that parameter become part of the marginalization term . note that while we treat all parameter equally in our notation , for euclidean parameter tangent- space and parameter-space coincide.engel et al . : direct sparse odometry 615second , jgeois assume to be the same for all residual belong to the same point , and evaluate only for the cen- ter pixel . again , this approximation be very good in practice . while it signiﬁcantly reduce the required computation , we have not observe a notable effect on accuracy for any of the used datasets . from the result linear system , an increment be com- put a dd¼h/c01band add to the current state xxnew ddþxx : ( 14 ) note that due to the first estimate jacobian approximation , a multiplicative formulation ( replace', 'system , an increment be com- put a dd¼h/c01band add to the current state xxnew ddþxx : ( 14 ) note that due to the first estimate jacobian approximation , a multiplicative formulation ( replace ðddþxxþ ( zz0with dd ( ðxx ( zz0þin ( 12 ) ) result in the exact same jacobian , thus a multiplicative update step xxnew logðdd ( exxþis equally valid . after each update step , we update zz0for all variable that be not part of the marginalization term , use zznew 0 xx ( zz0 and xx 0 . in practice , this include all depth value , as well a the pose of the new keyframe . each time a new keyframe be add , we perform up to 6 gauss-newton iter- ations , break early if ddis sufﬁciently small . we find that-since we never start far-away from the minimum—a levenberg-marquardt dampening ( which slow down convergence ) be not require . marginalization . when the active set of variable becomes too large , old variable be remove by marginalization use the schur complement . similar to [ 17 ] , we drop any residual term that would affect the sparsity pattern of h : when marginalize frame i , we ﬁrst marginalize all point inpi , as well a point that have not be observe in the last two keyframes . remaining observation of active point in frame iare drop from the system . marginalization proceeds a follow : let e0denote the part of the energy contain all residual that depend on state variable to be marginalize . we ﬁrst compute a gauss-newton approximation of e0around the current state estimate zz¼xx ( zz0 . this give e0ðxx ( zz0þ /c252ðxx/c0xx0þtbþðxx/c0xx0þthðxx/c0xx0þþc ¼2xxtðb/c0hxx0þ|ﬄﬄﬄﬄﬄﬄﬄ { zﬄﬄﬄﬄﬄﬄﬄ } ¼ : b0þxxthxxþðcþxxt 0hxx0/c0xxt 0bþ|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ { zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ } ¼ : c0 ; ( 15 ) where xx0denotes the current value ( evaluation point for r ) o f xx . the constant c ; c0can be drop , and h ; bare deﬁned a in ( 10 ) , ( 11 ) , ( 12 ) , and ( 13 ) . this be a quadratic function on xx , and we can apply the schur complement to marginalize a sub- set of variable . written a a linear system , it become haahab', ""( 10 ) , ( 11 ) , ( 12 ) , and ( 13 ) . this be a quadratic function on xx , and we can apply the schur complement to marginalize a sub- set of variable . written a a linear system , it become haahab hbahbb/c20/c21xxa xxb/c20/c21 ¼b0 a b0 b '' # ; ( 16 ) where bdenotes the block of variable we would like to mar- ginalize , and athe block of variable we would like to keep . applying the schur complement yield dhaaxxa¼cb0 a , with dhaa¼haa/c0habh/c01 bbhba ( 17 ) cb0 a¼b0 a/c0habh/c01 bbb0 b : ( 18 ) the residual energy on xxacan hence be write a e0/c0 xxa ( ðzz0þa/c1 ¼2xxt acb0 aþxxt adhaaxxa : ( 19 ) this be a quadratic function on xxand can be trivially add to the full photometric error ephoto during all subsequent optimization and marginalization operation , replace the corresponding non-linear term . note that this require the tangent space for zz0to remain the same for all variable that appear in e0during all subsequent optimization and mar- ginalization step . 3v isual odometry front -end the front end be the part of the algorithm that /c15determines the set f ; pi , and obsðpþthat make up the error term of ephoto . it decide which point and frame be use , and in which frame a point be visi- ble-in particular , this include outlier removal and occlusion detection . /c15provides initialization for new parameter , require for optimize the highly non-convex energy func- tion ephoto . as a rule of thumb , a linearization of the image iis only valid in a 1-2 pixel radius ; hence all parameter involve in compute p0should be ini- tialized sufﬁciently accurately for p0to be off by no more than 1-2 pixel . /c15decides when a point/frame should be marginalize . as such , the front-end need to replace many operation that in the indirect setting be accomplish by keypoint detector ( determine visibility , point selection ) and ini- tialization procedure such a ransac . note that many procedure describe here be speciﬁc to the monocular case . for instance , use a stereo camera make obtain initial depth value more"", ') and ini- tialization procedure such a ransac . note that many procedure describe here be speciﬁc to the monocular case . for instance , use a stereo camera make obtain initial depth value more straightforward , while integration of an imu can signiﬁcantly robustify—or even directly pro- vide—a pose initialization for new frame . 3.1 frame management our method always keep a window of up to nfactive keyframes ( we use nf¼7 ) . every new frame be initially track with respect to these reference frame ( step 1 ) . it be then either discard or use to create a new keyframe ( step 2 ) . once a new keyframe-and respective new points- be create , the total photometric error ( 8 ) be optimize . afterwards , we marginalize one or more frame ( step 3 ) . step 1 : initial frame tracking . when a new keyframe be cre- ated , all active point be project into it and slightly dilate , create a semi-dense depth map . new frame be track with respect to only this frame use conventional two-frame direct image alignment , a multi-scale image pyra- mid and a constant motion model to initialize . fig . 7 show some examples—we find that further increase the den- sity have little to no beneﬁt in term of accuracy or robustness , while signiﬁcantly increase runtime . note that when down-scaling the image , a pixel be assign a depth value if at least one of the source pixel have a depth value a in [ 24 ] , signiﬁcantly increase the density on coarser resolution . if the ﬁnal rmse for a frame be more than twice that of the frame before , we assume that direct image alignment616 ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 2018failed and attempt to recover by initialize with up to 27 different small rotation in different direction . this recov- ery-tracking be do on the coarse pyramid level only , and take approximately 0.5 m per try . note that this ran- sac-like procedure be only rarely invoke , such a when the camera move very quickly or shakily . tightly integrat- ing an imu would likely', ', and take approximately 0.5 m per try . note that this ran- sac-like procedure be only rarely invoke , such a when the camera move very quickly or shakily . tightly integrat- ing an imu would likely render this unnecessary . step 2 : keyframe creation . similar to orb-slam , our strategy be to initially take many keyframes ( around 5-10 keyframes per second ) , and sparsify them afterwards by early marginalizing redundant keyframes . we combine three criterion to determine if a new keyframe be require : 1 ) new keyframes need to be create a the ﬁeld of view change . we measure this by the mean square optical ﬂow ( from the last keyframe to the late frame ) f : ¼ð1 npn i¼1kp/c0p0k2þ1 2during initial coarse tracking . 2 ) camera translation cause occlusion and dis- occlusion , which require more keyframes to be take ( even though fmay be small ) . this be mea- sured by the mean ﬂow without rotation , i.e. , ft : ¼ð1 npn i¼1kp/c0p0 tk2þ1 2 , where ptis the warped point position with r¼i3/c23 . 3 ) if the camera exposure time change signiﬁcantly , a new keyframe should be take . this be measure by the relative brightness factor between two frame a : ¼jlogðeaj/c0aitjt/c01 iþj . these three quantity can be obtain easily a a by- product of initial alignmen t. finally , a new keyframe be take if wffþwftftþwaa > t kf , w h e r e wf ; wft ; wapro- vide a relative weighting of these three indicator , and tkf¼1by default . step 3 : keyframe marginalization . our marginalization strategy be a follow ( let i1 ... inbe the set of active key- frame , with i1being the new and inbeing the old ) : 1 ) we always keep the late two keyframes ( i1andi2 ) . 2 ) frames with less than 5 percent of their point visible ini1are marginalize . 3 ) if more than nfframes be active , we marginalize the one ( exclude i1and i2 ) which maximize a “ distance score ” sðiiþ , compute a sðiiþ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ dði ; 1þp x j2½3 ; n/c138nfigðdði ; jþþ/c15þ/c01 ; ( 20 ) where dði ; jþis the euclidean distance between key- frame iiandij , and /c15a small', '“ distance score ” sðiiþ , compute a sðiiþ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ dði ; 1þp x j2½3 ; n/c138nfigðdði ; jþþ/c15þ/c01 ; ( 20 ) where dði ; jþis the euclidean distance between key- frame iiandij , and /c15a small constant . this score function be heuristically design to keep active key- frame well-distributed in 3d space , with more key- frame close to the most recent one . a keyframe be marginalize by ﬁrst marginalize all point represent in it , and then the frame itself , use the mar- ginalization procedure from section 2.3 . to preserve the sparsity structure of the hessian , all observation of still exist point in the frame be drop from the system . while this be clearly suboptimal ( in practice about half of all residual be drop for this reason ) , it allow to efﬁciently optimize the energy function . fig . 8 show an example of a scene , highlight the active set of point and frame . 3.2 point management most exist direct method focus on utilize as much image data a possible . to achieve this in real time , they accumulate early , sub-optimal estimate ( linearizations / depth triangulation ) , and ignore-or approximate-correla- tions between different parameter . in this work , we follow a different approach , and instead heavily sub-sample data to allow process it in real time in a joint optimization frame- work . in fact , our experiment show that image data be highly redundant , and the beneﬁt of simply use more data point quickly ﬂattens off . note that in contrast to indirect method , our direct framework still allow to sample from across all available data , include weakly textured or repetitive region and edge , which do provide a real beneﬁt ( see section 4 ) . we aim at always keep a ﬁxed number npof active point ( we use np¼2 ; 000 ) , equally distribute across space and active frame , in the optimization . in a ﬁrst step , we identify npcandidate point in each new keyframe ( step 1 ) . candidate point be not immediately add into the opti- mization , but instead be track individually in subsequent frame ,', 'a ﬁrst step , we identify npcandidate point in each new keyframe ( step 1 ) . candidate point be not immediately add into the opti- mization , but instead be track individually in subsequent frame , generate a coarse depth value which will serve a initialization ( step 2 ) . when new point need to be add to fig . 8 . keyframe management . bottom : the six old keyframes in the optimization window , overlay with the point host in them ( already marginalize point be show in black ) . the top image show the full point cloud , as well a the position of all keyframes ( black camera frus- tum ) —active point and keyframes be show in red and blue respec- tively . the inlay show the newly add keyframe , overlay with all forward-warped active point , which will be use for initial alignment of subsequent frame . fig . 7 . example depth map use for initial frame track . the top row show the original image , the bottom row the color-coded depth map . since we aim at a ﬁxed number of point in the active optimization , they become more sparse in densely textured scene ( leave ) , while become similar in density to those of lsd-slam in scene where only few infor- mative image region be available to sample from ( right ) .engel et al . : direct sparse odometry 617the optimization , we choose a number of candidate point ( from across all frame in the optimization window ) to be activate , i.e. , add into the optimization ( step 3 ) . note that we choose npcandidates in each frame , however only keep npactive point across all active frame combine . this assure that we always have sufﬁcient candidate to acti- vate , even though some may become invalid a they leave the ﬁeld of view or be identiﬁed a outlier . step 1 : candidate point selection . our point selection strat- egy aim at select point that be ( 1 ) well-distributed in the image and ( 2 ) have sufﬁciently high image gradient magnitude with respect to their immediate surroundings . we obtain a region-adaptive gradient threshold by split the image into 32/c232blocks . for', 'and ( 2 ) have sufﬁciently high image gradient magnitude with respect to their immediate surroundings . we obtain a region-adaptive gradient threshold by split the image into 32/c232blocks . for each block , we then com- pute the threshold a /c22gþgth , where /c22gis the median absolute gradient over all pixel in that block , and gtha global con- stant ( we use gth¼7 ) . to obtain an equal distribution of point throughout the image , we split it into d/c2dblocks , and from each block select the pixel with large gradient if it surpass the region-adaptive threshold . otherwise , we do not select a pixel from that block . we find that it be often beneﬁcial to also include some point with weak gradient from region where no high-gradient point be present , capture infor- mation from weak intensity variation originate for exam- ple from smoothly change illumination across white wall . to achieve this , we repeat this procedure twice more , with decreased gradient threshold and block-size 2dand 4d , respectively . the block-size dis continuously adapt such that this procedure generate the desired amount of point ( if too many point be create it be increase for the next frame , otherwise it be decrease ) . fig . 9 show the select point candidate for some example scene . note that for for candidate point selection , we use the raw image prior to photometric correction . step 2 : candidate point tracking . point candidate be track in subsequent frame use a discrete search along the epipolar line , minimize the photometric error ( 4 ) . from the best match we compute a depth and associate variance , which be use to constrain the search interval for the subsequent frame . this track strategy be inspire by lsd-slam . note that the compute depth only serve a initialization once the point be activate . step 3 : candidate point activation . after a set of old point be marginalize , new point candidate be activate to replace them . again , we aim at maintain a uniformspatial distribution across the image . to this end , we ﬁrst', '. after a set of old point be marginalize , new point candidate be activate to replace them . again , we aim at maintain a uniformspatial distribution across the image . to this end , we ﬁrst project all active point onto the most recent keyframe . we then activate candidate point which—also project into this keyframe—maximize the distance to any exist point ( require large distance for candidate create during the second or third block-run ) . fig . 7 show the result distri- bution of point in a number of scene . outlier and occlusion detection . since the available image data generally contain much more information than can be use in real time , we attempt to identify and remove poten- tial outlier as early a possible . first , when search along the epipolar line during candidate tracking , point for which the minimum be not sufﬁciently distinct be perma- nently discard , greatly reduce the number of false match in repetitive area . second , point observation for which the photometric error ( 4 ) surpass a threshold be remove . the threshold be continuously adapt with respect to the median residual in the respective frame . for “ bad ” frame ( e.g. , frame that contain a lot of motion blur ) , the threshold will be high , such that not all observation be remove . for good frame , in turn , the threshold will be low , a we can afford to be more strict . 4r esults in this section we will extensively evaluate our direct sparse mono-v oalgorithm ( dso ) . we both compare it to other monocular slam/vo method , as well a evaluate the effect of important design and parameter choice . we use three datasets for evaluation : ( 1 ) the tum monovo dataset [ 8 ] , which provide 50 pho- tometrically calibrated sequence , comprise 105 minute of video record in dozen of different envi- ronments , indoors and outdoors ( see fig . 11 ) . since the dataset only provide loop-closure-ground-truth fig . 9 . candidate selection . the top row show the original image , the bottom row show the point choose a candidate to be add to the', '11 ) . since the dataset only provide loop-closure-ground-truth fig . 9 . candidate selection . the top row show the original image , the bottom row show the point choose a candidate to be add to the map ( 2,000 in each frame ) . points select on the ﬁrst pas be show in green , those select on the second and third pas in blue and red respectively . green candidate be evenly spread across gradient-rich area , while point add on the second and third pas also cover region with very weak intensity variation , but be much sparser . fig . 10 . results on euroc mav ( top ) and icl_nuim ( bottom ) datasets . translational rmse after sim ( 3 ) alignment . rt ( dash ) denote hard- enforced real-time execution . further , we evaluate dso with low set- ting at 5 time real-time speed , and orb-slam when restrict local loop-closures to point that have be observe at least once within the lasttmax=10 s.618 ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 2018 ( allow to evaluate track accuracy via the accu- mulated drift after a large loop ) , we evaluate use thealignment error ( ealign ) a deﬁned in the respective publication . ( 2 ) the euroc mav dataset [ 1 ] , which contain 11 ste- reo-inertial sequence comprise 19 minute of video , record in three different indoor environ- ments . for this dataset , no photometric calibration or exposure time be available , hence we omit photo- metric image correction and set ( /c21a¼/c21b¼0 ) . we evaluate in term of the absolute trajectory error ( eate ) , which be the translational rmse after simð3þ alignment . for this dataset we crop the beginning of each sequence since they contain very shaky motion meant to initialize the imu biases—we only use the part of the sequence where the mav be in the air . ( 3 ) the icl-nuim dataset [ 10 ] , which contain eight ray- trace sequence comprise 4.5 minute of video , from two indoor environment . for this dataset , pho- tometric image correction be not require , and all exposure time can be set to t¼1 .', 'eight ray- trace sequence comprise 4.5 minute of video , from two indoor environment . for this dataset , pho- tometric image correction be not require , and all exposure time can be set to t¼1 . again , we evalu- ate in term of the absolute trajectory error ( eate ) . methodology . we aim at an evaluation as comprehensive a possible give the available data , and thus run all sequence both forward and backwards , 5 time each ( to account for non-deterministic behaviour ) . for the euroc mav dataset we further run both the left and the right video separately . in total , this give 500 run for the tum- monovo dataset , 220 run for the euroc mav dataset , and 80 run for the icl-nuim dataset , which we run on 20 dedi- cated workstation . we remove the dependency on the host machine ’ s cpu speed by not enforce real-time execution , except where state otherwise : for orb-slam we play the video at 20 percent speed , whereas dso be run in a sequen- tialized , single-threaded implementation that run approxi- mately four time slow than real time . note that even though we do not enforce real-time execution for most of the experiment , we use the exact same parameter setting a for the real-time comparison . the result be summarize in the form of cumulative error plot ( see , e.g. , fig . 10 ) , which visualize for how manytracked sequence the respective error value ( eate/ealign ) be below a certain threshold ; 2thereby show both accuracy on sequence where a method work well , as well a robust- ness , i.e. , on how many sequence the method do not fail . the raw tracking result for all runs-as well a script to compute the ﬁgures-are provide in the supplementary material.3additional interesting analysis use the tum- monovo dataset—e.g. , the inﬂuence of the camera ’ s ﬁeld of view , the image resolution or the camera ’ s motion direc- tion-can be find in [ 8 ] . evaluated methods and parameter settings . we compare our method to the open-source implementation of ( monoc- ular ) orb-slam [ 20 ] . we also attempt to evaluate', 'direc- tion-can be find in [ 8 ] . evaluated methods and parameter settings . we compare our method to the open-source implementation of ( monoc- ular ) orb-slam [ 20 ] . we also attempt to evaluate against the open-source implementation of lsd-slam [ 5 ] and svo [ 9 ] , however both method consistently fail on most of the sequence . a major reason for this be that they assume brightness constancy ( ignore exposure change ) , while both real-world datasets use contain heavy expo- sure variation . to facilitate a fair comparison and allow application of the loop-closure metric from the tum-monovo dataset , we disable explicit loop-closu re detection and re-localiza- tion for orb-slam . note that everything else ( include local and global ba ) remain unchanged , still allow orb-slam to detect incremental loop-closures that can be find via the co-visibility representation alone . all parameter be set to the same value across all sequence and datasets . the only exception be the icl-nuim data- set : for this dataset we set gth¼3f o rd s o , a n dl o w e rt h e fast threshold for orb-slam to 2 , which we find to give best result . 4.1 quantitative comparison fig . 10 show the absolute trajectory rmse eateon the euroc mav dataset and the icl-nuim dataset for both method ( if an algorithm get lose within a sequence , we set fig . 11 . tum mono-vo dataset . a single image from each of the 50 tum mono-vo dataset sequence ( s_01 to s_50 ) use for evaluation and parameter study , overlay with the predicted depth map from dso . the full dataset contains over 105 minute of video ( 190 ’ 000 frame ) . note the wide range of environment cover , range from narrow indoor corridores to wide outdoor area , include forest . 2.on default setting , we run each method 10 time forward and 10 time backwards , result in 1,000/440/160 run . the respective error plot summarize all these run , and be scale to ﬁt the 500/ 220/80 scale . 3.http : //vision.in.tum.de/dsoengel et al . : direct sparse odometry 619eate¼1 ) . fig . 12 show the', '. the respective error plot summarize all these run , and be scale to ﬁt the 500/ 220/80 scale . 3.http : //vision.in.tum.de/dsoengel et al . : direct sparse odometry 619eate¼1 ) . fig . 12 show the alignment error ealign , as well a the rotation-drift erand scale-drift esfor the tum-monovo dataset . the full set of result for each evaluated trajectory be visualize in fig . 13 and fig . 14 . in addition to the non-real-time evaluation ( bold line ) , we evaluate both algorithm in a hard-enforced real-time set- ting on an intel i7-4910mq cpu ( dash line ) . in this mode , we enforce real-time by allow both orb-slam and dso to skip frame if track can not keep up-increasing the drift or potentially leading to complete loss of track . the direct , sparse approach clearly outperform orb-slam in accu- racy and robustness both on the tum-monovo dataset , as well a the synthetic icl_nuim dataset . on the euroc mav dataset , orb-slam achieve a good accuracy ( but low robustness ) . this be due to two major reason : ( 1 ) there be no photometric calibration available , and ( 2 ) the sequence contain many small loop or segment where the quadrocop- ter “ back-tracks ” the way it come , allow orb-slam ’ s local mapping component to implicitly close many small and some large loop , whereas our visual odometry formula- tion permanently marginalize all point and frame that leave the ﬁeld of view . we can validate this by prohibit orb-slam from match against any keypoints that have not be observe for more than tmax¼10s ( line with circle marker in fig . 10 ) : in this case , orb-slam perform similar to dso in term of accuracy , but be less robust . the slight difference in robustness for dso come from the fact that for real-time execution , track new frame and keyframe- creation be parallelize , thus new frame be track on the second-latest keyframe , instead of the late . in some rare cases—in particular during strong exposure changes-this cause initial image alignment to fail . to show the ﬂexibility of dso , we include result', 'keyframe , instead of the late . in some rare cases—in particular during strong exposure changes-this cause initial image alignment to fail . to show the ﬂexibility of dso , we include result when run at 5 time the speed they be record at,4with reduce setting ( np¼800points , nf¼6active frame , 424/c2320image resolution , /c204gauss-newton iteration after a keyframe be create ) : even with such extreme set- ting , dso achieve very good accuracy and robustness on all three datasets . note that dso be design a a pure visual adometry while orb-slam constitute a full slam system , includ- ing loop-closure detection & correction and re-localiza- tion—all these additional ability be neglect or switch off in this comparison.fig . 12 . results on tum-monovo dataset . accumulated rotational drift erand scale drift esafter a large loop , as well a the alignment error a deﬁned in [ 8 ] . since esis a multiplicative factor , we aggregate e0 s¼maxðes ; e/c01 sþ . the solid line corresponds to sequentialized , non- real-time execution , the dashed line to hard enforced real-time process- ing . for dso , we also show result obtain at low parameter setting , run at 5 time real-time speed . fig . 13 . full evaluation result . all error value for the euroc mav data- set ( leave ) and the icl_nuim dataset ( right ) : each square correspond to the ( color-coded ) absolute trajectory error eateover the full sequence . we run each of the 11 + 8 sequence ( horizontal axis ) forward ( “ fwd ” ) and backwards ( “ bwd ” ) , 10 time each ( vertical axis ) ; for the euroc mav dataset we far use the left and the right image stream . fig . 10 show these error value aggregate a cumulative error plot ( bold , con- tinuous line ) . fig . 14 . full evaluation result . all error value for the tum-monovo dataset ( also see fig . 11 ) . each square correspond to the ( color-coded ) alignment error ealign , a deﬁned in [ 8 ] . we run each of the 50 sequence ( horizontal axis ) forward ( “ fwd ” ) and backwards ( “ bwd ” ) , 10 time each ( vertical', 'to the ( color-coded ) alignment error ealign , a deﬁned in [ 8 ] . we run each of the 50 sequence ( horizontal axis ) forward ( “ fwd ” ) and backwards ( “ bwd ” ) , 10 time each ( vertical axis ) . fig . 12 show all these error value aggregate a cumu- lative error plot ( bold , continuous line ) . 4.all image be load , decode , and pinhole-rectiﬁed beforehand.620 ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 2018runtime . the required compute depend both on the number of tracked frame , as well a the number of key- frame create ( i.e. , on how far the camera move ) . on the tum monovo dataset , the average single-threaded runtime for initial frame alignment and candidate point tracking ( perform for each frame ) be 18 m ( 6.5 m on “ reduce ” setting ) . creating a new keyframe take 143 m ( 43 m on “ reduce ” setting ) , also in a single thread . 4.2 parameter studies this section aim at evaluate a number of different param- eter and algorithm design choice , use the tum-monovo dataset . photometric calibration . we analyze the inﬂuence of pho- tometric calibration , verify that it in fact increase accu- racy and robustness for direct method . to this end , we incrementally disable the different component : 1 ) exposure ( blue ) : set ti¼1and/c21a¼/c21b¼0 . 2 ) vignette ( green ) : set vðxþ¼1 ( and 1. ) . 3 ) response ( yellow ) : set g/c01¼identity ( and 1-2. ) . 4 ) brightness constancy ( black ) : set /c21a¼/c21b¼1 , i.e. , disable afﬁne brightness correction ( and 1-3. ) . fig . 15 show the result . as expect , dso perform signiﬁ- cantly well with full photometric calibration , in particular compare to a basic brightness constancy assumption ( a use in many other direct or semi-direct approach like lsd-slam or svo ) . in turn , orb-slam performs worse when use photometrically calibrate image . in fact , the photometric calibration only affect the keypoint selection ( fast thresholding ) : for this step , use the originalimages—in particular with enabled gamma', 'calibrate image . in fact , the photometric calibration only affect the keypoint selection ( fast thresholding ) : for this step , use the originalimages—in particular with enabled gamma correction—lead to a good point distribution . note that we observe the same behaviour for dso , and thus also perform point selec- tion on the original image ( see section 3.2 ) . in all remain experiment we use the original image for orb-slam , and ( if available ) a full photometric calibration for dso . amount of data . we analyze the effect of change the amount of data use , by vary the number of active point np , as well a the number of frame in the active win- dow nf . note that increase nfallows to keep more obser- vations per point : for any point we only ever keep observation in active frame ; thus the number of observa- tions when marginalize a point be limit to nf ( see section 2.3 ) . fig . 16 summarize the result . we can observe that the beneﬁt of simply use more data quickly ﬂattens off after np¼500points . at the same time , the number of active frame have little inﬂuence after nf¼7 , while increas- ing the runtime quadratically . we further evaluate a ﬁxed- lag marginalization strategy ( i.e. , always marginalize the old keyframe , instead of use the propose distance score ) a in [ 17 ] : this perform signiﬁcantly bad . selection of data . in addition to evaluate the effect of the number of residual use , it be interest to look at which data be used—in particular since one of the main beneﬁts of a direct approach be the ability to sample from all point , instead of only use corner . to this end , we vary the gradi- ent threshold for point selection , gth ; the result be summa- rized in fig . 17 . while there seem to be a sweet spot around gth¼7 ( ifgthis too large , for some scene not enough well- distributed point be available to sample from—if it be too low , too much weight will be give to data with a low signal- to-noise ratio ) , the overall impact be relatively low . more interestingly , we analyse the', 'be available to sample from—if it be too low , too much weight will be give to data with a low signal- to-noise ratio ) , the overall impact be relatively low . more interestingly , we analyse the effect of only use cor- ners , by restrict point candidate to fast corner only . we can clearly see that only use corner signiﬁcantly decrease performance . note that for low fast threshold , many false “ corner ” will be detect along edge , which our method can still use , in contrast to indirect method for which such point will be outlier . in fact , orb-slam achieve best performance use the default threshold of 20 . number of keyframes . we analyze the number of key- frame take by vary tkf ( see section 3.1 ) . for each value oftkfwe give the resulting average number of keyframes per second ; the default set tkf¼1results in eight key- frame per second , which be easily achieve in real time.fig . 15 . photometric calibration . errors on the tum-monovo dataset for orb-slam and dso , when incrementally disable photometric calibra- tion . while dso a a direct method clearly beneﬁts from photometric cal- ibration , orb-slam a an indirect approach performs well on the original image . we therefore use the original image for all other orb- slam evaluation . fig . 16 . amount of data use . errors on the tum-monovo dataset , when change the size of the optimization window ( top ) and the number of point ( bottom ) . using more than np¼500points or nf¼7active frame have only marginal impact . note that a real-time default setting , we use np¼2 ; 000andnf¼7 , mainly to obtain dense reconstructions.fig . 17 . selection of data use . errors on the tum-monovo dataset , when change the type of data use . left : errors for different gradient threshold gth , which seem to have a limited impact on the algorithms accuracy . right : errors when only use fast corner , at different threshold . using only fast corner signiﬁcantly reduce accuracy and robustness , show that the ability to use data from edge and weakly textured surface do', 'when only use fast corner , at different threshold . using only fast corner signiﬁcantly reduce accuracy and robustness , show that the ability to use data from edge and weakly textured surface do have a real beneﬁt.engel et al . : direct sparse odometry 621the result be summarize in fig . 18 . taking too few key- frame ( less than 4 per second ) reduce the robustness , mainly in situation with strong occlusion / dis-occlusions , e.g. , when walk through door . taking too many key- frame , on the other hand ( more than 15 per second ) , decrease accuracy . this be because take more keyframes cause them to be marginalize earlier ( since nfis ﬁxed ) , thereby accumulate linearizations around early ( and less accurate ) linearization point . residual pattern . we test different residual pattern for np , cover small or large area . the result be show in fig . 19 . 4.3 geometric versus photometric noise study the fundamental difference between the propose direct model and the indirect model be the noise assumption . the direct approach model photometric noise , i.e. , additive noise on pixel intensity . in contrast , the indirect approach model geometric noise , i . e . , a d d i t i v en o i s eo nt h e ðu ; vþ-position of a point in the image plane , assume that keypoint descriptor be robust to photometric noise . it therefore come at no surprise that the indirect approach be signiﬁcantly more robust to geometric noise in the data . in turn , the direct approach performs well in the presence of strong photometric noise—which key- point-descriptors ( operate on a purely local level ) fail to ﬁ l t e ro u t .w ev e r i f yt h i sb ya nalyzing track accuracy on the tum-monovo dataset , w hen artiﬁcially add ( a ) geometric noise , and ( b ) photometric noise to the image . geometric noise . for each frame , we separately generate a low-frequency random ﬂow-map ng : v ! r2by up- sample a 3/c23grid ﬁlled with uniformly distribute ran- dom value from ½/c0dg ; dg/c1382 ( use bicubic interpolation ) .we then perturb the', 'generate a low-frequency random ﬂow-map ng : v ! r2by up- sample a 3/c23grid ﬁlled with uniformly distribute ran- dom value from ½/c0dg ; dg/c1382 ( use bicubic interpolation ) .we then perturb the original image by shift each pixel xbyngðxþ i0 gðxþ : ¼iðxþngðxþþ : ( 21 ) this procedure simulate noise originate from ( unmod- eled ) roll shutter or inaccurate geometric camera calibra- tion . fig . 20 visualizes an example of the result noise pattern , as well a the accuracy of orb-slam and dso for different value of dg . as expect , we can clearly observe how dso ’ s performance quickly deteriorate with add geometric noise , whereas orb-slam be much less affected . this be because the ﬁrst step in the indirect pipeline— keypoint detection and extraction—is not affect by low- frequency geometric noise , a it operate on a purely local level . the second step then optimize a geometric noise model-which not surprisingly deal well with geometric noise . in the direct approach , in turn , geometric noise be not model , and thus have a much more severe effect—in fact , fordg > 1:5there likely exist no state for which all resid- uals be within the validity radius of the linearization of i ; thus optimization fail entirely ( which can be alleviate by use a coarser pyramid level ) . note that this result also suggest that the propose direct model be more susceptible to inaccurate intrinsic camera calibration than the indirect approach—in turn , it may beneﬁt more from accurate , non- parametric intrinsic calibration . photometric noise . for each frame , we separately generate a high-frequency random blur-map np : v ! r2by up- sample a 300/c2300grid ﬁlled with uniformly distribute random value in ½/c0dp ; dp/c1382 . we then perturb the original image by add anisotropic blur with standard deviation npðxþto pixel x i0 pðxþ : ¼z r2fðdd ; npðxþ2þiðxþddþddd ; ( 22 ) where fð/c1 ; npðxþ2þdenotes a 2d gaussian kernel with stan- dard deviation npðxþ . fig . 21 show the result . we can observe that dso be slightly more robust to', 'r2fðdd ; npðxþ2þiðxþddþddd ; ( 22 ) where fð/c1 ; npðxþ2þdenotes a 2d gaussian kernel with stan- dard deviation npðxþ . fig . 21 show the result . we can observe that dso be slightly more robust to photometric noise than orb-slam—this be because ( purely local ) key- point matching fails for high photometric noise , whereas a fig . 20 . geometric noise . effect of apply low-frequency geometric noise to the image , simulate geometric distortion such a a rolling shutter ( eval- uated on the tum-monovo dataset ) . the top row show an example image with dg¼2 . while the effect be hardly visible to the human eye ( observe that the close-up be slightly shift ) , it have a severe impact on slam accuracy , in particular when use a direct model . note that the distortion cause by a standard rolling shutter camera easily surpass dg¼3 . fig . 19 . residual pattern . errors on the tum-monovo dataset for some of the evaluated pattern np . using only a 3/c23neighborhood seem to perform slightly worse—using more than the propose 8-pixel pattern however seem to have little beneﬁt—at the same time , use a large neighbourhood increase the computational demand . note that these result may vary with low-level property of the use camera and lens , such a the point spread function.fig . 18 . number of keyframes . errors on the tum-monovo dataset , when change the number of keyframes take via the threshold tkf.622 ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 2018joint optimization of the photometric error well overcome the introduced distortion . to summarize : while the direct approach outperform the indirect approach on well-calibrated data , it be ill-suited in the presence of strong geometric noise , e.g. , originate from a roll shutter or inaccurate intrinsic calibration . in practice , this make the indirect model superior for smart- phone or off-the-shelf webcam , since these be design to capture video for human consumption-prioritizing reso- lution and light-sensitivity over', ', this make the indirect model superior for smart- phone or off-the-shelf webcam , since these be design to capture video for human consumption-prioritizing reso- lution and light-sensitivity over geometric precision . in turn , the direct approach offer superior performance on data cap- tured with dedicated camera for machine-vision , since these put more importance on geometric precision , rather than capture appeal image for human consumption . note that this can be resolve by tightly integrate the roll shutter into the model , a do , e.g. , in [ 15 ] , [ 18 ] , [ 19 ] . 4.4 qualitative results in addition to accurate camera tracking , dso compute 3d point on all gradient-rich area , include edges—resultingin point-cloud reconstruction similar to the semi-dense reconstruction of lsd-slam . the density then directly correspond to how many point we keep in the active win- dow np . fig . 23 show some example . fig . 22 show three more scene ( one from each dataset ) , together with some corresponding depth map . note that our approach be able to track through scene with very little texture , whereas indirect approach fail . all reconstruction show be sim- ply accumulate from the odometry , without integrate loop-closures . see the supplementary video for more quali- tative result , available online . failure modes . as for all monocular method , the pre- dominant cause of failure be degenerate motion in the form of almost-pure rotation , since no new point can be triangulate . this issue be be pliﬁed since dso be design a pure visual odometry , and do not reuse point once they leave the ﬁeld of view . in contrast , a full slam for- mulation ( a use by orb-slam ) allow to recycle previ- ously triangulated point once they re-enter the ﬁeld of fig . 21 . photometric noise . effect of apply high-frequent , non-isotropic blur to the image , simulate photometric noise ( evaluate on the tum- monovo dataset ) . the top row show an example image with dp¼6 , the effect be clearly visible . since the direct approach model', 'blur to the image , simulate photometric noise ( evaluate on the tum- monovo dataset ) . the top row show an example image with dp¼6 , the effect be clearly visible . since the direct approach model a photometric error , it be more robust to this type of noise than indirect method . fig . 23 . point density . 3d point cloud and some coarse depth map , i.e. , the most recent keyframe with all npactive point project into it ) for np¼500 ( top ) , np¼2 ; 000 ( middle ) , and np¼10 ; 000 ( bottom ) . fig . 22 . qualitative example . one scene from each dataset ( leave to right : v2_01_easy [ 1 ] , seq_38 [ 8 ] and ofﬁce_1 [ 10 ] ) , compute in real time with default setting . the bottom show some corresponding ( sparse ) depth maps-some scene contain very little texture , make them very challenge for indirect approaches.engel et al . : direct sparse odometry 623view ( implicit loop-closures a s discuss in section 4.1 ) , and thereby bridge over some period of degenerate cam- era motion . note that this issue stem from the employ windowed optimization stra tegy ( section 2.3 ) and be not inherent to the propose direct sparse model formulation ( section 2.2 ) . we have not observe signiﬁcant issue with non-lam- bertian surface , likely due to the concentration on high-gra- dient point . however , the propose direct model will fail when the scene light change drastically , e.g. , in the pres- ence of move light source or when the time of day change . while this be typically unproblematic for short- term visual odometry , it need to be address to facilitate life-long mapping , for instance by change the energy ( 4 ) to penalize gradient direction or magnitude difference instead of absolute intensity . 5c onclusion we have present a novel direct and sparse formulation for structure from motion . it combine the beneﬁts of direct method ( seamless ability to use & reconstruct all point instead of only corner ) with the ﬂexibility of sparse approach ( efﬁcient , joint optimization of all model param- eters ) . this be', 'direct method ( seamless ability to use & reconstruct all point instead of only corner ) with the ﬂexibility of sparse approach ( efﬁcient , joint optimization of all model param- eters ) . this be possible in real time by omit the geometric prior use by other direct method , and instead evaluate the photometric error for each point over a small neighbor- hood of pixel , to well-constrain the overall problem . fur- thermore , we incorporate full photometric calibration , complete the intrinsic camera model that traditionally only reﬂects the geometric component of the image forma- tion process . we have implement our direct & sparse model in the form of a monocular visual odometry algorithm , incremen- tally marginalize / eliminate old state to maintain real-time performance . to this end we have develop a front-end that perform data-selection and provide accu- rate initialization for optimize the highly non-convex energy function . our comprehensive evaluation on several hour of video show the superiority of the present for- mulation relative to state-of-the-art indirect method . we furthermore present an exhaustive parameter study , indi- cating that ( 1 ) simply use more data do not increase track accuracy ( although it make the 3d model denser ) , ( 2 ) use all point instead of only corner do provide a real gain in accuracy and robustness , and ( 3 ) incorporate photometric calibration do increase perfor- mance , in particular compare to the basic “ brightness constancy ” assumption . we have also show experimentally that the indirect approach—modeling a geometric error—is much more robust to geometric noise , e.g. , originate from a poor intrin- sic camera calibration or a rolling shutter . the direct approach be in turn more robust to photometric noise , and achieve superior accuracy on well-calibrated data . we believe this to be one of the main explanation for the recent revival of direct formulation after a dominance of indirect approach for more than a decade : for a long time , the pre- dominant', '. we believe this to be one of the main explanation for the recent revival of direct formulation after a dominance of indirect approach for more than a decade : for a long time , the pre- dominant source of digital image data be camera , which originally be design to capture image for human view- ing ( such a off-the-shelf webcam or integrate smartphonecameras ) . in this setting , the strong geometric distortion cause by roll shutter and imprecise lens favor the indirect approach . in turn , with 3d computer vision becom- ing an integral part of mass-market product ( include autonomous car and drone , as well a mobile device for vr and ar ) , camera be be develop speciﬁcally for this purpose , feature global shutter , precise lens , and high frame-rates—which allows direct formulation to real- ize their full potential . since the structure of the propose direct sparse energy formulation be the same a that of indirect method , it can be integrate with other optimization framework like ( double-windowed ) bundle adjustment [ 25 ] or incremental smoothing and mapping [ 14 ] . the main challenge here be the greatly increased degree of non-convexity compare to the indirect model , which originate from the inclusion of the image in the error function-this be likely to restrict the use of our model to video processing . acknowledgments this work be support through the erc consolidator grant “ 3d reloaded ” and through a google faculty research award . references [ 1 ] m. burri , et al. , “ the euroc micro aerial vehicle datasets , ” int . j . robot . res . , vol . 35 , pp . 1157–1163 , 2016 . [ 2 ] d. caruso , j. engel , and d. cremers , “ large-scale direct slam for omnidirectional camera , ” in proc . int . conf . intell . robot syst . , 2015 , pp . 141–148 . [ 3 ] j. civera , a. davison , and j. montiel , “ inverse depth parametriza- tion for monocular slam , ” ieee trans . robot . , vol . 24 , no . 5 , pp . 932–945 , oct. 2008 . [ 4 ] a. davison , i. reid , n. molton , and o. stasse , “ monoslam : real- time', 'depth parametriza- tion for monocular slam , ” ieee trans . robot . , vol . 24 , no . 5 , pp . 932–945 , oct. 2008 . [ 4 ] a. davison , i. reid , n. molton , and o. stasse , “ monoslam : real- time single camera slam , ” ieee trans . pattern anal . mach . intell . , vol . 29 , no . 6 , pp . 1052–1067 , jun . 2007 . [ 5 ] j. engel , t. sch €ops , and d. cremers , “ lsd-slam : large-scale direct monocular slam , ” in proc . eur . conf . comput . vis . , 2014 , pp . 834–849 . [ 6 ] j. engel , j. stueckler , and d. cremers , “ large-scale direct slam with stereo camera , ” in proc . ieee/rsj int . conf . intell . robot syst . , 2015 , pp . 1935–1942 . [ 7 ] j. engel , j. sturm , and d. cremers , “ scale-aware navigation of a low-cost quadrocopter with a monocular camera , ” robot . auton . syst. , vol . 62 , no . 11 , pp . 1646–1656 , 2014 . [ 8 ] j. engel , v. usenko , and d. cremers , “ a photometrically calibrate benchmark for monocular visual odometry , ” arxiv:1607.02555 , 2016 . [ 9 ] c. forster , m. pizzoli , and d. scaramuzza , “ svo : fast semi-direct monocular visual odometry , ” in proc . ieee int . conf . robot . autom . , 2014 , pp . 15–22 . [ 10 ] a. handa , t. whelan , j. mcdonald , and a. davison , “ a bench- mark for rgb-d visual odometry , 3d reconstruction and slam , ” inproc . ieee int . conf . robot . autom . , 2014 , pp . 1524–1531 . [ 11 ] g. p. huang , a. i. mourikis , and s. i. roumeliotis , “ a ﬁrst-esti- mate jacobian ekf for improve slam consistency , ” in proc . ieee/asme int . symp . exp . robot . , 2008 , pp . 619–624 . [ 12 ] h. jin , p. favaro , and s. soatto , “ real-time 3-d motion and struc- ture of point feature : front-end system for vision-based control and interaction , ” in proc . ieee conf . comput . vis . pattern recognit . , 2000 , pp . 778–779 . [ 13 ] h. jin , p. favaro , and s. soatto , “ a semi-direct approach to struc- ture from motion , ” visual comput . , vol . 19 , no . 6 , pp . 377–394 , 2003 . [ 14 ] m. kaess , h. johannsson , r. roberts , v. ila , j. leonard ,', 's. soatto , “ a semi-direct approach to struc- ture from motion , ” visual comput . , vol . 19 , no . 6 , pp . 377–394 , 2003 . [ 14 ] m. kaess , h. johannsson , r. roberts , v. ila , j. leonard , and f. dellaert , “ isam2 : incremental smoothing and mapping use the bayes tree , ” int . j . robot . res . , vol . 31 , no . 2 , pp . 217–236 , feb. 2012 . [ 15 ] c. kerl , j. stueckler , and d. cremers , “ dense continuous-time tracking and map with rolling shutter rgb-d camera , ” in proc . ieee int . conf . comput . vis . , 2015 , pp . 2264–2272.624 ieee transactions on pattern analysis and machine intelligence , vol . 40 , no . 3 , march 2018 [ 16 ] g. klein and d. murray , “ parallel tracking and mapping for small ar workspace , ” in proc . 6th ieee acm int . symp . mixed aug- mented reality , 2007 , pp . 225–234 . [ 17 ] s. leutenegger , s. lynen , m. bosse , r. siegwart , and p. furgale , “ keyframe-based visual–inertial odometry use nonlinear opti- mization , ” int . j . robot . res . , vol . 34 , no . 3 , pp . 314–334 , 2015 . [ 18 ] m. li , b. kim , and a. mourikis , “ real-time motion estimation on a cellphone use inertial sensing and a rolling-shutter camera , ” in proc . ieee int . conf . robot . autom . , 2013 , pp . 4712–4719 . [ 19 ] s. lovegrove , a. patron-perez , and g. sibley , “ spline fusion : a continuous-time representation for visual-inertial fusion with application to roll shutter camera , ” in proc . british mach . vis . conf . , 2013 , pp . 93.1–93.12 . [ 20 ] r. mur-artal , j. montiel , and j. tardos , “ orb-slam : a versatile and accurate monocular slam system , ” ieee trans . robot . , vol . 31 , no . 5 , pp . 1147–1163 , oct. 2015 . [ 21 ] r. newcombe , s. lovegrove , and a. davison , “ dtam : dense tracking and mapping in real-time , ” in proc . ieee int . conf . com- put . vis . , 2011 , pp . 2320–2327 . [ 22 ] m. pizzoli , c. forster , and d. scaramuzza , “ remode : probabilis- tic , monocular dense reconstruction in real time , ” in proc . ieee int . conf . robot . autom . , 2014 ,', 'pp . 2320–2327 . [ 22 ] m. pizzoli , c. forster , and d. scaramuzza , “ remode : probabilis- tic , monocular dense reconstruction in real time , ” in proc . ieee int . conf . robot . autom . , 2014 , pp . 2609–2616 . [ 23 ] r. ranftl , v. vineet , q. chen , and v. koltun , “ dense monocular depth estimation in complex dynamic scene , ” in proc . ieee conf . comput . vis . pattern recognit . , 2016 , pp . 4058–4066 . [ 24 ] t. sch €ops , j. engel , and d. cremers , “ semi-dense visual odometry for ar on a smartphone , ” in proc . ieee int . symp . mixed augmented reality , 2014 , pp . 145–150 . [ 25 ] h. strasdat , a. j. davison , j. m. m. montiel , and k. konolige , “ double window optimisation for constant time visual slam , ” in proc . ieee int . conf . comput . vis . , 2011 , pp . 2352–2359 . [ 26 ] j. st €uhmer , s. gumhold , and d. cremers , “ real-time dense geom- etry from a handheld camera , ” in proc . 32nd dagm conf . pattern recognit . , 2010 , pp . 11–20 . [ 27 ] l. valgaerts , a. bruhn , m. mainberger , and j. weickert , “ dense versus sparse approach for estimate the fundamental matrix , ” int . j. comput . vis . , vol . 96 , no . 2 , pp . 212–234 , 2012 . jakob engel receive the master ’ s degree in computer science from the technical university of munich , in 2011 , where he also spend 2012 to 2016 a full-time phd degree at the chair for computer vision and pattern recognition . he be a senior research scientist with oculus . in 2015 , he become a google phd fellow . in 2015 , he spend half a year at the intel visual computing lab . he join oculus research in july 2016 . vladlen koltun receive the phd degree in 2002 for new result in theoretical computational geome- try , spend three year at uc berkeley a a postdoc in the theory group , and join the stanford com- puter science faculty , in 2005 a a theoretician . he be the director of the intel visual computing lab . he switch to research in visual computing in 2007 and join intel a a principal researcher in 2015 to establish the visual computing', '. he be the director of the intel visual computing lab . he switch to research in visual computing in 2007 and join intel a a principal researcher in 2015 to establish the visual computing lab . daniel cremers receive the phd degree in com- puter science from the university of mannheim , germany . subsequently , he spend two year a a postdoctoral researcher with ucla and one year a a permanent researcher at siemens corporate research , princeton , new jersey . from 2005 until 2009 , he be associate professor with the univer- sity of bonn , germany . since 2009 he hold the chair for computer vision and pattern recognition with technical university of munich . in 2016 , he receive the gottfried-wilhelm leibniz award , the big award in german academia . `` for more information on this or any other computing topic , please visit our digital library at www.computer .org/publications/dlib.engel et al . : direct sparse odometry 625']",https://doi.org/10.3390/app14093945
7.pdf,"see discussion , st at , and author pr ofiles f or this public ation at : http : //www .researchgate.ne t/public ation/271823237 orb-slam : a versatile and accu rate monocular slam system article in ieee t ransactions on r obotics · oct ober 2015 doi : 10.1109/tro.2015.2463671 citations 7,144reads 28,578 3 author s : raul mur -art al univ ersity of z aragoza 7 publica tions 14,676 citations see profile j. m. m. montiel univ ersity of z aragoza 89 publica tions 14,566 citations see profile juan doming o tardos univ ersity of z aragoza 79 publica tions 24,322 citations see profile all c ontent f ollo wing this p age be uplo aded b y raul mur -art al on 18 sept ember 2015 . the user have r equest ed enhanc ement of the do wnlo aded file.this paper have be accept for publication in ieee transactions on robotics . doi : 10.1109/tro.2015.2463671 ieee xplore : http : //ieeexplore.ieee.org/xpl/articledetails.jsp ? arnumber=7219438 c 2015 ieee . personal use of this material be permit . permission from ieee must be obtain for all other us , in any current or future medium , include reprint /republishing this material for advertising or promotional purpose , create new collective work , for resale or redistribution to server or list , or reuse of any copyrighted component of this work in other works.ieee transactions on robotics 1 orb-slam : a versatile and accurate monocular slam system ra´ul mur-artal* , j. m. m. montiel , member , ieee , and juan d. tard ´os , member , ieee , abstract —this paper present orb-slam , a feature-based monocular slam system that operate in real time , in small and large , indoor and outdoor environment . the system be robust to severe motion clutter , allow wide baseline loop closing and relocalization , and include full automatic initialization . building on excellent algorithm of recent year , we design from scratch a novel system that use the same feature for all slam task : tracking , mapping , relocalization , and loop closing . a survival of the ﬁttest strategy that select the point and keyframes of the reconstruction lead to excellent robustness and generate a compact and trackable map that only grow if the scene content change , allow lifelong operation . we present an exhaustive evaluation in 27 sequence from the most popular datasets . orb- slam achieves unprecedented performance with respect to other state-of-the-art monocular slam approach . for the beneﬁt of the community , we make the source code public . index terms —lifelong mapping , localization , monocular vi- sion , recognition , slam i. i ntroduction bundle adjustment ( ba ) be know to provide ac- curate estimate of camera localization as well a a sparse geometrical reconstruction [ 1 ] , [ 2 ] , give that a strong network of match and good initial guess be provide . for long time this approach be consider unaffordable for real time application such a visual simultaneous localisation and mapping ( visual slam ) . visual slam have the goal of estimate the camera trajectory while reconstruct the environment . nowadays we know that to achieve accurate result at non-prohibitive computational cost , a real time slam algorithm have to provide ba with : corresponding observation of scene feature ( map point ) among a subset of select frame ( keyframes ) . as complexity grow with the number of keyframes , their selection should avoid unnecessary redundancy . a strong network conﬁguration of keyframes and point to produce accurate result , that be , a well spread set of keyframes observe point with signiﬁcant parallax and with plenty of loop closure match . an initial estimation of the keyframe pose and point location for the non-linear optimization . a local map in exploration where optimization be focus to achieve scalability . the ability to perform fast global optimization ( e.g . pose graph ) to close loop in real-time . this work be support by the direcci ´on general de investigaci ´on of spain under project dpi2012-32168 , the ministerio de educaci ´on scholarship fpu13/04175 and gobierno de arag ´on scholarship b121/13 . the author be with the instituto de investigaci ´on en ingenier ´ıa de arag ´on ( i3a ) , universidad de zaragoza , mar ´ıa de luna 1 , 50018 zaragoza , spain ( e-mail : raulmur @ unizar.es ; josemari @ unizar.es ; tardos @ unizar.es ) . * corresponding author.the ﬁrst real time application of ba be the visual odome- try work of mouragon et . al . [ 3 ] , follow by the ground break- ing slam work of klein and murray [ 4 ] , know a parallel tracking and mapping ( ptam ) . this algorithm , while limit to small scale operation , provide simple but effective method for keyframe selection , feature matching , point triangulation , camera localization for every frame , and relocalization after track failure . unfortunately several factor severely limit it application : lack of loop closing and adequate handling of occlusion , low invariance to viewpoint of the relocalization and the need of human intervention for map bootstrapping . in this work we build on the main idea of ptam , the place recognition work of g ´alvez-l ´opez and tard ´os [ 5 ] , the scale-aware loop closing of strasdat et . al [ 6 ] and the use of covisibility information for large scale operation [ 7 ] , [ 8 ] , to design from scratch orb-slam , a novel monocular slam system whose main contribution be : use of the same feature for all task : tracking , mapping , relocalization and loop closing . this make our system more efﬁcient , simple and reliable . we use orb feature [ 9 ] which allow real-time performance without gpus , provide good invariance to change in viewpoint and illumination . real time operation in large environment . thanks to the use of a covisibility graph , track and mapping be focus in a local covisible area , independent of global map size . real time loop closing base on the optimization of a pose graph that we call the essential graph . it be build from a span tree maintain by the system , loop closure link and strong edge from the covisibility graph . real time camera relocalization with signiﬁcant invari- ance to viewpoint and illumination . this allow recovery from track failure and also enhance map reuse . a new automatic and robust initialization procedure base on model selection that permit to create an initial map of planar and non-planar scene . asurvival of the ﬁttest approach to map point and keyframe selection that be generous in the spawning but very restrictive in the culling . this policy improve track- ing robustness , and enhances lifelong operation because redundant keyframes be discard . we present an extensive evaluation in popular public datasets from indoor and outdoor environment , include hand-held , car and robot sequence . notably , we achieve good camera localization accuracy than the state of the art in direct method [ 10 ] , which optimize directly over pixel intensity instead of feature reprojection error . we include a discussionieee transactions on robotics 2 in section ix-b on the possible cause that can make feature- base method more accurate than direct method . the loop closing and relocalization method here present be base on our previous work [ 11 ] . a preliminary version of the system be present in [ 12 ] . in the current paper we add the initialization method , the essential graph , and perfect all method involve . we also describe in detail all building block and perform an exhaustive experimental validation . to the best of our knowledge , this be the most complete and reliable solution to monocular slam , and for the beneﬁt of the community we make the source code public . demonstra- tion video and the code can be find in our project webpage1 . ii . r elated work a . place recognition the survey by williams et al . [ 13 ] compare several ap- proaches for place recognition and conclude that technique base on appearance , that be image to image matching , scale well in large environment than map to map or image to map method . within appearance base method , bag of word technique [ 14 ] , such a the probabilistic approach fab-map [ 15 ] , be to the fore because of their high efﬁciency . dbow2 [ 5 ] use for the ﬁrst time bag of binary word obtain from brief descriptor [ 16 ] along with the very efﬁcient fast feature detector [ 17 ] . this reduce in more than one order of magnitude the time need for feature extraction , compare to surf [ 18 ] and sift [ 19 ] feature that be use in bag of word approach so far . although the system demonstrate to be very efﬁcient and robust , the use of brief , neither rotation nor scale invariant , limit the system to in-plane trajectory and loop detection from similar viewpoint . in our previous work [ 11 ] , we propose a bag of word place recognizer build on dbow2 with orb [ 9 ] . orb be binary feature invariant to rotation and scale ( in a certain range ) , result in a very fast recognizer with good invariance to viewpoint . we demonstrate the high recall and robustness of the recognizer in four different datasets , require less than 39ms ( include feature extraction ) to retrieve a loop candidate from a 10k image database . in this work we use an improved version of that place recognizer , use covisibility information and return several hypothesis when query the database instead of just the best match . b. map initialization monocular slam require a procedure to create an initial map because depth can not be recover from a single image . one way to solve the problem be to initially track a known structure [ 20 ] . in the context of ﬁltering approach , point can be initialize with high uncertainty in depth use an inverse depth parametrization [ 21 ] , which hopefully will later converge to their real position . the recent semi-dense work of engel et al . [ 10 ] , follow a similar approach initialize the depth of the pixel to a random value with high variance . initialization method from two view either assumes locally scene planarity [ 4 ] , [ 22 ] and recover the relative camera pose 1http : //webdiis.unizar.es/ raulmur/orbslamfrom a homography use the method of faugeras et . al [ 23 ] , or compute an essential matrix [ 24 ] , [ 25 ] that model planar and general scene , use the ﬁve-point algorithm of nister [ 26 ] , which require to deal with multiple solution . both reconstruction method be not well constrain under low parallax and suffer from a twofold ambiguity solution if all point of a planar scene be close to one of the camera center [ 27 ] . on the other hand if a non-planar scene be see with parallax a unique fundamental matrix can be compute with the eight-point algorithm [ 2 ] and the relative camera pose can be recover without ambiguity . we present in section iv a new automatic approach base on model selection between a homography for planar scene and a fundamental matrix for non-planar scene . a statistical approach to model selection be propose by torr et al . [ 28 ] . under a similar rationale we have develop a heuristic initialization algorithm that take into account the risk of select a fundamental matrix in close to degenerate case ( i.e . planar , nearly planar , and low parallax ) , favor the selection of the homography . in the planar case , for the sake of safe operation , we refrain from initialize if the solution have a twofold ambiguity , a a corrupted solution could be select . we delay the initialization until the method produce a unique solution with signiﬁcant parallax . c. monocular slam monocular slam be initially solve by ﬁltering [ 20 ] , [ 21 ] , [ 29 ] , [ 30 ] . in that approach every frame be process by the ﬁlter to jointly estimate the map feature location and the camera pose . it have the drawback of waste computation in process consecutive frame with little new information and the accumulation of linearization error . on the other hand keyframe-based approach [ 3 ] , [ 4 ] estimate the map use only select frame ( keyframes ) allow to perform more costly but accurate bundle adjustment optimization , a mapping be not tie to frame-rate . strasdat et . al [ 31 ] demon- strated that keyframe-based technique be more accurate than ﬁltering for the same computational cost . the most representative keyframe-based slam system be probably ptam by klein and murray [ 4 ] . it be the ﬁrst work to introduce the idea of split camera tracking and mapping in parallel thread , and demonstrate to be successful for real time augment reality application in small environment . the original version be later improve with edge feature , a rotation estimation step during track and a good relocal- ization method [ 32 ] . the map point of ptam correspond to fast corner match by patch correlation . this make the point only useful for track but not for place recognition . in fact ptam do not detect large loop , and the relocalization be base on the correlation of low resolution thumbnail of the keyframes , yield a low invariance to viewpoint . strasdat et . al [ 6 ] present a large scale monocular slam system with a front-end base on optical ﬂow implement on a gpu , follow by fast feature match and motion- only ba , and a back-end base on sliding-window ba . loop closure be solve with a pose graph optimization with similarity constraint ( 7dof ) , that be able to correct the scaleieee transactions on robotics 3 drift appear in monocular slam . from this work we take the idea of loop closing with 7dof pose graph optimization and apply it to the essential graph deﬁned in section iii-d strasdat et . al [ 7 ] use the front-end of ptam , but per- form the track only in a local map retrieve from a covi- sibility graph . they propose a double window optimization back-end that continuously perform ba in the inner window , and pose graph in a limited-size outer window . however , loop closing be only effective if the size of the outer window be large enough to include the whole loop . in our system we take advantage of the excellent idea of use a local map base on covisibility , and build the pose graph from the covisibility graph , but apply them in a totally redesigned front- end and back-end . another difference be that , instead of use speciﬁc feature for loop detection ( surf ) , we perform the place recognition on the same tracked and map feature , obtain robust frame-rate relocalization and loop detection . pirker et . al [ 33 ] propose cd-slam , a very complete system include loop closing , relocalization , large scale oper- ation and effort to work on dynamic environment . however map initialization be not mention . the lack of a public implementation do not allow u to perform a comparison of accuracy , robustness or large-scale capability . the visual odometry of song et al . [ 34 ] us orb feature for track and a temporal sliding window ba back-end . in comparison our system be more general a they do not have global relocalization , loop closing and do not reuse the map . they be also use the know distance from the camera to the ground to limit monocular scale drift . lim et . al [ 25 ] , work publish after we submit our preliminary version of this work [ 12 ] , use also the same feature for track , map and loop detection . however the choice of brief limit the system to in-plane trajectory . their system only track point from the last keyframe so the map be not reuse if revisit ( similar to visual odometry ) and have the problem of grow unbounded . we compare qualitatively our result with this approach in section viii-e . the recent work of engel et . al [ 10 ] , know a lsd- slam , be able to build large scale semi-dense map , use direct method ( i.e . optimization directly over image pixel intensity ) instead of bundle adjustment over feature . their result be very impressive a the system be able to operate in real time , without gpu acceleration , build a semi-dense map , with more potential application for robotics than the sparse output generate by feature-based slam . nevertheless they still need feature for loop detection and their camera localization accuracy be signiﬁcantly low than in our system and ptam , a we show experimentally in section viii-b . this surprising result be discuss in section ix-b . in a halfway between direct and feature-based method be the semi-direct visual odometry svo of forster et al . [ 22 ] . without require to extract feature in every frame they be able to operate at high frame-rates obtain impressive result in quadracopters . however no loop detection be perform and the current implementation be mainly think for downward look camera . finally we want to discuss about keyframe selection . all visual slam work in the literature agree that run ba fig . 1 . orb-slam system overview , show all the step perform by the tracking , local mapping and loop closing thread . the main component of the place recognition module and the map be also show . with all the point and all the frame be not feasible . the work of strasdat et al . [ 31 ] show that the most cost- effective approach be to keep a much point a possible , while keep only non-redundant keyframes . the ptam approach be to insert keyframes very cautiously to avoid an excessive growth of the computational complexity . this restrictive keyframe insertion policy make the track fail in hard exploration condition . our survival of the ﬁttest strategy achieves unprecedented robustness in difﬁcult scenario by insert keyframes as quickly a possible , and remove later the redundant one , to avoid the extra cost . iii . s ystem overview a . feature choice one of the main design idea in our system be that the same feature use by the mapping and tracking be use for place recognition to perform frame-rate relocalization and loop detection . this make our system efﬁcient and avoid the need to interpolate the depth of the recognition feature from near slam feature a in previous work [ 6 ] , [ 7 ] . we requiere feature that need for extraction much less than 33ms per image , which exclude the popular sift ( 300ms ) [ 19 ] , surf ( 300ms ) [ 18 ] or the recent a-kaze ( 100ms ) [ 35 ] . to obtain general place recognition capability , we require rotation invariance , which exclude brief [ 16 ] and ldb [ 36 ] . we choose orb [ 9 ] , which be orient multi-scale fast corner with a 256 bit descriptor associate . they be ex- tremely fast to compute and match , while they have good invariance to viewpoint . this allow to match them from wide baseline , boost the accuracy of ba . we already show the good performance of orb for place recognition in [ 11 ] . while our current implementation make use of orb , the technique propose be not restrict to these feature . b . three threads : tracking , local mapping and loop closing our system , see an overview in fig . 1 , incorporates three thread that run in parallel : tracking , local mapping and loopieee transactions on robotics 4 closing . the tracking be in charge of localize the camera with every frame and decide when to insert a new keyframe . we perform ﬁrst an initial feature match with the previous frame and optimize the pose use motion-only ba . if the tracking be lose ( e.g . due to occlusion or abrupt movement ) , the place recognition module be use to perform a global relocalization . once there be an initial estimation of the camera pose and feature matchings , a local visible map be retrieve use the covisibility graph of keyframes that be maintain by the system , see fig . 2 ( a ) and fig . 2 ( b ) . then match with the local map point be search by reprojection , and camera pose be optimize again with all match . finally the track thread decides if a new keyframe be insert . all the tracking step be explain in detail in section v. the novel procedure to create an initial map be present in section iv . the local mapping process new keyframes and performs local ba to achieve an optimal reconstruction in the sur- rounding of the camera pose . new correspondence for un- matched orb in the new keyframe be search in connected keyframes in the covisibility graph to triangulate new point . some time after creation , base on the information gather during the tracking , an exigent point culling policy be apply in order to retain only high quality point . the local mapping be also in charge of cull redundant keyframes . we explain in detail all local mapping step in section vi . the loop closing search for loop with every new keyframe . if a loop be detect , we compute a similarity trans- formation that inform about the drift accumulate in the loop . then both side of the loop be align and duplicate point be fuse . finally a pose graph optimization over similarity constraint [ 6 ] be perform to achieve global consistency . the main novelty be that we perform the optimization over the essential graph , a sparser subgraph of the covisibility graph which be explain in section iii-d . the loop detection and correction step be explain in detail in section vii . we use the levenberg-marquardt algorithm implement in g2o [ 37 ] to carry out all optimization . in the appendix we describe the error term , cost function , and variable involve in each optimization . c. map points , keyframes and their selection each map point pistores : its 3d position xw ; iin the world coordinate system . the view direction ni , which be the mean unit vector of all it viewing direction ( the ray that join the point with the optical center of the keyframes that observe it ) . a representative orb descriptor di , which be the as- sociated orb descriptor whose ham distance be minimum with respect to all other associate descriptor in the keyframes in which the point be observe . the maximum dmax and minimum dmin distance at which the point can be observe , accord to the scale invariance limit of the orb feature . each keyframe kistores : the camera pose tiw , which be a rigid body transforma- tion that transform point from the world to the camera coordinate system . ( a ) keyframes ( blue ) , current cam- era ( green ) , mappoints ( black , red ) , current local mappoints ( red ) ( b ) covisibility graph ( c ) spanning tree ( green ) and loop closure ( red ) ( d ) essential graph fig . 2 . reconstruction and graph in the sequence fr3long ofﬁce household from the tum rgb-d benchmark [ 38 ] . the camera intrinsics , include focal length and princi- pal point . all the orb feature extract in the frame , associate or not to a map point , whose coordinate be undistorted if a distortion model be provide . map point and keyframes be create with a generous pol- icy , while a late very exigent cull mechanism be in charge of detect redundant keyframes and wrongly match or not trackable map point . this permit a ﬂexible map expansion during exploration , which boost track robustness under hard condition ( e.g . rotation , fast movement ) , while it size be bound in continual revisits to the same environment , i.e . lifelong operation . additionally our map contain very few outlier compare with ptam , at the expense of contain less point . culling procedure of map point and keyframes be explain in sections vi-b and vi-e respectively . d. covisibility graph and essential graph covisibility information between keyframes be very useful in several task of our system , and be represent a an undirected weighted graph a in [ 7 ] . each node be a keyframe and an edge between two keyframes exist if they share observation of the same map point ( at least 15 ) , be the weight of the edge the number of common map point . in order to correct a loop we perform a pose graph opti- mization [ 6 ] that distribute the loop closing error along the graph . in order not to include all the edge provide by the covisibility graph , which can be very dense , we propose to build an essential graph that retain all the node ( keyframes ) , but less edge , still preserve a strong network that yieldsieee transactions on robotics 5 accurate result . the system build incrementally a span tree from the initial keyframe , which provide a connected subgraph of the covisibility graph with minimal number of edge . when a new keyframe be insert , it be include in the tree link to the keyframe which share most point observation , and when a keyframe be erase by the culling policy , the system update the link affect by that keyframe . the essential graph contain the spanning tree , the subset of edge from the covisibility graph with high covisibility ( min= 100 ) , and the loop closure edge , result in a strong network of camera . fig . 2 show an example of a covisibility graph , span tree and associate essential graph . as show in the experiment of section viii-e , when perform the pose graph optimization , the solution be so accurate that an additional full bundle adjustment optimization barely improve the solution . the efﬁciency of the essential graph and the inﬂuence of the minis show at the end of section viii-e. e. bags of words place recognition the system have embed a bag of word place recognition module , base on dbow22 [ 5 ] , to perform loop detection and relocalization . visual word be just a discretization of the descriptor space , which be know a the visual vocabulary . the vocabulary be create ofﬂine with the orb descriptor extract from a large set of image . if the image be general enough , the same vocabulary can be use for different environ- ments get a good performance , a show in our previous work [ 11 ] . the system build incrementally a database that contain an invert index , which store for each visual word in the vocabulary , in which keyframes it have be see , so that query the database can be do very efﬁciently . the database be also update when a keyframe be delete by the culling procedure . because there exist visual overlap between keyframes , when query the database there will not exist a unique keyframe with a high score . the original dbow2 take this overlap into account , add up the score of image that be close in time . this have the limitation of not include keyframes view the same place but insert at a different time . instead we group those keyframes that be connect in the covisibility graph . in addition our database return all keyframe match whose score be high than the 75 % of the best score . an additional beneﬁt of the bag of word representation for feature matching be report in [ 5 ] . when we want to compute the correspondence between two set of orb feature , we can constraint the brute force match only to those feature that belong to the same node in the vocabulary tree at a certain level ( we select the second out of six ) , speed up the search . we use this trick when search match for triangulate new point , and at loop detection and relocalization . we also reﬁne the correspondence with an orientation consistency test , see [ 11 ] for detail , that discard outlier ensure a coherent rotation for all correspondence . 2https : //github.com/dorian3d/dbow2iv . a utomatic mapinitialization the goal of the map initialization be to compute the relative pose between two frame to triangulate an initial set of map point . this method should be independent of the scene ( planar or general ) and should not require human intervention to select a good two-view conﬁguration , i.e . a conﬁguration with signiﬁcant parallax . we propose to compute in parallel two geometrical model , a homography assume a planar scene and a fundamental matrix assume a non-planar scene . we then use a heuristic to select a model and try to recover the relative pose with a speciﬁc method for the select model . our method only initialize when it be certain that the two- view conﬁguration be safe , detect low-parallax case and the well-known twofold planar ambiguity [ 27 ] , avoid to initialize a corrupted map . the step of our algorithm be : 1 ) find initial correspondence : extract orb feature ( only at the ﬁnest scale ) in the current frame fcand search for match xc $ xrin the reference frame fr . if not enough match be find , reset the reference frame . 2 ) parallel computation of the two model : compute in parallel thread a homography hcrand a fundamental matrix fcr : xc=hcrxr xt cfcrxr= 0 ( 1 ) with the normalized dlt and 8-point algorithms respec- tively a explain in [ 2 ] inside a ransac scheme . to make homogeneous the procedure for both model , the number of iteration be preﬁxed and the same for both model , along with the point to be use at each iteration , 8 for the fundamental matrix , and 4 of them for the homography . at each iteration we compute a score smfor each model m ( hfor the homography , ffor the fundamental matrix ) : sm=x i","['see discussion , st at , and author pr ofiles f or this public ation at : http : //www .researchgate.ne t/public ation/271823237 orb-slam : a versatile and accu rate monocular slam system article in ieee t ransactions on r obotics · oct ober 2015 doi : 10.1109/tro.2015.2463671 citations 7,144reads 28,578 3 author s : raul mur -art al univ ersity of z aragoza 7 publica tions 14,676 citations see profile j. m. m. montiel univ ersity of z aragoza 89 publica tions 14,566 citations see profile juan doming o tardos univ ersity of z aragoza 79 publica tions 24,322 citations see profile all c ontent f ollo wing this p age be uplo aded b y raul mur -art al on 18 sept ember 2015 . the user have r equest ed enhanc ement of the do wnlo aded file.this paper have be accept for publication in ieee transactions on robotics . doi : 10.1109/tro.2015.2463671 ieee xplore : http : //ieeexplore.ieee.org/xpl/articledetails.jsp ? arnumber=7219438 c 2015 ieee . personal use of this material be permit . permission from ieee must be obtain for all other us , in any current or future medium , include reprint /republishing this material for advertising or promotional purpose , create new collective work , for resale or redistribution to server or list , or reuse of any copyrighted component of this work in other works.ieee transactions on robotics 1 orb-slam : a versatile and accurate monocular slam system ra´ul mur-artal* , j. m. m. montiel , member , ieee , and juan d. tard ´os , member , ieee , abstract —this paper present orb-slam , a feature-based monocular slam system that operate in real time , in small and large , indoor and outdoor environment . the system be robust to severe motion clutter , allow wide baseline loop closing and relocalization , and include full automatic initialization . building on excellent algorithm of recent year , we design from scratch a novel system that use the same feature for all slam task : tracking , mapping , relocalization , and loop closing . a survival of the ﬁttest strategy that select the point', ', we design from scratch a novel system that use the same feature for all slam task : tracking , mapping , relocalization , and loop closing . a survival of the ﬁttest strategy that select the point and keyframes of the reconstruction lead to excellent robustness and generate a compact and trackable map that only grow if the scene content change , allow lifelong operation . we present an exhaustive evaluation in 27 sequence from the most popular datasets . orb- slam achieves unprecedented performance with respect to other state-of-the-art monocular slam approach . for the beneﬁt of the community , we make the source code public . index terms —lifelong mapping , localization , monocular vi- sion , recognition , slam i. i ntroduction bundle adjustment ( ba ) be know to provide ac- curate estimate of camera localization as well a a sparse geometrical reconstruction [ 1 ] , [ 2 ] , give that a strong network of match and good initial guess be provide . for long time this approach be consider unaffordable for real time application such a visual simultaneous localisation and mapping ( visual slam ) . visual slam have the goal of estimate the camera trajectory while reconstruct the environment . nowadays we know that to achieve accurate result at non-prohibitive computational cost , a real time slam algorithm have to provide ba with : \\x0fcorresponding observation of scene feature ( map point ) among a subset of select frame ( keyframes ) . \\x0fas complexity grow with the number of keyframes , their selection should avoid unnecessary redundancy . \\x0fa strong network conﬁguration of keyframes and point to produce accurate result , that be , a well spread set of keyframes observe point with signiﬁcant parallax and with plenty of loop closure match . \\x0fan initial estimation of the keyframe pose and point location for the non-linear optimization . \\x0fa local map in exploration where optimization be focus to achieve scalability . \\x0fthe ability to perform fast global optimization ( e.g . pose graph ) to close loop in real-time . this', 'optimization . \\x0fa local map in exploration where optimization be focus to achieve scalability . \\x0fthe ability to perform fast global optimization ( e.g . pose graph ) to close loop in real-time . this work be support by the direcci ´on general de investigaci ´on of spain under project dpi2012-32168 , the ministerio de educaci ´on scholarship fpu13/04175 and gobierno de arag ´on scholarship b121/13 . the author be with the instituto de investigaci ´on en ingenier ´ıa de arag ´on ( i3a ) , universidad de zaragoza , mar ´ıa de luna 1 , 50018 zaragoza , spain ( e-mail : raulmur @ unizar.es ; josemari @ unizar.es ; tardos @ unizar.es ) . * corresponding author.the ﬁrst real time application of ba be the visual odome- try work of mouragon et . al . [ 3 ] , follow by the ground break- ing slam work of klein and murray [ 4 ] , know a parallel tracking and mapping ( ptam ) . this algorithm , while limit to small scale operation , provide simple but effective method for keyframe selection , feature matching , point triangulation , camera localization for every frame , and relocalization after track failure . unfortunately several factor severely limit it application : lack of loop closing and adequate handling of occlusion , low invariance to viewpoint of the relocalization and the need of human intervention for map bootstrapping . in this work we build on the main idea of ptam , the place recognition work of g ´alvez-l ´opez and tard ´os [ 5 ] , the scale-aware loop closing of strasdat et . al [ 6 ] and the use of covisibility information for large scale operation [ 7 ] , [ 8 ] , to design from scratch orb-slam , a novel monocular slam system whose main contribution be : \\x0fuse of the same feature for all task : tracking , mapping , relocalization and loop closing . this make our system more efﬁcient , simple and reliable . we use orb feature [ 9 ] which allow real-time performance without gpus , provide good invariance to change in viewpoint and illumination . \\x0freal time operation in large environment . thanks to the use', 'use orb feature [ 9 ] which allow real-time performance without gpus , provide good invariance to change in viewpoint and illumination . \\x0freal time operation in large environment . thanks to the use of a covisibility graph , track and mapping be focus in a local covisible area , independent of global map size . \\x0freal time loop closing base on the optimization of a pose graph that we call the essential graph . it be build from a span tree maintain by the system , loop closure link and strong edge from the covisibility graph . \\x0freal time camera relocalization with signiﬁcant invari- ance to viewpoint and illumination . this allow recovery from track failure and also enhance map reuse . \\x0fa new automatic and robust initialization procedure base on model selection that permit to create an initial map of planar and non-planar scene . \\x0fasurvival of the ﬁttest approach to map point and keyframe selection that be generous in the spawning but very restrictive in the culling . this policy improve track- ing robustness , and enhances lifelong operation because redundant keyframes be discard . we present an extensive evaluation in popular public datasets from indoor and outdoor environment , include hand-held , car and robot sequence . notably , we achieve good camera localization accuracy than the state of the art in direct method [ 10 ] , which optimize directly over pixel intensity instead of feature reprojection error . we include a discussionieee transactions on robotics 2 in section ix-b on the possible cause that can make feature- base method more accurate than direct method . the loop closing and relocalization method here present be base on our previous work [ 11 ] . a preliminary version of the system be present in [ 12 ] . in the current paper we add the initialization method , the essential graph , and perfect all method involve . we also describe in detail all building block and perform an exhaustive experimental validation . to the best of our knowledge , this be the most complete and reliable solution to', 'method involve . we also describe in detail all building block and perform an exhaustive experimental validation . to the best of our knowledge , this be the most complete and reliable solution to monocular slam , and for the beneﬁt of the community we make the source code public . demonstra- tion video and the code can be find in our project webpage1 . ii . r elated work a . place recognition the survey by williams et al . [ 13 ] compare several ap- proaches for place recognition and conclude that technique base on appearance , that be image to image matching , scale well in large environment than map to map or image to map method . within appearance base method , bag of word technique [ 14 ] , such a the probabilistic approach fab-map [ 15 ] , be to the fore because of their high efﬁciency . dbow2 [ 5 ] use for the ﬁrst time bag of binary word obtain from brief descriptor [ 16 ] along with the very efﬁcient fast feature detector [ 17 ] . this reduce in more than one order of magnitude the time need for feature extraction , compare to surf [ 18 ] and sift [ 19 ] feature that be use in bag of word approach so far . although the system demonstrate to be very efﬁcient and robust , the use of brief , neither rotation nor scale invariant , limit the system to in-plane trajectory and loop detection from similar viewpoint . in our previous work [ 11 ] , we propose a bag of word place recognizer build on dbow2 with orb [ 9 ] . orb be binary feature invariant to rotation and scale ( in a certain range ) , result in a very fast recognizer with good invariance to viewpoint . we demonstrate the high recall and robustness of the recognizer in four different datasets , require less than 39ms ( include feature extraction ) to retrieve a loop candidate from a 10k image database . in this work we use an improved version of that place recognizer , use covisibility information and return several hypothesis when query the database instead of just the best match . b. map initialization monocular slam require a procedure to create', 'recognizer , use covisibility information and return several hypothesis when query the database instead of just the best match . b. map initialization monocular slam require a procedure to create an initial map because depth can not be recover from a single image . one way to solve the problem be to initially track a known structure [ 20 ] . in the context of ﬁltering approach , point can be initialize with high uncertainty in depth use an inverse depth parametrization [ 21 ] , which hopefully will later converge to their real position . the recent semi-dense work of engel et al . [ 10 ] , follow a similar approach initialize the depth of the pixel to a random value with high variance . initialization method from two view either assumes locally scene planarity [ 4 ] , [ 22 ] and recover the relative camera pose 1http : //webdiis.unizar.es/ \\x18raulmur/orbslamfrom a homography use the method of faugeras et . al [ 23 ] , or compute an essential matrix [ 24 ] , [ 25 ] that model planar and general scene , use the ﬁve-point algorithm of nister [ 26 ] , which require to deal with multiple solution . both reconstruction method be not well constrain under low parallax and suffer from a twofold ambiguity solution if all point of a planar scene be close to one of the camera center [ 27 ] . on the other hand if a non-planar scene be see with parallax a unique fundamental matrix can be compute with the eight-point algorithm [ 2 ] and the relative camera pose can be recover without ambiguity . we present in section iv a new automatic approach base on model selection between a homography for planar scene and a fundamental matrix for non-planar scene . a statistical approach to model selection be propose by torr et al . [ 28 ] . under a similar rationale we have develop a heuristic initialization algorithm that take into account the risk of select a fundamental matrix in close to degenerate case ( i.e . planar , nearly planar , and low parallax ) , favor the selection of the homography . in the planar case , for the sake of safe', 'risk of select a fundamental matrix in close to degenerate case ( i.e . planar , nearly planar , and low parallax ) , favor the selection of the homography . in the planar case , for the sake of safe operation , we refrain from initialize if the solution have a twofold ambiguity , a a corrupted solution could be select . we delay the initialization until the method produce a unique solution with signiﬁcant parallax . c. monocular slam monocular slam be initially solve by ﬁltering [ 20 ] , [ 21 ] , [ 29 ] , [ 30 ] . in that approach every frame be process by the ﬁlter to jointly estimate the map feature location and the camera pose . it have the drawback of waste computation in process consecutive frame with little new information and the accumulation of linearization error . on the other hand keyframe-based approach [ 3 ] , [ 4 ] estimate the map use only select frame ( keyframes ) allow to perform more costly but accurate bundle adjustment optimization , a mapping be not tie to frame-rate . strasdat et . al [ 31 ] demon- strated that keyframe-based technique be more accurate than ﬁltering for the same computational cost . the most representative keyframe-based slam system be probably ptam by klein and murray [ 4 ] . it be the ﬁrst work to introduce the idea of split camera tracking and mapping in parallel thread , and demonstrate to be successful for real time augment reality application in small environment . the original version be later improve with edge feature , a rotation estimation step during track and a good relocal- ization method [ 32 ] . the map point of ptam correspond to fast corner match by patch correlation . this make the point only useful for track but not for place recognition . in fact ptam do not detect large loop , and the relocalization be base on the correlation of low resolution thumbnail of the keyframes , yield a low invariance to viewpoint . strasdat et . al [ 6 ] present a large scale monocular slam system with a front-end base on optical ﬂow implement on a gpu , follow by fast', 'of the keyframes , yield a low invariance to viewpoint . strasdat et . al [ 6 ] present a large scale monocular slam system with a front-end base on optical ﬂow implement on a gpu , follow by fast feature match and motion- only ba , and a back-end base on sliding-window ba . loop closure be solve with a pose graph optimization with similarity constraint ( 7dof ) , that be able to correct the scaleieee transactions on robotics 3 drift appear in monocular slam . from this work we take the idea of loop closing with 7dof pose graph optimization and apply it to the essential graph deﬁned in section iii-d strasdat et . al [ 7 ] use the front-end of ptam , but per- form the track only in a local map retrieve from a covi- sibility graph . they propose a double window optimization back-end that continuously perform ba in the inner window , and pose graph in a limited-size outer window . however , loop closing be only effective if the size of the outer window be large enough to include the whole loop . in our system we take advantage of the excellent idea of use a local map base on covisibility , and build the pose graph from the covisibility graph , but apply them in a totally redesigned front- end and back-end . another difference be that , instead of use speciﬁc feature for loop detection ( surf ) , we perform the place recognition on the same tracked and map feature , obtain robust frame-rate relocalization and loop detection . pirker et . al [ 33 ] propose cd-slam , a very complete system include loop closing , relocalization , large scale oper- ation and effort to work on dynamic environment . however map initialization be not mention . the lack of a public implementation do not allow u to perform a comparison of accuracy , robustness or large-scale capability . the visual odometry of song et al . [ 34 ] us orb feature for track and a temporal sliding window ba back-end . in comparison our system be more general a they do not have global relocalization , loop closing and do not reuse the map . they be also use the', 'for track and a temporal sliding window ba back-end . in comparison our system be more general a they do not have global relocalization , loop closing and do not reuse the map . they be also use the know distance from the camera to the ground to limit monocular scale drift . lim et . al [ 25 ] , work publish after we submit our preliminary version of this work [ 12 ] , use also the same feature for track , map and loop detection . however the choice of brief limit the system to in-plane trajectory . their system only track point from the last keyframe so the map be not reuse if revisit ( similar to visual odometry ) and have the problem of grow unbounded . we compare qualitatively our result with this approach in section viii-e . the recent work of engel et . al [ 10 ] , know a lsd- slam , be able to build large scale semi-dense map , use direct method ( i.e . optimization directly over image pixel intensity ) instead of bundle adjustment over feature . their result be very impressive a the system be able to operate in real time , without gpu acceleration , build a semi-dense map , with more potential application for robotics than the sparse output generate by feature-based slam . nevertheless they still need feature for loop detection and their camera localization accuracy be signiﬁcantly low than in our system and ptam , a we show experimentally in section viii-b . this surprising result be discuss in section ix-b . in a halfway between direct and feature-based method be the semi-direct visual odometry svo of forster et al . [ 22 ] . without require to extract feature in every frame they be able to operate at high frame-rates obtain impressive result in quadracopters . however no loop detection be perform and the current implementation be mainly think for downward look camera . finally we want to discuss about keyframe selection . all visual slam work in the literature agree that run ba fig . 1 . orb-slam system overview , show all the step perform by the tracking , local mapping and loop closing thread . the', 'selection . all visual slam work in the literature agree that run ba fig . 1 . orb-slam system overview , show all the step perform by the tracking , local mapping and loop closing thread . the main component of the place recognition module and the map be also show . with all the point and all the frame be not feasible . the work of strasdat et al . [ 31 ] show that the most cost- effective approach be to keep a much point a possible , while keep only non-redundant keyframes . the ptam approach be to insert keyframes very cautiously to avoid an excessive growth of the computational complexity . this restrictive keyframe insertion policy make the track fail in hard exploration condition . our survival of the ﬁttest strategy achieves unprecedented robustness in difﬁcult scenario by insert keyframes as quickly a possible , and remove later the redundant one , to avoid the extra cost . iii . s ystem overview a . feature choice one of the main design idea in our system be that the same feature use by the mapping and tracking be use for place recognition to perform frame-rate relocalization and loop detection . this make our system efﬁcient and avoid the need to interpolate the depth of the recognition feature from near slam feature a in previous work [ 6 ] , [ 7 ] . we requiere feature that need for extraction much less than 33ms per image , which exclude the popular sift ( \\x18300ms ) [ 19 ] , surf ( \\x18300ms ) [ 18 ] or the recent a-kaze ( \\x18100ms ) [ 35 ] . to obtain general place recognition capability , we require rotation invariance , which exclude brief [ 16 ] and ldb [ 36 ] . we choose orb [ 9 ] , which be orient multi-scale fast corner with a 256 bit descriptor associate . they be ex- tremely fast to compute and match , while they have good invariance to viewpoint . this allow to match them from wide baseline , boost the accuracy of ba . we already show the good performance of orb for place recognition in [ 11 ] . while our current implementation make use of orb , the technique propose be not restrict to these', 'the accuracy of ba . we already show the good performance of orb for place recognition in [ 11 ] . while our current implementation make use of orb , the technique propose be not restrict to these feature . b . three threads : tracking , local mapping and loop closing our system , see an overview in fig . 1 , incorporates three thread that run in parallel : tracking , local mapping and loopieee transactions on robotics 4 closing . the tracking be in charge of localize the camera with every frame and decide when to insert a new keyframe . we perform ﬁrst an initial feature match with the previous frame and optimize the pose use motion-only ba . if the tracking be lose ( e.g . due to occlusion or abrupt movement ) , the place recognition module be use to perform a global relocalization . once there be an initial estimation of the camera pose and feature matchings , a local visible map be retrieve use the covisibility graph of keyframes that be maintain by the system , see fig . 2 ( a ) and fig . 2 ( b ) . then match with the local map point be search by reprojection , and camera pose be optimize again with all match . finally the track thread decides if a new keyframe be insert . all the tracking step be explain in detail in section v. the novel procedure to create an initial map be present in section iv . the local mapping process new keyframes and performs local ba to achieve an optimal reconstruction in the sur- rounding of the camera pose . new correspondence for un- matched orb in the new keyframe be search in connected keyframes in the covisibility graph to triangulate new point . some time after creation , base on the information gather during the tracking , an exigent point culling policy be apply in order to retain only high quality point . the local mapping be also in charge of cull redundant keyframes . we explain in detail all local mapping step in section vi . the loop closing search for loop with every new keyframe . if a loop be detect , we compute a similarity trans- formation that inform about the', 'in detail all local mapping step in section vi . the loop closing search for loop with every new keyframe . if a loop be detect , we compute a similarity trans- formation that inform about the drift accumulate in the loop . then both side of the loop be align and duplicate point be fuse . finally a pose graph optimization over similarity constraint [ 6 ] be perform to achieve global consistency . the main novelty be that we perform the optimization over the essential graph , a sparser subgraph of the covisibility graph which be explain in section iii-d . the loop detection and correction step be explain in detail in section vii . we use the levenberg-marquardt algorithm implement in g2o [ 37 ] to carry out all optimization . in the appendix we describe the error term , cost function , and variable involve in each optimization . c. map points , keyframes and their selection each map point pistores : \\x0fits 3d position xw ; iin the world coordinate system . \\x0fthe view direction ni , which be the mean unit vector of all it viewing direction ( the ray that join the point with the optical center of the keyframes that observe it ) . \\x0fa representative orb descriptor di , which be the as- sociated orb descriptor whose ham distance be minimum with respect to all other associate descriptor in the keyframes in which the point be observe . \\x0fthe maximum dmax and minimum dmin distance at which the point can be observe , accord to the scale invariance limit of the orb feature . each keyframe kistores : \\x0fthe camera pose tiw , which be a rigid body transforma- tion that transform point from the world to the camera coordinate system . ( a ) keyframes ( blue ) , current cam- era ( green ) , mappoints ( black , red ) , current local mappoints ( red ) ( b ) covisibility graph ( c ) spanning tree ( green ) and loop closure ( red ) ( d ) essential graph fig . 2 . reconstruction and graph in the sequence fr3long ofﬁce household from the tum rgb-d benchmark [ 38 ] . \\x0fthe camera intrinsics , include focal length and princi- pal point . \\x0fall', 'graph fig . 2 . reconstruction and graph in the sequence fr3long ofﬁce household from the tum rgb-d benchmark [ 38 ] . \\x0fthe camera intrinsics , include focal length and princi- pal point . \\x0fall the orb feature extract in the frame , associate or not to a map point , whose coordinate be undistorted if a distortion model be provide . map point and keyframes be create with a generous pol- icy , while a late very exigent cull mechanism be in charge of detect redundant keyframes and wrongly match or not trackable map point . this permit a ﬂexible map expansion during exploration , which boost track robustness under hard condition ( e.g . rotation , fast movement ) , while it size be bound in continual revisits to the same environment , i.e . lifelong operation . additionally our map contain very few outlier compare with ptam , at the expense of contain less point . culling procedure of map point and keyframes be explain in sections vi-b and vi-e respectively . d. covisibility graph and essential graph covisibility information between keyframes be very useful in several task of our system , and be represent a an undirected weighted graph a in [ 7 ] . each node be a keyframe and an edge between two keyframes exist if they share observation of the same map point ( at least 15 ) , be the weight \\x12of the edge the number of common map point . in order to correct a loop we perform a pose graph opti- mization [ 6 ] that distribute the loop closing error along the graph . in order not to include all the edge provide by the covisibility graph , which can be very dense , we propose to build an essential graph that retain all the node ( keyframes ) , but less edge , still preserve a strong network that yieldsieee transactions on robotics 5 accurate result . the system build incrementally a span tree from the initial keyframe , which provide a connected subgraph of the covisibility graph with minimal number of edge . when a new keyframe be insert , it be include in the tree link to the keyframe which share most point observation ,', 'a connected subgraph of the covisibility graph with minimal number of edge . when a new keyframe be insert , it be include in the tree link to the keyframe which share most point observation , and when a keyframe be erase by the culling policy , the system update the link affect by that keyframe . the essential graph contain the spanning tree , the subset of edge from the covisibility graph with high covisibility ( \\x12min= 100 ) , and the loop closure edge , result in a strong network of camera . fig . 2 show an example of a covisibility graph , span tree and associate essential graph . as show in the experiment of section viii-e , when perform the pose graph optimization , the solution be so accurate that an additional full bundle adjustment optimization barely improve the solution . the efﬁciency of the essential graph and the inﬂuence of the \\x12minis show at the end of section viii-e. e. bags of words place recognition the system have embed a bag of word place recognition module , base on dbow22 [ 5 ] , to perform loop detection and relocalization . visual word be just a discretization of the descriptor space , which be know a the visual vocabulary . the vocabulary be create ofﬂine with the orb descriptor extract from a large set of image . if the image be general enough , the same vocabulary can be use for different environ- ments get a good performance , a show in our previous work [ 11 ] . the system build incrementally a database that contain an invert index , which store for each visual word in the vocabulary , in which keyframes it have be see , so that query the database can be do very efﬁciently . the database be also update when a keyframe be delete by the culling procedure . because there exist visual overlap between keyframes , when query the database there will not exist a unique keyframe with a high score . the original dbow2 take this overlap into account , add up the score of image that be close in time . this have the limitation of not include keyframes view the same place but insert at a', 'score . the original dbow2 take this overlap into account , add up the score of image that be close in time . this have the limitation of not include keyframes view the same place but insert at a different time . instead we group those keyframes that be connect in the covisibility graph . in addition our database return all keyframe match whose score be high than the 75 % of the best score . an additional beneﬁt of the bag of word representation for feature matching be report in [ 5 ] . when we want to compute the correspondence between two set of orb feature , we can constraint the brute force match only to those feature that belong to the same node in the vocabulary tree at a certain level ( we select the second out of six ) , speed up the search . we use this trick when search match for triangulate new point , and at loop detection and relocalization . we also reﬁne the correspondence with an orientation consistency test , see [ 11 ] for detail , that discard outlier ensure a coherent rotation for all correspondence . 2https : //github.com/dorian3d/dbow2iv . a utomatic mapinitialization the goal of the map initialization be to compute the relative pose between two frame to triangulate an initial set of map point . this method should be independent of the scene ( planar or general ) and should not require human intervention to select a good two-view conﬁguration , i.e . a conﬁguration with signiﬁcant parallax . we propose to compute in parallel two geometrical model , a homography assume a planar scene and a fundamental matrix assume a non-planar scene . we then use a heuristic to select a model and try to recover the relative pose with a speciﬁc method for the select model . our method only initialize when it be certain that the two- view conﬁguration be safe , detect low-parallax case and the well-known twofold planar ambiguity [ 27 ] , avoid to initialize a corrupted map . the step of our algorithm be : 1 ) find initial correspondence : extract orb feature ( only at the ﬁnest scale ) in the current frame', 'planar ambiguity [ 27 ] , avoid to initialize a corrupted map . the step of our algorithm be : 1 ) find initial correspondence : extract orb feature ( only at the ﬁnest scale ) in the current frame fcand search for match xc $ xrin the reference frame fr . if not enough match be find , reset the reference frame . 2 ) parallel computation of the two model : compute in parallel thread a homography hcrand a fundamental matrix fcr : xc=hcrxr xt cfcrxr= 0 ( 1 ) with the normalized dlt and 8-point algorithms respec- tively a explain in [ 2 ] inside a ransac scheme . to make homogeneous the procedure for both model , the number of iteration be preﬁxed and the same for both model , along with the point to be use at each iteration , 8 for the fundamental matrix , and 4 of them for the homography . at each iteration we compute a score smfor each model m ( hfor the homography , ffor the fundamental matrix ) : sm=x i\\x00 \\x1am\\x00 d2 cr ( xi c ; xi r ; m ) \\x01 +\\x1am ( d2 rc\\x00 xi c ; xi r ; m ) \\x01\\x01 \\x1am ( d2 ) = ( \\x00\\x00d2ifd2 < tm 0 ifd2\\x15tm ( 2 ) whered2 crandd2 rcare the symmetric transfer error [ 2 ] from one frame to the other . tmis the outlier rejection threshold base on the 2test at 95 % ( th= 5:99 , tf= 3:84 , assume a standard deviation of 1 pixel in the measurement error ) . \\x00is deﬁned equal to thso that both model score equally for the same din their inlier region , again to make the process homogeneous . we keep the homography and fundamental matrix with high score . if no model could be find ( not enough inliers ) , we restart the process again from step 1 . 3 ) model selection : if the scene be planar , nearly planar or there be low parallax , it can be explain by a homography . however a fundamental matrix can also be find , but the problem be not well constrain [ 2 ] and any attempt to recover the motion from the fundamental matrix would yieldieee transactions on robotics 6 wrong result . we should select the homography a the reconstruction method will correctly initialize from a plane or it will detect the low parallax case and', 'yieldieee transactions on robotics 6 wrong result . we should select the homography a the reconstruction method will correctly initialize from a plane or it will detect the low parallax case and refuse the initialization . on the other hand a non-planar scene with enough parallax can only be explain by the fundamental matrix , but a homography can also be find explain a subset of the match if they lie on a plane or they have low parallax ( they be far away ) . in this case we should select the fundamental matrix . we have find that a robust heuristic be to compute : rh=sh sh+sf ( 3 ) and select the homography if rh > 0:45 , which adequately capture the planar and low parallax case . otherwise , we select the fundamental matrix . 4 ) motion and structure from motion recovery : once a model be select we retrieve the motion hy- potheses associate . in the case of the homography we retrieve 8 motion hypothesis use the method of faugeras et . al [ 23 ] . the method propose cheriality test to select the valid solution . however these test fail if there be low parallax a point easily go in front or back of the camera , which could yield the selection of a wrong solution . we propose to directly triangulate the eight solution , and check if there be one solution with most point see with parallax , in front of both camera and with low reprojection error . if there be not a clear winner solution , we do not initialize and continue from step 1 . this technique to disambiguate the solution make our initialization robust under low parallax and the twofold ambiguity conﬁguration , and could be consider the key of the robustness of our method . in the case of the fundamental matrix , we convert it in an essential matrix use the calibration matrix k : erc=ktfrck ( 4 ) and then retrieve 4 motion hypothesis with the singular value decomposition method explain in [ 2 ] . we trian- gulate the four solution and select the reconstruction a do for the homography . 5 ) bundle adjustment : finally we perform a full ba , see the appendix', 'method explain in [ 2 ] . we trian- gulate the four solution and select the reconstruction a do for the homography . 5 ) bundle adjustment : finally we perform a full ba , see the appendix for detail , to reﬁne the initial reconstruction . an example of a challenging initialization in the outdoor newcollege robot sequence [ 39 ] be show in fig . 3 . it can be see how ptam and lsd-slam have initialize all point in a plane , while our method have wait until there be enough parallax , initialize correctly from the fundamental matrix . v. t racking in this section we describe the step of the track thread that be perform with every frame from the camera . the camera pose optimization , mention in several step , consist inmotion-only ba , which be describe in the appendix . fig . 3 . top : ptam , middle lsd-slam , bottom : orb-slam , some time after initialization in the newcollege sequence [ 39 ] . ptam and lsd-slam initialize a corrupted planar solution while our method have automatically initialize from the fundamental matrix when it have detect enough parallax . depending on which keyframes be manually select , ptam be also able to initialize well . a. orb extraction we extract fast corner at 8 scale level with a scale factor of 1.2 . for image resolution from 512\\x02384 to752\\x02480 pixel we find suitable to extract 1000 corner , for high resolution , a the 1241\\x02376 in the kitti dataset [ 40 ] we extract 2000 corner . in order to ensure an homogeneous distribution we divide each scale level in a grid , try to extract at least 5 corner per cell . then we detect corner in each cell , adapt the detector threshold if not enough corner be find . the amount of corner retain per cell be also adapt if some cell contain no corner ( textureless or low contrast ) . the orientation and orb descriptor be then compute on the retain fast corner . the orb descriptor be use in all feature match , in contrast to the search by patch correlation in ptam . b . initial pose estimation from previous frame if tracking be successful for last', 'corner . the orb descriptor be use in all feature match , in contrast to the search by patch correlation in ptam . b . initial pose estimation from previous frame if tracking be successful for last frame , we use a constant velocity motion model to predict the camera pose and perform a guided search of the map point observe in the last frame . if not enough match be find ( i.e . motion model be clearly violate ) , we use a wider search of the map point around their position in the last frame . the pose be then optimize with the found correspondences.ieee transactions on robotics 7 c. initial pose estimation via global relocalization if the tracking be lose , we convert the frame into bag of word and query the recognition database for keyframe candidate for global relocalization . we compute correspon- dences with orb associate to map point in each keyframe , a explain in section iii-e. we then perform alternatively ransac iteration for each keyframe and try to ﬁnd a camera pose use the pnp algorithm [ 41 ] . if we ﬁnd a camera pose with enough inliers , we optimize the pose and perform a guided search of more match with the map point of the candidate keyframe . finally the camera pose be again optimize , and if support with enough inliers , track procedure continue . d. track local map once we have an estimation of the camera pose and an initial set of feature match , we can project the map into the frame and search more map point correspondence . to bind the complexity in large map , we only project a local map . this local map contain the set of keyframes k1 , that share map point with the current frame , and a set k2with neighbor to the keyframesk1in the covisibility graph . the local map also have a reference keyframe kref2k 1which share most map point with current frame . now each map point see in k1andk2is search in the current frame a follow : 1 ) compute the map point projection xin the current frame . discard if it lay out of the image bound . 2 ) compute the angle between the current viewing ray v and', 'in the current frame a follow : 1 ) compute the map point projection xin the current frame . discard if it lay out of the image bound . 2 ) compute the angle between the current viewing ray v and the map point mean view direction n. discard if v\\x01n < co ( 60\\x0e ) . 3 ) compute the distance dfrom map point to camera center . discard if it be out of the scale invariance region of the map point d =2 [ dmin ; dmax ] . 4 ) compute the scale in the frame by the ratio d=d min . 5 ) compare the representative descriptor dof the map point with the still unmatched orb feature in the frame , at the predicted scale , and near x , and associate the map point with the best match . the camera pose be ﬁnally optimize with all the map point find in the frame . e. new keyframe decision the last step be to decide if the current frame be spawn a a new keyframe . as there be a mechanism in the local mapping to cull redundant keyframes , we will try to insert keyframes as fast a possible , because that make the track more robust to challenge camera movement , typically rotation . to insert a new keyframe all the following condition must be meet : 1 ) more than 20 frame must have pass from the last global relocalization . 2 ) local mapping be idle , or more than 20 frame have pass from last keyframe insertion . 3 ) current frame track at least 50 point . 4 ) current frame track less than 90 % point than kref . instead of use a distance criterion to other keyframes a ptam , we impose a minimum visual change ( condition4 ) . condition 1 ensure a good relocalization and condition 3 a good tracking . if a keyframe be insert when the local mapping be busy ( second part of condition 2 ) , a signal be send to stop local bundle adjustment , so that it can process as soon a possible the new keyframe . vi . l ocal mapping in this section we describe the step perform by the local mapping with every new keyframe ki . a. keyframe insertion at ﬁrst we update the covisibility graph , add a new node forkiand update the edge result from the share map', 'the step perform by the local mapping with every new keyframe ki . a. keyframe insertion at ﬁrst we update the covisibility graph , add a new node forkiand update the edge result from the share map point with other keyframes . we then update the spanning tree linkingkiwith the keyframe with most point in common . we then compute the bag of word representation of the keyframe , that will help in the data association for triangu- lating new point . b . recent map points culling map point , in order to be retain in the map , must pass a restrictive test during the ﬁrst three keyframes after creation , that ensure that they be trackable and not wrongly triangulate , i.e due to spurious data association . a point must fulﬁll these two condition : 1 ) the tracking must ﬁnd the point in more than the 25 % of the frame in which it be predict to be visible . 2 ) if more than one keyframe have pass from map point creation , it must be observe from at least three keyframes . once a map point have pass this test , it can only be remove if at any time it be observe from less than three keyframes . this can happen when keyframes be cull and when local bundle adjustment discard outlier observation . this policy make our map contain very few outlier . c. new map point creation new map point be create by triangulate orb from connect keyframes kcin the covisibility graph . for each unmatched orb in kiwe search a match with other un- match point in other keyframe . this matching be do a explain in section iii-e and discard those match that do not fulﬁll the epipolar constraint . orb pair be triangulate , and to accept the new point , positive depth in both camera , parallax , reprojection error and scale consistency be check . initially a map point be observe from two keyframes but it could be match in others , so it be project in the rest of connect keyframes , and correspondence be search a detail in section v-d. d. local bundle adjustment the local ba optimize the currently process keyframe ki , all the keyframes connect to it', 'rest of connect keyframes , and correspondence be search a detail in section v-d. d. local bundle adjustment the local ba optimize the currently process keyframe ki , all the keyframes connect to it in the covisibility graph kc , and all the map point see by those keyframes . all other keyframes that see those point but be not connect to theieee transactions on robotics 8 currently process keyframe be include in the optimization but remain ﬁxed . observations that be mark a outlier be discard at the middle and at the end of the optimization . see the appendix for more detail about this optimization . e. local keyframe culling in order to maintain a compact reconstruction , the local mapping try to detect redundant keyframes and delete them . this be beneﬁcial a bundle adjustment complexity grow with the number of keyframes , but also because it enable lifelong operation in the same environment a the number of keyframes will not grow unbounded , unless the visual content in the scene change . we discard all the keyframes in kcwhose 90 % of the map point have be see in at least other three keyframes in the same or ﬁner scale . the scale condition ensure that map point maintain keyframes from which they be measure with most accuracy . this policy be inspire by the one propose in the work of tan et . al [ 24 ] , where keyframes be discard after a process of change detection . vii . l oop closing the loop closing thread take ki , the last keyframe pro- cessed by the local mapping , and try to detect and close loop . the step be next described . a. loop candidates detection at ﬁrst we compute the similarity between the bag of word vector of kiand all it neighbor in the covisibility graph ( \\x12min= 30 ) and retain the low score smin . then we query the recognition database and discard all those keyframes whose score be low than smin . this be a similar operation to gain robustness a the normalize score in dbow2 , which be compute from the previous image , but here we use covisibility information . in addition all those', 'than smin . this be a similar operation to gain robustness a the normalize score in dbow2 , which be compute from the previous image , but here we use covisibility information . in addition all those keyframes directly connect to kiare discard from the result . to accept a loop candidate we must detect consecutively three loop candidate that be consistent ( keyframes connect in the covisibility graph ) . there can be several loop candidate if there be several place with similar appearance to ki . b. compute the similarity transformation in monocular slam there be seven degree of freedom in which the map can drift , three translation , three rotation and a scale factor [ 6 ] . therefore to close a loop we need to compute a similarity transformation from the current keyframe kito the loop keyframe klthat inform u about the error accumulate in the loop . the computation of this similarity will serve also as geometrical validation of the loop . we ﬁrst compute correspondence between orb associate to map point in the current keyframe and the loop candidate keyframes , follow the procedure explain in section iii-e. at this point we have 3d to 3d correspondence for each loop candidate . we alternatively perform ransac iteration with each candidate , try to ﬁnd a similarity transformation use the method of horn [ 42 ] . if we ﬁnd a similarity silwith enough inliers , we optimize it ( see the appendix ) , and performa guide search of more correspondence . we optimize it again and , if silis support by enough inliers , the loop with klis accept . c. loop fusion the ﬁrst step in the loop correction be to fuse duplicated map point and insert new edge in the covisibility graph that will attach the loop closure . at ﬁrst the current keyframe posetiwis correct with the similarity transformation sil and this correction be propagate to all the neighbor of ki , concatenate transformation , so that both side of the loop get align . all map point see by the loop keyframe and it neighbor be project into kiand it neighbor and match', 'to all the neighbor of ki , concatenate transformation , so that both side of the loop get align . all map point see by the loop keyframe and it neighbor be project into kiand it neighbor and match be search in a narrow area around the projection , a do in section v-d. all those map point match and those that be inliers in the computation of silare fuse . all keyframes involve in the fusion will update their edge in the covisibility graph effectively create edge that attach the loop closure . d. essential graph optimization to effectively close the loop , we perform a pose graph optimization over the essential graph , describe in section iii-d , that distribute the loop closing error along the graph . the optimization be perform over similarity transformation to correct the scale drift [ 6 ] . the error term and cost function be detail in the appendix . after the optimization each map point be transform accord to the correction of one of the keyframes that observe it . viii . e xperiments we have perform an extensive experimental validation of our system in the large robot sequence of newcollege [ 39 ] , evaluate the general performance of the system , in 16 hand- hold indoor sequence of the tum rgb-d benchmark [ 38 ] , evaluate the localization accuracy , relocalization and lifelong capability , and in 10 car outdoor sequence from the kitti dataset [ 40 ] , evaluate real-time large scale operation , local- ization accuracy and efﬁciency of the pose graph optimization . our system run in real time and process the image exactly at the frame rate they be acquire . we have carry out all experiment with an intel core i7-4700mq ( 4 core @ 2.40ghz ) and 8gb ram . orb-slam have three main thread , that run in parallel with other task from ros and the operating system , which introduce some randomness in the result . for this reason , in some experiment , we report the median from several run . a . system performance in the newcollege dataset the newcollege dataset [ 39 ] contain a 2.2km sequence from a robot traverse a', 'reason , in some experiment , we report the median from several run . a . system performance in the newcollege dataset the newcollege dataset [ 39 ] contain a 2.2km sequence from a robot traverse a campus and adjacent park . the sequence be record by a stereo camera at 20 fps and a resolu- tion512\\x02382 . it contain several loop and fast rotation that make the sequence quite challenge for monocular vision . to the best of our knowledge there be no other monocular system in the literature able to process this whole sequence . for example strasdat et al . [ 7 ] , despite be able to closeieee transactions on robotics 9 table ii loop closing times in newcollege loop detection ( m ) loop correction ( s ) loop keyframesessential graph edgescandidates detectionsimilarity transformationfusionessential graph optimizationtotal ( s ) 1 287 1347 4.71 20.77 0.20 0.26 0.51 2 1082 5950 4.14 17.98 0.39 1.06 1.52 3 1279 7128 9.82 31.29 0.95 1.26 2.27 4 2648 12547 12.37 30.36 0.97 2.30 3.33 5 3150 16033 14.71 41.28 1.73 2.80 4.60 6 4496 21797 13.52 48.68 0.97 3.62 4.69 table i tracking and mapping times in newcollege thread operationmedian ( m ) mean ( m ) std ( m ) trackingorb extraction 11.10 11.42 1.61 initial pose est . 3.38 3.45 0.99 track local map 14.84 16.01 9.98 total 30.57 31.60 10.39 local mappingkeyframe insertion 10.29 11.88 5.03 map point culling 0.10 3.18 6.70 map point creation 66.79 72.96 31.48 local ba 296.08 360.41 171.11 keyframe culling 8.07 15.79 18.98 total 383.59 464.27 217.89 fig . 4 . example of loop detect in the newcollege sequence . we draw the inlier correspondence support the similarity transformation find . loop and work in large scale environment , only show monocular result for a small part of this sequence . as an example of our loop closing procedure we show in fig . 4 the detection of a loop with the inliers that support the similarity transformation . fig . 5 show the reconstruction before and after the loop closure . in red it be show the local map , which after the loop closure extend along', 'inliers that support the similarity transformation . fig . 5 show the reconstruction before and after the loop closure . in red it be show the local map , which after the loop closure extend along both side of the loop closure . the whole map after process the full sequence at it real frame-rate be show in fig . 6 . the big loop on the right do not perfectly align because it be traverse in opposite direction and the place recognizer be not able to ﬁnd loop closure . we have extract statistic of the time spend by each thread in this experiment . table i show the result for the fig . 5 . map before and after a loop closure in the newcollege sequence . the loop closure match be draw in blue , the trajectory in green , and the local map for the tracking at that moment in red . the local map be extend along both side of the loop after it be close . tracking and the local mapping . tracking work at frame-rates around 25-30hz , be the most demanding task to track the local map . if need this time could be reduce limit the number of keyframes that be include in the local map . in the local mapping thread the most demanding task be local bundle adjustment . the local ba time varies if the robot be explore or in a well mapped area , because during exploration bundle adjustment be interrupt if track insert a new keyframe , a explain in section v-e . in case of not need new keyframes local bundle adjustment perform a generous number of preﬁxed iteration . table ii show the result for each of the 6 loop clo- sures find . it can be see how the loop detection increase sublinearly with the number of keyframes . this be due toieee transactions on robotics 10 fig . 6 . orb-slam reconstruction of the full sequence of newcollege . the big loop on the right be traverse in opposite direction and not visual loop closure be find , therefore they do not perfectly align . the efﬁcient querying of the database that only compare the subset of image with word in common , which demonstrate the potential of bag of word for place recognition .', 'they do not perfectly align . the efﬁcient querying of the database that only compare the subset of image with word in common , which demonstrate the potential of bag of word for place recognition . our essential graph include edge around 5 time the number of keyframes , which be a quite sparse graph . b. localization accuracy in the tum rgb-d benchmark the tum rgb-d benchmark [ 38 ] be an excellent dataset to evaluate the accuracy of camera localization a it provide several sequence with accurate ground truth obtain with an external motion capture system . we have discard all those sequence that we consider that be not suitable for pure monocular slam system , a they contain strong rotation , no texture or no motion . for comparison we have also execute the novel , direct , semi-dense lsd-slam [ 10 ] and ptam [ 4 ] in the benchmark . we compare also with the trajectory generate by rgbd- slam [ 43 ] which be provide for some of the sequence in the benchmark website . in order to compare orb-slam , lsd-slam and ptam with the ground truth , we align the keyframe trajectories use a similarity transformation , a scale be unknown , and measure the absolute trajectory error ( ate ) [ 38 ] . in the case of rgbd-slam we align the trajec- tory with a rigid body transformation , but also a similarity to check if the scale be well recover . lsd-slam initializes from random depth value and take time to converge , there- fore we have discard the ﬁrst 10 keyframes when compare with the ground truth . for ptam we manually select two frame from which we get a good initialization . table iii show the median result over 5 execution in each of the 16 sequence select . it can be see that orb-slam be able to process all the sequence , except for fr3nostructure texture far ( fr3 nstr texfar ) . this be a planar scene that because of the camera trajectory with respect to the plane have two possible interpretation , i.e . the twofold ambiguity describe in [ 27 ] . our initialization method detect the ambiguity and for safety refuse to', 'camera trajectory with respect to the plane have two possible interpretation , i.e . the twofold ambiguity describe in [ 27 ] . our initialization method detect the ambiguity and for safety refuse to initialize . ptam initialize select sometimesthe true solution and others the corrupted one , in which case the error be unacceptable . we have not notice two different reconstruction from lsd-slam but the error in this sequence be very high . in the rest of the sequence , ptam and lsd-slam exhibit less robustness than our method , loose track in eight and three sequence respectively . in term of accuracy orb-slam and ptam be similar in open trajectory , while orb-slam achieve high accuracy when detect large loop a in the sequence fr3nostructure texture near withloop ( fr3 nstr texnear ) . the most surprising result be that both ptam and orb- slam be clearly more accurate than lsd-slam and rgbd-slam . one of the possible cause can be that they reduce the map optimization to a pose-graph optimization be sensor measurement be discard , while we perform bundle adjustment and jointly optimize camera and map over sensor measurement , which be the gold standard algorithm to solve structure from motion [ 2 ] . we further discuss this result in section ix-b . another interesting result be that lsd-slam seem to be less robust to dynamic object than our system a see in fr2desk with person andfr3walking xyz . we have notice that rgbd-slam have a bias in the scale infr2sequences , a align the trajectory with 7 dof signiﬁcantly reduce the error . finally it should be note that engel et al . [ 10 ] report that ptam have less accuracy than lsd-slam in fr2xyzwith an rmse of 24.28cm . however , the paper do not give enough detail on how those result be obtain , and we have be unable to reproduce them . c. relocalization in the tum rgb-d benchmark we perform two relocalization experiment in the tum rgb-d benchmark . in the ﬁrst experiment we build a map with the ﬁrst 30 second of the sequence fr2xyzand perform global relocalization', 'benchmark we perform two relocalization experiment in the tum rgb-d benchmark . in the ﬁrst experiment we build a map with the ﬁrst 30 second of the sequence fr2xyzand perform global relocalization with every successive frame and evaluate the accuracy of the recovered pose . we perform the same experiment with ptam for comparison . fig . 7 show the keyframes use to create the initial map , the pose of the relocalized frame and the ground truth for those frame . it can be see that ptam be only able to relocalize frame which be near to the keyframes due to the little invariance of it relocalization method . table iv show the recall and the error with respect to the ground truth . orb-slam accurately relocalizes more than the double of frame than ptam . in the second experiment we create an initial map with se- quence fr3sitting xyzand try to relocalize all frame from fr3walking xyz . this be a challenging experiment a there be big occlusion due to people move in the scene . here ptam ﬁnds no relocalizations while our system relocalizes 78 % of the frame , a can be see in table iv . fig . 8 show some example of challenge relocalizations perform by our system in these experiment . d. lifelong experiment in the tum rgb-d benchmark previous relocalization experiment have show that our system be able to localize in a map from very different view- point and robustly under moderate dynamic change . this property in conjunction with our keyframe cull procedureieee transactions on robotics 11 table iii keyframe localization error comparison in the tum rgb-d benchmark [ 38 ] absolute keyframe trajectory rmse ( cm ) orb-slam ptam lsd-slamrgbd- slam fr1xyz 0.90 1.15 9.00 1.34 ( 1.34 ) fr2xyz 0.30 0.20 2.15 2.61 ( 1.42 ) fr1ﬂoor 2.99 x 38.07 3.51 ( 3.51 ) fr1desk 1.69 x 10.65 2.58 ( 2.52 ) fr2360 kidnap3.81 2.63 x 393.3 ( 100.5 ) fr2desk 0.88 x 4.57 9.50 ( 3.94 ) fr3long ofﬁce3.45 x 38.53 - fr3nstr texfarambiguity detected4.92 / 34.7418.31 - fr3nstr texnear1.39 2.74 7.54 - fr3str texfar0.77 0.93 7.95 - fr3str texnear1.58 1.04', 'fr2desk 0.88 x 4.57 9.50 ( 3.94 ) fr3long ofﬁce3.45 x 38.53 - fr3nstr texfarambiguity detected4.92 / 34.7418.31 - fr3nstr texnear1.39 2.74 7.54 - fr3str texfar0.77 0.93 7.95 - fr3str texnear1.58 1.04 x - fr2desk person0.63 x 31.73 6.97 ( 2.00 ) fr3sit xyz0.79 0.83 7.73 - fr3sit halfsph1.34 x 5.87 - fr3walk xyz1.24 x 12.44 - fr3walk halfsph1.74 x x - results for orb-slam , ptam and lsd-slam be the median over 5 execution in each sequence . the trajectory have be align with 7dof with the ground truth . trajectories for rgbd-slam be take from the benchmark website , only available for fr1 and fr2 sequence , and have be align with 6dof and 7dof ( result between bracket ) . x mean that the tracking be lose at some point and a signiﬁcant portion of the sequence be not process by the system . allow to operate lifelong in the same environment under different viewpoint and some dynamic change . in the case of a completely static scenario our system be able to maintain the number of keyframes bound even if the camera be look at the scene from different viewpoint . we demonstrate it in a custom sequence be the camera be look at the same desk during 93 second but perform a trajectory so that the viewpoint be always change . we compare the evolution of the number of keyframes in our map and those generate by ptam in fig . 9 . it can be see how ptam be always insert keyframes , while our mechanism to prune redundant keyframes make it number to saturate . while the lifelong operation in a static scenario should be a requirement of any slam system , more interesting be the case where dynamic change occur . we analyze the behavior of our system in such scenario by run consecutively the dynamic sequence from fr3 : sit xyz , sit halfsphere , sit rpy , 0.8 1 1.2 1.4 1.6−1−0.7−0.4−0.10.20.5 top view x [ m ] y [ m ] ptam r kfs gt0.8 1 1.2 1.4 1.611.21.41.61.82 frontal view x [ m ] z [ m ] ptam 0.8 1 1.2 1.4 1.6−1−0.7−0.4−0.10.20.5 top view x [ m ] y [ m ] orb−slam0.8 1 1.2 1.4 1.611.21.41.61.82 frontal view x [ m ] z [ m ]', 'gt0.8 1 1.2 1.4 1.611.21.41.61.82 frontal view x [ m ] z [ m ] ptam 0.8 1 1.2 1.4 1.6−1−0.7−0.4−0.10.20.5 top view x [ m ] y [ m ] orb−slam0.8 1 1.2 1.4 1.611.21.41.61.82 frontal view x [ m ] z [ m ] orb−slamfig . 7 . relocalization experiment in fr2xyz . map be initially create during the ﬁrst 30 second of the sequence ( kfs ) . the goal be to relocalize subsequent frame . successful relocalizations ( r ) of our system and ptam be show . the ground truth ( gt ) be only show for the frame to relocalize . fig . 8 . example of challenge relocalizations ( severe scale change , dynamic object ) that our system successfully find in the relocalization experiment . walk xyz , walk halfspehere and walk rpy . all the sequence focus the camera to the same desk but perform different trajectory , while people be move and change some object like chair . fig . 10 ( a ) show the evolution of the total number of keyframes in the map , and fig . 10 ( b ) show for each keyframe it frame of creation and destruction , show how long the keyframes have survive in the map . it can be see that during the ﬁrst two sequence the map size grows a all the view of the scene be be see for the ﬁrst time . in fig . 10 ( b ) we can see that several keyframes create during these two ﬁrst sequence be maintain in the map during the whole experiment . during the sequence sit rpyand walk xyzthe map do not grow , because the map create so far explain well the scene . in contrast , during the last two sequence , more keyframes be insert show that there be some novelty in the scene that be not yet represent , ieee transactions on robotics 12 table iv results for the relocalization experiments initial map relocalization system kfsrmse ( cm ) recall ( % ) rmse ( cm ) max . error ( cm ) fr2xyz . 2769 frame to relocalize ptam 37 0.19 34.9 0.26 1.52 orb-slam 24 0.19 78.4 0.38 1.67 fr3walking xyz . 859 frame to relocalize ptam 34 0.83 0.0 - - orb-slam 31 0.82 77.9 1.32 4.95 0 10 20 30 40 50 60 70 80 90 100020406080100120 time [ s ] keyframes orb−slam ptam fig', '78.4 0.38 1.67 fr3walking xyz . 859 frame to relocalize ptam 34 0.83 0.0 - - orb-slam 31 0.82 77.9 1.32 4.95 0 10 20 30 40 50 60 70 80 90 100020406080100120 time [ s ] keyframes orb−slam ptam fig . 9 . lifelong experiment in a static environment where the camera be always look at the same place from different viewpoint . ptam be always insert keyframes , while orb-slam be able to prune redundant keyframes and maintain a bounded-size map . due probably to dynamic change . finally fig . 10 ( c ) show a histogram of the keyframes accord to the time they have survive with respect to the remain time of the sequence from it moment of creation . it can be see that most of the keyframes be destroy by the culling procedure soon after creation , and only a small subset survive until the end of the experiment . on one hand , this show that our system have a generous keyframe spawning policy , which be very useful when perform abrupt motion in exploration . on the other hand the system be eventually able to select a small representative subset of those keyframes . in these lifelong experiment we have show that our map grow with the content of the scene but not with the time , and that be able to store the dynamic change of the scene which could be useful to perform some scene understanding by accumulate experience in an environment . e. large scale and large loop closing in the kitti dataset the odometry benchmark from the kitti dataset [ 40 ] contains 11 sequence from a car drive around a residential area with accurate ground truth from gps and a velodyne laser scanner . this be a very challenging dataset for monocular vision due to fast rotation , area with lot of foliage , which make more difﬁcult data association , and relatively high car speed , be the sequence record at 10 fps . we play the sequence at the real frame-rate they be record and orb- slam be able to process all the sequence by the exception of sequence 01 which be a highway with few trackable close object . sequences 00 , 02 , 05 , 06 , 07 , 09 contain', 'they be record and orb- slam be able to process all the sequence by the exception of sequence 01 which be a highway with few trackable close object . sequences 00 , 02 , 05 , 06 , 07 , 09 contain loop that 0 1000 2000 3000 4000 5000 6000020406080100 frameskeyframesxyz halfsphere rpy xyz halfsphere rpysitting sit sit walk walk walking ( a ) evolution of the number of keyframes in the map 0 1000 2000 3000 4000 5000 6000050100150200250300350400 frameskeyframe idxyz halfsphere rpy xyz halfsphere rpysitting sit sit walk walk walking ( b ) keyframe creation and destruction . each horizontal line corresponds to a keyframe , from it creation frame until it destruction 0 10 20 30 40 50 60 70 80 90 100010203040 % survival with respect to the remain sequence time % keyframes ( c ) histogram of the survival time of all spawn keyframes with respect to the remain time of the experiment fig . 10 . lifelong experiment in a dynamic environment from the tum rgb- d benchmark . be correctly detect and close by our system . sequence 09 contain a loop that can be detect only in a few frame at the end of the sequence , and our system not always detect it ( the result provide be for the execution in which it be detect ) . qualitative comparison of our trajectory and the ground truth be show in fig . 11 and fig . 12 . as in the tum rgb-d benchmark we have align the keyframe trajectory of our system and the ground truth with a similarity transformation . we can compare qualitatively our result from fig . 11 and fig . 12 with the result provide for sequence 00 , 05 , 06 , 07 and 08 by the recent monocular slam approach of lim et . al [ 25 ] in their ﬁgure 10 . orb-slam produce clearly more accurate trajectory for all those sequence by the exception of sequence 08 in which they seem to suffer less drift . table v show the median rmse error of the keyframe trajectory over ﬁve execution in each sequence . we also provide the dimension of the map to put in context the error . the result demonstrate that our system be very accurate beingieee', 'the keyframe trajectory over ﬁve execution in each sequence . we also provide the dimension of the map to put in context the error . the result demonstrate that our system be very accurate beingieee transactions on robotics 13 −300 −200 −100 0 100 200 300−1000100200300400500 x [ m ] y [ m ] ground truth orb−slam + 7dof alignment −300 −200 −100 0 100 200 300−1000100200300400500 x [ m ] y [ m ] ground truth orb−slam + global ba + 7dof alignment −300 −200 −100 0 100 200 300−150−100−50050100150200250300350400 x [ m ] y [ m ] ground truth orb−slam + 7dof alignment −300 −200 −100 0 100 200 300−150−100−50050100150200250300350400 x [ m ] y [ m ] ground truth orb−slam + global ba + 7dof alignment −200 −150 −100 −50 0−100−50050100 x [ m ] y [ m ] ground truth orb−slam + 7dof alignment −200 −150 −100 −50 0−100−50050100 x [ m ] y [ m ] ground truth orb−slam + global ba + 7dof alignment fig . 11 . sequences 00 , 05 and 07 from the odometry benchmark of the kitti dataset . left : point and keyframe trajectory . center : trajectory and ground truth . right : trajectory after 20 iteration of full ba . the output of our system be quite accurate , while it can be slightly improve with some iteration of ba . −100 0 100 200 300 400 500 600−20002004006008001000 x [ m ] y [ m ] ground truth orb−slam + 7dof alignment ( a ) sequence 02 −100 0 100 200 300 400 500−50050100150200 x [ m ] y [ m ] ground truth orb−slam + 7dof alignment ( b ) sequence 03 −50 −40 −30 −20 −10 0 10 20 30 40 50−50050100150200250300350400450 x [ m ] y [ m ] ground truth orb−slam + 7dof alignment ( c ) sequence 04 −200 −150 −100 −50 0 50 100 150 200−200−1000100200300 x [ m ] y [ m ] ground truth orb−slam + 7dof alignment ( d ) sequence 06 −400 −300 −200 −100 0 100 200 300 400 500−100−50050100150200250300350400450 x [ m ] y [ m ] ground truth orb−slam + 7dof alignment ( e ) sequence 08 −150 −100 −50 0 50 100 150 200 250 300 350−1000100200300400500600 x [ m ] y [ m ] ground truth orb−slam + 7dof alignment ( f ) sequence 09 0 100 200 300 400 500 600', '+ 7dof alignment ( e ) sequence 08 −150 −100 −50 0 50 100 150 200 250 300 350−1000100200300400500600 x [ m ] y [ m ] ground truth orb−slam + 7dof alignment ( f ) sequence 09 0 100 200 300 400 500 600 700−100−50050100150 x [ m ] y [ m ] ground truth orb−slam + 7dof alignment ( g ) sequence 10 fig . 12 . orb-slam keyframe trajectory in sequence 02 , 03 , 04 ,06 , 08 , 09 and 10 from the odometry benchmark of the kitti dataset . sequence 08 do not contain loop and drift ( especially scale ) be not corrected.ieee transactions on robotics 14 table v results of our system in the kitti dataset . orb-slam + global ba ( 20 it . ) sequencedimension ( m\\x02m ) kfsrmse ( m ) rmse ( m ) time ba ( s ) kitti 00 564\\x02496 1391 6.68 5.33 24.83 kitti 01 1157\\x021827 x x x x kitti 02 599\\x02946 1801 21.75 21.28 30.07 kitti 03 471\\x02199 250 1.59 1.51 4.88 kitti 04 0:5\\x02394 108 1.79 1.62 1.58 kitti 05 479\\x02426 820 8.23 4.85 15.20 kitti 06 23\\x02457 373 14.68 12.34 7.78 kitti 07 191\\x02209 351 3.36 2.26 6.28 kitti 08 808\\x02391 1473 46.58 46.68 25.60 kitti 09 465\\x02568 653 7.62 6.62 11.33 kitti 10 671\\x02177 411 8.68 8.80 7.64 the trajectory error typically around the 1 % of it dimension , sometimes less a in sequence 03 with an error of the 0.3 % or high a in sequence 08 with the 5 % . in sequence 08 there be no loop and drift can not be correct , which make clear the need of loop closure to achieve accurate reconstruction . in this experiment we have also check how much the reconstruction can be improve by perform 20 iteration offull ba , see the appendix for detail , at the end of each sequence . we have notice that some iteration of full ba slightly improve the accuracy in the trajectory with loop but it have negligible effect in open trajectory , which mean that the output of our system be already very accurate . in any case if the most accurate result be need our algorithm provide a set of match , which deﬁne a strong camera network , and an initial guess , so that full ba converge in few iteration . finally we want to show the efﬁcacy of our loop closing', 'our algorithm provide a set of match , which deﬁne a strong camera network , and an initial guess , so that full ba converge in few iteration . finally we want to show the efﬁcacy of our loop closing approach and the inﬂuence of the \\x12minused to include edge in the essential graph . we have select the sequence 09 ( a very long sequence with a loop closure at the end ) , and in the same execution we have evaluate different loop closing strategy . in table vi we show the keyframe trajectory rmse and the time spend in the optimization in different case : without loop closing , if we directly apply a full ba ( 20 or 100 iteration ) , if we apply only pose graph optimization ( 10 iteration with different number of edge ) and if we apply pose graph optimization and full ba afterwards . the result clearly show that before loop closure , the solution be so far from the optimal , that ba have convergence problem . even after 100 iteration still the error be very high . on the other hand essential graph optimization show fast convergence and more accurate result . it can be see that the choice of \\x12minhas not signiﬁcant effect in accuracy but decrease the number of edge the time can be signiﬁcantly reduce . performing an additional ba after the pose graph optimization slightly improve the accuracy while increase substantially the time.table vi comparison of loop closing strategies in kitti 09 method time ( s ) pose graph edges rmse ( m ) - - - 48.77 ba ( 20 ) 14.64 - 49.90 ba ( 100 ) 72.16 - 18.82 eg ( 200 ) 0.38 890 8.84 eg ( 100 ) 0.48 1979 8.36 eg ( 50 ) 0.59 3583 8.95 eg ( 15 ) 0.94 6663 8.88 eg ( 100 ) + ba ( 20 ) 13.40 1979 7.22 first row show result without loop closing . number between bracket for ba ( bundle adjustment ) mean number of levenberg-marquardt ( lm ) iteration , while for eg ( essential graph ) be the \\x12min to build the essential graph . all eg optimization perform 10 lm iteration . 100 0 100 200 300 x [ m ] 200 100 0100200300400500600y [ m ] ground truth estimated ( a ) without loop closing 100 0 100', 'to build the essential graph . all eg optimization perform 10 lm iteration . 100 0 100 200 300 x [ m ] 200 100 0100200300400500600y [ m ] ground truth estimated ( a ) without loop closing 100 0 100 200 300 x [ m ] 200 100 0100200300400500600y [ m ] ground truth estimated ( b ) ba ( 20 ) 100 0 100 200 300 x [ m ] 200 100 0100200300400500600y [ m ] ground truth estimated ( c ) eg ( 100 ) 100 0 100 200 300 x [ m ] 200 100 0100200300400500600y [ m ] ground truth estimated ( d ) eg ( 100 ) + ba ( 20 ) fig . 13 . comparison of different loop closing strategy in kitti 09 . ix . c onclusions and discussion a . conclusions in this work we have present a new monocular slam system with a detailed description of it building block and an exhaustive evaluation in public datasets . our system have demonstrate that it can process sequence from indoor and outdoor scene and from car , robot and hand-held motion . the accuracy of the system be typically below 1 cm in small indoor scenario and of a few meter in large outdoor scenario ( once we have align the scale with the ground truth ) . currently ptam by klein and murray [ 4 ] be consider the most accurate slam method from monocular video in real time . it be not coincidence that the backend of ptam be bundle adjustment , which be well know to be the gold standard method for the ofﬂine structure from motion problem [ 2 ] . one of the main success of ptam , and the early work of mouragnon [ 3 ] , be to bring that knowledge into the robotics slam community and demonstrate it real time performance.ieee transactions on robotics 15 the main contribution of our work be to expand the versatility of ptam to environment that be intractable for that system . to achieve this , we have design from scratch a new monoc- ular slam system with some new idea and algorithm , but also incorporate excellent work develop in the past few year , such a the loop detection of g ´alvez-l ´opez and tard ´os [ 5 ] , the loop closing procedure and covisibility graph of strasdat et.al [ 6 ] , [ 7 ] , the', 'work develop in the past few year , such a the loop detection of g ´alvez-l ´opez and tard ´os [ 5 ] , the loop closing procedure and covisibility graph of strasdat et.al [ 6 ] , [ 7 ] , the optimization framework g2o by kuemmerle et . al [ 37 ] and orb feature by rubble et . al [ 9 ] . to the best of our knowledge , no other system have demonstrate to work in as many different scenario and with such accuracy . therefore our system be currently the most reliable and complete solution for monocular slam . our novel policy to spawn and cull keyframes , permit to create keyframes every few frame , which be eventually remove when consider redundant . this ﬂexible map expansion be really useful in poorly condition exploration trajectory , i.e . close to pure rotation or fast movement . when operate repeatedly in the same environment , the map only grow if the visual content of the scene change , store a history of it different visual appearance . interesting result for long- term mapping could be extract analyze this history . finally we have also demonstrate that orb feature have enough recognition power to enable place recognition from severe viewpoint change . moreover they be so fast to extract and match ( without the need of multi-threading or gpu accel- eration ) that enable real time accurate tracking and mapping . b. sparse/feature-based vs. dense/direct methods recent real-time monocular slam algorithm such a dtam [ 44 ] and lsd-slam [ 10 ] be able to perform dense or semi dense reconstruction of the environment , while the camera be localize by optimize directly over image pixel intensity . these direct approach do not need feature ex- traction and thus avoid the corresponding artifact . they be also more robust to blur , low-texture environment and high- frequency texture like asphalt [ 45 ] . their denser reconstruc- tions , a compare to the sparse point map of our system or ptam , could be more useful for other task than just camera localization . however , direct method have their own limitation .', 'reconstruc- tions , a compare to the sparse point map of our system or ptam , could be more useful for other task than just camera localization . however , direct method have their own limitation . firstly , these method assume a surface reﬂectance model that in real scene produce it own artifact . the photometric consistency limit the baseline of the match , typically narrow than those that feature allow . this have a great impact in recon- struction accuracy , which require wide baseline observation to reduce depth uncertainty . direct method , if not correctly model , be quite affect by rolling-shutter , auto-gain and auto-exposure artifact ( a in the tum rgb-d benchmark ) . finally , because direct method be in general very computa- tionally demand , the map be just incrementally expand a in dtam , or map optimization be reduce to a pose graph , discard all sensor measurement a in lsd-slam . in contrast , feature-based method be able to match feature with a wide baseline , thanks to their good invariance to viewpoint and illumination change . bundle adjustment jointly optimize camera pose and point over sensor measurements.in the context of structure and motion estimation , torr and zisserman [ 46 ] already point the beneﬁts of feature-based against direct method . in this work we provide experimental evidence ( see section viii-b ) of the superior accuracy of feature-based method in real-time slam . we consider that the future of monocular slam should incorporate the best of both approach . c. future work the accuracy of our system can still be improve incorpo- rating point at inﬁnity in the tracking . these point , which be not see with sufﬁcient parallax and our system do not include in the map , be very informative of the rotation of the camera [ 21 ] . another open way be to upgrade the sparse map of our system to a denser and more useful reconstruction . thanks to our keyframe selection , keyframes comprise a compact summary of the environment with a very high pose accuracy and rich information of', ""system to a denser and more useful reconstruction . thanks to our keyframe selection , keyframes comprise a compact summary of the environment with a very high pose accuracy and rich information of covisibility . therefore the orb-slam sparse map can be an excellent initial guess and skeleton , on top of which a dense and accurate map of the scene can be build . a ﬁrst effort in this line be present in [ 47 ] . appendix non-linear optimizations \\x0fbundle adjustment ( ba ) [ 1 ] : map point 3d location xw ; j2r3and keyframe pose tiw2se ( 3 ) , wherewstands for the world reference , be optimize minimize the reprojection error with respect to the matched keypoints xi ; j2r2 . the error term for the observation of a map point jin a keyframe ii : ei ; j=xi ; j\\x00\\x19i ( tiw ; xw ; j ) ( 5 ) where\\x19iis the projection function : \\x19i ( tiw ; xw ; j ) = '' fi ; uxi ; j zi ; j+ci ; u fi ; vyi ; j zi ; j+ci ; v # \\x02 xi ; jyi ; jzi ; j\\x03t=riwxw ; j+tiw ( 6 ) where riw2so ( 3 ) andtiw2r3are respectively the rotation and translation part of tiw , and ( fi ; u ; fi ; v ) and ( ci ; u ; ci ; v ) be the focal length and principle point associate to camera i . the cost function to be minimize be : c=x i ; j\\x1ah ( et i ; j \\x001 i ; jei ; j ) ( 7 ) where\\x1ahis the huber robust cost function and i ; j= \\x1b2 i ; ji2\\x022is the covariance matrix associate to the scale at which the keypoint be detect . in case of full ba ( use in the map initialization explain in section iv and in the experiment in section viii-e ) we optimize all point and keyframes , by the exception of the ﬁrst keyframe which remain ﬁxed a the origin . in local ba ( see section vi-d ) all point include in the local area be optimize , while a subset of keyframes be ﬁxed . in pose optimization , or motion-only ba , ( see section v ) all point be ﬁxed and only the camera pose be optimized.ieee transactions on robotics 16 \\x0fpose graph optimization over sim ( 3 ) constraints [ 6 ] : given a pose graph of binary edge ( see section vii-d ) we deﬁne the error in an edge a : ei ; j= logsim ( 3 ) ("", 'on robotics 16 \\x0fpose graph optimization over sim ( 3 ) constraints [ 6 ] : given a pose graph of binary edge ( see section vii-d ) we deﬁne the error in an edge a : ei ; j= logsim ( 3 ) ( sijsjws\\x001 iw ) ( 8 ) where sijis the relative sim ( 3 ) transformation between both keyframes compute from the se ( 3 ) pose just before the pose graph optimization and set the scale factor to 1 . in the case of the loop closure edge this relative transformation be compute with the method of horn [ 42 ] . the logsim3 [ 48 ] transforms to the tangent space , so that the error be a vector in r7 . the goal be to optimize the sim ( 3 ) keyframe pose minimize the cost function : c=x i ; j ( et i ; j\\x03i ; jei ; j ) ( 9 ) where \\x03i ; jis the information matrix of the edge , which , a in [ 48 ] , we set to the identity . we ﬁx the loop closure keyframe to ﬁx the 7 degree of gauge freedom . although this method be a rough approximation of a full ba , we demonstrate experimentally in section viii-e that it have signiﬁcantly faster and good convergence than ba . \\x0frelative sim ( 3 ) optimization : given a set of nmatchesi ) j ( keypoints and their associated 3d map point ) between keyframe 1and keyframe 2 , we want to optimize the relative sim ( 3 ) transformation s12 ( see section vii-b ) that minimize the reprojection error in both image : e1=x1 ; i\\x00\\x191 ( s12 ; x2 ; j ) e2=x2 ; j\\x00\\x192 ( s\\x001 12 ; x1 ; i ) ( 10 ) and the cost function to minimize be : c=x n\\x00 \\x1ah ( et 1 \\x001 1 ; ie1 ) +\\x1ah ( et 2 \\x001 2 ; je2 ) \\x01 ( 11 ) where 1 ; iand 2 ; iare the covariance matrices associ- ated to the scale in which keypoints in image 1 and image 2 be detect . in this optimization the point be ﬁxed . references [ 1 ] b. triggs , p. f. mclauchlan , r. i. hartley , and a. w. fitzgibbon , “ bundle adjustment a modern synthesis , ” in vision algorithm : theory and practice , 2000 , pp . 298–372 . [ 2 ] r. hartley and a. zisserman , multiple view geometry in computer vision , 2nd ed . cambridge university press , 2004 . [ 3 ] e. mouragnon , m. lhuillier , m. dhome , f.', ', 2000 , pp . 298–372 . [ 2 ] r. hartley and a. zisserman , multiple view geometry in computer vision , 2nd ed . cambridge university press , 2004 . [ 3 ] e. mouragnon , m. lhuillier , m. dhome , f. dekeyser , and p. sayd , “ real time localization and 3d reconstruction , ” in computer vision and pattern recognition , 2006 ieee computer society conference on , vol . 1 , 2006 , pp . 363–370 . [ 4 ] g. klein and d. murray , “ parallel tracking and mapping for small ar workspace , ” in ieee and acm international symposium on mixed and augmented reality ( ismar ) , nara , japan , november 2007 , pp . 225–234 . [ 5 ] d. g ´alvez-l ´opez and j. d. tard ´os , “ bags of binary word for fast place recognition in image sequence , ” ieee transactions on robotics , vol . 28 , no . 5 , pp . 1188–1197 , 2012 . [ 6 ] h. strasdat , j. m. m. montiel , and a. j. davison , “ scale drift-aware large scale monocular slam. ” in robotics : science and systems ( rss ) , zaragoza , spain , june 2010 . [ 7 ] h. strasdat , a. j. davison , j. m. m. montiel , and k. konolige , “ double window optimisation for constant time visual slam , ” in ieee international conference on computer vision ( iccv ) , barcelona , spain , november 2011 , pp . 2352–2359 . [ 8 ] c. mei , g. sibley , and p. newman , “ closing loop without place , ” in ieee/rsj international conference on intelligent robots and systems ( iros ) , taipei , taiwan , october 2010 , pp . 3738–3744 . [ 9 ] e. rublee , v . rabaud , k. konolige , and g. bradski , “ orb : an efﬁcient alternative to sift or surf , ” in ieee international conference on computer vision ( iccv ) , barcelona , spain , november 2011 , pp . 2564– 2571 . [ 10 ] j. engel , t. sch ¨ops , and d. cremers , “ lsd-slam : large-scale di- rect monocular slam , ” in european conference on computer vision ( eccv ) , zurich , switzerland , september 2014 , pp . 834–849 . [ 11 ] r. mur-artal and j. d. tard ´os , “ fast relocalisation and loop closing in keyframe-based slam , ” in ieee international conference on robotics', ', switzerland , september 2014 , pp . 834–849 . [ 11 ] r. mur-artal and j. d. tard ´os , “ fast relocalisation and loop closing in keyframe-based slam , ” in ieee international conference on robotics and automation ( icra ) , hong kong , china , june 2014 , pp . 846–853 . [ 12 ] —— , “ orb-slam : tracking and map recognizable feature , ” in mvigro workshop at robotics science and systems ( rss ) , berkeley , usa , july 2014 . [ 13 ] b. williams , m. cummins , j. neira , p. newman , i. reid , and j. d. tard ´os , “ a comparison of loop closing technique in monocular slam , ” robotics and autonomous systems , vol . 57 , no . 12 , pp . 1188–1197 , 2009 . [ 14 ] d. nister and h. stewenius , “ scalable recognition with a vocabulary tree , ” in ieee computer society conference on computer vision and pattern recognition ( cvpr ) , vol . 2 , new york city , usa , june 2006 , pp . 2161–2168 . [ 15 ] m. cummins and p. newman , “ appearance-only slam at large scale with fab-map 2.0 , ” the international journal of robotics research , vol . 30 , no . 9 , pp . 1100–1123 , 2011 . [ 16 ] m. calonder , v . lepetit , c. strecha , and p. fua , “ brief : binary robust independent elementary features , ” in european conference on computer vision ( eccv ) , hersonissos , greece , september 2010 , pp . 778–792 . [ 17 ] e. rosten and t. drummond , “ machine learn for high-speed corner detection , ” in european conference on computer vision ( eccv ) , graz , austria , may 2006 , pp . 430–443 . [ 18 ] h. bay , t. tuytelaars , and l. van gool , “ surf : speeded up robust features , ” in european conference on computer vision ( eccv ) , graz , austria , may 2006 , pp . 404–417 . [ 19 ] d. g. lowe , “ distinctive image feature from scale-invariant keypoints , ” international journal of computer vision , vol . 60 , no . 2 , pp . 91–110 , 2004 . [ 20 ] a. j. davison , i. d. reid , n. d. molton , and o. stasse , “ monoslam : real-time single camera slam , ” ieee transactions on pattern analysis and machine intelligence , vol . 29 , no . 6 ,', '. [ 20 ] a. j. davison , i. d. reid , n. d. molton , and o. stasse , “ monoslam : real-time single camera slam , ” ieee transactions on pattern analysis and machine intelligence , vol . 29 , no . 6 , pp . 1052–1067 , 2007 . [ 21 ] j. civera , a. j. davison , and j. m. m. montiel , “ inverse depth parametrization for monocular slam , ” ieee transactions on robotics , vol . 24 , no . 5 , pp . 932–945 , 2008 . [ 22 ] c. forster , m. pizzoli , and d. scaramuzza , “ svo : fast semi-direct monocular visual odometry , ” in proc . ieee intl . conf . on robotics and automation , hong kong , china , june 2014 , pp . 15–22 . [ 23 ] o. d. faugeras and f. lustman , “ motion and structure from motion in a piecewise planar environment , ” international journal of pattern recognition and artiﬁcial intelligence , vol . 2 , no . 03 , pp . 485–508 , 1988 . [ 24 ] w. tan , h. liu , z. dong , g. zhang , and h. bao , “ robust monocular slam in dynamic environment , ” in ieee international symposium on mixed and augmented reality ( ismar ) , adelaide , australia , october 2013 , pp . 209–218 . [ 25 ] h. lim , j. lim , and h. j. kim , “ real-time 6-dof monocular visual slam in a large-scale environment , ” in ieee international conference on robotics and automation ( icra ) , hong kong , china , june 2014 , pp . 1532–1539 . [ 26 ] d. nist ´er , “ an efﬁcient solution to the ﬁve-point relative pose prob- lem , ” ieee transactions on pattern analysis and machine intelligence , vol . 26 , no . 6 , pp . 756–770 , 2004 . [ 27 ] h. longuet-higgins , “ the reconstruction of a plane surface from two perspective projection , ” proceedings of the royal society of london . series b . biological sciences , vol . 227 , no . 1249 , pp . 399–410 , 1986 . [ 28 ] p. h. torr , a. w. fitzgibbon , and a. zisserman , “ the problem of degeneracy in structure and motion recovery from uncalibrated image sequence , ” international journal of computer vision , vol . 32 , no . 1 , pp . 27–44 , 1999 . [ 29 ] a. chiuso , p. favaro , h. jin , and s. soatto , “', 'and motion recovery from uncalibrated image sequence , ” international journal of computer vision , vol . 32 , no . 1 , pp . 27–44 , 1999 . [ 29 ] a. chiuso , p. favaro , h. jin , and s. soatto , “ structure from motion causally integrate over time , ” ieee transactions on pattern analysis and machine intelligence , vol . 24 , no . 4 , pp . 523–535 , 2002 . [ 30 ] e. eade and t. drummond , “ scalable monocular slam , ” in ieee com- puter society conference on computer vision and pattern recognition ( cvpr ) , vol . 1 , new york city , usa , june 2006 , pp . 469–476 . [ 31 ] h. strasdat , j. m. m. montiel , and a. j. davison , “ visual slam : why ﬁlter ? ” image and vision computing , vol . 30 , no . 2 , pp . 65–77 , 2012.ieee transactions on robotics 17 [ 32 ] g. klein and d. murray , “ improving the agility of keyframe-based slam , ” in european conference on computer vision ( eccv ) , marseille , france , october 2008 , pp . 802–815 . [ 33 ] k. pirker , m. ruther , and h. bischof , “ cd slam-continuous local- ization and mapping in a dynamic world , ” in ieee/rsj international conference on intelligent robots and systems ( iros ) , san francisco , usa , september 2011 , pp . 3990–3997 . [ 34 ] s. song , m. chandraker , and c. c. guest , “ parallel , real-time monocular visual odometry , ” in ieee international conference on robotics and automation ( icra ) , 2013 , pp . 4698–4705 . [ 35 ] p. f. alcantarilla , j. nuevo , and a. bartoli , “ fast explicit diffusion for accelerated feature in nonlinear scale space , ” in british machine vision conference ( bmvc ) , bristol , uk , 2013 . [ 36 ] x. yang and k.-t. cheng , “ ldb : an ultra-fast feature for scalable augmented reality on mobile device , ” in ieee international symposium on mixed and augmented reality ( ismar ) , 2012 , pp . 49–57 . [ 37 ] r. kuemmerle , g. grisetti , h. strasdat , k. konolige , and w. burgard , “ g2o : a general framework for graph optimization , ” in ieee interna- tional conference on robotics and automation ( icra ) , shanghai , china', ', h. strasdat , k. konolige , and w. burgard , “ g2o : a general framework for graph optimization , ” in ieee interna- tional conference on robotics and automation ( icra ) , shanghai , china , may 2011 , pp . 3607–3613 . [ 38 ] j. sturm , n. engelhard , f. endres , w. burgard , and d. cremers , “ a benchmark for the evaluation of rgb-d slam system , ” in ieee/rsj international conference on intelligent robots and systems ( iros ) , vilamoura , portugal , october 2012 , pp . 573–580 . [ 39 ] m. smith , i. baldwin , w. churchill , r. paul , and p. newman , “ the new college vision and laser data set , ” the international journal of robotics research , vol . 28 , no . 5 , pp . 595–599 , 2009 . [ 40 ] a. geiger , p. lenz , c. stiller , and r. urtasun , “ vision meet robotics : the kitti dataset , ” the international journal of robotics research , vol . 32 , no . 11 , pp . 1231–1237 , 2013 . [ 41 ] v . lepetit , f. moreno-noguer , and p. fua , “ epnp : an accurate o ( n ) solution to the pnp problem , ” international journal of computer vision , vol . 81 , no . 2 , pp . 155–166 , 2009 . [ 42 ] b. k. p. horn , “ closed-form solution of absolute orientation use unit quaternion , ” journal of the optical society of america a , vol . 4 , no . 4 , pp . 629–642 , 1987 . [ 43 ] f. endres , j. hess , j. sturm , d. cremers , and w. burgard , “ 3-d mapping with an rgb-d camera , ” ieee transactions on robotics , vol . 30 , no . 1 , pp . 177–187 , 2014 . [ 44 ] r. a. newcombe , s. j. lovegrove , and a. j. davison , “ dtam : dense tracking and mapping in real-time , ” in ieee international conference on computer vision ( iccv ) , barcelona , spain , november 2011 , pp . 2320– 2327 . [ 45 ] s. lovegrove , a. j. davison , and j. ibanez-guzm ´an , “ accurate visual odometry from a rear parking camera , ” in ieee intelligent vehicles symposium ( iv ) , 2011 , pp . 788–793 . [ 46 ] p. h. torr and a. zisserman , “ feature base method for structure and motion estimation , ” in vision algorithms : theory and practice . springer , 2000', '( iv ) , 2011 , pp . 788–793 . [ 46 ] p. h. torr and a. zisserman , “ feature base method for structure and motion estimation , ” in vision algorithms : theory and practice . springer , 2000 , pp . 278–294 . [ 47 ] r. mur-artal and j. d. tardos , “ probabilistic semi-dense mapping from highly accurate feature-based monocular slam , ” in robotics : science and systems ( rss ) , rome , italy , july 2015 . [ 48 ] h. strasdat , “ local accuracy and global consistency for efﬁcient visual slam , ” ph.d. dissertation , imperial college , london , october 2012 . ra´ul mur artal be bear in zaragoza , spain in 1989 . he receive the industrial engineering degree ( mention in industrial automation and robotics ) in 2012 and the m.s . degree in systems and com- puter engineering in 2013 from the university of zaragoza , where he be currently work towards the phd . degree with the i3a robotics , perception and real-time group . his research interest include visual localization and long-term mapping . j. m . m. montiel be bear in arnedo , spain , in 1967 . he receive the m.s . and ph.d. degree in electrical engineering from the universidad de zaragoza , spain , in 1992 and 1996 , respectively . he be currently a full professor with the depar- tamento de inform ´atica e ingenier ´ıa de sistemas , universidad de zaragoza , where he be in charge of perception and computer vision research grant and course . his current interest include , real-time vision localization and semantic mapping for rigid and non rigid environment , and the transference of this technology to robotic and nonrobotic application domain . prof . mart ´ınez montiel be a member of the i3a robotics , perception , and real-time group . he have be award several spanish mec grant to fund research at the university of oxford , uk , and at imperial college london , uk . juan d. tard ´oswas bear in huesca , spain , in 1961 . he earn the m.s . and ph.d. degree in electrical engineering from the university of zaragoza , spain , in 1985 and 1991 , respectively . he be', '. juan d. tard ´oswas bear in huesca , spain , in 1961 . he earn the m.s . and ph.d. degree in electrical engineering from the university of zaragoza , spain , in 1985 and 1991 , respectively . he be full professor with the departamento de inform ´atica e ingenier ´ıa de sistemas , university of zaragoza , where he be in charge of course in robotics , computer vision , and artiﬁcial intelligence . his research interest include slam , perception and mobile robotics . prof. tard ´os be a member of the i3a robotics , perception , and real-time group view publication stats']",https://doi.org/10.1109/TRO.2015.2463671
4.pdf,"deepvo : a deep learning approach for monocular visual odometry vikram mohanty shubh agrawal shaswat datta arna ghosh vishnu d. sharma debashish chakravarty indian institute of technology kharagpur kharagpur , west bengal , india 721302 fvikram.mohanty , shubh.agrawal111 , shaswatdatta , arna.ghosh , vd , dc g @ iitkgp.ac.in abstract — deep learning base technique have be adopt with precision to solve a lot of standard com- puter vision problem , some of which be image clas- siﬁcation , object detection and segmentation . despite the widespread success of these approach , they have not yet be exploit largely for solve the standard perception relate problem encounter in autonomous navigation such a visual odometry ( vo ) , structure from motion ( sfm ) and simultaneous localization and mapping ( slam ) . this paper analyze the problem of monocular visual odometry use a deep learning- base framework , instead of the regular ’ feature detec- tion and track ’ pipeline approach . several experi- ments be perform to understand the inﬂuence of a known/unknown environment , a conventional trackable feature and pre-trained activation tune for object clas- siﬁcation on the network ’ s ability to accurately estimate the motion trajectory of the camera ( or the vehicle ) . based on these observation , we propose a convolutional neural network architecture , best suit for estimate the object ’ s pose under know environment condition , and display promise result when it come to infer the actual scale use just a single camera in real-time . i. i ntroduction in recent year , convolutional neural networks ( cnns ) have be employ successfully for numerous application in computer vision and robotics such a object detection [ 32 ] , clas- siﬁcation [ 20 ] , semantic segmentation [ 23 ] and many others , often outperform the conventional feature-based method . however , a few exception exist to this trend ; notably - structure from motion ( sfm ) , simultaneous localization and mapping ( slam ) and visual odometry ( vo ) be some of the traditional perception problem , for which deep learning technique have not be exploit in alarge manner . in this paper , we analyze the prob- lem of visual odometry use a deep learning- base framework . in robot navigation , odometry be deﬁned a the process of fuse data from different motion sen- sors to estimate the change in the robot ’ s position over time . this process of determine the trajec- tory play an important part in robotics , form the basis of path planning and control . tradi- tionally , this problem have be tackle use data from rotary encoders , imu and gps [ 27 ] . while this approach have be practically successful in solve the problem in hand , it be still prone to un- favorable condition like wheel slip in uneven terrain and lack of gps signal . recently , this problem have be solve just by use data from the camera ( sequence of image ) . this process of incrementally estimate the robot ’ s pose ( position and orientation ) by analyze the motion change in the associated camera image be know a visual odometry [ 37 ] . a standard visual odometry approach generally follow the following step ( for both monocular and stereo vision case ) [ 24 ] : 1 ) image acquisition at two time instance 2 ) image correction such a rectiﬁcation and lens distortion removal 3 ) feature detection in the two image ( such a corner use surf [ 2 ] , orb [ 35 ] or fast [ 34 ] ) 4 ) feature track between the two image to obtain the optical ﬂow 5 ) estimation of motion use the obtain op- tical ﬂow and the camera parameters.arxiv:1611.06069v1 [ cs.cv ] 18 nov 2016on the deep learning front , there have be huge technological advancement regard the applica- tions of cnns . it have be show that these deep network be adept in extract various abstract feature from image . our work propose a deep learning-based framework for analyze the problem of visual odometry , motivate from the observation that instead of geometric feature descriptor , cnns can be use to extract high-level feature from image . using these feature , we estimate the transformation matrix between two consecutive scene to recreate the vehicle ’ s trajectory . another signiﬁcant contribution of this paper be use only monocular vision to estimate the vehicle ’ s position in true scale , which can not be do solely by pure geometry base method . this be possible since the training network be able to learn the cam- era intrinsic parameter and scale . we hope that this framework will open up further research into the associated ﬁelds of simultaneous localization and mapping ( slam ) and structure from motion ( sfm ) as well . ii . r elated work a . visual odometry the problem of visual odometry have be tradi- tionally tackle by two method - feature-based and direct ( ” appearance-based ” ) . while the ﬁrst approach relies on detecting and track a sparse set of salient image feature such a line and cor- ners , the latter relies directly on the pixel intensity value to extract motion information . feature-based method use a variety of feature detector to detect salient feature point such a fast ( features from accelerated segment test ) [ 34 ] , surf ( speeded up robust features ) [ 2 ] , brief ( binary robust independent elementary features ) [ 4 ] , orb ( oriented fast and rotated brief ) [ 35 ] and harris [ 13 ] corner detector . these feature point be then track in the next sequential frame use a feature point tracker , the most common one be the klt tracker [ 41 ] , [ 38 ] . the result thus obtain be the optical ﬂow , follow which the ego-motion can then be es- timated use the camera parameter a propose by nister [ 30 ] . this general approach of detect feature point and track them be follow bymost paper ( in both monocular vision and stereo vision base approach ) a be the case in [ 25 ] and [ 16 ] . more recent work in this area employ the ptam approach [ 17 ] , which be a robust feature tracking-based slam algorithm , with an added advantage of run in real-time by parallelize the motion estimation and mapping task [ 3 ] , [ 42 ] , [ 18 ] . direct or ” appearance-based ” method for visual odometry rely directly on the pixel intensity value in an image , and minimize error directly in sensor space , while subsequently avoid feature match- ing and tracking . these method however require a planarity assumption ( e.g . homography ) . early di- rect monocular slam method like [ 15 ] and [ 26 ] make use of ﬁltering algorithm for structure from motion , while in [ 39 ] and [ 31 ] non-linear least square estimation be use . other approach like dtam [ 28 ] compute a dense depth-map for each key-frame , which be use for align the whole image to ﬁnd the camera pose . this be do by minimize a global energy function . since this approach be computationally intensive , heavy gpu parallelization be require . to mitigate this heavy computational requirement , the method describe in [ 8 ] be propose . recently , fast direct monocular slam have also be achieve by the lsd-slam algorithm [ 7 ] . aside from these two approach , the other notable method be a semi-direct approach to the problem , which combine the successful factor of feature-based method ( track many feature , parallel tracking and mapping ) with the accuracy and speed of direct method . this be explore in the work by scaramuzza et.al . [ 10 ] b . deep learning approaches with the advent of cnns [ 22 ] , numerous com- puter vision task have be solve very efﬁ- ciently and with high accuracy by these archi- tectures a compare to traditional geometry-based approach . classiﬁcation problem such a the imagenet large scale visual recognition com- petition ( ilsvrc ) [ 36 ] , [ 20 ] , regression problem like depth regression [ 6 ] , object detection [ 32 ] and segmentation problem [ 23 ] have all be solve by these network . however , the domain of structure from motion , slam and visual odometry be still untouched bythe advance in deep learning . recently , optical ﬂow between two image have be obtain by network such a flownet [ 9 ] and epicflow [ 33 ] . homography between two image have also be estimate use deep network in [ 5 ] . nicolai , skeele et al . apply deep learning technique to learn odometry , but use laser data from a lidar [ 29 ] . the only visual odometry approach use deep learning that the author be aware of the work of konda and memisevic [ 19 ] . their approach however be limit to stereo visual odom- etry . agrawal et al . [ 1 ] propose the use of ego- motion vector a a weak supervisory signal for feature learning . for infer egomotion , their training approach treat the whole problem a a classiﬁcation task . as oppose to this , we treat the visual odometry estimation a a regression problem . iii . m ethodology the pipeline can be divide into two stage : data preprocessing and the cnn framework , design speciﬁcally for different experiment . a . data preprocessing for our experiment , the kitti vision bench- mark [ 11 ] be use . the visual odometry dataset provide by kitti consists of stereo-vision se- quences collect while drive the vehicle in different environment . since this work focus on monocular vision , the video sequence collect from a single camera be consider . of the 21 sequence available , 11 sequence with ground truth trajectory be use for training and test sequence . these 11 sequence be far sort into training and test dataset , a per the need of our experiment . the original ground truth pose information be available in term of a sequence of 3x4 transformation matrix which describe the motion of a vehicle between 0thtime step to tthtime step . these matrix be process to generate the ground truth data in a new form describe the differential change in translational motion ( dx , dz , dq ) of the vehicle , for all sub- sequent image in pair i tand i t+1 ( where i tis image at tthtime step and i t+1is image at ( t+1 ) th time step ) along two designated translational ax ( x , z ) . each of the original image sequence of size 1241 x376 be warp and downsampledto 256 x256 , a the architecture we propose be inspire by alexnet [ 21 ] , which restrict input to square sized image only . later , a dataset of image pair be generate consisting of image at tthtime step and the corresponding image at ( t+1 ) thtime step . thus , the ﬁnal process dataset could be represent a : it , it+1 , ( dx , dz , dq ) t","['deepvo : a deep learning approach for monocular visual odometry vikram mohanty shubh agrawal shaswat datta arna ghosh vishnu d. sharma debashish chakravarty indian institute of technology kharagpur kharagpur , west bengal , india 721302 fvikram.mohanty , shubh.agrawal111 , shaswatdatta , arna.ghosh , vd , dc g @ iitkgp.ac.in abstract — deep learning base technique have be adopt with precision to solve a lot of standard com- puter vision problem , some of which be image clas- siﬁcation , object detection and segmentation . despite the widespread success of these approach , they have not yet be exploit largely for solve the standard perception relate problem encounter in autonomous navigation such a visual odometry ( vo ) , structure from motion ( sfm ) and simultaneous localization and mapping ( slam ) . this paper analyze the problem of monocular visual odometry use a deep learning- base framework , instead of the regular ’ feature detec- tion and track ’ pipeline approach . several experi- ments be perform to understand the inﬂuence of a known/unknown environment , a conventional trackable feature and pre-trained activation tune for object clas- siﬁcation on the network ’ s ability to accurately estimate the motion trajectory of the camera ( or the vehicle ) . based on these observation , we propose a convolutional neural network architecture , best suit for estimate the object ’ s pose under know environment condition , and display promise result when it come to infer the actual scale use just a single camera in real-time . i. i ntroduction in recent year , convolutional neural networks ( cnns ) have be employ successfully for numerous application in computer vision and robotics such a object detection [ 32 ] , clas- siﬁcation [ 20 ] , semantic segmentation [ 23 ] and many others , often outperform the conventional feature-based method . however , a few exception exist to this trend ; notably - structure from motion ( sfm ) , simultaneous localization and mapping ( slam ) and visual odometry ( vo ) be some of', 'feature-based method . however , a few exception exist to this trend ; notably - structure from motion ( sfm ) , simultaneous localization and mapping ( slam ) and visual odometry ( vo ) be some of the traditional perception problem , for which deep learning technique have not be exploit in alarge manner . in this paper , we analyze the prob- lem of visual odometry use a deep learning- base framework . in robot navigation , odometry be deﬁned a the process of fuse data from different motion sen- sors to estimate the change in the robot ’ s position over time . this process of determine the trajec- tory play an important part in robotics , form the basis of path planning and control . tradi- tionally , this problem have be tackle use data from rotary encoders , imu and gps [ 27 ] . while this approach have be practically successful in solve the problem in hand , it be still prone to un- favorable condition like wheel slip in uneven terrain and lack of gps signal . recently , this problem have be solve just by use data from the camera ( sequence of image ) . this process of incrementally estimate the robot ’ s pose ( position and orientation ) by analyze the motion change in the associated camera image be know a visual odometry [ 37 ] . a standard visual odometry approach generally follow the following step ( for both monocular and stereo vision case ) [ 24 ] : 1 ) image acquisition at two time instance 2 ) image correction such a rectiﬁcation and lens distortion removal 3 ) feature detection in the two image ( such a corner use surf [ 2 ] , orb [ 35 ] or fast [ 34 ] ) 4 ) feature track between the two image to obtain the optical ﬂow 5 ) estimation of motion use the obtain op- tical ﬂow and the camera parameters.arxiv:1611.06069v1 [ cs.cv ] 18 nov 2016on the deep learning front , there have be huge technological advancement regard the applica- tions of cnns . it have be show that these deep network be adept in extract various abstract feature from image . our work propose a deep learning-based framework for', 'advancement regard the applica- tions of cnns . it have be show that these deep network be adept in extract various abstract feature from image . our work propose a deep learning-based framework for analyze the problem of visual odometry , motivate from the observation that instead of geometric feature descriptor , cnns can be use to extract high-level feature from image . using these feature , we estimate the transformation matrix between two consecutive scene to recreate the vehicle ’ s trajectory . another signiﬁcant contribution of this paper be use only monocular vision to estimate the vehicle ’ s position in true scale , which can not be do solely by pure geometry base method . this be possible since the training network be able to learn the cam- era intrinsic parameter and scale . we hope that this framework will open up further research into the associated ﬁelds of simultaneous localization and mapping ( slam ) and structure from motion ( sfm ) as well . ii . r elated work a . visual odometry the problem of visual odometry have be tradi- tionally tackle by two method - feature-based and direct ( ” appearance-based ” ) . while the ﬁrst approach relies on detecting and track a sparse set of salient image feature such a line and cor- ners , the latter relies directly on the pixel intensity value to extract motion information . feature-based method use a variety of feature detector to detect salient feature point such a fast ( features from accelerated segment test ) [ 34 ] , surf ( speeded up robust features ) [ 2 ] , brief ( binary robust independent elementary features ) [ 4 ] , orb ( oriented fast and rotated brief ) [ 35 ] and harris [ 13 ] corner detector . these feature point be then track in the next sequential frame use a feature point tracker , the most common one be the klt tracker [ 41 ] , [ 38 ] . the result thus obtain be the optical ﬂow , follow which the ego-motion can then be es- timated use the camera parameter a propose by nister [ 30 ] . this general approach of detect feature point and', 'the result thus obtain be the optical ﬂow , follow which the ego-motion can then be es- timated use the camera parameter a propose by nister [ 30 ] . this general approach of detect feature point and track them be follow bymost paper ( in both monocular vision and stereo vision base approach ) a be the case in [ 25 ] and [ 16 ] . more recent work in this area employ the ptam approach [ 17 ] , which be a robust feature tracking-based slam algorithm , with an added advantage of run in real-time by parallelize the motion estimation and mapping task [ 3 ] , [ 42 ] , [ 18 ] . direct or ” appearance-based ” method for visual odometry rely directly on the pixel intensity value in an image , and minimize error directly in sensor space , while subsequently avoid feature match- ing and tracking . these method however require a planarity assumption ( e.g . homography ) . early di- rect monocular slam method like [ 15 ] and [ 26 ] make use of ﬁltering algorithm for structure from motion , while in [ 39 ] and [ 31 ] non-linear least square estimation be use . other approach like dtam [ 28 ] compute a dense depth-map for each key-frame , which be use for align the whole image to ﬁnd the camera pose . this be do by minimize a global energy function . since this approach be computationally intensive , heavy gpu parallelization be require . to mitigate this heavy computational requirement , the method describe in [ 8 ] be propose . recently , fast direct monocular slam have also be achieve by the lsd-slam algorithm [ 7 ] . aside from these two approach , the other notable method be a semi-direct approach to the problem , which combine the successful factor of feature-based method ( track many feature , parallel tracking and mapping ) with the accuracy and speed of direct method . this be explore in the work by scaramuzza et.al . [ 10 ] b . deep learning approaches with the advent of cnns [ 22 ] , numerous com- puter vision task have be solve very efﬁ- ciently and with high accuracy by these archi- tectures a compare to', '. [ 10 ] b . deep learning approaches with the advent of cnns [ 22 ] , numerous com- puter vision task have be solve very efﬁ- ciently and with high accuracy by these archi- tectures a compare to traditional geometry-based approach . classiﬁcation problem such a the imagenet large scale visual recognition com- petition ( ilsvrc ) [ 36 ] , [ 20 ] , regression problem like depth regression [ 6 ] , object detection [ 32 ] and segmentation problem [ 23 ] have all be solve by these network . however , the domain of structure from motion , slam and visual odometry be still untouched bythe advance in deep learning . recently , optical ﬂow between two image have be obtain by network such a flownet [ 9 ] and epicflow [ 33 ] . homography between two image have also be estimate use deep network in [ 5 ] . nicolai , skeele et al . apply deep learning technique to learn odometry , but use laser data from a lidar [ 29 ] . the only visual odometry approach use deep learning that the author be aware of the work of konda and memisevic [ 19 ] . their approach however be limit to stereo visual odom- etry . agrawal et al . [ 1 ] propose the use of ego- motion vector a a weak supervisory signal for feature learning . for infer egomotion , their training approach treat the whole problem a a classiﬁcation task . as oppose to this , we treat the visual odometry estimation a a regression problem . iii . m ethodology the pipeline can be divide into two stage : data preprocessing and the cnn framework , design speciﬁcally for different experiment . a . data preprocessing for our experiment , the kitti vision bench- mark [ 11 ] be use . the visual odometry dataset provide by kitti consists of stereo-vision se- quences collect while drive the vehicle in different environment . since this work focus on monocular vision , the video sequence collect from a single camera be consider . of the 21 sequence available , 11 sequence with ground truth trajectory be use for training and test sequence . these 11 sequence be far sort into training and', 'from a single camera be consider . of the 21 sequence available , 11 sequence with ground truth trajectory be use for training and test sequence . these 11 sequence be far sort into training and test dataset , a per the need of our experiment . the original ground truth pose information be available in term of a sequence of 3x4 transformation matrix which describe the motion of a vehicle between 0thtime step to tthtime step . these matrix be process to generate the ground truth data in a new form describe the differential change in translational motion ( dx , dz , dq ) of the vehicle , for all sub- sequent image in pair i tand i t+1 ( where i tis image at tthtime step and i t+1is image at ( t+1 ) th time step ) along two designated translational ax ( x , z ) . each of the original image sequence of size 1241 x376 be warp and downsampledto 256 x256 , a the architecture we propose be inspire by alexnet [ 21 ] , which restrict input to square sized image only . later , a dataset of image pair be generate consisting of image at tthtime step and the corresponding image at ( t+1 ) thtime step . thus , the ﬁnal process dataset could be represent a : \\x08it , it+1 , ( dx , dz , dq ) t\\x00 > ( t+1 ) this be the base input image and ground truth label format . however , for different experiment , this base data be convert into other realizable format , or augment with additional data , which be explain in the late subsection . b . hardware and software all the demonstrated experiment be per- form on an intel xeon @ 4 x 3.3 ghz machine load with 32 gb ddr3 ram and nvidia gtx 970 . to evaluate our approach for learn visual odometry and gpu base implementation , we choose caffe [ 14 ] , develop by the berkeley vision and learning center . all the data pre- processing be program in python , use as- sociated library for compatibility with the python binding of caffe . c. deep learning framework we design a cnn architecture , partly base on the original alexnet [ 21 ] , tune to take a input simultaneously - the paired image in se-', 'with the python binding of caffe . c. deep learning framework we design a cnn architecture , partly base on the original alexnet [ 21 ] , tune to take a input simultaneously - the paired image in se- quence ( i t , it+1 ) , with an objective to regress the targeted label ( dx , dz , dq ) . all weight in the network ’ s convolutional layer have a gaussian initialization , whereas the fully connect layer be initialize use the xavier algorithm [ 12 ] . the network be design to compute a l2 ( eu- clidean ) loss . based on the different experiment perform for the propose analysis , the network architecture be far tuned speciﬁc to each task , with the detail describe below . 1 ) testing on an unknown environment : from the 11 sequence in the dataset , 7 be consider for training and 4 for test . here , the test sequence be choose such that they belong to different environmental condition a compare to the training sequence . the network architecture consist of two parallel alexnet-based cascade convolutional layer concatenate at the end ofthe ﬁnal convolutional layer to generate fully connect layer , which be smoothly stack to regress the target variable ( dx , dz , dq ) ( figure 1 ) . fig . 1 : architecture use for unknown case the network take 3 input in the form of i t , it+1 and the pose ( dx , dz , dq ) between them . the two data input correspond to image sequence be feed into the convolutional cascade which convolve in parallel , and then concatenate at the end to generate a ﬂattened ( image batch size x 8192 ) vector . this vector be feed into custom design fully connect layer that converge to ( image batch size x 3 ) and be feed along with the ground truth label to an euclidean loss layer to minimize the loss . the same architecture , ignore the dropout layer , be use in test phase . 2 ) testing on a known environment : the train- ing sequence and test sequence be take from a random permutation of the entire dataset into two different proportion : 80:20 and 50:50 from all the 11 sequence individually . this en-', ': the train- ing sequence and test sequence be take from a random permutation of the entire dataset into two different proportion : 80:20 and 50:50 from all the 11 sequence individually . this en- sured that both training and test set contain similar environment sequence . the network architecture adopt be exactly the same a the previous experiment . the only difference from the previous experiment be in the preparation of the training set and testing set , with the motivation to observe the network ’ s behavior in a known or unknown environment . this provide an insight into the nature of the visual odometry problem . the experiment help in understand if the propose network architecture be robust to new environment or require a prior knowledge of the scene . the model be train twice independently , once for the 80:20 and once for 50:50 trainingto test set ratio scenario . the major motivation for train the model in two different ratio be to analyze the amount of data require by the network to sufﬁciently learn about the environment to be able to accurately estimate the trajectory . ( a ) original image ( b ) fast feature fig . 2 : representation of fast feature in the network 3 ) testing on an unknown environment with prior feature : for this task , in addition to the schema use in the ﬁrst experiment , fast [ 34 ] feature be add a a prior input to the net- work ( figure 2 ) . the feature for each image be append to the rgb data to generate a 4-dimensional feature set for the each input im- age . the image data thus obtain and the pose ground truth be segregate into 7 training and 4 test sequence . the network architecture , same a the previous experiment , follow the the same procedure a employ in the ﬁrst experiment . this experiment be perform with an objective to observe the inﬂuence of a prior feature , con- ventionally use for a feature-based approach for solve the visual odometry problem , in improve the accuracy of pose estimation . fig . 3 : alexnet-based architecture for unknown environment with', 'ventionally use for a feature-based approach for solve the visual odometry problem , in improve the accuracy of pose estimation . fig . 3 : alexnet-based architecture for unknown environment with pre-trained network4 ) testing on an unknown environment use pre-trained network . : this experiment be per- form use a network architecture consist of two alexnet-based cascade convolutional lay- er pre-trained on the imagenet database . the network be ﬁne-tuned by train on part of dataset sequence while the rest be use a test sequence . here , the output activation of the ﬁnal convolutional layer in the original alexnet architecture be extract and serve a the input instead of a standard rgb image . the learnable part of the architecture comprise of 1 convolution layer and 4 fully connect layer ( figure 3 ) . this experiment be design with the motivation to understand the effect of pre-trained activation train on object classiﬁcation label for the task of estimate the odometry vector . iv . e xperimental results for the experiment describe in section 3.3 , the result be show for comparison of the network prediction with the ground truth and to observe the loss in training and test phase . the network be observe to pass any arbitrary image pair through it layer , compute the layer activation and estimate the odometry vector at an average of 9ms , display real-time capability . it be far observe that this do not depend on the nature of the scene . a . test results : unknown environment for this evaluation , the testing be perform on an environment completely unknown to the network . in such condition , the estimate position deviate too much from the ground truth , a show in figure 4 . the training and test loss for this network be show in figure 5 . as can be observe from the plot , the training loss decline very fast with the number of iteration . on the other hand , the loss during test oscillates around a ﬁxed value with small variation . this show that al- though the network be able to reduce the the loss on a know', 'the number of iteration . on the other hand , the loss during test oscillates around a ﬁxed value with small variation . this show that al- though the network be able to reduce the the loss on a know environment , the lack of knowledge of a scene do not help in estimate the odometry vector . therefore , even after a signiﬁcant number of iteration , the testing loss do not fall . fig . 4 : prediction v ground truth : unknown environment fig . 5 : training and testing loss : unknown en- vironment b . test results : known environment this experiment be perform on a know environment , with data segregate into training and test sequence in ratio of 80-20 and 50-50 . figure 6 and 9 show a signiﬁcant improvement in the prediction of odometry vector in a sequence , part of which be already know to the network . figure 6 , 7 and 8 be the result for data break into 50-50 ratio . figure 7 give an insight into the deviation , which be observe to be increase with time . therefore , it can be conclude that the error in odometry accumulates over time result in the predicted trajectory drift away from the ground truth . the loss , similar to deviation , show great im-fig . 6 : comparison of the predict output with the ground truth ( 50-50 proportion of training and test data ) : known environment fig . 7 : deviation from the ground truth for test in know environment ( 50-50 proportion of training and test data ) : known environment provement in performance in know environment over unknown environment . the test loss follow the training loss and show a steep drop with increase in number of iteration . figure 9 , 10 and 11 depict the result with same methodology but for a separation of data in 80-20 proportion ( training and test data ) . c. test results : using a trackable prior feature in an unknown environment in this part , we use fast feature a prior along with the rgb image . as observed from figure 12 , this network display similar behavior in term of training and test loss a that of a net- fig . 8 : training and testing', 'fast feature a prior along with the rgb image . as observed from figure 12 , this network display similar behavior in term of training and test loss a that of a net- fig . 8 : training and testing loss for test in know environment ( 50-50 proportion of training and test data ) fig . 9 : comparison of the predict output with the ground truth ( 4:1 ratio of training and test data ) fig . 10 : deviation from the ground truth for test in know environment ( 80-20 proportion of training and test data ) fig . 11 : training and testing loss for test in know environment ( 80-20 proportion of training and test data ) fig . 12 : training and testing loss for test in un- known environment with prior feature work in an unknown environment . this experiment consist of few test iteration case . v. d iscussions the result from the experiment perform be highly encouraging . the author believe that the result not only suggest that the architecture present can be try out on robotic platform , but also provide u a deep understanding of how this network deal with the visual odometry problem . from the result of test on a know environ- ment , it be clear that more the network learn about a particular environment , the good it get at pre- dicting the visual odometry . this be in alignment with the general perception . also , this supportsthe hypothesis that the network treat the problem of visual odometry a speciﬁc to a particular scene . this be far support on compare these result to that of 1stexperiment . in case of predict visual odometry data on unseen image , the network performs fairly poor . inspired by this ﬁnding , the author delve deep into understand the signiﬁcance of feature re- quired for scene understanding . [ 1 ] present the use of ego-motion vector a a weak supervisory signal for feature learn . they show the effectiveness of the feature learn on simple task like scene and object recognition . motivated by this , the author use the pre-trained weight of alexnet [ 21 ] train on object classiﬁcation for the pre-', 'of the feature learn on simple task like scene and object recognition . motivated by this , the author use the pre-trained weight of alexnet [ 21 ] train on object classiﬁcation for the pre- sented network . however the result obtain be not supportive of the fact , thus show that the feature extract from the pre-trained network be not generic to the problem of visual odometry . the author try out the idea of provide prior information about the scene to improve the predic- tion accuracy on unknown environment . there- fore , the fast feature of the scene be use along with the feature extract by the convolu- tional layer of the network . a . future work the result of predict visual odometry in know environment show the error drift with time . therefore , the predicted trajectory also seem to show more deviation from ground truth with time . to tackle this issue , the author feel that the use of recurrent network would be more appropri- ate . the presence of recurrent connection would enable the network to correct the error incur from ground truth continuously . it would also be interest to explore far on the fusion of conventional trackable feature a a prior to the high level feature generate by the cnns . use of generative network to predict the next scene from an estimate ego-motion vector and update the ego-motion vector use a feedback loop could be use to correct the accumulate error . the mechanism be know to function in the human brain [ 40 ] and a similar architecture can be use in artiﬁcial system too.vi . c onclusions the propose network demonstrate promise result , when provide with a prior knowledge of the environment , while display the expect opposite response in case of an unknown environ- ment . the network , when provide with a prior of fast feature , and train on an unknown environment , show a similar behavior a that of the network subject to an unknown environment without any prior . it may be conclude that the propose cnn design for the purpose of vi- sual odometry be able to learn feature', 'behavior a that of the network subject to an unknown environment without any prior . it may be conclude that the propose cnn design for the purpose of vi- sual odometry be able to learn feature similar to fast , and a manual addition of these feature only contribute to redundancy . when deploy on known environment , the network architecture be able to learn the actual scale in real time , which be not possible for monocular visual odometry use geometric method . references [ 1 ] p. agrawal , j. carreira , and j. malik . learning to see by move . in proceedings of the ieee international conference on computer vision , page 37–45 , 2015 . [ 2 ] h. bay , t. tuytelaars , and l. van gool . surf : speeded up robust feature . page 404–417 , 2006 . [ 3 ] m. bl ¨osch , s. weiss , d. scaramuzza , and r. siegwart . vision base mav navigation in unknown and unstructured envi- ronments . in robotics and automation ( icra ) , 2010 ieee international conference on , page 21–28 . ieee , 2010 . [ 4 ] m. calonder , v . lepetit , c. strecha , and p. fua . brief : binary robust independent elementary feature . in european conference on computer vision , page 778–792 . springer , 2010 . [ 5 ] d. detone , t. malisiewicz , and a. rabinovich . deep image homography estimation . arxiv preprint arxiv:1606.03798 , 2016 . [ 6 ] d. eigen , c. puhrsch , and r. fergus . depth map prediction from a single image use a multi-scale deep network . in advances in neural information process system , page 2366–2374 , 2014 . [ 7 ] j. engel , t. sch ¨ops , and d. cremers . lsd-slam : large-scale direct monocular slam . in european conference on computer vision , page 834–849 . springer , 2014 . [ 8 ] j. engel , j. sturm , and d. cremers . semi-dense visual odometry for a monocular camera . in proceedings of the ieee international conference on computer vision , page 1449– 1456 , 2013 . [ 9 ] p. fischer , a. dosovitskiy , e. ilg , p. h ¨ausser , c. hazırbas ¸ , v . golkov , p. van der smagt , d. cremers , and t. brox . flownet : learning optical ﬂow', 'vision , page 1449– 1456 , 2013 . [ 9 ] p. fischer , a. dosovitskiy , e. ilg , p. h ¨ausser , c. hazırbas ¸ , v . golkov , p. van der smagt , d. cremers , and t. brox . flownet : learning optical ﬂow with convolutional network . arxiv preprint arxiv:1504.06852 , 2015 . [ 10 ] c. forster , m. pizzoli , and d. scaramuzza . svo : fast semi- direct monocular visual odometry . in 2014 ieee international conference on robotics and automation ( icra ) , page 15– 22 . ieee , 2014 . [ 11 ] a. geiger , p. lenz , and r. urtasun . are we ready for autonomous driving ? the kitti vision benchmark suite . in conference on computer vision and pattern recognition ( cvpr ) , 2012 . [ 12 ] x. glorot and y . bengio . understanding the difﬁculty of train deep feedforward neural network . in aistats , volume 9 , page 249–256 , 2010 . [ 13 ] c. harris and m. stephens . a combined corner and edge detector . in alvey vision conference , volume 15 , page 50 . citeseer , 1988 . [ 14 ] y . jia , e. shelhamer , j. donahue , s. karayev , j . long , r. girshick , s. guadarrama , and t. darrell . caffe : convolu- tional architecture for fast feature embed . arxiv preprint arxiv:1408.5093 , 2014 . [ 15 ] h. jin , p. favaro , and s. soatto . a semi-direct approach to structure from motion . the visual computer , 19 ( 6 ) :377–394 , 2003 . [ 16 ] a. e. johnson , s. b. goldberg , y . cheng , and l. h. matthies . robust and efﬁcient stereo feature track for visual odom- etry . in robotics and automation , 2008 . icra 2008 . ieee international conference on , page 39–46 . ieee , 2008 . [ 17 ] g. klein and d. murray . parallel track and map for small ar workspace . in mixed and augmented reality , 2007 . ismar 2007 . 6th ieee and acm international symposium on , page 225–234 . ieee , 2007 . [ 18 ] l. kneip , m. chli , r. siegwart , et al . robust real-time visual odometry with a single camera and an imu . in bmvc , page 1–11 , 2011 . [ 19 ] k. konda and r. memisevic . learning visual odometry with a convolutional network . in international conference', 'visual odometry with a single camera and an imu . in bmvc , page 1–11 , 2011 . [ 19 ] k. konda and r. memisevic . learning visual odometry with a convolutional network . in international conference on computer vision theory and applications , 2015 . [ 20 ] a. krizhevsky , i. sutskever , and g. e. hinton . imagenet classiﬁcation with deep convolutional neural network . in advances in neural information process system , page 1097–1105 , 2012 . [ 21 ] a. krizhevsky , i. sutskever , and g. e. hinton . imagenet classiﬁcation with deep convolutional neural network . in advances in neural information process system , page 1097–1105 , 2012 . [ 22 ] y . lecun and y . bengio . convolutional network for image , speech , and time series . the handbook of brain theory and neural network , 3361 ( 10 ) :1995 , 1995 . [ 23 ] j . long , e. shelhamer , and t. darrell . fully convolutional network for semantic segmentation . in proceedings of the ieee conference on computer vision and pattern recogni- tion , page 3431–3440 , 2015 . [ 24 ] m. maimone , y . cheng , and l. matthies . two year of visual odometry on the mar exploration rover . journal of field robotics , 24 ( 3 ) :169–186 , 2007 . [ 25 ] l. h. matthies . dynamic stereo vision . 1989 . [ 26 ] n. molton , a. j. davison , and i. reid . locally planar patch feature for real-time structure from motion . in bmvc , page 1–10 , 2004 . [ 27 ] t. moore and d. stouch . a generalized extend kalman ﬁlter implementation for the robot operating system . in intelligent autonomous systems 13 , page 335–348 . springer , 2016 . [ 28 ] r. a. newcombe , s. j. lovegrove , and a. j. davison . dtam : dense track and map in real-time . in 2011 international conference on computer vision , page 2320– 2327 . ieee , 2011 . [ 29 ] a. nicolai , r. skeele , c. eriksen , and g. a. hollinger . deep learning for laser base odometry estimation . [ 30 ] d. nist ´er . an efﬁcient solution to the ﬁve-point relative pose problem . ieee transaction on pattern analysis and machine intelligence , 26 ( 6 )', 'for laser base odometry estimation . [ 30 ] d. nist ´er . an efﬁcient solution to the ﬁve-point relative pose problem . ieee transaction on pattern analysis and machine intelligence , 26 ( 6 ) :756–770 , 2004 . [ 31 ] a. pretto , e. menegatti , and e. pagello . omnidirectional dense large-scale mapping and navigation base on meaning- ful triangulation . in robotics and automation ( icra ) , 2011 ieee international conference on , page 3289–3296 . ieee,2011 . [ 32 ] s. ren , k. he , r. girshick , and j . sun . faster r-cnn : towards real-time object detection with region proposal network . in advances in neural information process system , page 91– 99 , 2015 . [ 33 ] j. revaud , p. weinzaepfel , z. harchaoui , and c. schmid . epicﬂow : edge-preserving interpolation of correspondence for optical ﬂow . in proceedings of the ieee conference on computer vision and pattern recognition , page 1164–1172 , 2015 . [ 34 ] e. rosten and t. drummond . machine learn for high-speed corner detection . page 430–443 , 2006 . [ 35 ] e. rublee , v . rabaud , k. konolige , and g. bradski . orb : an efﬁcient alternative to sift or surf . page 2564–2571 , 2011 . [ 36 ] o. russakovsky , j. deng , h. su , j. krause , s. satheesh , s. ma , z. huang , a. karpathy , a. khosla , m. bernstein , a. c. berg , and l. fei-fei . imagenet large scale visual recognition challenge . international journal of computer vision ( ijcv ) , 115 ( 3 ) :211–252 , 2015 . [ 37 ] d. scaramuzza and f. fraundorfer . visual odometry [ tutorial ] . ieee robotics & automation magazine , 18 ( 4 ) :80–92 , 2011 . [ 38 ] j. shi and c. tomasi . good feature to track . in computer vision and pattern recognition , 1994 . proceedings cvpr ’ 94. , 1994 ieee computer society conference on , page 593–600 . ieee , 1994 . [ 39 ] g. silveira , e. malis , and p. rives . an efﬁcient direct approach to visual slam . ieee transaction on robotics , 24 ( 5 ) :969–979 , 2008 . [ 40 ] m. synofzik , g. v osgerau , and a. newen . beyond the com- parator model : a multifactorial two-step', 'direct approach to visual slam . ieee transaction on robotics , 24 ( 5 ) :969–979 , 2008 . [ 40 ] m. synofzik , g. v osgerau , and a. newen . beyond the com- parator model : a multifactorial two-step account of agency . consciousness and cognition , 17 ( 1 ) :219–239 , 2008 . [ 41 ] c. tomasi and t. kanade . detection and tracking of point feature . school of computer science , carnegie mellon univ . pittsburgh , 1991 . [ 42 ] s. weiss , m. w. achtelik , s. lynen , m. c. achtelik , l. kneip , m. chli , and r. siegwart . monocular vision for long-term micro aerial vehicle state estimation : a compendium . journal of field robotics , 30 ( 5 ) :803–831 , 2013 .']",https://doi.org/10.1186/s40064-016-3573-7
6.pdf,"citation : zhao , y.-l. ; hong , y.-t. ; huang , h.-p . comprehensive performance evaluation between visual slam and lidar slam for mobile robots : theories and experiments . appl . sci . 2024 ,14 , 3945. http : //doi.org/10.3390/ app14093945 academic editors : jos émiguel molina mart ínez and andrea prati received : 9 january 2024 revised : 11 april 2024 accepted : 2 may 2024 published : 6 may 2024 copyright : ©2024 by the author . licensee mdpi , basel , switzerland . this article be an open access article distribute under the term and condition of the creative commons attribution ( cc by ) license ( http : // creativecommons.org/licenses/by/ 4.0/ ) . apply science article comprehensive performance evaluation between visual slam and lidar slam for mobile robots : theories and experiments yu-lin zhao , yi-tian hong and han-pang huang * department of mechanical engineering , national taiwan university , taipei 106 , taiwan ; f07522844 @ ntu.edu.tw ( y.-l.z . ) ; r12522813 @ ntu.edu.tw ( y.-t.h . ) *correspondence : hanpang @ ntu.edu.tw ; tel . : +886-2-33662700 ; fax : +886-2-23676064 abstract : slam ( simultaneous localization and mapping ) , primarily rely on camera or lidar ( light detection and ranging ) sensor , play a crucial role in robotics for localization and envi- ronmental reconstruction . this paper assess the performance of two lead method , namely orb-slam3 and sc-lego-loam , focus on localization and mapping in both indoor and out- door environment . the evaluation employ artificial and cost-effective datasets incorporate data from a 3d lidar and an rgb-d ( color and depth ) camera . a practical approach be introduce for calculate ground-truth trajectory and during benchmarking , reconstruction map base on ground truth be establish . to assess the performance , ate and rpe be utilize to evaluate the accuracy of localization ; standard deviation be employ to compare the stability during the localization process for different method . while both algorithms exhibit satisfactory positioning accuracy , their performance be suboptimal in scenario with inadequate texture . furthermore , 3d reconstruction map establish by the two approach be also provide for direct observation of their difference and the limitation encounter during map construction . moreover , the research include a comprehensive comparison of computational performance metric , encompass central processing unit ( cpu ) utilization , memory usage , and an in-depth analysis . this evaluation reveal that visual slam require more cpu resource than lidar slam , primarily due to additional data storage requirement , emphasize the impact of environmental factor on resource requirement . in conclusion , lidar slam be more suitable for the outdoors due to it comprehensive nature , while visual slam excels indoors , compensate for sparse aspect in lidar slam . to facilitate further research , a technical guide be also provide for the researcher in related field . keywords : 3d slam ; visual slam ; lidar slam ; 3d reconstruction ; robotics 1 . introduction slam have emerge a a prominent subject within the robotics community in recent year , lead to the development of numerous associate open-source project . these project encompass a wide range of application , include autonomous vehicle , drone , architectural surveying , and scene scanning . slam stand for simultaneous localization and mapping , represent a vital technique with immense potential . it enable a robot to robustly ascertain it position ( localization ) while concurrently construct a real-time representation of previously unexplored environment ( map ) [ 1 ] . various sensor be employ to gauge distance in the real world and capture pertinent feature . among these sensor , camera and lidar system be the most prevalent . consequently , the slam domain primarily encompass visual-based slam and lidar-based slam , contingent upon the input from the specific sensor . in our previous study , we discuss mainly map for mobile robot in outdoor environment involve lidar slam and visual slam [ 2 ] . thus , this study primarily revolve around the performance comparison between visual slam and lidar slam in term of localization and mapping for mobile robot operate in real-world environment . appl . sci . 2024 ,14 , 3945. http : //doi.org/10.3390/app14093945 http : //www.mdpi.com/journal/applsciappl . sci . 2024 ,14 , 3945 2 of 20 with an emphasis on maintain continuity and universality , we strive to opt for cost- effective solution . therefore , the chosen algorithm representative for these two slam method be orb-slam3 [ 3 ] and sc-lego-loam [ 4 ] , respectively . visual slam ( vslam ) mainly infer the motion of the camera and the surround environment base on the continuous color image stream . to ensure minimal distortion and reduce calculation relate to the epipolar geometry-based positional relationship , depth image stream be often integrate into the camera ’ s position . as exemplify by our research focus , orb-slam3 [ 3 ] be a feature-based slam system that employ pinhole and fisheye lens model to facilitate visual , visual-inertial , and multi-map slam by monocular , stereo , and rgb-d camera . based mainly on orb-slam2 [ 5 ] and orb- slam visual-inertial [ 6 ] , orb-slam3 introduce several novel method , include a multi-map system , bag-of-words for loop closure , and advance mid-term and long-term data association technique . lidar slam can be roughly divide into 2d and 3d lidar slam , generate 2d and 3d map separately . early lidar slam system , such a gmapping [ 7 ] , and simul- taneous localization , mapping , and moving object tracking ( slam-mot ) [ 8 ] primarily utilized 2d lidar sensor and be often base on the extended kalman filter ( ekf ) algorithm [ 9 ] . ekf be the nonlinear version of the kalman filter ( kf ) , which approxi- mat the state of a nonlinear system through a linear system that employ a first-order taylor expansion . as hardware advance and algorithms become increasingly refined , optimization-based technique and the integration of multiple scan channel with 3d lidar play increasingly significant role in the field of lidar slam . sc-lego-loam [ 4 ] be an expansion of the loam ( lidar odometry and mapping ) [ 10 ] approach and in- corporates a scan context loop detection technique [ 11 ] that utilize a global descriptor obtain through the scan context to detect loop . the main objective of sc-lego-loam be to perform real-time odometry and mapping by efficiently process the point cloud data acquire from a 3d lidar sensor and to achieve accurate and robust localization and mapping . an inertial measurement unit ( imu ) can also be use a a supplement to the slam position relationship , provide vision with fast-moving positioning . however , the external parameter matrix from the camera or lidar to imu must be obtain through physical measurement and sensor calibration . if the external reference calibration be not accurate enough , the position effect of the experimental restoration may be seriously wrong . therefore , the experiment involve in this paper do not use imu data . three-dimensional reconstruction serf a both an application and a component of slam technology . it furnish the essential three-dimensional geometric data necessary for construct environment map and play a pivotal role in realize the perception , positioning , and navigation function of the slam algorithm within the environment . typically , these 3d reconstruction representation take the form of point cloud model , mesh model , and geometric model . for example , kinect fusion [ 12 ] can use the rgb-d kinect camera to achieve instant 3d reconstruction of the environment through dense point cloud sampling and real-time tracking algorithm . furthermore , algorithm from the loam series can also find application in the architecture , engineering , and construction ( aec ) field [ 13 ] . the dataset determine the upper limit of the algorithm , so another focus of this paper be to record datasets use 3d lidar scan and rgb-d camera stream . current public datasets differ with target scene , record sensor data , and ground truth collection method . for example , the tum ( technical university of munich ) dataset [ 14,15 ] be suitable for pure visual slam and visual-imu slam , where the motion capture system record the ground truth . another example be the kitti ( karlsruhe institute of technology and toyota technological institute ) dataset [ 16 ] , which also provide high-quality data . kitti employ real-time kinematic gps ( rtk-gps , [ 17 ] ) in conjunction with inertial navigation systems ( ins ) to establish ground truth data . but it use a stereo camera sensor instead of an rgb-d camera sensor.appl . sci . 2024 ,14 , 3945 3 of 20 in previous slam research or common service robot in the market , slam technology be often use only in one specific scenario . as slam technology advance with modern technology , robot be require to continuously navigate between indoor and outdoor envi- ronments . however , different environment may be suitable for different slam algorithm . our research aim to assist researcher in gain a more comprehensive understanding of the performance of various slam approach . moreover , the literature [ 2,18 ] , be also useful for u to choose suitable location for conduct multiple experiment , thereby enhance the variability and assess the performance of various slam system . the summary of this research work can be succinctly encapsulate a follow : ( a ) a systematic review of the slam algorithm include gmapping , orb-slam3 , sc-lego-loam , and cartographer be introduce for the comparison . ( b ) the robot plat- form use for implementation , the process to establish datasets include rgb-d and 3d lidar data , and the mapping approach utilize in this study be introduce . in addition , the benchmark calculate by the cartographer algorithm for the trajectory error estimation be acquire . ( c ) the comparative analysis be divide into three main section : localization , mapping , and performance evaluation . the localization focus on evaluate the accuracy of pose tracking by ate and rpe , which can be acquire by calculate the euclidean distance between the ground-truth pose and the estimate pose [ 19 ] . the mapping section focus on evaluate 3d reconstruction performance , consider factor such a the quality of reconstructed model and adaptability to vary environmental condition . additionally , the performance evaluation encompass metric such a cpu utilization and memory usage . effective resource management be deem crucial for real-time application and overall system usability . 2 . theories this section offer a concise overview of pertinent slam ( simultaneous localization and mapping ) algorithm , specifically gmapping , orb-slam3 , sc-lego-loam , and the cartographer algorithm . gmapping stand out a one of the most widely-used slam algorithm ; however , it rely on odometry data a the input , make it suitable solely a a reference for positional effect in section 4 . the primary focus of evaluation lie on orb-slam3 and sc-lego-loam . additionally , the cartographer algorithm come into play for calculate ground-truth trajectory . ultimately , 3d reconstruction be employ to assess the mapping outcome . 2.1 . gmapping gmapping combine the wheel odometry of the robot with current observation data so that the uncertainty can be reduce . however , it depend severely on the existence of odometer , which affect it robust performance [ 7 ] . to solve the error accumulation problem of learn grid map with rbpfs ( rao-blackwellized particle filter ) , gmapping present proposal distribution of statistic and adaptive resampling to improve the accuracy of particle filter . the adaptive resampling mechanism in gmapping be base on a threshold nth , which be usually set to half of the total particle number , and the currently effective number of particle neff , which can be calculate by equation ( 1 ) . neff=1 n ∑ i=1","['citation : zhao , y.-l. ; hong , y.-t. ; huang , h.-p . comprehensive performance evaluation between visual slam and lidar slam for mobile robots : theories and experiments . appl . sci . 2024 ,14 , 3945. http : //doi.org/10.3390/ app14093945 academic editors : jos émiguel molina mart ínez and andrea prati received : 9 january 2024 revised : 11 april 2024 accepted : 2 may 2024 published : 6 may 2024 copyright : ©2024 by the author . licensee mdpi , basel , switzerland . this article be an open access article distribute under the term and condition of the creative commons attribution ( cc by ) license ( http : // creativecommons.org/licenses/by/ 4.0/ ) . apply science article comprehensive performance evaluation between visual slam and lidar slam for mobile robots : theories and experiments yu-lin zhao , yi-tian hong and han-pang huang * department of mechanical engineering , national taiwan university , taipei 106 , taiwan ; f07522844 @ ntu.edu.tw ( y.-l.z . ) ; r12522813 @ ntu.edu.tw ( y.-t.h . ) *correspondence : hanpang @ ntu.edu.tw ; tel . : +886-2-33662700 ; fax : +886-2-23676064 abstract : slam ( simultaneous localization and mapping ) , primarily rely on camera or lidar ( light detection and ranging ) sensor , play a crucial role in robotics for localization and envi- ronmental reconstruction . this paper assess the performance of two lead method , namely orb-slam3 and sc-lego-loam , focus on localization and mapping in both indoor and out- door environment . the evaluation employ artificial and cost-effective datasets incorporate data from a 3d lidar and an rgb-d ( color and depth ) camera . a practical approach be introduce for calculate ground-truth trajectory and during benchmarking , reconstruction map base on ground truth be establish . to assess the performance , ate and rpe be utilize to evaluate the accuracy of localization ; standard deviation be employ to compare the stability during the localization process for different method . while both algorithms exhibit satisfactory positioning accuracy ,', 'of localization ; standard deviation be employ to compare the stability during the localization process for different method . while both algorithms exhibit satisfactory positioning accuracy , their performance be suboptimal in scenario with inadequate texture . furthermore , 3d reconstruction map establish by the two approach be also provide for direct observation of their difference and the limitation encounter during map construction . moreover , the research include a comprehensive comparison of computational performance metric , encompass central processing unit ( cpu ) utilization , memory usage , and an in-depth analysis . this evaluation reveal that visual slam require more cpu resource than lidar slam , primarily due to additional data storage requirement , emphasize the impact of environmental factor on resource requirement . in conclusion , lidar slam be more suitable for the outdoors due to it comprehensive nature , while visual slam excels indoors , compensate for sparse aspect in lidar slam . to facilitate further research , a technical guide be also provide for the researcher in related field . keywords : 3d slam ; visual slam ; lidar slam ; 3d reconstruction ; robotics 1 . introduction slam have emerge a a prominent subject within the robotics community in recent year , lead to the development of numerous associate open-source project . these project encompass a wide range of application , include autonomous vehicle , drone , architectural surveying , and scene scanning . slam stand for simultaneous localization and mapping , represent a vital technique with immense potential . it enable a robot to robustly ascertain it position ( localization ) while concurrently construct a real-time representation of previously unexplored environment ( map ) [ 1 ] . various sensor be employ to gauge distance in the real world and capture pertinent feature . among these sensor , camera and lidar system be the most prevalent . consequently , the slam domain primarily encompass visual-based slam and lidar-based', 'real world and capture pertinent feature . among these sensor , camera and lidar system be the most prevalent . consequently , the slam domain primarily encompass visual-based slam and lidar-based slam , contingent upon the input from the specific sensor . in our previous study , we discuss mainly map for mobile robot in outdoor environment involve lidar slam and visual slam [ 2 ] . thus , this study primarily revolve around the performance comparison between visual slam and lidar slam in term of localization and mapping for mobile robot operate in real-world environment . appl . sci . 2024 ,14 , 3945. http : //doi.org/10.3390/app14093945 http : //www.mdpi.com/journal/applsciappl . sci . 2024 ,14 , 3945 2 of 20 with an emphasis on maintain continuity and universality , we strive to opt for cost- effective solution . therefore , the chosen algorithm representative for these two slam method be orb-slam3 [ 3 ] and sc-lego-loam [ 4 ] , respectively . visual slam ( vslam ) mainly infer the motion of the camera and the surround environment base on the continuous color image stream . to ensure minimal distortion and reduce calculation relate to the epipolar geometry-based positional relationship , depth image stream be often integrate into the camera ’ s position . as exemplify by our research focus , orb-slam3 [ 3 ] be a feature-based slam system that employ pinhole and fisheye lens model to facilitate visual , visual-inertial , and multi-map slam by monocular , stereo , and rgb-d camera . based mainly on orb-slam2 [ 5 ] and orb- slam visual-inertial [ 6 ] , orb-slam3 introduce several novel method , include a multi-map system , bag-of-words for loop closure , and advance mid-term and long-term data association technique . lidar slam can be roughly divide into 2d and 3d lidar slam , generate 2d and 3d map separately . early lidar slam system , such a gmapping [ 7 ] , and simul- taneous localization , mapping , and moving object tracking ( slam-mot ) [ 8 ] primarily utilized 2d lidar sensor and be often base on the', 'early lidar slam system , such a gmapping [ 7 ] , and simul- taneous localization , mapping , and moving object tracking ( slam-mot ) [ 8 ] primarily utilized 2d lidar sensor and be often base on the extended kalman filter ( ekf ) algorithm [ 9 ] . ekf be the nonlinear version of the kalman filter ( kf ) , which approxi- mat the state of a nonlinear system through a linear system that employ a first-order taylor expansion . as hardware advance and algorithms become increasingly refined , optimization-based technique and the integration of multiple scan channel with 3d lidar play increasingly significant role in the field of lidar slam . sc-lego-loam [ 4 ] be an expansion of the loam ( lidar odometry and mapping ) [ 10 ] approach and in- corporates a scan context loop detection technique [ 11 ] that utilize a global descriptor obtain through the scan context to detect loop . the main objective of sc-lego-loam be to perform real-time odometry and mapping by efficiently process the point cloud data acquire from a 3d lidar sensor and to achieve accurate and robust localization and mapping . an inertial measurement unit ( imu ) can also be use a a supplement to the slam position relationship , provide vision with fast-moving positioning . however , the external parameter matrix from the camera or lidar to imu must be obtain through physical measurement and sensor calibration . if the external reference calibration be not accurate enough , the position effect of the experimental restoration may be seriously wrong . therefore , the experiment involve in this paper do not use imu data . three-dimensional reconstruction serf a both an application and a component of slam technology . it furnish the essential three-dimensional geometric data necessary for construct environment map and play a pivotal role in realize the perception , positioning , and navigation function of the slam algorithm within the environment . typically , these 3d reconstruction representation take the form of point cloud model , mesh model , and', ', positioning , and navigation function of the slam algorithm within the environment . typically , these 3d reconstruction representation take the form of point cloud model , mesh model , and geometric model . for example , kinect fusion [ 12 ] can use the rgb-d kinect camera to achieve instant 3d reconstruction of the environment through dense point cloud sampling and real-time tracking algorithm . furthermore , algorithm from the loam series can also find application in the architecture , engineering , and construction ( aec ) field [ 13 ] . the dataset determine the upper limit of the algorithm , so another focus of this paper be to record datasets use 3d lidar scan and rgb-d camera stream . current public datasets differ with target scene , record sensor data , and ground truth collection method . for example , the tum ( technical university of munich ) dataset [ 14,15 ] be suitable for pure visual slam and visual-imu slam , where the motion capture system record the ground truth . another example be the kitti ( karlsruhe institute of technology and toyota technological institute ) dataset [ 16 ] , which also provide high-quality data . kitti employ real-time kinematic gps ( rtk-gps , [ 17 ] ) in conjunction with inertial navigation systems ( ins ) to establish ground truth data . but it use a stereo camera sensor instead of an rgb-d camera sensor.appl . sci . 2024 ,14 , 3945 3 of 20 in previous slam research or common service robot in the market , slam technology be often use only in one specific scenario . as slam technology advance with modern technology , robot be require to continuously navigate between indoor and outdoor envi- ronments . however , different environment may be suitable for different slam algorithm . our research aim to assist researcher in gain a more comprehensive understanding of the performance of various slam approach . moreover , the literature [ 2,18 ] , be also useful for u to choose suitable location for conduct multiple experiment , thereby enhance the variability and assess', 'of various slam approach . moreover , the literature [ 2,18 ] , be also useful for u to choose suitable location for conduct multiple experiment , thereby enhance the variability and assess the performance of various slam system . the summary of this research work can be succinctly encapsulate a follow : ( a ) a systematic review of the slam algorithm include gmapping , orb-slam3 , sc-lego-loam , and cartographer be introduce for the comparison . ( b ) the robot plat- form use for implementation , the process to establish datasets include rgb-d and 3d lidar data , and the mapping approach utilize in this study be introduce . in addition , the benchmark calculate by the cartographer algorithm for the trajectory error estimation be acquire . ( c ) the comparative analysis be divide into three main section : localization , mapping , and performance evaluation . the localization focus on evaluate the accuracy of pose tracking by ate and rpe , which can be acquire by calculate the euclidean distance between the ground-truth pose and the estimate pose [ 19 ] . the mapping section focus on evaluate 3d reconstruction performance , consider factor such a the quality of reconstructed model and adaptability to vary environmental condition . additionally , the performance evaluation encompass metric such a cpu utilization and memory usage . effective resource management be deem crucial for real-time application and overall system usability . 2 . theories this section offer a concise overview of pertinent slam ( simultaneous localization and mapping ) algorithm , specifically gmapping , orb-slam3 , sc-lego-loam , and the cartographer algorithm . gmapping stand out a one of the most widely-used slam algorithm ; however , it rely on odometry data a the input , make it suitable solely a a reference for positional effect in section 4 . the primary focus of evaluation lie on orb-slam3 and sc-lego-loam . additionally , the cartographer algorithm come into play for calculate ground-truth trajectory . ultimately , 3d reconstruction', '4 . the primary focus of evaluation lie on orb-slam3 and sc-lego-loam . additionally , the cartographer algorithm come into play for calculate ground-truth trajectory . ultimately , 3d reconstruction be employ to assess the mapping outcome . 2.1 . gmapping gmapping combine the wheel odometry of the robot with current observation data so that the uncertainty can be reduce . however , it depend severely on the existence of odometer , which affect it robust performance [ 7 ] . to solve the error accumulation problem of learn grid map with rbpfs ( rao-blackwellized particle filter ) , gmapping present proposal distribution of statistic and adaptive resampling to improve the accuracy of particle filter . the adaptive resampling mechanism in gmapping be base on a threshold nth , which be usually set to half of the total particle number , and the currently effective number of particle neff , which can be calculate by equation ( 1 ) . neff=1 n ∑ i=1\\x00ew ( i ) \\x012 ( 1 ) where nis the total number of the particle and ew ( i ) be the normalized weight of the particle i . under the condition that neffis less than nth , adaptive resampling will be perform . this method combine robot encoder data with current observation data so that the uncertainty of robot can be reduce . however , it depend heavily on the existence ofappl . sci . 2024 ,14 , 3945 4 of 20 odometer , which affect it robust performance . furthermore , the number of particle become large and influence the calculation speed in a large scale scene . 2.2 . orb-slam3 orb-slam3 [ 3 ] be a highly efficient system design for visual slam . it operate through three concurrent thread : tracking , local mapping , and loop and map merging , alongside an independent full bundle adjustment ( ba ) thread for map enhancement . bundle adjustment [ 20 ] be a non-linear optimization method utilize in the orb- slam3 algorithm ; it estimate accurate camera 3d location and pose by minimize the reprojection error with respect to the match key point on the digital image . the se ( 3 ) ,', 'utilize in the orb- slam3 algorithm ; it estimate accurate camera 3d location and pose by minimize the reprojection error with respect to the match key point on the digital image . the se ( 3 ) , special euclidean group , represent the special euclidean group within lie group algebra . it be a six-dimensional vector encompass three rotational degree of freedom and three translational degree of freedom . the se ( 3 ) manifold possess a topological structure that render it differentiable . consequently , se ( 3 ) can effectively determine an alignment . the error can be formulate a ei , j=xi , j−πi ( tiw , xw , j ) ( 2 ) and the minimized cost function be represent a equation ( 3 ) , c=∑ i , jρh ( et i , jω−1 i , jei , j ) ( 3 ) where ρhis the huber robust cost function and ωi , j=σ2 i , ji2×2is the covariance matrix relate to the scale of the detected key point . in orb-slam3 , the track thread process image data , extract key feature point , and match them between consecutive frame to determine the current frame ’ s position . to enhance pose estimation , this thread employ a motion-only ba , refine the accuracy of the calculated pose . additionally , keyframes be discern through predefined criterion . the local mapping thread implement the track thread ’ s decision , generate new keyframes and update the local map . a culling mechanism be employ to systematically eliminate redundant keyframes and point , thereby bolster the robustness of track . furthermore , the local bundle adjustment optimizes all point within the local area for map accuracy . the loop and map merge thread operates within the atlas multi-map system , search for similar scene to achieve loop closure and integrate map smoothly . lastly , the full ba thread independently perform pose-graph optimization to refine the entire map without compromise real-time performance . by coordinate these thread , orb-slam3 delivers real-time and optimal performance for diverse application require accurate and up-to-date map . 2.3 . sc-lego-loam sc-lego-loam [', 'performance . by coordinate these thread , orb-slam3 delivers real-time and optimal performance for diverse application require accurate and up-to-date map . 2.3 . sc-lego-loam sc-lego-loam [ 4 ] , an extension of the lego-loam [ 21 ] method , incorporate a loop detection technique base on the scan context for enhanced loop identification . the process begin with the 3d lidar data point obtain from the scan , which be initially categorize into ground and segment point use a ground segmentation method . subsequently , an image segmentation method refine the segmented point into distinct cluster , each assign a specific label for identification purpose . by evaluate the roughness value cof both ground and segment point and compare them with a predefined threshold cth , the classification of edge and planar feature be then carry out . the roughness c be define a follow : c=1 |s|· ∥ri∥∥∑ j∈s , j̸=i ( rj−ri ) ∥ ( 4 ) where riand rjare the euclidean distance from point piand pjin set s to lidar , respectively.appl . sci . 2024 ,14 , 3945 5 of 20 the estimation of the robot ’ s motion follow , involve a comparison between the current feature set and that of the previous time step , with label match enhance the precision of match result . the subsequent application of a two-step levenberg– marquardt ( l–m ) optimization compute the pose transformation matrix . loop detection be facilitate through the integration of a scan context method , transform the 3d point cloud into polar coordinate and extract global descriptor . utilizing a k-dimensional ( kd ) tree and conduct the near neighbor search , the frame with the high similarity score be identify a the loopback frame , facilitate loop detection . the similarity score between query and the candidate point cloud can be calculate by d ( iq , ic ) =1 nsns ∑ j=1 ( 1−cq j·cc j ∥cq j∥∥cc j∥ ) ( 5 ) where nsrepresents axial sector number in scan context iand cq j , cc jrepresent column vector at the same index in query and the candidate frame , respectively . the final stage', '∥cq j∥∥cc j∥ ) ( 5 ) where nsrepresents axial sector number in scan context iand cq j , cc jrepresent column vector at the same index in query and the candidate frame , respectively . the final stage encompass map optimization via the iterative closest point ( icp , [ 22 ] ) method and the integration of positional data , culminate in the determination of an accurate final position and estimation . 2.4 . cartographer the cartographer [ 23 ] stand a an open-source real-time 2d and 3d mapping library and slam system , with it primary development carry out by google . the original source code be introduce in 2016 [ 23 ] and have be update subsequently . consequently , we be presently utilize the late release version , which receive it most recent update in 2021 . the cartographer system employ a dual approach , incorporate both local and global optimization technique to rectify error and estimate pose . these two optimization method be relatively independent and align with the distinct mapping process : the front end , responsible for convert scan into submaps , and the back end , task with link submaps into the global map . the latter can also be view a manage loop closure . 1 . scans-to-submap ( local map ) initially , the lidar raw data be transform into scan without any pose estimation . a sequence of consecutive scan be then employ to construct a submap , represent a a probability grid . to enhance the quality of the scan , a bicubic interpolation method be utilize for filtering and smooth noisy data . this refined scan output be refer to a a grid scan . the scan matching process , utilize a ceres-based [ 24 ] scan matcher , be employ to perform this task , simplify a a nonlinear least square problem . argmin ξk ∑ k=1 ( 1−msmooth ( tξhk ) ) 2 ( 6 ) where tξtransforms hkfrom the scan frame to the submap frame accord to the scan pose . the function msmooth : r2→ris a smooth version of the probability value in the local submap . consequently , this process enable the estimation of the pose , with the grid scan', 'to the scan pose . the function msmooth : r2→ris a smooth version of the probability value in the local submap . consequently , this process enable the estimation of the pose , with the grid scan subsequently incorporate into the submap . 2 . submaps-to-map ( global map ) as the front-end process gradually accumulate error , the back end characterize this optimization a a nonlinear least square problem , akin to the local mapping step , term sparse pose adjustment ( spa ) . this challenge involve handle scan pose and submap pose , incorporate constraint relate to relative pose and their associate covariance matrix . to prevent an excessive computational burden when explore the entire domain , the cartographer employ a branch-and-bound ( bab ) scan matchingappl . sci . 2024 ,14 , 3945 6 of 20 approach . its algorithm can be summarize a a method for refine the search process to obtain an optimal pose estimate . 2.5 . three-dimensional reconstruction in this section , for the goal of 3d reconstruction , two map generate separately by two slam method be briefly introduce . 1 . rgb-d point cloud an rgb-d point cloud be a data representation that combine color information from a traditional rgb ( red , green , and blue ) image with depth information to create a three- dimensional reconstruction of a scene or object . figure 1 show a typical rgb-d point cloud reconstruction process . appl . sci . 2024 , 14 , x for peer review 6 of 21 as the front-end process gradually accumulate error , the back end characterize this optimization a a nonlinear least square problem , akin to the local mapping step , term sparse pose adjustment ( spa ) . this challenge involve handle scan pose and submap pose , incorporate constraint rela ted to relative pose and their associate covariance matrix . to prevent an excessive computational burden when explore the entire domain , the cartographer employ a branch-and-bound ( bab ) scan match approach . its algorithm can be summarize a a method for re ﬁning the search process to obtain an', 'burden when explore the entire domain , the cartographer employ a branch-and-bound ( bab ) scan match approach . its algorithm can be summarize a a method for re ﬁning the search process to obtain an optimal pose estimate . 2.5 . three-dimensional reconstruction in this section , for the goal of 3d recons truction , two map generate separately by two slam method be brie ﬂy introduce . 1 . rgb-d point cloud an rgb-d point cloud be a data representati on that combine color information from a traditional rgb ( red , green , and blue ) image with depth information to create a three- dimensional reconstruction of a scene or ob ject . figure 1 show a typical rgb-d point cloud reconstruction process . figure 1 . overview of a typical rgb- d reconstruction process . initially , the preprocessed input data be align and use for local reconstruction . next , we can determine the position and or ientation of the come ra by establish association between visual feature . this involve the process of match observed feature or landmark in the environment wi th their corresponding feature in a known or reference map . and loop closure detect an d correct error in the estimated trajectory . finally , base on the estimate camera pose , the input data be integrate into the 3d reconstruction mapping . 2 . lidar point cloud lidar point cloud map , generate by lidar slam , provide information regard the position of object . this be achieve through the emission of laser beam by the lidar sensor , which be then re ﬂected back from the object to calculate their respective position . due to the prop erties of 3d lidar beam be emi tted in a speci ﬁc angular range and cover multiple directio n at the same time , the point cloud map posse the advantage of fast and wide-ranging simultaneous generation . as a result , it be well-suited for application involve large-scale environment . to alleviate computational load and down-sampling , some mapping algorithm will preserve feature like point , line , and surface a data point . 3 . dataset and', 'involve large-scale environment . to alleviate computational load and down-sampling , some mapping algorithm will preserve feature like point , line , and surface a data point . 3 . dataset and benchmark 3.1 . robot platform this section describe the robot platform setup use in the experiment , which be modi ﬁed from [ 25 ] . the di ﬀerential wheel mobile robot deploy several sensor , computer , and power . the sensor contain stereolab depth camera zed2 , lslidar 16- line mechanical lidar , and wheel odometry . thus , we collect wheel odometry , rgb and figure 1 . overview of a typical rgb-d reconstruction process . initially , the preprocessed input data be align and use for local reconstruction . next , we can determine the position and orientation of the camera by establish associ- ations between visual feature . this involve the process of match observed feature or landmark in the environment with their corresponding feature in a known or ref- erence map . and loop closure detects and corrects error in the estimated trajectory . finally , base on the estimate camera pose , the input data be integrate into the 3d reconstruction mapping . 2 . lidar point cloud lidar point cloud map , generate by lidar slam , provide information regard the position of object . this be achieve through the emission of laser beam by the lidar sensor , which be then reflect back from the object to calculate their respective position . due to the property of 3d lidar beam be emit in a specific angular range and cover multiple direction at the same time , the point cloud map posse the advantage of fast and wide-ranging simultaneous generation . as a result , it be well-suited for application involve large-scale environment . to alleviate computational load and down-sampling , some mapping algorithm will preserve feature like point , line , and surface a data point . 3 . dataset and benchmark 3.1 . robot platform this section describe the robot platform setup use in the experiment , which be modify from [ 25 ] . the differential wheel', ', and surface a data point . 3 . dataset and benchmark 3.1 . robot platform this section describe the robot platform setup use in the experiment , which be modify from [ 25 ] . the differential wheel mobile robot deploy several sensor , computer , and power . the sensor contain stereolab depth camera zed2 , lslidar 16-line mechanical lidar , and wheel odometry . thus , we collect wheel odometry , rgb and depth image , and 3d lidar scan . the transform matrix of the different sensor frame be also calculate a figure 2 . the mobile robot ( figure 2 ) be construct by our laboratory . the position of the depth camera and 3d lidar mount on the robot have be accurately measure by the 3d model file and the extrinsic calibration parameter have be compute use the d–h ( denavit–hartenberg ) parameter table in the field of robotics . it be worth mention that the robot also have 2d lidar ; however , it be notappl . sci . 2024 ,14 , 3945 7 of 20 used because of poor accuracy . the computer deploy an intel core i7-9700te ( 8 core @ 3.8 ghz ) , which be use for all the dataset record and slam experiment . and 64-bit ubuntu 20.04 be use for all of the experiment . appl . sci . 2024 , 14 , x for peer review 7 of 21 depth image , and 3d lidar scan . the transform matrix of the di ﬀerent sensor frame be also calculate a figure 2 . the mobile robot ( figure 2 ) be construct by our laboratory . the position of the depth camera and 3d lidar mount on the robot have be accurately measure by the 3d model ﬁle and the extrinsic calibration parameter have be compute use the d–h ( denav it–hartenberg ) parameter table in the ﬁeld of robotics . it be worth mention that the robot also have 2d lidar ; however , it be not use because of poor accuracy . the computer depl oyed an intel core i7-9700te ( 8 core @ 3.8ghz ) , which be use for all the dataset record and slam experiment . and 64-bit ubuntu 20.04 be use for all of the experiment . figure 2 . hardware and coordinate system . 3.2 . benchmark this section primarily focus on the acquisition', 'record and slam experiment . and 64-bit ubuntu 20.04 be use for all of the experiment . figure 2 . hardware and coordinate system . 3.2 . benchmark this section primarily focus on the acquisition of cost-e ﬀective datasets with the ground truth serve a the benchmark . in es sence , our task be to devise a methodology for obtain ground truth data and craft scenario that align with our speci ﬁc evaluation requirement for assess the perf ormance of various algorithm . in order to manage cost , it become crucial to obtain datasets that be economically viable while still serve a a valid benchmark for algorithmic e ﬀectiveness . as a result , we need to derive ground truth value base on the available sensor and data source . fortunately , the wheeled robot be typically design to travel on ﬂat ground and often have ﬁxed sensor . thus , the 2d slam algorithm aid in derive the ground truth , encompass both the robot ’ s pose and their corresponding lidar timestamps . additionally , we utilize interpolation to enhance the data content and synchron ize lidar and image timestamps , contribute to the accurate calculation of error between camera pose obtain from orb-slam3 and ground truth . finally , leverage the cartographer a our foundation , we have propose a technique for calculate ground truth . at the same time , we select scene with engineering drawing or tile a reference while record dataset scene . in this stud y , w e r ecorded re al-world d a tasets from two scene on the campus of national taiwan university ( ntu ) in taipei , taiwan . these scene include ( 1 ) the corridor on the sixth ﬂoor of the college of engineering buildi ng a an indoor environment and ( 2 ) the second ﬂoor outdoor venue of the college of engineering building a an outdoor environment . combined with compare the engineering drawing of the building , we believe that the accuracy of this dataset ca n be guarantee . in addition , the paper [ 23 ] show that the relative error of the cartographer be less than 1 % , which be within our error margin', ', we believe that the accuracy of this dataset ca n be guarantee . in addition , the paper [ 23 ] show that the relative error of the cartographer be less than 1 % , which be within our error margin . at this point , we have obtain benchmark for compare localization e ﬀects . 3.3 . mapping ( 3d reconstruction ) our subsequent goal be to obtain the mapping benchmark by expand upon our establish localization . as illustrated in figure 3 , this paper introduce two distinct rgb- figure 2 . hardware and coordinate system . 3.2 . benchmark this section primarily focus on the acquisition of cost-effective datasets with the ground truth serve a the benchmark . in essence , our task be to devise a methodology for obtain ground truth data and craft scenario that align with our specific evaluation requirement for assess the performance of various algorithm . in order to manage cost , it become crucial to obtain datasets that be economically viable while still serve a a valid benchmark for algorithmic effectiveness . as a result , we need to derive ground truth value base on the available sensor and data source . fortunately , the wheeled robot be typically design to travel on flat ground and often have fix sensor . thus , the 2d slam algorithm aid in derive the ground truth , encompass both the robot ’ s pose and their corresponding lidar timestamps . additionally , we utilize interpolation to enhance the data content and synchronize lidar and image timestamps , contribute to the accurate calculation of error between camera pose obtain from orb-slam3 and ground truth . finally , leverage the cartographer a our foundation , we have propose a technique for calculate ground truth . at the same time , we select scene with engineering drawing or tile a reference while record dataset scene . in this study , we record real-world datasets from two scene on the campus of national taiwan university ( ntu ) in taipei , taiwan . these scene include ( 1 ) the corridor on the sixth floor of the college of engineering building a an', 'datasets from two scene on the campus of national taiwan university ( ntu ) in taipei , taiwan . these scene include ( 1 ) the corridor on the sixth floor of the college of engineering building a an indoor environment and ( 2 ) the second floor outdoor venue of the college of engineering building a an outdoor environment . combined with compare the engineering drawing of the building , we believe that the accuracy of this dataset can be guarantee . in addition , the paper [ 23 ] show that the relative error of the cartographer be less than 1 % , which be within our error margin . at this point , we have obtain benchmark for compare localization effect . 3.3 . mapping ( 3d reconstruction ) our subsequent goal be to obtain the mapping benchmark by expand upon our establish localization . as illustrated in figure 3 , this paper introduce two distinct rgb-d reconstruction process base on different data association method . figure 3a show a rgb-d reconstruction with loop closure , rely on orb-slam3 . in contrast , figure 3b illustrate rgb-d reconstruction base on ground truth , prevent additional loop closure a acquire ground truth incorporate loopback correction . to optimize the effect of mapping and reduce the amount of calculation , we use three filter when construct the map . ( i ) pass-through filter be employ to filter data along a specific dimension efficiently . it can remove error cause by the inherent characteristic of the system and the imaging principle . ( ii ) voxel filter be utilized to reduce point sampling and mitigate bias . ( iii ) statistical outlier removal filter serve the purpose of eliminate outliers.appl . sci . 2024 ,14 , 3945 8 of 20 appl . sci . 2024 , 14 , x for peer review 8 of 21 d reconstruction process base on di ﬀerent data association method . figure 3a show a rgb-d reconstruction with loop closure , re lie on orb-slam3 . in contrast , figure 3b illustrate rgb-d reconstruction base on gr ound truth , prevent additional loop closure a acquire ground truth incorporat es loopback', 'with loop closure , re lie on orb-slam3 . in contrast , figure 3b illustrate rgb-d reconstruction base on gr ound truth , prevent additional loop closure a acquire ground truth incorporat es loopback correction . to optimize the e ﬀect of mapping and reduce the amount of calculation , we use three ﬁlters when construct the map . ( i ) pass-through ﬁlters be employ to ﬁlter data along a speci ﬁc dimension eﬃciently . it can remove error cause by th e inherent characteristic of the system and the imaging principle . ( ii ) voxel ﬁlters be utilized to reduce point sampling and mitigate bias . ( iii ) statistical outlier removal ﬁlters serve the purpose of eliminate outlier . ( a ) rgb-d reconstruction base on orb-slam3 . ( b ) rgb-d reconstruction base on ground truth . figure 3 . two type of rgb-d reconstruction processing method be refer to in this paper . ( a ) based on orb-slam3 , rgb-d reconstruction with a loop closure . ( b ) based on ground truth , rgb- d reconstruction . 4 . comprehensive performance evaluation the evaluation and comparison of the pu rposed slam algorithm can be divide into two category : localization and mapping in an indoor environment and localization and mapping in an outdoor environment . the organization aim to compare the overall performance of slam algorithm in two environment . there be ﬁve outdoor datasets and ﬁve indoor datasets , include 3d lidar data , rgb-d image , and the corresponding benchmark . in this paper , the trajectory path for the self-recorded indoor dataset be about 90 m , while the outdoor dataset have a traj ectory path of about 40 m. additionally , when run slam result ( both trajectory and mapping ) , program ( ros-noetic-rviz 1.14.20 on ros netics and debug message ) be employ to ensure that both slam method in every running achieve loop detecti on and result in the best slam outcome . during the recording of datasets in this paper , the mobile robot move along the same route ﬁve time with the same starting and end position each time . therefore , in table 1', 'the best slam outcome . during the recording of datasets in this paper , the mobile robot move along the same route ﬁve time with the same starting and end position each time . therefore , in table 1 , for example , i1 represent the ﬁrst recording in an indoor environment , while o5 represent the ﬁfth recording in an outdoor environment . the number in the descriptor only indicate the order of dataset record ing in each environment . in the indoor environment , we use the third dataset ( i 3 ) for our implementation . in the outdoor environment , we use the fourth dataset ( o 4 ) for our implementation . it have be modi ﬁed in the manuscript . table 1 . ate with se ( 3 ) umeyama a lignment in an indoor environment ( unit : m ) . gmapping orb-slam3 sc-lego- loam gmapping orb-slam3 sc-lego- loam dataset i1 max 4.91518 0.78351 2.38522 dataset i4 max 1.88854 0.46741 0.56568 mean 2.68788 0.48961 0.5693 mean 0.90588 0.27761 0.27293 rmse 3.05934 0.50410 0.69376 rmse 1.09383 0.29589 0.29494 figure 3 . two type of rgb-d reconstruction processing method be refer to in this paper . ( a ) based on orb-slam3 , rgb-d reconstruction with a loop closure . ( b ) based on ground truth , rgb-d reconstruction . 4 . comprehensive performance evaluation the evaluation and comparison of the purposed slam algorithm can be divide into two category : localization and mapping in an indoor environment and localization and mapping in an outdoor environment . the organization aim to compare the overall performance of slam algorithm in two environment . there be five outdoor datasets and five indoor datasets , include 3d lidar data , rgb-d image , and the corresponding benchmark . in this paper , the trajectory path for the self-recorded indoor dataset be about 90 m , while the outdoor dataset have a trajectory path of about 40 m. additionally , when run slam result ( both trajectory and mapping ) , program ( ros-noetic-rviz 1.14.20 on ros netics and debug message ) be employ to ensure that both slam method in every running achieve loop', ', when run slam result ( both trajectory and mapping ) , program ( ros-noetic-rviz 1.14.20 on ros netics and debug message ) be employ to ensure that both slam method in every running achieve loop detection and result in the best slam outcome . during the recording of datasets in this paper , the mobile robot move along the same route five time with the same starting and end position each time . therefore , in table 1 , for example , i1 represent the first recording in an indoor environment , while o5 represent the fifth recording in an outdoor environment . the number in the descriptor only indicate the order of dataset recording in each environment . in the indoor environment , we use the third dataset ( i3 ) for our implementation . in the outdoor environment , we use the fourth dataset ( o4 ) for our implementation . it have be modify in the manuscript . table 1 . ate with se ( 3 ) umeyama alignment in an indoor environment ( unit : m ) . gmappingorb- slam3sc-lego- loamgmappingorb- slam3sc-lego- loam max 4.91518 0.78351 2.38522 max 1.88854 0.46741 0.56568 mean 2.68788 0.48961 0.5693 mean 0.90588 0.27761 0.27293 rmse 3.05934 0.50410 0.69376 rmse 1.09383 0.29589 0.29494dataset i1 std 1.4611 0.11998 0.39648dataset i4 std 0.61307 0.10238 0.1118 max 2.39814 0.65616 0.73112 max 1.31456 0.79217 0.92859 mean 1.23568 0.39849 0.35394 mean 0.50987 0.3197 0.4894 rmse 1.48806 0.40884 0.38915 rmse 0.65464 0.36851 0.53221dataset i2 std 0.82911 0.09143 0.16174dataset i5 std 0.41060 0.18328 0.20911 max 4.80831 0.38631 1.07315 max 3.06494 0.61711 1.13675 mean 2.45052 0.16167 0.39211 mean 1.55796 0.32941 0.41554 rmse 2.87831 0.17358 0.44974 rmse 1.83484 0.35018 0.47196dataset i3 std 1.50985 0.0632 0.22025average std 0.96475 0.11205 0.21988appl . sci . 2024 ,14 , 3945 9 of 20 4.1 . indoor environment the indoor datasets describe a rectangular corridor ( low-texture , low-structure ) with a total length of over 90 m. due to different robot speed , the duration of the datasets be approximately 350 to 450 s. 4.1.1 . localization', 'rectangular corridor ( low-texture , low-structure ) with a total length of over 90 m. due to different robot speed , the duration of the datasets be approximately 350 to 450 s. 4.1.1 . localization figure 4 display the ground truth of the robot pose obtain from one of the indoor datasets ( i3 ) . other datasets have similar trajectory . the absolute trajectory error ( ate ) and relative pose error ( rpe ) be utilize to evaluate slam algorithms [ 26 ] . ate measure the difference between the actual and estimated value of camera pose , directly show the advantage and disadvantage of the algorithm . relative pose error be a measure of the difference between the estimate pose and the ground truth pose at each point in time along the trajectory . in addition , se ( 3 ) transformation parameter be calculate use the umeyama algorithm [ 27 ] to align the two value . appl . sci . 2024 , 14 , x for peer review 9 of 21 std 1.4611 0.11998 0.39648 std 0.61307 0.10238 0.1118 dataset i2 max 2.39814 0.65616 0.73112 dataset i5 max 1.31456 0.79217 0.92859 mean 1.23568 0.39849 0.35394 mean 0.50987 0.3197 0.4894 rmse 1.48806 0.40884 0.38915 rmse 0.65464 0.36851 0.53221 std 0.82911 0.09143 0.16174 std 0.41060 0.18328 0.20911 dataset i3 max 4.80831 0.38631 1.07315 average max 3.06494 0.61711 1.13675 mean 2.45052 0.16167 0.39211 mean 1.55796 0.32941 0.41554 rmse 2.87831 0.17358 0.44974 rmse 1.83484 0.35018 0.47196 std 1.50985 0.0632 0.22025 std 0.96475 0.11205 0.21988 4.1 . indoor environment the indoor datasets describe a rectangular corridor ( low-texture , low-structure ) with a total length of over 90 m. due to di ﬀerent robot speed , the duration of the datasets be approximately 350 to 450 s. 4.1.1 . localization figure 4 display the ground truth of the ro bot pose obtain from one of the indoor datasets ( i3 ) . other datasets have similar traj ectories . the absolute trajectory error ( ate ) and relative pose error ( rpe ) be utilize to evaluate slam algorithms [ 26 ] . ate measure the diﬀerence between the actual and estimate', 'similar traj ectories . the absolute trajectory error ( ate ) and relative pose error ( rpe ) be utilize to evaluate slam algorithms [ 26 ] . ate measure the diﬀerence between the actual and estimate va lues of camera pose , directly show the advantage and disadvantage of the algorithm . relative pose error be a measure of the diﬀerence between the estimate pose and the gr ound truth pose at each point in time along the trajectory . in addition , se ( 3 ) transf ormation parameter be calculate use the umeyama algorithm [ 27 ] to align the two value . figure 4 . the ground truth of the robot pose obtain from one of the indoor datasets ( i3 ) . four metric be choose to assess the algorithm , comprise the maximum value ( max ) , mean value ( mean ) , root mean square error ( rmse ) , and standard deviation ( std ) . the rmse can quantify the accuracy of the constructed trajectory through the following mathematical expression 2 1ˆ , t tn txx rmsne=\\uf0e6\\uf0f6\\uf0e7\\uf0f7\\uf0e8\\uf0f8− =\\uf0e5 ( 7 ) where ˆtx represent the ground truth of the localization of the mobile robot , tx be the measured value , n be the total number of position , and t stand for the serial number . figure 4 . the ground truth of the robot pose obtain from one of the indoor datasets ( i3 ) . four metric be choose to assess the algorithm , comprise the maximum value ( max ) , mean value ( mean ) , root mean square error ( rmse ) , and standard deviation ( std ) . the rmse can quantify the accuracy of the constructed trajectory through the follow- ing mathematical expression rmse =s ∑n t=1\\x00 xt−ˆxt\\x012 n , ( 7 ) where ˆxtrepresents the ground truth of the localization of the mobile robot , xtis the measured value , ni the total number of position , and tstands for the serial number . the std reflect the degree of stability of the build trajectory error and the mathemati- cal expression be σ=s ∑n t=1\\x00 xt−xt\\x012 n−1 , ( 8 ) where xtis the mean value . additionally , gmapping serf a an extra reference . however , give that gmapping be a 2d lidar slam algorithm utilize odometry a an', 'be σ=s ∑n t=1\\x00 xt−xt\\x012 n−1 , ( 8 ) where xtis the mean value . additionally , gmapping serf a an extra reference . however , give that gmapping be a 2d lidar slam algorithm utilize odometry a an additional input , it involvement be limit to the evaluation of localization.appl . sci . 2024 ,14 , 3945 10 of 20 the result of the metric ate ( with se ( 3 ) umeyama alignment ) be show in table 1 . it be obvious that in indoor environment , orb-slam3 have small error , highlight in bold font , and outperform other method . moreover , orb-slam3 also demonstrate small standard deviation , indicate high stability for localization . due to the smooth surface of the indoor floor , the robot be susceptible to slip , lead to inaccurate mileage count value . consequently , this issue adversely impact the precision of the gmapping algorithm . as illustrated in figure 5 , we conduct a comparison of rpe in an indoor environment ( i3 ) , unveil the discernible impact of loop closure on orb-slam3 and sc-lego-loam , respectively . analysis of the four sub-figures allow u to draw the following conclusion . ( 1 ) in indoor environment , loop closure detection effectively correct the result of the two slam algorithm , enhance their accuracy and robustness . ( 2 ) the rpe of visual slam be positively correlate with the robot ’ s speed and change in the field of view , with the most pronounced performance observe in figure 5a . the local maximum value occur at corner , while the rpe approach zero in the initial and final static state . ( 3 ) conversely , the correlation of lidar slam ’ s rpe be relatively inconspicuous . we attribute this to the lidar ’ s 360-degree horizontal range , which scan the operator behind it , introduce a different dynamic . appl . sci . 2024 , 14 , x for peer review 10 of 21 the std re ﬂects the degree of stability of th e build trajectory error and the mathematical expression be ( ) 2 1 , 1n tttxx nσ=− = −\\uf0e5 ( 8 ) where tx be the mean value . additionally , gmapping serf a an extra reference . however , give', 'of th e build trajectory error and the mathematical expression be ( ) 2 1 , 1n tttxx nσ=− = −\\uf0e5 ( 8 ) where tx be the mean value . additionally , gmapping serf a an extra reference . however , give that gmapping be a 2d lidar slam algorithm utilize odometry a an additional input , it involvement be limit to the evaluation of localization . the result of the metric ate ( with se ( 3 ) umeyama alignment ) be show in table 1 . it be obvious that in indoor environment , orb-slam3 have small error , highlight in bold font , and outperform other method . moreover , orb-slam3 also demonstrate small standard deviation , indicate high stability for localization . due to the smooth surface of the indoor ﬂoor , the robot be susceptible to slip , lead to inaccurate mileage count value . consequently , this issue adversely impact the precision of the gmapping algorithm . as illustrated in figure 5 , we conduc ted a comparison of rpe in an indoor environment ( i3 ) , unveil the discernible impact of loop clos ure on orb-slam3 and sc-lego-loam , respectively . analysis of the four sub- ﬁgures allow u to draw the following conclusion . ( 1 ) in indoor environment , loop closure detection e ﬀectively correct the result of the two slam algorith m , enhance their accuracy and robustness . ( 2 ) the rpe of visual slam be positively correlate with the robot ’ s speed and change in the ﬁeld of view , with the most pronounced performance observe in figure 5a . the local maximum value occur at corner , while the rpe approach zero in the initial and ﬁnal static state . ( 3 ) conversely , the correlation of lidar slam ’ s rpe be relatively inconspicuous . we a ttribute this to the lidar ’ s 360-degree horizontal range , which scan the operator behind it , introduce a di ﬀerent dynamic . ( a ) ( b ) ( c ) ( d ) figure 5 . i3 rpe with regard to the translation part ( m ) for delta = 1 ( frame ) use consecutive pair ( with se ( 3 ) umeyama alignment , while proc essing pan rotation and scale ) : ( a ) orb-slam3 with loop closure ; ( b )', 'to the translation part ( m ) for delta = 1 ( frame ) use consecutive pair ( with se ( 3 ) umeyama alignment , while proc essing pan rotation and scale ) : ( a ) orb-slam3 with loop closure ; ( b ) sc-lego-loam with loop closure ; ( c ) orb-slam3 without loop closure ; and ( d ) sc-lego-loam without loop closure . figure 5 . i3 rpe with regard to the translation part ( m ) for delta = 1 ( frame ) use consecutive pair ( with se ( 3 ) umeyama alignment , while process pan rotation and scale ) : ( a ) orb-slam3 with loop closure ; ( b ) sc-lego-loam with loop closure ; ( c ) orb-slam3 without loop closure ; and ( d ) sc-lego-loam without loop closure . 4.1.2 . mapping 1 . lidar slam map analysis the robot trajectory be illustrate by the colored line be show in figure 6 , which present from a top-view perspective of the lidar slam map result , the yellow point cloud denote the low elevation correspond to the floor , while the red point cloud represent the high elevation correspond to the ceiling . the central point cloud in the map capture reflection from exterior high-rise object encounter when traverse corridor window . after complete a circuit around the building , it be observe that the 3d reconstruct map of lidar slam present the architectural structure of the indoor environment in a comprehensive geometrically precise manner . however , when view locally , the map exhibit sparse detail and there be a lack of discernibility on planar features.appl . sci . 2024 ,14 , 3945 11 of 20 appl . sci . 2024 , 14 , x for peer review 11 of 21 4.1.2 . mapping 1 . lidar slam map analysis the robot trajectory be illustrate by the colored line be show in figure 6 , which present from a top-view perspective of the lidar slam map result , the yellow point cloud denote the low elevation correspond to the ﬂoor , while the red point cloud represent the high elevation corresp onding to the ceiling . the central point cloud in the map capture re ﬂections from exterior high-rise object encounter when traverse corridor window . after', 'represent the high elevation corresp onding to the ceiling . the central point cloud in the map capture re ﬂections from exterior high-rise object encounter when traverse corridor window . after completi ng a circuit around the building , it be observe that the 3d reconstruct map of lidar slam present the architectural structure of the indoor environment in a comprehensive geometrically precise manner . however , when view locally , the map exhibi t sparser detail and there be a lack of discernibility on planar feature . figure 6 . a top-view perspective of the lidar slam map result . the colored point cloud depict the robot ’ s pose . the white box represent speci ﬁc scenario : ( 1 . ) multiple corner . ( 2 . ) extended corridor . 2 . visual slam base on two map method figure 7 depicts local reconstruct map base on the ground truth or orb-slam3 within a depth range of 7 m , with the recons truction area and perspective correspond to the white ﬁeld of view ( fov ) trapezoid and arrow label ( 1 ) and ( 2 ) in figure 6 . ( a ) ( b ) figure 6 . a top-view perspective of the lidar slam map result . the colored point cloud depict the robot ’ s pose . the white box represent specific scenario : ( 1 . ) multiple corner . ( 2 . ) extended corridor . 2 . visual slam base on two map method figure 7 depicts local reconstruct map base on the ground truth or orb-slam3 within a depth range of 7 m , with the reconstruction area and perspective correspond to the white field of view ( fov ) trapezoid and arrow label ( 1 ) and ( 2 ) in figure 6 . appl . sci . 2024 , 14 , x for peer review 11 of 21 4.1.2 . mapping 1 . lidar slam map analysis the robot trajectory be illustrate by the colored line be show in figure 6 , which present from a top-view perspective of the lidar slam map result , the yellow point cloud denote the low elevation correspond to the ﬂoor , while the red point cloud represent the high elevation corresp onding to the ceiling . the central point cloud in the map capture re ﬂections from exterior high-rise object', 'correspond to the ﬂoor , while the red point cloud represent the high elevation corresp onding to the ceiling . the central point cloud in the map capture re ﬂections from exterior high-rise object encounter when traverse corridor window . after completi ng a circuit around the building , it be observe that the 3d reconstruct map of lidar slam present the architectural structure of the indoor environment in a comprehensive geometrically precise manner . however , when view locally , the map exhibi t sparser detail and there be a lack of discernibility on planar feature . figure 6 . a top-view perspective of the lidar slam map result . the colored point cloud depict the robot ’ s pose . the white box represent speci ﬁc scenario : ( 1 . ) multiple corner . ( 2 . ) extended corridor . 2 . visual slam base on two map method figure 7 depicts local reconstruct map base on the ground truth or orb-slam3 within a depth range of 7 m , with the recons truction area and perspective correspond to the white ﬁeld of view ( fov ) trapezoid and arrow label ( 1 ) and ( 2 ) in figure 6 . ( a ) ( b ) appl . sci . 2024 , 14 , x for peer review 12 of 21 ( c ) ( d ) figure 7 . local reconstruct indoor map within a depth range of 7 m. ( a ) the ﬁrst perspective ( 1 ) base on ground truth ; ( b ) the ﬁrst perspective ( 1 ) ba sed on orb-slam3 ; ( c ) the second perspective ( 2 ) base on ground truth ; ( d ) the second perspective ( 2 ) base on orb-slam3 . firstly , a comparison of th e 3d reconstruction effect between the two visual slam mapping approach be perform . in the first perspective , figure 7a , b , the reconstruction from orb-slam3 , exhibit multiple image overlap phenomenon on the fire hydrant on the wall , the door in the background , and the wind ow frame on the right wall . this result in distorted and shift object shape . in contrast , the reconstruction base d on the ground truth appear more detailed , continuous , and visually superior , closely align with the actual scene . moving to the second perspective within the', 'contrast , the reconstruction base d on the ground truth appear more detailed , continuous , and visually superior , closely align with the actual scene . moving to the second perspective within the indoor environment , figure 7c , d , the ﬁre extinguisher on the wall and it signag e similarly appear more blurred in the reconstruction from orb-slam3 , consistent with the issue observe in the previous location . this problem arise from the di ﬀerences in data source and algorithm between the orb-slam3 and cartographer model . or b-slam3 calculates camera pose base on feature point match for each image , lead to cumulative error in the z-axis direction due to robot vibration during move ment . this result in discontinuous height diﬀerences in the overlay of each frame during mapping . in contrast , the cartographer model utilize 3d lidar data with wheel odometry for calculation , impose ﬁxed constraint on the pose in the z-axis direction . this ensure consistency in height along the z-axis in the ground truth , result in a smooth overlay of each frame during mapping . 3 . comparison between lidar slam and visual slam in this section , we compare visual slam and lidar slam map base on three aspect . the ﬁrst aspect to be discuss be the ability to identify item . in figure 7 , the map construct by visual slam display more detailed and rich surface feature . for example , the ﬁre hydrant on the wall surface and th e color of surround object be reconstruct , enhance map identi ﬁability and closely resemble the real scene . in contrast , the lidar slam reconstruction only outline the structural aspect of the environmental architecture . the scan characteristic of lidar result in uniform point cloud on the same plane of corridor wall , ma king it challenge to distinguish feature and position of individual object in ﬁner detail . as a result , the visual discernibility be comparatively low in lidar slam . the second aspect be deficiency cause by in door lighting source . defects , such a a lack of point cloud , be', 'detail . as a result , the visual discernibility be comparatively low in lidar slam . the second aspect be deficiency cause by in door lighting source . defects , such a a lack of point cloud , be observable in the reconstruction base on visual slam on both the floor and ceiling . this be attribute to the inst allation of fluorescent light on the ceiling , cause reflection on the floor that introduc e error in depth measurement base on the figure 7 . local reconstruct indoor map within a depth range of 7 m. ( a ) the first perspective ( 1 ) base on ground truth ; ( b ) the first perspective ( 1 ) base on orb-slam3 ; ( c ) the second perspective ( 2 ) base on ground truth ; ( d ) the second perspective ( 2 ) base on orb-slam3.appl . sci . 2024 ,14 , 3945 12 of 20 firstly , a comparison of the 3d reconstruction effect between the two visual slam mapping approach be perform . in the first perspective , figure 7a , b , the reconstruction from orb-slam3 , exhibit multiple image overlap phenomenon on the fire hydrant on the wall , the door in the background , and the window frame on the right wall . this result in distorted and shift object shape . in contrast , the reconstruction base on the ground truth appear more detailed , continuous , and visually superior , closely align with the actual scene . moving to the second perspective within the indoor environment , figure 7c , d , the fire extinguisher on the wall and it signage similarly appear more blurred in the reconstruction from orb-slam3 , consistent with the issue observe in the previous location . this problem arise from the difference in data source and algorithm between the orb- slam3 and cartographer model . orb-slam3 calculates camera pose base on feature point match for each image , lead to cumulative error in the z-axis direction due to robot vibration during movement . this result in discontinuous height difference in the overlay of each frame during mapping . in contrast , the cartographer model utilize 3d lidar data with wheel odometry for', 'during movement . this result in discontinuous height difference in the overlay of each frame during mapping . in contrast , the cartographer model utilize 3d lidar data with wheel odometry for calculation , impose fix constraint on the pose in the z-axis direction . this ensure consistency in height along the z-axis in the ground truth , result in a smooth overlay of each frame during mapping . 3 . comparison between lidar slam and visual slam in this section , we compare visual slam and lidar slam map base on three aspect . the first aspect to be discuss be the ability to identify item . in figure 7 , the map construct by visual slam display more detailed and rich surface feature . for example , the fire hydrant on the wall surface and the color of surround object be reconstruct , enhance map identifiability and closely resemble the real scene . in contrast , the lidar slam reconstruction only outline the structural aspect of the environmental architecture . the scan characteristic of lidar result in uniform point cloud on the same plane of corridor wall , make it challenge to distinguish feature and position of individual object in finer detail . as a result , the visual discernibility be comparatively low in lidar slam . the second aspect be deficiency cause by indoor lighting source . defects , such a a lack of point cloud , be observable in the reconstruction base on visual slam on both the floor and ceiling . this be attribute to the installation of fluorescent light on the ceiling , cause reflection on the floor that introduce error in depth measurement base on the binocular disparity of zed2 . these error be subsequently mitigate through filter during the mapping process . conversely , the scanning nature of 3d lidar in lidar slam be unaffected by indoor lighting source and it do not experience the same issue . the third aspect be the blind spot of local map . due to the limited fov inherent in the camera ’ s image principle , certain part of the scene may not be capture , such a corner during turn at', '. the third aspect be the blind spot of local map . due to the limited fov inherent in the camera ’ s image principle , certain part of the scene may not be capture , such a corner during turn at intersection . this issue result in the absence of object around corner , a depict in figure 8 . however , in lidar slam , the 3d lidar use in this experiment provide a 360-degree scan of the environment , eliminate blind spot in the lidar reconstruction result . appl . sci . 2024 , 14 , x for peer review 13 of 21 binocular disparity of zed2 . these error be subsequently mitigate through filter during the mapping process . conversely , the sc anning nature of 3d lidar in lidar slam be unaffected by indoor lighting source and it do not experien ce the same issue . the third aspect be the blind spot of local map . due to the limited fov inherent in the camera ’ s image principle , certain part of the scene may not be capture , such a corner during turn at intersection . this issue result in the absence of object around corner , a depict in figure 8 . however , in lidar slam , the 3d lidar use in this experiment provide a 360-degree scan of the environment , eliminate blind spot in the lidar reconstruction result . figure 8 . blind spot in local mapping around corner . 4.2 . outdoor environment the outdoor datasets describe a rectangular pathway over the yard with a total length of over 40 m. the duration of the datasets be approximately 200 s with a stable robot speed . 4.2.1 . localization figure 9 display the ground truth of th e robot pose obtain from one of the outdoor datasets ( o4 ) . other datasets start at slightly di ﬀerent location but have similar journey . the result of the metric absolute trajectory error ( with se ( 3 ) umeyama alignment ) be show in table 2 . in outdoor environment , sc-lego-loam demonstrate be tter position accuracy , namely low error highlight in bold font , particularly in scenario with rich texture compare to indoor environment . moreover , sc-lego-loam also demonstrate small standard', 'tter position accuracy , namely low error highlight in bold font , particularly in scenario with rich texture compare to indoor environment . moreover , sc-lego-loam also demonstrate small standard deviation , indicate high stability for localization . figure 8 . blind spot in local mapping around corners.appl . sci . 2024 ,14 , 3945 13 of 20 4.2 . outdoor environment the outdoor datasets describe a rectangular pathway over the yard with a total length of over 40 m. the duration of the datasets be approximately 200 s with a stable robot speed . 4.2.1 . localization figure 9 display the ground truth of the robot pose obtain from one of the outdoor datasets ( o4 ) . other datasets start at slightly different location but have similar journey . the result of the metric absolute trajectory error ( with se ( 3 ) umeyama alignment ) be show in table 2 . in outdoor environment , sc-lego-loam demonstrate well posi- tioning accuracy , namely low error highlight in bold font , particularly in scenario with rich texture compare to indoor environment . moreover , sc-lego-loam also demonstrate small standard deviation , indicate high stability for localization . appl . sci . 2024 , 14 , x for peer review 14 of 21 figure 9 . the ground truth of the robot pose be obtain ed from one of the outdoor datasets ( o4 ) . table 2 . ate with se ( 3 ) umeyama a lignment in an outdoor environment ( unit : m ) . gmapping orb-slam3 sc-lego- loam gmapping orb-slam3 sc-lego- loam dataset o1 max 0.18999 0.62861 0.16018 dataset o4 max 0.27816 1.87328 0.16234 mean 0.13034 0.36844 0.08666 mean 0.131768 0.34227 0.05432 rmse 0.13735 0.39105 0.09413 rmse 0.14738 0.38218 0.06009 std 0.04333 0.13104 0.03674 std 0.06602 0.17004 0.02568 dataset o2 max 0.19983 0.62096 0.16714 dataset o5 max 0.23663 0.66725 0.17237 mean 0.10030 0.30119 0.06705 mean 0.10678 0.34068 0.06945 rmse 0.11021 0.32724 0.07239 rmse 0.11844 0.36635 0.07693 std 0.04567 0.12794 0.02728 std 0.05125 0.13473 0.03308 dataset o3 max 0.20131 0.78556 0.19174 average max 0.22118 0.91513', '0.34068 0.06945 rmse 0.11021 0.32724 0.07239 rmse 0.11844 0.36635 0.07693 std 0.04567 0.12794 0.02728 std 0.05125 0.13473 0.03308 dataset o3 max 0.20131 0.78556 0.19174 average max 0.22118 0.91513 0.17075 mean 0.09545 0.31753 0.07116 mean 0.11292 0.33402 0.06972 rmse 0.10189 0.34955 0.0776 rmse 0.12306 0.36327 0.07622 std 0.03564 0.14614 0.03095 std 0.04839 0.14198 0.03074 as show in figure 10 , we conduct a comparison between rpe in an outdoor environment ( o4 ) , reveal the discernible impact of loopback on orb-slam3 and sc- lego-loam , respectively . this further underscore the bene ﬁcial impact of loop closure detection in error elimination . however , unli ke the indoor environment discuss in the preceding section , the outdoor environmen t possess rich texture information . consequently , the role of loop closure in lidar slam be mitigate and the operator ’ s inﬂuence on the slam result be not a substantial . ( a ) ( b ) figure 9 . the ground truth of the robot pose be obtain from one of the outdoor datasets ( o4 ) . table 2 . ate with se ( 3 ) umeyama alignment in an outdoor environment ( unit : m ) . gmappingorb- slam3sc-lego- loamgmappingorb- slam3sc-lego- loam max 0.18999 0.62861 0.16018 max 0.27816 1.87328 0.16234 mean 0.13034 0.36844 0.08666 mean 0.131768 0.34227 0.05432 rmse 0.13735 0.39105 0.09413 rmse 0.14738 0.38218 0.06009dataset o1 std 0.04333 0.13104 0.03674dataset o4 std 0.06602 0.17004 0.02568 max 0.19983 0.62096 0.16714 max 0.23663 0.66725 0.17237 mean 0.10030 0.30119 0.06705 mean 0.10678 0.34068 0.06945 rmse 0.11021 0.32724 0.07239 rmse 0.11844 0.36635 0.07693dataset o2 std 0.04567 0.12794 0.02728dataset o5 std 0.05125 0.13473 0.03308 max 0.20131 0.78556 0.19174 max 0.22118 0.91513 0.17075 mean 0.09545 0.31753 0.07116 mean 0.11292 0.33402 0.06972 rmse 0.10189 0.34955 0.0776 rmse 0.12306 0.36327 0.07622dataset o3 std 0.03564 0.14614 0.03095average std 0.04839 0.14198 0.03074appl . sci . 2024 ,14 , 3945 14 of 20 as show in figure 10 , we conduct a comparison between rpe in an outdoor en-', '0.07622dataset o3 std 0.03564 0.14614 0.03095average std 0.04839 0.14198 0.03074appl . sci . 2024 ,14 , 3945 14 of 20 as show in figure 10 , we conduct a comparison between rpe in an outdoor en- vironment ( o4 ) , reveal the discernible impact of loopback on orb-slam3 and sc- lego-loam , respectively . this further underscore the beneficial impact of loop closure detection in error elimination . however , unlike the indoor environment discuss in the preceding section , the outdoor environment possess rich texture information . conse- quently , the role of loop closure in lidar slam be mitigate and the operator ’ s influence on the slam result be not a substantial . appl . sci . 2024 , 14 , x for peer review 14 of 21 figure 9 . the ground truth of the robot pose be obtain ed from one of the outdoor datasets ( o4 ) . table 2 . ate with se ( 3 ) umeyama a lignment in an outdoor environment ( unit : m ) . gmapping orb-slam3 sc-lego- loam gmapping orb-slam3 sc-lego- loam dataset o1 max 0.18999 0.62861 0.16018 dataset o4 max 0.27816 1.87328 0.16234 mean 0.13034 0.36844 0.08666 mean 0.131768 0.34227 0.05432 rmse 0.13735 0.39105 0.09413 rmse 0.14738 0.38218 0.06009 std 0.04333 0.13104 0.03674 std 0.06602 0.17004 0.02568 dataset o2 max 0.19983 0.62096 0.16714 dataset o5 max 0.23663 0.66725 0.17237 mean 0.10030 0.30119 0.06705 mean 0.10678 0.34068 0.06945 rmse 0.11021 0.32724 0.07239 rmse 0.11844 0.36635 0.07693 std 0.04567 0.12794 0.02728 std 0.05125 0.13473 0.03308 dataset o3 max 0.20131 0.78556 0.19174 average max 0.22118 0.91513 0.17075 mean 0.09545 0.31753 0.07116 mean 0.11292 0.33402 0.06972 rmse 0.10189 0.34955 0.0776 rmse 0.12306 0.36327 0.07622 std 0.03564 0.14614 0.03095 std 0.04839 0.14198 0.03074 as show in figure 10 , we conduct a comparison between rpe in an outdoor environment ( o4 ) , reveal the discernible impact of loopback on orb-slam3 and sc- lego-loam , respectively . this further underscore the bene ﬁcial impact of loop closure detection in error elimination . however , unli ke the indoor environment', 'of loopback on orb-slam3 and sc- lego-loam , respectively . this further underscore the bene ﬁcial impact of loop closure detection in error elimination . however , unli ke the indoor environment discuss in the preceding section , the outdoor environmen t possess rich texture information . consequently , the role of loop closure in lidar slam be mitigate and the operator ’ s inﬂuence on the slam result be not a substantial . ( a ) ( b ) appl . sci . 2024 , 14 , x for peer review 15 of 21 ( c ) ( d ) figure 10 . o4 rpe with regard to the translation part ( m ) for delta = 1 ( frame ) use consecutive pair ( with se ( 3 ) umeyama alignment , while process pan rotation and scale ) : ( a ) orb- slam3 with loop closure ; ( b ) sc-lego-loam with loop closure ; ( c ) orb-slam3 without loop closure ; and ( d ) sc-lego-loam with out loop closure . 4.2.2 . mapping 1 . lidar slam map analysis figure 11 show lidar slam mapping in di ﬀerent view and the real scene and the robot trajectory be represent a colored line s. figure 11a , b be present a top and side view of the reconstructed map , respectively . figure 11c provide the real scene of the outdoor environment . by circle around the cent ral platform during data collection , it be evident from figure 11a , b that the 3d lida r reconstruction faithfully capture the complex architectural structure surround the platform , include every beam , column , and the stair on either side . ( a ) figure 10 . o4 rpe with regard to the translation part ( m ) for delta = 1 ( frame ) use consecutive pair ( with se ( 3 ) umeyama alignment , while process pan rotation and scale ) : ( a ) orb-slam3 with loop closure ; ( b ) sc-lego-loam with loop closure ; ( c ) orb-slam3 without loop closure ; and ( d ) sc-lego-loam without loop closure . 4.2.2 . mapping 1 . lidar slam map analysis figure 11 show lidar slam mapping in different view and the real scene and the robot trajectory be represent a colored line . figure 11a , b be present a top and side view of the reconstructed map ,', 'figure 11 show lidar slam mapping in different view and the real scene and the robot trajectory be represent a colored line . figure 11a , b be present a top and side view of the reconstructed map , respectively . figure 11c provide the real scene of the outdoor environment . by circle around the central platform during data collection , it be evident from figure 11a , b that the 3d lidar reconstruction faithfully capture the complex architectural structure surround the platform , include every beam , column , and the stair on either side . appl . sci . 2024 , 14 , x for peer review 15 of 21 ( c ) ( d ) figure 10 . o4 rpe with regard to the translation part ( m ) for delta = 1 ( frame ) use consecutive pair ( with se ( 3 ) umeyama alignment , while process pan rotation and scale ) : ( a ) orb- slam3 with loop closure ; ( b ) sc-lego-loam with loop closure ; ( c ) orb-slam3 without loop closure ; and ( d ) sc-lego-loam with out loop closure . 4.2.2 . mapping 1 . lidar slam map analysis figure 11 show lidar slam mapping in di ﬀerent view and the real scene and the robot trajectory be represent a colored line s. figure 11a , b be present a top and side view of the reconstructed map , respectively . figure 11c provide the real scene of the outdoor environment . by circle around the cent ral platform during data collection , it be evident from figure 11a , b that the 3d lida r reconstruction faithfully capture the complex architectural structure surround the platform , include every beam , column , and the stair on either side . ( a ) figure 11 . cont .appl . sci . 2024 ,14 , 3945 15 of 20 appl . sci . 2024 , 14 , x for peer review 16 of 21 ( b ) ( c ) figure 11 . lidar slam outdoor mapping and the real scene . ( a ) top view of the reconstructed map ; ( b ) side view of the reconstructed map ; ( c ) the real scene . 2 . visual slam base on two map method figure 12 depicts local reconstruct map base on the ground truth or orb-slam3 within a depth range of 7 m , with the recons truction area and perspective correspond', 'visual slam base on two map method figure 12 depicts local reconstruct map base on the ground truth or orb-slam3 within a depth range of 7 m , with the recons truction area and perspective correspond to the white fov trapezoid and arrow label ( 1 ) and ( 2 ) in figure 11 . ( a ) ( b ) figure 11 . lidar slam outdoor mapping and the real scene . ( a ) top view of the reconstructed map ; ( b ) side view of the reconstructed map ; ( c ) the real scene . 2 . visual slam base on two map method figure 12 depicts local reconstruct map base on the ground truth or orb-slam3 within a depth range of 7 m , with the reconstruction area and perspective correspond to the white fov trapezoid and arrow label ( 1 ) and ( 2 ) in figure 11 . appl . sci . 2024 , 14 , x for peer review 16 of 21 ( b ) ( c ) figure 11 . lidar slam outdoor mapping and the real scene . ( a ) top view of the reconstructed map ; ( b ) side view of the reconstructed map ; ( c ) the real scene . 2 . visual slam base on two map method figure 12 depicts local reconstruct map base on the ground truth or orb-slam3 within a depth range of 7 m , with the recons truction area and perspective correspond to the white fov trapezoid and arrow label ( 1 ) and ( 2 ) in figure 11 . ( a ) ( b ) figure 12 . cont .appl . sci . 2024 ,14 , 3945 16 of 20 appl . sci . 2024 , 14 , x for peer review 17 of 21 ( c ) ( d ) figure 12 . local reconstruct outdoor map within a depth range of 7 m. ( a ) the ﬁrst perspective ( 1 ) base on ground truth ; ( b ) the ﬁrst perspective ( 1 ) base on orb-slam3 ; ( c ) the second perspective ( 2 ) base on ground truth ; ( d ) the second perspective ( 2 ) base on orb-slam3 . in the ﬁrst perspective , the reconstruction from orb-slam3 , figure 12b , exhibit phenomenon of multiple image overlap on the beam of the building , outdoor air condition unit , and pa tterns on the ﬂoor . this result in a blurry appearance of object and the ﬂoor pattern appear distort a a consequence . however , the reconstruction base on ground truth , figure 12a , while show', 'pa tterns on the ﬂoor . this result in a blurry appearance of object and the ﬂoor pattern appear distort a a consequence . however , the reconstruction base on ground truth , figure 12a , while show slight distortion in some subtle area , maintain an overall ﬁner and more continuous e ﬀect . moving to the second perspective , figure 12c , d , similar issue be observe in the reconstruction from orb-slam3 , include blur ﬂoor pattern and double-image displacement in the architectural structure . the reason for these problem be consistent with the indoor condition discuss in the previous section and will not be reiterate here . 3 . comparison between lidar slam and visual slam in this section , a comparison between vi sual slam and lidar slam be conduct . by examine the visual slam-based reconstruction map , figure 13a , and the lidar slam-based reconstruction map , figure 13b , we discuss the following three point sequentially . ( a ) ( b ) figure 13 . ( a ) visual slam base on ground truth and ( b ) lidar slam base on 3d lidar . the colored point cloud depict the robot ’ s pose . the white box represent speci ﬁc scenario : ( 1 . ) reconstruction with loop . ( 2 . ) reconstruction without loop . figure 12 . local reconstruct outdoor map within a depth range of 7 m. ( a ) the first perspective ( 1 ) base on ground truth ; ( b ) the first perspective ( 1 ) base on orb-slam3 ; ( c ) the second perspective ( 2 ) base on ground truth ; ( d ) the second perspective ( 2 ) base on orb-slam3 . in the first perspective , the reconstruction from orb-slam3 , figure 12b , exhibit phe- nomena of multiple image overlap on the beam of the building , outdoor air condition unit , and pattern on the floor . this result in a blurry appearance of object and the floor pattern appear distort a a consequence . however , the reconstruction base on ground truth , figure 12a , while show slight distortion in some subtle area , maintain an overall finer and more continuous effect . moving to the second perspective , figure 12c , d , similar', 'on ground truth , figure 12a , while show slight distortion in some subtle area , maintain an overall finer and more continuous effect . moving to the second perspective , figure 12c , d , similar issue be observe in the reconstruction from orb-slam3 , include blurred floor pattern and double-image displacement in the architectural structure . the reason for these problem be consistent with the indoor condition discuss in the previous section and will not be reiterate here . 3 . comparison between lidar slam and visual slam in this section , a comparison between visual slam and lidar slam be conduct . by examine the visual slam-based reconstruction map , figure 13a , and the lidar slam- base reconstruction map , figure 13b , we discuss the following three point sequentially . appl . sci . 2024 , 14 , x for peer review 17 of 21 ( c ) ( d ) figure 12 . local reconstruct outdoor map within a depth range of 7 m. ( a ) the ﬁrst perspective ( 1 ) base on ground truth ; ( b ) the ﬁrst perspective ( 1 ) base on orb-slam3 ; ( c ) the second perspective ( 2 ) base on ground truth ; ( d ) the second perspective ( 2 ) base on orb-slam3 . in the ﬁrst perspective , the reconstruction from orb-slam3 , figure 12b , exhibit phenomenon of multiple image overlap on the beam of the building , outdoor air condition unit , and pa tterns on the ﬂoor . this result in a blurry appearance of object and the ﬂoor pattern appear distort a a consequence . however , the reconstruction base on ground truth , figure 12a , while show slight distortion in some subtle area , maintain an overall ﬁner and more continuous e ﬀect . moving to the second perspective , figure 12c , d , similar issue be observe in the reconstruction from orb-slam3 , include blur ﬂoor pattern and double-image displacement in the architectural structure . the reason for these problem be consistent with the indoor condition discuss in the previous section and will not be reiterate here . 3 . comparison between lidar slam and visual slam in this section , a comparison between', 'be consistent with the indoor condition discuss in the previous section and will not be reiterate here . 3 . comparison between lidar slam and visual slam in this section , a comparison between vi sual slam and lidar slam be conduct . by examine the visual slam-based reconstruction map , figure 13a , and the lidar slam-based reconstruction map , figure 13b , we discuss the following three point sequentially . ( a ) ( b ) figure 13 . ( a ) visual slam base on ground truth and ( b ) lidar slam base on 3d lidar . the colored point cloud depict the robot ’ s pose . the white box represent speci ﬁc scenario : ( 1 . ) reconstruction with loop . ( 2 . ) reconstruction without loop . figure 13 . ( a ) visual slam base on ground truth and ( b ) lidar slam base on 3d lidar . the colored point cloud depict the robot ’ s pose . the white box represent specific scenario : ( 1 . ) reconstruction with loop . ( 2 . ) reconstruction without loop.appl . sci . 2024 ,14 , 3945 17 of 20 the first aspect be the difference in the size of the reconstructed area . from the result of both mapping approach , under the same robot trajectory , the global map reconstruct by visual slam cover only a portion of the lidar slam global map . this difference arise from the distinct maximum measurement distance provide by the two sensor . in hardware specification , the optimal measurement depth for the camera be approximately 20 m and exceed this range introduce large error . therefore , in visual slam reconstruction , the mapping range be set to pixel within a depth of 7 m to ensure more accurate mapping . in contrast , the 3d lidar use in this experiment have a maximum scanning range of up to 150 m , result in a broad map coverage by lidar slam while maintain precision in distant area . the second aspect be object recognizability . in outdoor scene , while lidar slam provide a more comprehensive environmental structure and outline , it lack the capability to recognize local object and feature . this limitation , similar to that in an indoor', ', while lidar slam provide a more comprehensive environmental structure and outline , it lack the capability to recognize local object and feature . this limitation , similar to that in an indoor environ- ment , arise from the scan nature of lidar , where point cloud on the same plane exhibit no height difference , make it challenge to distinguish feature between each point cloud . consequently , lidar slam can not recognize object as effectively a visual slam , which utilize complete color and feature shape for object identification . the third aspect be about defect and blind spot in local map . in the mapping analysis of visual slam , we observe the issue of multiple image overlap cause by vibration . additionally , a in indoor mapping , the limited field of view ( fov ) result in some local environment that can not be capture , create blind spot in the mapping result . however , in lidar slam , the blind spot issue be also eliminate by the usage of a 360-degree 3d lidar for the reconstruction . moreover , the sc-lego-loam algorithm use in lidar slam leverage feature set match between timestamps for pose estimation and it detection range be broad . therefore , the impact of vibration on map effectiveness be milder in lidar slam , with no apparent ghost phenomenon . 4.3 . computational resource analysis to quantitatively demonstrate the computational resource requirement of the two slam method , we simultaneously record the cpu resource allocation during the map creation process . in this study , we run 10 datasets for the nondeterministic nature of the system and show average result for the cpu utilization under various type of task and memory usage increase ratio in table 3 . “ user ” be responsible for time spend by normal process execute in user mod [ 28 ] . “ system ” show time spend by execute process in kernel mode . and “ idle ” be the portion of cpu time during which the processor be inactive . in addition to these three main state , there be other type of cpu usage ; therefore , these three type will', 'mode . and “ idle ” be the portion of cpu time during which the processor be inactive . in addition to these three main state , there be other type of cpu usage ; therefore , these three type will not add up to 100 % . moreover , “ memory ” be the temporary storage for active process . finally , the increase ratio be compute by subtract the normal memory usage occupancy rate from the active occupancy rate and divide the result by the normal memory usage occupancy rate . according to table 3 , we conduct comparison from three different perspective and the result be present in table 4 . firstly , we compare the cpu utilization statistic of the two slam method indoors and outdoors ( scenario 1 ) . in indoor environment , visual slam occupy 51.25 % more cpu compare to lidar slam , while outdoors , it occupy 35.02 % more than lidar slam . this result confirm that visual slam have high computational requirement than lidar slam . the reason behind this lie in the fact that the map construct by visual slam store more information . next , we compare each cpu utilization of the two slam method indoors and outdoors with the original cpu utilization without run any program ( scenario 2 ) . the cpu utilization of visual slam during the execution of indoor and outdoor datasets be 77.80 % and 70.67 % high compare to the original usage , respectively . on the other hand , for lidar slam , it be 17.55 % and 26.41 % high than the original usage , respectively . the relatively minor increase in indoor cpu utilization may be due to the simpler structure of indoor wall , result in a low point cloud volume and low density for mapping.appl . sci . 2024 ,14 , 3945 18 of 20 table 3 . average result for the cpu utilization under various type of task and memory usage increase ratio * ( unit : % ) . cpu utilization memory usage user sys idle occupancy rate increasing ratio org 14.93 3.03 73.08 27.20 na orb_i 26.54 3.13 60.83 34.20 22.18 orb_o 25.48 3.20 61.80 33.69 12.12 sc_i 17.55 3.08 70.25 31.49 2.40 sc_o 18.87 3.32 68.45 30.42 2.68 *', 'occupancy rate increasing ratio org 14.93 3.03 73.08 27.20 na orb_i 26.54 3.13 60.83 34.20 22.18 orb_o 25.48 3.20 61.80 33.69 12.12 sc_i 17.55 3.08 70.25 31.49 2.40 sc_o 18.87 3.32 68.45 30.42 2.68 * note that the notation ‘ org ’ denote the average cpu usage in user without slam algorithm run . ‘ orb_i ’ , ‘ orb_o ’ , ‘ sc_i ’ , and ‘ sc_o ’ mean the average amount of cpu time spend in user mode indoors use the orb algorithm , outdoors use the orb algorithm , indoors use sc-the lego-loam algorithm , and outdoors use sc-lego-loam algorithm , respectively . table 4 . three comparison among different perspective * ( unit : % ) . scenario 1 scenario 2 scenario 3 in terms of cpu usage in userin terms of cpu usage in userin terms of memory increasing ratio orb_i vs. sc_i 51.25 orb_i vs. org 77.80 orb_i 22.18 orb_o vs. sc_o 35.02 orb_o vs. org 70.67 orb_o 12.12 sc_i vs. org 17.55 sc_i 2.40 sc_o vs. org 26.41 sc_o 2.68 * the comparison of a to b refers to the latter , b . the notation carry the same meaning a those in table 3 . finally , due to the indoor dataset recording time be approximately 350 to 450 s and the outdoor dataset recording time be around 200 s , we compare the memory usage in- crease ratio of the two slam method indoors relative to outdoors ( scenario 3 ) . it can be observe that the memory usage increase ratio of visual slam indoors be approximately twice that of outdoors , consistent with the doubled dataset recording time . however , in lidar slam , the memory usage increase ratio outdoors be slightly high than indoors , possibly due to the large scanning range and point cloud volume in outdoor environment . 5 . conclusions this study undertake a thorough assessment of slam algorithm in both indoor and outdoor environment , leverage state-of-the-art technique . two pivotal slam algorithm , namely orb-slam3 and sc-lego-loam , be utilized , along with the ground truth creation method cartographer . additionally , two 3d reconstruction method employ rgb-d and lidar point cloud be employ . the', 'orb-slam3 and sc-lego-loam , be utilized , along with the ground truth creation method cartographer . additionally , two 3d reconstruction method employ rgb-d and lidar point cloud be employ . the experiment entail the collection of real-world datasets across varied setting , with a focus on analyze the algorithm ’ efficacy in term of localization , mapping , and computational resource demand . the ensuing finding be derive from the outcome : • in term of localization performance , orb-slam3 exhibit superior position accu- racy in indoor environment , while sc-lego-loam excels in outdoor setting . this distinction arise from the nature of slam algorithm , which benefit from environ- mental intricacy , make it easy for the robot to calculate and match feature ; • in term of mapping capability , the effectiveness of mapping be closely tie to the sen- sor ’ s characteristic and each have it unique drawback . for instance , slam map use depth camera may suffer from imperfection and blind spot due to indoor lighting source and limitation on fov , respectively . three-dimensional lidar-basedappl . sci . 2024 ,14 , 3945 19 of 20 mapping may lack the level of detail and textural information require for precise ob- ject recognition . nevertheless , in accordance with the distinct mapping spatial scope between indoor and outdoor environment , lidar slam be more suitable in an out- door environment due to it comprehensive characteristic . in contrast , visual slam be more suitable in an indoor environment due to it recognizability characteristic and ability to compensate for the sparse aspects inherent in lidar slam ; • regarding performance evaluation , quantitative analysis of computational resource requirement indicate that visual slam demand more cpu resource compare to lidar slam , attribute this to the storage of additional depth and color data . the study also explore cpu utilization and memory usage increase ratio , emphasize the impact of environmental factor on resource demand . as part of future work , expand', 'depth and color data . the study also explore cpu utilization and memory usage increase ratio , emphasize the impact of environmental factor on resource demand . as part of future work , expand the datasets to encompass indoor and outdoor environment , more complex robotic experiment scenario , the ability to identify object , and longer record time will allow for a comprehensive comparison of slam perfor- mance in these diverse situation . at the same time , the use of more sensor should also be consider to test the performance of more sensor fusion algorithm . author contributions : conceptualization , y.-l.z . and h.-p .h . ; funding acquisition , h.-p .h . ; method- ology , y.-l.z . and y.-t.h . ; project administration , h.-p .h . ; resources , h.-p .h . ; software , y.-l.z . ; super- vision , h.-p .h . ; validation , y.-l.z . and y.-t.h . ; visualization , y.-l.z . and y.-t.h . ; writing—original draft , y.-l.z . and y.-t.h . ; writing—review and edit , y.-l.z. , y.-t.h . and h.-p .h . all author have read and agree to the publish version of the manuscript . funding : this research be fund in part by the national science and technology council , taiwan , under grant most 112-2221-e-002-149-my3 . institutional review board statement : not applicable . informed consent statement : not applicable . data availability statement : the original contribution present in the study be include in the article , further inquiry can be direct to the corresponding author . conflicts of interest : the author declare no conflict of interest . references 1 . zang , q. ; zhang , k. ; wang , l. ; wu , l. an adaptive orb-slam3 system for outdoor dynamic environments . sensors 2023 ,23 , 1359 . [ crossref ] [ pubmed ] 2 . hong , y.-t. ; huang , h.-p . a comparison of outdoor 3d reconstruction between visual slam and lidar slam . in proceedings of the 2023 international automatic control conference ( cacs ) , penghu , taiwan , 26–29 october 2023 ; pp . 1–6 . [ crossref ] 3 . campos , c. ; elvira , r. ; rodr íguez , j.j.g . ;', 'in proceedings of the 2023 international automatic control conference ( cacs ) , penghu , taiwan , 26–29 october 2023 ; pp . 1–6 . [ crossref ] 3 . campos , c. ; elvira , r. ; rodr íguez , j.j.g . ; montiel , j.m.m . ; tard ós , j.d . orb-slam3 : an accurate open-source library for visual , visual–inertial , and multimap slam . ieee trans . robot . 2021 ,37 , 1874–1890 . [ crossref ] 4 . xue , g. ; wei , j. ; li , r. ; cheng , j. lego-loam-sc : an improved simultaneous localization and mapping method fusing lego-loam and scan context for underground coalmine . sensors 2022 ,22 , 520 . [ crossref ] [ pubmed ] 5 . mur-artal , r. ; tard ós , j.d . orb-slam2 : an open-source slam system for monocular , stereo , and rgb-d cameras . ieee trans . robot . 2017 ,33 , 1255–1262 . [ crossref ] 6 . mur-artal , r. ; tard ós , j.d . visual-inertial monocular slam with map reuse . ieee robot . autom . lett . 2017 ,2 , 796–803 . [ crossref ] 7 . grisetti , g. ; stachniss , c. ; burgard , w. improved techniques for grid mapping with rao-blackwellized particle filters . ieee trans . robot . 2007 ,23 , 34 . [ crossref ] 8 . wang , c.-c. ; thorpe , c. ; thrun , s. ; hebert , m. ; durrant-whyte , h. simultaneous localization , mapping and moving object tracking . int . j . robot . res . 2007 ,26 , 889–916 . [ crossref ] 9 . bailey , t. ; nieto , j. ; guivant , j. ; stevens , m. ; nebot , e. consistency of the ekf-slam algorithm . in proceedings of the 2006 ieee/rsj international conference on intelligent robots and systems , beijing , china , 9–15 october 2006 ; pp . 3562–3568 . [ crossref ] 10 . zhang , j. ; singh , s. loam : lidar odometry and mapping in real-time . robot . sci . syst . 2014 ,2 , 9 . [ crossref ] 11 . kim , g. ; kim , a. scan context : egocentric spatial descriptor for place recognition within 3d point cloud map . in proceedings of the 2018 ieee/rsj international conference on intelligent robots and systems ( iros ) , madrid , spain , 1–5 october 2018 ; pp . 4802–4809 . [ crossref ] appl . sci . 2024 ,14 , 3945 20', 'of the 2018 ieee/rsj international conference on intelligent robots and systems ( iros ) , madrid , spain , 1–5 october 2018 ; pp . 4802–4809 . [ crossref ] appl . sci . 2024 ,14 , 3945 20 of 20 12 . izadi , s. ; kim , d. ; hilliges , o. ; molyneaux , d. ; newcombe , r. ; kohli , p . ; shotton , j. ; hodges , s. ; freeman , d. ; davison , a. ; et al . kinectfusion : real-time 3d reconstruction and interaction use a move depth camera . in proceedings of the 24th annual acm symposium on user interface software and technology , santa barbara , ca , usa , 16–19 october 2011 ; pp . 559–568 . [ crossref ] 13 . yarovoi , a. ; cho , y.k . review of simultaneous localization and mapping ( slam ) for construction robotics application . autom . constr . 2024 ,162 , 105344 . [ crossref ] 14 . schubert , d. ; goll , t. ; demmel , n. ; usenko , v . ; stückler , j. ; cremers , d. the tum vi benchmark for evaluating visual-inertial odometry . in proceedings of the 2018 ieee/rsj international conference on intelligent robots and systems ( iros ) , madrid , spain , 1–5 october 2018 ; pp . 1680–1687 . [ crossref ] 15 . sturm , j. ; engelhard , n. ; endres , f. ; burgard , w. ; cremers , d. a benchmark for the evaluation of rgb-d slam system . in proceedings of the 2012 ieee/rsj international conference on intelligent robots and systems , vilamoura-algarve , portugal , 7–12 october 2012 ; pp . 573–580 . [ crossref ] 16 . geiger , a. ; lenz , p . ; stiller , c. ; urtasun , r. vision meet robotics : the kitti dataset . int . j . robot . res . 2013 ,32 , 1231–1237 . [ crossref ] 17 . langley , r.b . rtk gps . gps world 1998 ,9 , 70–76 . 18 . ye , w. ; zhao , y. ; vela , p . characterizing slam benchmarks and methods for the robust perception age . arxiv 2019 , arxiv:1905.07808 . 19 . forster , c. ; zhang , z. ; gassner , m. ; werlberger , m. ; scaramuzza , d. svo : semidirect visual odometry for monocular and multicamera systems . ieee trans . robot . 2017 ,33 , 249–265 . [ crossref ] 20 . mur-artal , r. ; montiel , j.m.m . ; tard ós', 'm. ; scaramuzza , d. svo : semidirect visual odometry for monocular and multicamera systems . ieee trans . robot . 2017 ,33 , 249–265 . [ crossref ] 20 . mur-artal , r. ; montiel , j.m.m . ; tard ós , j.d . orb-slam : a versatile and accurate monocular slam system . ieee trans . robot . 2015 ,31 , 1147–1163 . [ crossref ] 21 . shan , t. ; englot , b. lego-loam : lightweight and ground-optimized lidar odometry and mapping on variable terrain . in proceedings of the 2018 ieee/rsj international conference on intelligent robots and systems ( iros ) , madrid , spain , 1–5 october 2018 ; pp . 4758–4765 . [ crossref ] 22 . segal , a. ; haehnel , d. ; thrun , s. generalized-icp . robot . sci . syst . 2009 ,2 , 435 . [ crossref ] 23 . hess , w. ; kohler , d. ; rapp , h. ; andor , d. real-time loop closure in 2d lidar slam . in proceedings of the 2016 ieee international conference on robotics and automation ( icra ) , stockholm , sweden , 16–21 may 2016 ; pp . 1271–1278 . [ crossref ] 24 . agarwal , s. ; mierle , k. ceres solver : tutorial & reference . available online : http : //ceres-solver.org/ ( access on 1 december 2023 ) . 25 . zhao , y.l . ; huang , h.p . ; chen , t.l . ; chiang , p .c . ; chen , y.h . ; yeh , j.h . ; huang , c.h . ; lin , j.f . ; weng , w.t . a smart sterilization robot system with chlorine dioxide for spray disinfection . ieee sens . j . 2021 ,21 , 22047–22057 . [ crossref ] 26 . grupp , m. evo : python package for the evaluation of odometry and slam . available online : http : //github.com/ michaelgrupp/evo ( access on 1 december 2023 ) . 27 . umeyama , s. least-squares estimation of transformation parameter between two point pattern . ieee trans . pattern anal . mach . intell . 1991 ,13 , 376–380 . [ crossref ] 28 . rodola , g. psutil on pypi.org . available online : http : //pypi.org/project/psutil ( access on 1 december 2023 ) . disclaimer/publisher ’ s note : the statement , opinion and data contain in all publication be solely those of the individual author ( s ) and contributor ( s ) and', '( access on 1 december 2023 ) . disclaimer/publisher ’ s note : the statement , opinion and data contain in all publication be solely those of the individual author ( s ) and contributor ( s ) and not of mdpi and/or the editor ( s ) . mdpi and/or the editor ( s ) disclaim responsibility for any injury to people or property result from any idea , method , instruction or product refer to in the content .']",https://doi.org/10.48550/arXiv.1607.02565
1.pdf,"see discussion , st at , and author pr ofiles f or this public ation at : http : //www .researchgate.ne t/public ation/358523574 a comprehensive su rvey of visual slam algorithms article in robotics · februar y 2022 doi : 10.3390/r obotics11010024 citations 266reads 7,176 5 author s , include : andr éa mac ario barr os atomic ener gy and alt ernativ e ener gi commission 6 publica tions 276 citations see profile yoann moline atomic ener gy and alt ernativ e ener gi commission 21 publica tions 310 citations see profile frédérick carr el atomic ener gy and alt ernativ e ener gi commission 139 publica tions 1,499 citations see profile maug an michel atomic ener gy and alt ernativ e ener gi commission 20 publica tions 348 citations see profile all c ontent f ollo wing this p age be uplo aded b y andr éa mac ario barr o on 11 f ebruar y 2022 . the user have r equest ed enhanc ement of the do wnlo aded file./gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045 /gid00001 /gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046citation : macario barros , a. ; michel , m. ; moline , y. ; corre , g. ; carrel , f. a comprehensive survey of visual slam algorithms . robotics 2022 ,11 , 24. http : //doi.org/10.3390/ robotics11010024 academic editor : dario richiedei received : 12 december 2021 accepted : 7 february 2022 published : 10 february 2022 publisher ’ s note : mdpi stay neutral with regard to jurisdictional claim in publish map and institutional afﬁl- iations . copyright : © 2022 by the author . licensee mdpi , basel , switzerland . this article be an open access article distribute under the term and condition of the creative commons attribution ( cc by ) license ( http : // creativecommons.org/licenses/by/ 4.0/ ) . robotics review a comprehensive survey of visual slam algorithms andréa macario barros * , maugan michel , yoann moline , gwenolé corre and frédérick carrel laboratoire capteurs et architectures électroniques ( lcae ) , laboratoire d ’ intégration des systèmes et de technologies ( list ) , commissariat à l ’ énergie atomique et aux énergies alternatives ( cea ) , 91400 saclay , france ; maugan.michel @ cea.fr ( m.m . ) ; yoann.moline @ cea.fr ( y.m . ) ; gwenole.corre @ cea.fr ( g.c . ) ; frederick.carrel @ cea.fr ( f.c . ) *correspondence : andrea.barros @ cea.fr ; tel . : +33-1-69-08-22-59 abstract : simultaneous localization and mapping ( slam ) technique be widely research , since they allow the simultaneous creation of a map and the sensor ’ pose estimation in an unknown environment . visual-based slam technique play a signiﬁcant role in this ﬁeld , a they be base on a low-cost and small sensor system , which guarantee those advantage compare to other sensor- base slam technique . the literature present different approach and method to implement visual-based slam system . among this variety of publication , a beginner in this domain may ﬁnd problem with identify and analyze the main algorithm and select the most appropriate one accord to his or her project constraint . therefore , we present the three main visual-based slam approach ( visual-only , visual-inertial , and rgb-d slam ) , provide a review of the main algorithm of each approach through diagram and ﬂowcharts , and highlight the main advantage and disadvantage of each technique . furthermore , we propose six criterion that ease the slam algorithm ’ s analysis and consider both the software and hardware level . in addition , we present some major issue and future direction on visual-slam ﬁeld , and provide a general overview of some of the exist benchmark datasets . this work aim to be the ﬁrst step for those initiate a slam project to have a good perspective of slam technique ’ main element and characteristic . keywords : embed slam ; evaluation criterion ; rgb-d slam ; visual-inertial slam ; visual-slam ; 3d reconstruction 1 . introduction simultaneous localization and mapping ( slam ) technology , ﬁrst propose by smith in 1986 [ 1 ] , be use in an extensive range of application , especially in the domain of augmented reality ( ar ) [ 2–4 ] and robotics [ 5–7 ] . the slam process aim at map an unknown environment and simultaneously locate a sensor system in this environment through the signal provide by the sensor ( s ) . in robotics , the construction of a map be a crucial task , since it allow the visualization of landmark , facilitate the environment ’ s visualization . in addition , it can help in the state estimation of the robot , relocate it , and decrease estimation error when re-visiting register area [ 8 ] . the map construction come with two other task : localization and path planning . according to stachniss [ 9 ] , the mapping problem may be describe by examine three question consider the robot ’ s perspective : what do the world look like ? where be i ? and how can i reach a give location ? the ﬁrst question be clariﬁed by the mapping task , which search to construct a map , i.e. , a model of the environment . to do so , it require the location of the observed landmark , i.e. , the answer for the second question , provide by the localization task . the localization task search to determine the robot ’ s pose , i.e. , it orientation and position and , consequently , locate the robot on the map . depending on the ﬁrst two task , the path planning clear up the last question , and seek to estimate a trajectory for the robot to achieve a give location . it rely on the current robot ’ s pose , provide by the localization task , and on the environment ’ s characteristic , provide by the mapping task . slam be a solution that integrate both the mapping and localization task . robotics 2022 ,11 , 24. http : //doi.org/10.3390/robotics11010024 http : //www.mdpi.com/journal/roboticsrobotics 2022 ,11 , 24 2 of 27 visual-based slam algorithm can be consider especially attractive , due to their sensor conﬁguration simplicity , miniaturize size , and low cost . therefore , numerous visual-based technique be propose in the literature , which make the choice of the most suitable one accord to one ’ s project constraint difﬁcult . the visual-based approach can be divide into three main category : visual-only slam , visual-inertial ( vi ) slam , and rgb-d slam . the ﬁrst one refers to the slam technique base only on 2d image provide by a monocular or stereo camera . they present a high technical difﬁculty due to their limited visual input [ 10 ] . the robustness in the sensor ’ s track of the visual-slam algorithm may be increase by add an inertial measurement unit ( imu ) , which can be find in their miniaturize size and low cost , while achieve high accuracy , essential aspect to many application that require lightweight design , such a autonomous race car [ 11 ] . in addition , visual-based slam system may employ a depth sensor and process the depth information by apply a rgb-d-based approach . to obtain a general overview and an introduction to the slam problem , the work by durrant-white and bailey [ 12,13 ] propose a slam tutorial presenting from the problem description to the environment representation . in addition , cadena et al . [ 8 ] analyze the main open problem and future perspective of the slam . considering the review and survey of visual-based technique , yousif et al . [ 14 ] and fuentes-pacheco et al . [ 15 ] present an overview of the main concept use in the visual-only slam technique and the fundamental algorithm . yousif et al . [ 14 ] also brieﬂy describe the rgb-d-based slam problem . taketomi et al . [ 10 ] and covolan et al . [ 3 ] present an overview of the main concept use in the visual-based slam approach , focus on visual-only and rgb-d-based approach and describe the main algorithm . the recent publication by servieres et al . [ 16 ] propose a classiﬁcation of the main visual-based slam algorithm and perform a historical research . gui et al . [ 17 ] and chen et al . [ 18 ] present the main concept and algorithm of visual- inertial slam and visual-inertial odometry approach , consider the ﬁltering-based and optimization-based perspective . in [ 17 ] , the technique up to 2015 be present , while in [ 18 ] , the algorithm up to 2018 be also include . an overview of the main concept and technique in visual-inertial navigation can also be find in [ 19 ] . concerning the rgb-d approach , chen et al . [ 20 ] present a global perspective from the main concept use in rgb-d modeling . a recent survey by zhang et al . [ 21 ] present an overview of the main concept and describe the principal rgb-d-based slam algorithm . as one can see , there be several review and survey in the literature about visual-based slam technique ; still , most of them be limited to just one or two of the three main approach and do not address the algorithm in detail . so , a review that address the three approach and the fundamental algorithm be necessary to help researcher and student to initiate their work on visual-slam technique to obtain an overview of this large domain . thus , this paper provide a review of the most representative visual-based slam technique and an overview of each method ’ s main advantage and disadvantage . this article present three main contribution : 1—an explanation of the most representative visual-based slam algorithm through the construction of diagram and ﬂowcharts . this approach will be helpful to the reader , a it provide an overview of the slam technique when initiate a project and allow the reader to have a ﬁrst contact with the visual-based slam algorithm . 2—as far a we know , this be the ﬁrst review article that present the three main visual-based approach , perform an individual analysis of each method and a general analysis of the approach . 3—focusing on the reader initiate their study on the slam algorithm , we propose six main criterion to be observe in the different technique and implementation to be consider accord to one ’ s application . the requirement consider , from a software level , slam technique , such a loop closure , to a hardware-level approach , such a slam on soc implementation . this paper be organize a follow . section 2 present the main essential concept of the three select approach . section 3 present the six criterion that we establish to evaluate the slam algorithm and the most representative slam technique accordingrobotics 2022 ,11 , 24 3 of 27 to the present approach . section 4 present some of the recent major issue face by the visual-slam community and point out future direction to deal with these problem . section 5 provide a general overview of some of the most signiﬁcant publicly available benchmark datasets . finally , our conclusion be present in section 6 . 2 . visual-based slam concepts this section present concept relate to visual-based slam and odometry algo- rithms , and the main characteristic of the visual-based approach cover in this paper . the visual-based slam technique use one or more camera in the sensor system , receive 2d image a the source of information . in general , the visual-based slam algorithm be divide into three main thread : initialization , track , and map [ 10 ] . figure 1 show a general view of the three main part generally present in visual-based slam approach . initializationtracking + mapping input data determination of the global coordinate system dense reconstructionsparse reconstruction semi-dense reconstruction2d image 2d image + imu data 2d image + depth data+ + roll pitch yaw figure 1 . general component of a visual-based slam . the depth and inertial data may be add to the 2d visual input to generate a sparse map ( generate with the orb-slam3 algorithm [ 22 ] in the mh_01 sequence [ 23 ] ) , semi-dense map ( obtain with the lsd-slam [ 24 ] in the dataset provide by the author ) , and a dense reconstruction ( reprinted from [ 25 ] ) . as one can see in the figure , in visual-slam system , the input can be a 2d image , both a 2d image and imu data , or a 2d image and depth data , depend on the used approach , i.e. , visual-only ( section 2.1 ) , visual-inertial ( section 2.2 ) , or rgb-d-based ( section 2.3 ) , respectively . the initialization determine the global coordinate and build an initial map , use to perform the two main step : tracking and mapping . the track process be responsible for the continuous estimation of the sensor ’ s pose . in general , the algorithm establish 2d–3d correspondence between the current frame and map , constitute a problem call perspective-n-points . there be several way to solve this problem , epnp be one of the most representative solution [ 26 ] . the mapping process be in charge of compute and expand the 3d structure a the camera move . the depth data computa- tion differs accord to the employ algorithm ( section 3 address individually each algorithm provide detailed explanation ) . finally , the mapping process shall result in a sparse , semi-dense , or dense 3d reconstruction , accord to the implement technique . although we mainly refer to the concept a belonging to the slam methodology , we consider , in this paper , both visual-slam and visual-odometry ( vo ) technique , since they be closely relate . the vo algorithm also seek to estimate a robot ’ s position through camera a a source of information . the main difference between visual-slam and vo lie in consider , or not , the global consistency of the estimate trajectory and map [ 14 ] .robotics 2022 ,11 , 24 4 of 27 while vo perform only local optimization , visual-slam algorithm also employ loop closure detection ( see section 3 ) , be capable of correct drift accumulate at the end of the robot ’ s trajectory . 2.1 . visual-only slam the visual-only slam system be base on 2d image processing . after the image ’ acquisition from more than one point of view , the system perform the initialization process to deﬁne a global coordinate system and reconstruct an initial map . in the feature-based algorithm rely on ﬁlters ( ﬁltering-based algorithm ) , the ﬁrst step consist of initialize the map point with high uncertainty , which may converge later to their actual position . this procedure be follow by track , which attempt to estimate the camera pose . simultaneously , the mapping process include new point in the 3d reconstruction a more unknown scene be observe . the visual-only slam system may use a monocular or stereo camera . the monocular camera-based slam be a well-explored domain give the small size of the sensor ( the small of all the presented approach ) , it low price , easy calibration , and reduced power consumption [ 27 ] . despite these advantage , the monocular-based system offer a high complexity in system ’ s initialization , since at least two different view be necessary to determine an initial depth , and pose estimation and problem concern drift and scale estimation . this last problem may be compensate by stereo camera , which present the main advantage to feature the stereo view in only one frame . however , the sensor ’ s size be more signiﬁcant than a simple monocular camera . in addition , it require more processing for each frame , mainly due to the need for an image rectiﬁcation process in the stereo matching stage . the visual-only slam category can be divide into two main method : feature-based and direct . 2.1.1 . feature-based methods slam algorithm base on feature consider a certain number of point of interest , call keypoints . they can be detect in several image and match by compare their descriptor ; this process provide the camera pose estimation information . the descriptor data and keypoint location compose the feature , i.e. , the data use by the algorithm to process the tracking and mapping . as the feature-based method do not use all the frame information , they be suitable to ﬁgure in embedded implementation . however , the feature extraction may fail in a textureless environment [ 28 ] , as well a it generate a sparse map , provide less information than a dense one . 2.1.2 . direct methods in contrast with the feature-based method , the direct method use the sensor data without pre-processing , consider pixel ’ intensity , and minimize the photometric er- ror . there be many different algorithm base on this methodology , and depend on the chosen technique , the reconstruction may be dense , semi-dense , or sparse . the reconstruc- tion density be a substantial constraint to the algorithm ’ s real-time operation , since the joint optimization of both structure and camera position be more computationally expensive for dense and semi-dense reconstruction than for a sparse one [ 29 ] . figure 2 show the main difference between feature-based ( indirect ) and direct method accord to their front-end and back-end , that be , the part of the algorithm responsible for sensor ’ s data abstraction and the part responsible for the interpretation of the abstract data , respectively.robotics 2022 ,11 , 24 5 of 27 input image feature processing tracking and sparse map front-end back-end tracking and sparse map front-end back-endinput image tracking and dense map back-end figure 2 . general difference between feature-based and direct method . top : main step follow by the feature-based method , result in a sparse reconstruction ( map generate with the orb-slam3 algorithm [ 22 ] in the mh_01/euroc sequence [ 23 ] ) . bottom : main step follow by a direct method , that may result in a sparse ( generate from the reconstruction of sequence_02 /tum monovo [ 30 ] with the dso algorithm [ 31 ] ) or dense reconstruction ( reprinted from [ 25 ] ) , accord to the chosen technique . 2.2 . visual-inertial slam the vi-slam approach incorporate inertial measurement to estimate the structure and the sensor pose . the inertial data be provide by the use of an inertial measurement unit ( imu ) , which consist of a combination of gyroscope , accelerometer , and , additionally , magnetometer device . this way , the imu be capable of provide information relative to the angular rate ( gyroscope ) and acceleration ( accelerometer ) along the x- , y- , and z-axes , and , additionally , the magnetic ﬁeld around the device ( magnometer ) . while add an imu may increase the information richness of the environment and provide high accuracy , it also increase the algorithm ’ s complexity , especially during the initialization step , since , besides the initial estimation of the camera pose , the algorithm also have to estimate the imu pose . vi-slam algorithm can be divide accord to the type of fusion between the camera and imu data , which can be loosely or tightly couple . the loosely couple method do not merge the imu state to estimate the full pose : instead , the imu data be use to estimate the orientation and change in the sensor ’ s position [ 18 ] . on the other side , the tightly couple method be base on the fusion of camera and imu data into a motion equation , result in a state estimation that consider both data . in addition , vi-slam algorithms present different implementation accord to their back-end approach , which can be ﬁltering-based or optimization-based . the front-end of ﬁltering-based approach for vi-slam relies on feature extraction , while optimization- base method ( also know a keyframe-based approach ) rely on global optimization , which increase the system ’ s accuracy , as well a the algorithm ’ s computational cost.robotics 2022 ,11 , 24 6 of 27 2.3 . rgb-d slam slam system base on rgb-d data start to attract more attention with the advent of microsoft ’ s kinect in 2010 . rgb-d sensor consist of a monocular rgb camera and a depth sensor , allow slam system to directly acquire the depth information with a feasible accuracy accomplish in real-time by low-cost hardware . as the rgb-d device directly provide the depth map to the slam system , the general framework of slam base on this approach differ from the other one already present . most of the rgb-d-based system make use of the iterative closest point ( icp ) al- gorithm to locate the sensor , fuse the depth map to obtain the reconstruction of the whole structure . rgb-d system present advantage such a provide color image data and dense depth map without any pre-processing step , hence decrease the complexity of the slam initialization [ 10 ] . despite this , this approach be most suitable to indoor environment , and require large memory and power consumption [ 32 ] . 3 . visual-slam algorithms each consider approach present in section 2 include several algorithm , make it difﬁcult to select the most suitable slam or odometry algorithm accord to one ’ s project constraint . therefore , we present the most representative algorithm of each approach , select base on literature feedback , to accomplish a brief review of each one , and a systematic analysis base on six select criterion that , in general , be present a limit factor of slam project . besides the propose criterion , it be also necessary to characterize the scene and application , since some scenario may present speciﬁc attribute that may imply speciﬁc evaluation criterion , such a the analysis present in [ 33 ] . the author consider the autonomous drive application characteristic , which imply a set of speciﬁc criterion , such a the required accuracy , scalability , dynamicity , etc . thus , consider the general approach of the slam system , we establish six criterion that inﬂuence system dimension , accuracy , and hardware implementation . they be : algorithm type , map density , global optimization , loop closure , availability , and embed implementation : • algorithm type : this criterion indicate the methodology adopt by the algorithm . for the visual-only algorithm , we divide them into feature-based , hybrid , and direct method . considering the visual-inertial algorithm , they must be ﬁltering-based or optimization-based method . lastly , the rgb-d approach can be divide concern their tracking method , which can be direct , hybrid , or feature-based . • map density : in general , dense reconstruction require more computational resource than a sparse one , have an impact on memory usage and computational cost . on the other hand , it provide a more detailed and accurate reconstruction , which may be a key factor in a slam project . • global optimization : slam algorithm may include global map optimization , which refer to the technique that search to compensate the accumulative error introduce by the camera movement , consider the consistency of the entire structure . • loop closure : the loop closing detection refers to the capability of the slam algorithm to identify the image that be previously detect by the algorithm to estimate and correct the drift accumulate during the sensor movement . • availability : several slam algorithm be open source and make available by the author or have their implementation make available by third party , facilitate their usage and reproduction . • embedded implementation : the embedded slam implementation be an emerge ﬁeld use in several application , especially in robotics and automobile domain . this criterion depend on each algorithm ’ s hardware constraint and speciﬁcity , since there must be a trade-off between algorithm architecture in term of energy consumption , memory , and processing usage . we assemble the main publication we find present fully embed slam system in platform such a microcontrollers and fpga boards.robotics 2022 ,11 , 24 7 of 27 in the following , we present the select slam algorithms consider the most repre- sentative of each of the three present approach accord to their publication year . 3.1 . visual-only slam the select visual-only slam algorithm be present in figure 3 and explain in the following subsection . monoslam2007 ptam 2007dtam2011 svo 2014lsd-slam2014 orb-slam22017 cnn-vslam 2017dso2018 figure 3 . timeline represent the most representative visual-only slam algorithm . 3.1.1 . monoslam ( 2007 ) the ﬁrst monocular slam algorithm be monoslam , which be propose by david- son et al . [ 27 ] in 2007 . the ﬁrst step of the algorithm consist of the system ’ s initialization . then , it update the state vector consider a constant velocity motion model , where the camera motion and environment structure be estimate in real-time use an extended kalman ﬁlter ( ekf ) . the algorithm be represent by figure 4 . monoslam operate in real-time and be make available by the author . moreover , since monoslam be base on ekf , an already well-covered topic , several embed implementation base on this algorithm be find in the literature . in [ 34,35 ] , vincke et al . base their implementation on the monoslam algorithm , combine multiple sensor and a multi-processor architecture to evaluate it implementation . in [ 34 ] , the author use an arm + dsp + gpu architec- ture ( omap3530 architecture ) to implement the localization , reconstruction , and feature detection . they combine this architecture with a co-processor atmega168 use for data pre-processing and robot control . in [ 35 ] , they base the architecture on a combination of multi-cpus + gpus provide by the use of an omap4430 architecture . the author implement the different task of the algorithm into both single-core and dual-core arm architecture , and compare their performance . in addition , they parallelize the matching and initialization task use the arm and neon processor provide by the omap4430 . monoslam require a known target for the initialization step , which be not always accessible . in addition , the algorithm ’ s complexity increase proportionally with the size of the environment . this algorithm neither employ global optimization technique nor loop closure detection . at last , it only reconstruct a map of landmark , which may be a drawback regard the application that require a more accurate reconstruction . initializationprediction cinematic model ekf tracking shi & tomasi depth estimation partticle filter correlationcorrection ekf noise addition figure 4 . diagram represent the monoslam algorithm . 3.1.2 . parallel tracking and mapping ( 2007 ) another pioneer algorithm be the parallel tracking and mapping ( ptam ) [ 36 ] algo- rithm . ptam be the ﬁrst algorithm to separate tracking and mapping into two differentrobotics 2022 ,11 , 24 8 of 27 thread and to apply the concept of keyframes to the mapping thread . first , the mapping thread perform the map initialization . new keyframes be add to the system a the camera move and the initial map be expand . triangulation between two consecutive keyframes calculate the new point ’ s depth information . the track thread compute the camera pose , and for each new frame , it estimate an initial pose for perform the projection of the map point on the image . ptam use the correspondence to compute the camera pose by minimize the reprojection error . figure 5 represent the step perform by the ptam algorithm . new frameprior pose estimationcoarsest- scale feature searchingcamera pose updatingfinal pose estimationmap points projectionfine-scale feature search new keyframe ? update keyframe data associationintegrate keyframe add new featureslocally converged ? globally converged ? local bundle adjustmentglobal bundle adjustment repeat cycleupdate data associationtracking thread m a p p i n g t h r e a dyesno yes no noyes figure 5 . diagram represent the ptam algorithm . ptam allow the map representation by a large number of feature and performs global optimization . despite these advantage , the ptam algorithm present a high com- plexity due to the bundle adjustment step . in addition , it do not count with loop closure , and the generated map be more suitable to identify landmark . furthermore , it require the user ’ s interaction to establish the initial keyframes , and it present a non-negligible power consumption , which make it unsuitable for low-cost embed system [ 37 ] . 3.1.3 . dense tracking and mapping ( 2011 ) dense track and map ( dtam ) , propose by newcombe et al . [ 38 ] , be the ﬁrst fully direct method in the literature . the algorithm be divide into two main part : dense mapping and dense tracking . the ﬁrst stage search to estimate the depth value by deﬁning data cost volume represent the average photometric error of multiple frame compute for the inverse depth of the current frame . the inverse depth that minimize the photometric error be select to integrate the reconstruction . in the dense tracking stage , dtam estimate the motion parameter by align an image from the dense model project in a virtual camera and the current frame . figure 6 show a general view of the dtam algorithm . the algorithm provide an accurate and detailed reconstruction , but this level of density reconstruction impact the computational cost to store and process the data . as a consequence , to achieve real-time operation , the algorithm require state-of-the-art gpus [ 10 ] . the author in [ 39 ] employ a cpu + gpu architecture of different iphone model to implement a fully dense algorithm base on dtam . they use the cpu for the tracking task and the gpu for depth estimation and frame fusion . dtam do not implement loop closure technique or global optimization . new frameptam ’ s tracking dense trackingdepth map estimationcost volume 3d surface figure 6 . diagram represent the dtam algorithm.robotics 2022 ,11 , 24 9 of 27 3.1.4 . semi-direct visual odometry ( 2014 ) the semi-direct visual odometry ( svo ) algorithm [ 40 ] combine the advantage of both feature-based and direct method . the algorithm be divide into two main thread : motion estimation and mapping . the ﬁrst thread search to estimate the sensor ’ s motion parameter , which consist of minimize the photometric error . the mapping thread be base on probabilistic depth ﬁlters , and it search to estimate the optimum depth value for each 2d feature . when the algorithm achieve a low uncertainty , it insert the 3d point in the reconstruction , a show in figure 7 . svo enable direct pixel correspondence and the usage of a probabilistic mapping method . in addition , the algorithm be capable of operate with a high frame rate , since it do not need to extract feature for every frame [ 41 ] , which enable it operation in a low-cost embedded system , a with the embedded platform consider by [ 40 ] that consist in an odroid-u2 . nonetheless , svo present a limited accuracy due to the short-term data association [ 22 ] . svo do not implement global optimization technique or loop closure . the author already propose an extended version of the svo , svo 2.0 [ 42 ] , in which the algorithm be capable of process stereo data and imu information . last frameimage alignementpose and structure refinementfeature alignment keyframe ? motion estimation thread new frameframe queue update depth-filtersno converged ? yes : insert point feature extraction yes initialize depth filtersmap mapping threadminimization of photometric errorcoordinates correspondence fast corner extractionbayesian framework figure 7 . diagram represent the svo algorithm . adapted from [ 40 ] . 3.1.5 . large-scale direct monocular slam ( 2014 ) the large-scale direct monocular slam ( lsd-slam ) [ 24 ] be a direct algorithm that perform a semi-dense reconstruction . the algorithm consists of three main step : tracking , depth map estimation , and map optimization . the ﬁrst step minimize the photometric error to estimate the sensor ’ s pose . next , the lsd-slam performs the keyframe selection in the depth map estimation step . if it add a new keyframe to the algorithm , it initialize it depth map ; otherwise , it reﬁnes the depth map of the current keyframe by perform several small-baseline stereo comparison . finally , in the map optimization step , the lsd- slam incorporate the new keyframe in the map and optimize it by apply a pose–graph optimization algorithm . figure 8 illustrate the procedure . this technique allow the real- time construction of large-scale map and employ global optimization and loop closure . in addition , by combine the absence of feature extraction , characteristic of the direct method , with a semi-dense reconstruction , this method improve it efﬁciency , enable embedded implementation . boikos and christos-savvas in [ 29,43 ] use cpu + fpga architectures to implement the lsd-slam algorithm . in [ 29 ] , the author implement two accelerator on the fpga to perform more expensive task of the track thread ; that be , jacobian calculation , as well a residual and weight calculation . the arm cpu be use to implement the other task of the algorithm . in [ 43 ] , the author implement the direct track thread on the fpga , while the cpu be responsible for memory , hardware control , and parameter setup . the lsd-slam map estimation be base essentially on pose-graph optimization [ 22 ] and the algorithm achieve low accuracy than others , such a ptam and orb-slam [ 41 ] .robotics 2022 ,11 , 24 10 of 27 new frametrack on current keyframe estimate se3 transformationnew keyframe ? yeskeyframe creation depth map propagation and regularizationadd keyframe to map find close keyframes estimate sim3 edge no refine keyframe small-baseline stereo merge into keyframe regularize depth maptracking depth map estimation map optimization figure 8 . diagram represent the lsd-slam algorithm . adapted from [ 24 ] . 3.1.6 . orb-slam 2.0 ( 2017 ) the orb-slam2 algorithm [ 44 ] , originate from orb-slam [ 41 ] , be consider the state of the art of feature-based algorithm . it work in three parallel thread : tracking , local mapping , and loop closing . the ﬁrst thread locate the sensor by ﬁnding feature correspondence and minimize the reprojection error . the local mapping thread be responsible for the map management operation . the last thread , loop closing , be in charge of detect new loop and correct the drift error in the loop . after process the three thread , the algorithm also consider the whole structure and estimate motion consistency by perform a full bundle adjustment . figure 9 represent the thread that constitute the algorithm . orb-slam2 considers the monocular , stereo and rgb-d approach , and implement global optimization and loop closure technique . nonetheless , the tracking failure situation may lead to a lose state if the system do not recognize a high-similarity frame [ 45 ] . in addition , this method need to acquire the image with the same frame rate a it process them , which make real-time operation in embedded platform difﬁcult [ 46 ] . this be in spite of the fact that several embed implementation may be find in the literature . yu et al . [ 47 ] use a cpu to run the orb-slam algorithm and abouzahir et al . [ 46 ] implement the algorithm in different cpu- and gpu-based platform , and evaluate the performance of each thread on the platform . new frame extract orbtrack local map tracking new keyframe decision keyframe local mappingkeyframe insertionkeyframe management and points creation place recognition and map module pose estimation or relocalisation local bundle adjustmentlocal keyframes culling loop closingfull bundle adjustmentloop correction loop detection levenberg–marquardt optimization optimization essential graph figure 9 . diagram represent the orb-slam 2.0 algorithm . adapted from [ 41 ] . 3.1.7 . cnn-slam ( 2017 ) cnn-slam [ 48 ] be one of the ﬁrst work to present a real-time slam system base on convolutional neural network ( cnn ) . the algorithm may be divide into two different pipeline : one apply in every input frame and another in every keyframe . the ﬁrst be responsible for the camera pose estimation by minimize the photometric error between the current frame and the near keyframe . in parallel , for every keyframe , the depth be predict by a cnn . in addition , the algorithm predict the semantic segmentation for each frame . after these process step , the algorithm perform a pose-graph optimization to obtain a globally optimize pose estimation , a show in figure 10.robotics 2022 ,11 , 24 11 of 27 new frame cnn depth predictionkeyframe initialization global map and semantic label fusioncnn semantic segmentationpose-graph optimizationcamera pose estimationdepth refinement resnet architecturekey-frame figure 10 . diagram represent the cnn-slam algorithm . adapted from [ 48 ] . this algorithm do not suffer from absolute scale limitation , since it use depth pre- diction to perform the scale estimation [ 48 ] . in addition , it count with global optimization and loop closure . the author need to employ a cpu+gpu architecture to run the algorithm in real-time . 3.1.8 . direct sparse odometry ( 2018 ) the direct sparse odometry ( dso ) algorithm [ 31 ] combine a direct approach with a sparse reconstruction . the dso algorithm consider a window of the most recent frame . it perform a continuous optimization by apply a local bundle adjustment that optimize the keyframes window and the inverse depth map . the algorithm divide the image into several block and select the high intensity point . the dso considers exposure time and lens distortion in the optimization to increase the algorithm ’ s robustness . initially , this algorithm do not include global optimization or loop closure , but xiang et al . [ 49 ] propose an extension of the dso algorithm , include loop closure detection and pose- graph optimization . the dso main step be represent in figure 11 . track on current keyframe image alignment multi-scale image pyramid constant motion modelnew keyframe ? yeskeyframe creation candidate point selection tracking on next frame activation joint optimization optimization of the photometric erroradd keyframe in the optimization window of active keyframes marginalize old keyframes and pointsno refine active keyframesnew frame figure 11 . diagram represent the dso algorithm . 3.1.9 . general comments in this section , we present the main visual-only-based slam algorithm . table 1 summarize the main characteristic and analyzed criterion for the presented visual-only slam algorithm . table 1 . main aspect relate to the visual-only slam approach . method type map density global optim . * loop closure embed . implem . ** availability monoslam feature-based sparse no no [ 34,35 ] [ 50 ] ptam feature-based sparse yes no [ 51 ] [ 52 ] dtam direct dense no no [ 39 ] [ 53 ] svo hybrid sparse no no [ 40 ] [ 54 ] lsd direct semi-dense yes yes [ 29,43 ] [ 55 ] orb-slam feature-based sparse yes yes [ 46,47 ] [ 56 ] cnn-slam direct semi-dense yes yes - [ 57 ] dso direct sparse no no - [ 58 ] * global optimization . ** embedded implementation.robotics 2022 ,11 , 24 12 of 27 the main beneﬁts and drawback of each method be individually address . con- sidering a general point of view , the visual-only-based slam algorithm may be consider a well-explored ﬁeld , since most of the algorithm be make available by the author , which also have consequence for the embedded slam implementation find in the literature . the embedded implementation present in table 1consider the full slam algorithm implementation and work that do not perform essential modiﬁcations in the originally propose technique . however , it be possible to ﬁnd in the literature several embed implementation base on fundamental concept of the presented algorithm . for instance , the monoslam principle have be use for the development and imple- mentation of several other slam on soc implementation , such a the heterogeneous architecture recently propose by piat et al . [ 59 ] . furthermore , the grow development of the cnn-based slam algorithm can be notice . besides the presented cnn-slam , other algorithm be find in the literature , such a the cnn-svo [ 28 ] algorithm that use depth prediction to initialize the depth ﬁlters . developments of the hardware imple- mentation of cnn-based slam algorithm have be grow since the launch of the ai accelerator xilinx deep-learning processor unit [ 60 ] in 2019 . this hardware already enable the progress on embedded implementation of cnn-based algorithm : one ex- ample be the work present in [ 61 ] that use an fpga platform to perform a cnn-based feature extractor . 3.2 . visual-inertial slam a timeline represent the select visual-inertial algorithm be present in figure 12 and the algorithm be explain in the following subsection . msckf2007 okvis 2014rovio2015 viorb 2017vins-mono2018vi-dso orb-slam3 2020 figure 12 . timeline represent the most representative visual-inertial slam algorithm . 3.2.1 . multi-state constraint kalman filter ( 2007 ) the multi-state constraint kalman ﬁlter ( msckf ) [ 62 ] can be implement use both monocular and stereo camera [ 63 ] . the algorithm ’ s pipeline consist of three main step : propagation , image registration , and update . in the ﬁrst step , the msckf consider the discretization of a continuous-time imu model to obtain the propagation of the ﬁlter state and covariance . then , the image registration perform the state augmentation each time a new image be record . this estimation be add in the state and covariance matrix to initiate the image processing module ( feature extraction ) . finally , the algorithm perform the ﬁlter update . figure 13 represent the algorithm . the msckf be consider one of the fast ﬁlter-based method in the literature [ 64 ] , a consequence of it low computational cost [ 63 ] , which make this algorithm suitable for embedded implementation . delmerico and scaramuzza [ 65 ] use different hardware platform base on cpu architecture to implement visual-inertial slam algorithm . the author implement the algorithm in three different embed boards—intel nuc , up board , and odroid . however , the jacobian calculation perform by the algorithm may cause inconsistency and loss of accuracy [ 66 ] .robotics 2022 ,11 , 24 13 of 27 new imu datapropagation filter state covariance model pose estimation augmentation : state vector , covariance matrixupdate ekf updatenew image registrationimage processing feature tracking and matching figure 13 . diagram represent the msckf algorithm . 3.2.2 . open keyframe-based visual-inertial slam ( 2014 ) open keyframe-based visual-inertial slam ( okvis ) [ 67 ] be an optimization-based method . it combine the imu data and reprojection term into an objective function , allow- ing the algorithm to jointly optimize both the weighted reprojection error and temporal error from imu . the algorithm build a local map , and then the subsequent keyframes be select accord to the keypoints match area . the algorithm can be depict a show in figure 14 . the okvis algorithm present a low memory usage when compare with other algorithm ( this will be explain in the following subsection ) , such a vins-mono , viorb , and rovio [ 18 ] , enable it embedded implementation . already mention , the work of delmerico and scaramuzza [ 65 ] use different cpu platform to implement the okvis algorithm . however , to achieve real-time performance in the up board and odroid , the author need to reduce the number of keypoints , the keyframe window , and the imu-linked frame . nikolic et al . [ 68 ] use an fpga-cpu architecture to evaluate the okvis algorithm ’ s performance . the author take advantage of the logic block on the fpga to implement the image processing technique and accelerate the keypoint detection process . however , it be demonstrate that the algorithm be less accurate than others [ 18 ] . new framefeatures detection imu datalast pose propagationfeatures matchinglocal map insertionoptimization in recent framesis it keyframe ? yes noinsertion on optimization window marginalization operationdrop frame landmarksmarginalize frame and speed/bias state figure 14 . diagram represent the okvis algorithm . 3.2.3 . robust visual inertial odometry ( 2015 ) the robust visual inertial odometry ( rovio ) algorithm [ 69 ] be another ﬁlter-based method that use the ekf approach , and similar to other ﬁlter-based method , it use the imu data to state propagation , and the camera data to ﬁlter update . however , besides per- form the feature extraction , rovio execute the extraction of multi-level patch around the feature , a illustrate by figure 15 . the patch be use by the prediction and update step to obtain the innovation term , i.e. , the calculation of the error between the frame and the projection of the multi-level patch into the frame . the rovio algorithm achieves good accuracy and robustness under a low resource utilization [ 18,65 ] , be suitable for embedded implementation [ 65 ] . however , the algorithm prove to be more sensitive to per-frame processing time [ 65 ] and less accurate than other algorithms , such a vi-dso [ 70 ] . 3.2.4 . visual inertial orb-slam ( 2017 ) the visual-inertial orb-slam ( viorb ) algorithm [ 71 ] be base on the already pre- sented orb-slam algorithm [ 44 ] . as such , the system also count with three main thread : tracking , local mapping , and loop closing . in viorb , the track thread estimate the sensor pose , velocity , and imu bias . additionally , this thread perform the joint optimiza- tion of the reprojection error of the matched point and imu error data . the local mapping thread adopt a different culling policy consider the imu operation . finally , the loop closing thread implement a place recognition module to identify the keyframes alreadyrobotics 2022 ,11 , 24 14 of 27 visit by the sensor . furthermore , the algorithm perform an optimization to minimize the accumulated error . figure 16 seek to illustrate the main difference between the orb- slam algorithm ( see figure 9 ) and it visual-inertial version . the viorb algorithm be the ﬁrst visual-inertial method to employ map reuse , and it present high-performance accuracy [ 64,70,72 ] and memory usage [ 18 ] . nonetheless , the imu initialization take between 10 to 15 s [ 71 ] , and no embedded implementation be find . in [ 22 ] , the author propose the orb-slam3 algorithm , which be base on orb-slam2 and viorb algorithm . the system present a reduced initialization time compare to it predecessor , viorb . new featureprediction is it visible ? yes nowarp patch updategood statistics ? large uncertainty ? yes no extract patch if possibleyespre-alignment no delete featureintensity errorsimu-driven motion model innovation term figure 15 . diagram represent the feature handle perform by the rovio algorithm . adapted from [ 69 ] . new framecamera pose predictionoptimize frame by minimize the reprojection error local map points projection and matchingtracking imu data new keyframe decision keyframe local mappinglocal ba imu + reprojection errorkeyframe management temporal constraintloop closing orb-slam loop closingplace recognition module pose-graph optimizationfull ba figure 16 . diagram represent the viorb algorithm . 3.2.5 . monocular visual-inertial system ( 2018 ) monocular visual-inertial system ( vins-mono ) [ 73 ] be a monocular visual-inertial state estimator . it start with a measurement process responsible for feature extraction and tracking , and a pre-integration of the imu data between the frame . then , the algorithm perform an initialization process to provide the initial value for a non-linear optimiza- tion process that minimize the visual and inertial error . the vins also implement a relocalization and a pose-graph optimization module that merge the imu measurement and feature observation . figure 17 illustrate the vins-mono algorithm . the algorithm can also be apply consider binocular and stereo approach [ 74 ] . the vins-mono already demonstrate to achieve high accuracy when compare to other algorithm . yet , it present the high memory usage when compare to algorithms such a rovio , viorb , and okvis [ 18 ] . this be despite the fact that , since it only consider pose and velocity from the late imu state during the optimization process , this algorithm still demonstrate it suitability in embedded implementation [ 73 ] .robotics 2022 ,11 , 24 15 of 27 new frameimu datapreprocessing feature detection and tracking imu pre-integrationinitializationrelocalization module states from loop closureloop detectionoptimization-based vio pose-graph optimization and map reuse keyframe database figure 17 . diagram represent the vins-mono algorithm . 3.2.6 . visual-inertial direct sparse odometry ( 2018 ) the visual-inertial direct sparse odometry ( vi-dso ) algorithm [ 70 ] be base on the already present dso algorithm [ 31 ] . the algorithm search to minimize an energy function that combine the photometric and inertial error , which be build consider a nonlinear dynamic model . figure 18 show an overview of the vi-dso algorithm that illustrate it main difference concern the dso technique . the vi-dso be an extension of dso that consider the inertial information , which result in good accuracy and robustness than the original dso and other algorithm , like rovio [ 70 ] . however , the initialization procedure relies on bundle adjustment , which make the initialization slow [ 22 ] . the algorithm do not perform global optimization and loop closure detection , and embed implementation be not find in the literature . new framecoarse tracking on current keyframe image alignment + imu errornew keyframe ? yeskeyframe creation candidate point selection tracking on next frame activationjoint optimization optimization of pose , imu-biases and velocitiesadd keyframe in the optimization window of active keyframes marginalization policyno refine active keyframesinitialization value partial marginalization figure 18 . diagram represent the vi-dso algorithm . 3.2.7 . orb-slam3 ( 2020 ) the already mention orb-slam3 algorithm [ 75 ] be a technique that combine the orb-slam and viorb algorithms . as with it predecessor , the algorithm be divide into three main thread : tracking , local mapping and , instead of loop closing , loop closing and map merging . in addition , orb-slam3 maintain a multi-map representation call atlas , which maintain an active map use by the track thread , and non-active map use for relocalization and place recognition . the ﬁrst two thread follow the same principle a viorb , while map merging be add to the last thread . the loop closing and map merge thread us all the map in atlas to identify common part and perform loop correction or merge map and change the active map , depend on the location of the overlapped area . another important aspect of orb-slam3 concern the propose initial- ization technique that rely on the maximum-a-posteriori algorithm individually apply to the visual and inertial estimation , which be later jointly optimize . this algorithm can be use with monocular , stereo , and rgb-d camera , and implement global optimiza- tions and loop closure technique . however , author in [ 76 ] demonstrate signiﬁcant error result of orb-slam3 online performance . in [ 77 ] , the algorithm obtain a good performance , but fail to process all the sequence , and obtain inaccurate estimate in outdoor sequences.robotics 2022 ,11 , 24 16 of 27 3.2.8 . general comments this section present seven main visual-inertial slam algorithm , as long a an individual analysis of each of them . table 2 summarize the main characteristic and analyzed criterion for the presented visual-inertial slam algorithm . table 2 . main aspect relate to the visual-inertial slam approach . all approach present tightly couple sensor fusion . method type map density global optim . * loop closure embed . implem . ** availability msckf filtering-based sparse no no [ 65 ] [ 78,79 ] okvis optimization-based sparse no no [ 65,68 ] [ 80 ] rovio filtering-based sparse no no [ 65 ] [ 81 ] vins optimization-based sparse yes yes [ 65,74 ] [ 82 ] viorb optimization-based sparse yes yes - - vi-dso optimization-based sparse no no - [ 83 ] orb-slam3 optimization-based sparse yes yes - [ 84 ] * global optimization . ** embedded implementation . in a general analysis , the addition of an imu to visual-based slam algorithm have the primary purpose of increase the system ’ s robustness , which be already demonstrate to be true [ 2,22,70 ] . we observe great literature feedback from the algorithm make available by their author , which directly inﬂuenced the embedded implementation find in the literature . unlike it visual-only version , we do not ﬁnd an embedded version of the viorb algorithm , since the original article do not provide an open-source version , and the more recent one , the open-source orb-slam3 , be recently publish in 2020 [ 22 ] . as for the inertial version of the dso algorithm , the author do not provide an open-source implementation ; however , an implementation by third party may be find [ 83 ] , even though it require optimization . the visual-inertial slam-based approach represent a grow ﬁeld , and several recent article have be publish , combine the imu tech- nologies with a large variety of sensor [ 85–87 ] . limiting our research to the visual-slam technique , we could ﬁnd several article propose solution to increase the performance of the vi-based slam algorithm ’ s initialization step [ 75,88,89 ] . 3.3 . rgb-d slam the most representative slam algorithm base on rgb-d sensor , i.e. , consider rgb image and depth information directly , be present in figure 19 , accord to their publish year , and explain in the following subsection . kinectfusion2011 slam++2013 rgbdslamv2 2014orb-slam22017 dvo 2013 figure 19 . timeline represent the most representative rgb-d-based slam algorithm . 3.3.1 . kinectfusion ( 2011 ) the kinectfusion algorithm [ 90 ] be the ﬁrst algorithm base on an rgb-d sensor to operate in real-time . the algorithm include four main step : the measurement , pose estimation , reconstruction update , and surface prediction . in the ﬁrst step , the rgb image and depth data be use to generate a vertex and a normal map . in the pose estimation step , the algorithm apply the icp alignment between the current surface and the predicted one ( provide by the previous step ) . then , the reconstruction update step integrate the new depth frame into the 3d reconstruction , which be raycasted into the new estimate frame to obtain a new dense surface prediction . the kinectfusion algorithm be capable of goodrobotics 2022 ,11 , 24 17 of 27 mapping in maximum medium-sized room [ 90 ] . however , it accumulate drift error , since it do not perform loop close [ 91 ] . nardi et al . , in [ 92 ] , propose an implementation for the kinectfusion and test it in different cpu- and gpu-based platform . bodin et al . [ 93 ] use the framework propose by [ 92 ] to implement the kinectfusion in two different cpu and gpu platform . an overview of the step perform by the algorithm be show in figure 20 . input datameasurement surface vertex and normal mappose estimation icp alignementupdate reconstructionsurface prediction depth data track surface integrationlive depth track depth map alignement figure 20 . diagram represent the kinectfusion algorithm . adapted from [ 90 ] . 3.3.2 . slam++ ( 2013 ) the slam++ algorithm [ 94 ] be an object-oriented slam algorithm that take advan- tage of previously know scene contain repeated object and structure , such a a classroom . after the system initialization , slam++ operate in four step : camera pose esti- mation , object insertion , and pose update , pose–graph optimization , and surface rendering . the ﬁrst step estimate the current camera pose by apply the icp algorithm , consider dense multi-object prediction in the current slam graph . next , the algorithm search to identify object in the current frame use the database information . the third step insert the consider object in the slam graph by perform a pose–graph optimization operation . finally , the algorithm render the object in the graph , a show in figure 21 . slam++ performs loop closure detection and , by consider the object ’ s repeatability , it increase it efﬁciency and scene description . nevertheless , the algorithm be most suitable for already know scene . surface measurementframe model optimize se3object detectiontracking convergenceframe model optimize se3camera pose estimationobject insertion and pose update pose-graph optimization surface rendering figure 21 . diagram represent the slam++ algorithm . adapted from [ 94 ] . 3.3.3 . dense visual odometry ( 2013 ) the dense visual odometry slam ( dvo-slam ) algorithm , propose by kerl et al . [ 95 ] , be a keyframe-based technique . it minimize the photometric error between the keyframes to acquire the depth value and pixel coordinate , as well a camera motion . the algorithm calculates , for each input frame , an entropy value that be compare to a threshold value . the same principle be use for loop detection , although it use a different threshold value . the map be represent by a slam graph where the vertex have camera pose , and edge be the transformation between keyframes . this algorithm be robust to textureless scene and performs loop closure detection . the map representation relies on a representation of the keyframes , and the algorithm do not perform an explicit map reconstruction . figure 22 show an overview of the dvo algorithm.robotics 2022 ,11 , 24 18 of 27 rgb-d datacamera motion estimation photometric and geometric error minimizationkeyframe selection based on the calculation of an entropy ratio keyframe map insertion loop closure detection nearest neighbour searchmap representation graph of camera posesmap optimization non-linear least square optimization figure 22 . diagram represent the dvo algorithm . 3.3.4 . rgbdslamv2 ( 2014 ) the rgbdslamv2 [ 96 ] be one of the most popular rgb-d-based algorithm and relies on feature extraction . it perform the ransac algorithm to estimate the transformation between the matched feature and the icp algorithm to obtain pose estimation . finally , the system execute a global optimization and loop closure to eliminate the accumulated error . in addition , this method propose use an environment measurement model ( emm ) to validate the transformation obtain between the frame . the algorithm be base on sift feature , which degrade it real-time performance . rgbdslamv2 present a high computation consumption and require a slow movement by the sensor for it correct operation [ 91 ] . figure 23 represent the algorithm . input framepoint cloud subsampling and storage features extraction and matchingtransformation estimationtranformation validationinput depthpoint cloud subsampling and storageinput data tranfor- mationsoptimi- zations trajecto- rymap creation figure 23 . diagram represent the rgbdslamv2 algorithm . adapted from [ 96 ] . 3.3.5 . general comments section 3.3 individually present the most representative rgb-d-based technique . table 3 summarize the main characteristic and analyzed criterion for the presented algorithm . table 3 . main aspect relate to the rgb-d-based slam approach . method tracking method map density loop closure embed . implem . * availability kinectfusion direct dense no [ 92,93 ] [ 97 ] slam++ hybrid dense yes - - rgbdslamv2 feature-based dense yes - [ 98 ] dvo direct dense yes - [ 99 ] orb-slam 2.0 feature-based dense yes - [ 56 ] * embedded implementation . rgb-d-based slam algorithm represent an alternative solution to the visual-only and visual-inertial slam . in general , they construct dense map , enable them to represent the environment in great detail . in addition , it be a more robust approach regard low- texture environment thanks to the depth sensor . concerning embedded implementation , it be possible to ﬁnd , in the literature , several solution search to accelerate part of the rgb-d-based algorithm that usually require more computation load , such a the icp algorithm . beshaw et al . [ 100 ] and williams et al . [ 101 ] propose different architecture to accelerate the icp algorithm , and gautier et al . [ 102 ] implement the icp and the volumetric integration algorithm in a heterogeneous architecture . recent publication haverobotics 2022 ,11 , 24 19 of 27 focus on develop robust rgb-d slam algorithm consider dynamic environment condition [ 103–105 ] . 4 . open problems and future directions although the slam domain have be widely study for year , there be still several open problem . the current state of the art of slam and odometry algorithms increasingly seek to reinforce the algorithm ’ s robustness , optimize computational resource usage , and evolve the environment ’ s understanding in the map representation [ 8 ] . concerning the robustness , slam and odometry technique still present some major issue that undermine algorithms ’ robustness [ 8 ] . one of them be the tracking failure [ 106 ] ; face some challenge or long-term scenario , the algorithm may still fail to recognize and associate feature in the current received image , result in inaccurate pose estimation . this may have conse- quences in loop closure technique [ 107 ] and relocalizations [ 8,108 ] . as a solution to this issue , author have be explore new method to deal with the slam problem . recent work propose the incorporation of deep learning and spectral technique [ 109,110 ] to increase the system ’ s robustness ; some main example the deep-learning-based algorithm be discuss in section 4.1 . another main issue that decrease the slam algorithm ’ robustness be the assumption of static scenario , while the real world present dynamic environment ; this may cause failure in track [ 111 ] and reconstruction [ 112 ] . dealing with dynamic scene may be consider a challenge , since it require the algorithm to detect the dynamic object , avoid the tracking of the object , and exclude it from the map [ 113 ] . as mention in section 3.3.5 , several work have be publish propose solution to this central issue ; more representative example be discuss in section 4.3 . besides the robustness , recent slam algorithm seek to consider the usage of the computational resource [ 8 ] . this current topic lead to the open problem of memory usage by map storage [ 8 ] . storing the map in a long-term operation may considerably increase the memory usage , which may have consequence for memory-limited system operation , e.g. , embed slam . however , it be already possible to ﬁnd , in the literature , work propose solution for this topic . one example be the work of opdenbosch et al . [ 114 ] , who propose an efﬁcient map compression , and demonstrate it ability to signiﬁcantly reduce the map ’ s data and size without lose relevant information . in addition to map storage , another major issue that inﬂuences resource usage be map sparsity . dense and semi-dense map provide a more detailed representation of the environment , but this feature have con- sequence for resource usage . it have already be demonstrate that sparse map present low power consumption compare to semi-dense and dense ones—wan et al . [ 115 ] . con- sequently , they may be more suitable for an embedded implementation , although they provide few detail . currently , the slam algorithm also seek to evolve our understanding of the environ- ment in the performed reconstruction [ 8 ] . besides obtain the geometric information , the algorithm obtain information about the environment by recognize object within it , for example . an evolve slam category that enable this good environment abstraction be the semantic-based slam . the semantic slam be a trend topic on slam , and some main example be discuss in section 4.2 . following this , we brieﬂy discuss some re- cent and relevant article that we believe be representative a future direction of the visual-slam and visual-odometry ﬁelds . 4.1 . deep learning-based algorithms one remarkable algorithm that incorporate deep learning concept be the undeepvo [ 116 ] . this monocular visual-odometry algorithm can perform pose and depth estimation via a deep neural network . the author train undeepvo with unsupervised learn use stereo image ; additionally , they consider both spatial and temporal dense information in the loss function of the training . this method prove to be more accurate and robust than other monocular method , such a the orb-slam ( without loop closure ) .robotics 2022 ,11 , 24 20 of 27 recently , the same research group propose the deepslam [ 117 ] . the system consider a tracking-net and mapping-net trained use unsupervised learning , and consider spatial and temporal geometry in the loss function . the algorithm also contain a loop-net to perform loop detection . deepslam present a good performance than other monocular algorithm , a the orb-slam , and good robustness than orb-slam and lsd-slam . another relevant algorithm base on deep learning be the df-slam [ 118 ] . df-slam follow a framework similar to orb-slam , but instead of use the hand-made feature , explain in section 2.1.1 , it use deep local feature describe by the tfeat network . the author provide several result compare df-slam to orb-slam2 ; for most se- quences , the propose algorithm obtain a good performance . recently , it be possible to ﬁnd , in the literature , several overview [ 119–121 ] that address deep learning-based algorithm apply to depth estimation and the main concept of slam ’ s direction . more method that use deep learning technique be discuss in section 4.3 a a solution to dynamic slam algorithm . 4.2 . semantic-based algorithms incorporating semantic information on the visual-slam problem be a grow ﬁeld , and have be attract more attention in recent year . one important and recent study in this area be present in [ 122 ] . the author propose a new methodology for data association that incorporate information from an object detector , propose a solution that can represent both data association and landmark class in a factor graph solution . this method present reduced error compare to other solution incorporate semantic data association technique . more method contain semantic data be discuss in section 4.3 a a solution to dynamic slam algorithm . as this ﬁeld grow , it be also necessary to establish method to validate the semantic-based algorithm . authors in [ 123 ] introduce a new synthetically generate benchmark dataset that , besides the traditional ground truth of the trajectory , contain semantic label , information about the scene composition , ground truth 3d model , and the pose of the object . in addition , they propose evaluation metric that may assess the semantic-based algorithm ’ performance . 4.3 . dynamic slam algorithms research study into the slam algorithm consider dynamic environment be essential to increase the algorithms ’ robustness to more realistic situation . firstly , in [ 124 ] , then in [ 125 ] , sun et al . propose a motion removal technique to deal with the environment ’ s dynamicity in rgb-d approach . in [ 125 ] , the removal algorithm may be divide into two part ; ﬁrst , it identiﬁes the move object and update the foreground model use the error cause by the object in the image . then , it perform the foreground segmentation . the algorithm obtain good performance , especially in high-dynamics environment , than some state-of-the-art technique , such a dvo . an essential algorithm robust to dynamic scene be the dynamic-slam propose by xiao et al . [ 126 ] ; this method incorporate both deep learning and semantic technique . the system employ a cnn to detect dynamic object at a semantic level ; it separate the dynamic and static feature , consider the dynamic one a outlier . in addition , they propose a compensation algorithm to increase the detection accuracy and a feature-based framework . the track thread incorporate the semantic data , discard or reserve the feature . dynamic-slam present a great accuracy than other method such a lsd-slam , svo , and ptam ; and good robustness compare to orb-slam2 . dynaslam ii [ 127 ] be another relevant method that incorporate semantic segmen- tation to track dynamic object . this algorithm be base on orb-slam2 and performs semantic segmentation and feature extraction at each new frame . this algorithm do not make assumption about the dynamic object and perform the data association of dynamic and static feature . static feature be use to estimate the initial camera pose , and then trajectory , bound box , and 3d point be optimize . dynaslamii show to present a performance comparable to other state-of-the-art algorithm , such a the orb-slam2.robotics 2022 ,11 , 24 21 of 27 5 . datasets and benchmarking among all the slam algorithm in the literature , it be essential to achieve a fair comparison between them to determine which one present a good performance in cer- tain situation . several benchmarking datasets with different characteristic be propose in the literature to explore the slam capability and robustness . here , we present the publicly available benchmark dataset use to evaluate the presented slam algorithm in their original article . the tum rgb-d dataset [ 128 ] consists of several image sequence contain color and depth image record in indoor environment with a microsoft kinetic in two different platform : robot and handheld . the system be synchronize with a motion-capture system to provide the ground truth . in addition , the author propose two metric to evaluate the local accuracy and the global consistency of the trajectory ; they be relative pose error and absolute trajectory error , respectively . the kitti dataset [ 129 ] contains outdoor sequence record by color and grayscale stereo camera . the kitti also present data from a 3d laser scanner and the ground truth provide by an ins/gps . the sensor system be synchronize and mount on a car . in addition , the author provide tracklets for a dynamic object classiﬁcation and benchmark to evaluate robotics task , such a visual odometry and slam . another main benchmark dataset be the icl-nuim [ 130 ] . the dataset focus on rgb-d algorithm and provide data for the evaluation of the 3d reconstruction through eight synthetically generate indoor scene . a handheld rgb-d camera generate the sequence , and the ground truth consist of a 3d surface model and the estimate trajectory by a slam algorithm [ 131 ] . the euroc benchmark dataset [ 23 ] be widely use to evaluate visual-only and visual-inertial slam and odometry algorithm . the data be collect in two indoor environment by a micro aerial vehicle ( mav ) , and it provide eleven sequence of stereo image and imu data . the ground truth be obtain by a total station and a motion capture system . a dataset commonly use to evaluate monocular system be the tum monovo [ 30 ] . it contain several photometrically calibrate indoor and outdoor sequence provide by two handheld non-stereo monocular camera . due to the variety of the scene , the author do not provide a ground-truth from the pose , but they perform large sequence that start and end at the same position , allow the evaluation of the loop drift . lastly , a dataset provide for visual-inertial system evaluation be the tum vi dataset [ 132 ] . it provide several indoor and outdoor sequence capture by a stereo camera synchronize with an imu . the sensor system be handheld , and , a for the tum monovo , it be impossible to establish the ground truth for the entire sequence . however , they provide the ground truth via a motion capture system for the beginning and end of the system . table 4 summarize the main benchmark datasets characteristic present in this work . table 4 . main aspect relate to the present benchmark datasets . dataset year env . * platform sensor system ground-truth availability tum rgb-d 2012 indoor robot/handheld rgb-d camera motion capture [ 133 ] kitti 2013 outdoor car stereo-cameras ins/gps [ 134 ] 3d laser scanner icl-nuim 2014 indoor handheld rgb-d camera 3d surface model [ 135 ] slam estimation euroc 2016 indoor mav stereo-cameras total station [ 136 ] imu motion capture tum monovo 2016 indoor/outdoor handheld non-stereo camera - [ 137 ] tum vi 2018 indoor/outdoor handheld stereo-camera motion capture [ 138 ] imu ( partially ) * environment : indoor or outdoor.robotics 2022 ,11 , 24 22 of 27 6 . conclusions the visual-based slam technique represent a wide ﬁeld of research thanks to their robustness and accuracy provide by a cheap and small sensor system . the literature present many different visual-slam algorithm that make researcher ’ choice difﬁcult , without criterion , when it come to evaluate their beneﬁts and drawback . in this paper , we introduce the main visual-based slam approach and a brief description and sys- tematic analysis of a set of the most exemplary technique of each approach . to guide the choice among all the algorithm , we propose six criterion that be limit factor to several slam project : the algorithm type , the density of the reconstructed map , the presence of global optimization and loop closure technique , it availability , and the embedded implementation already perform . researchers can consider each criterion accord to their application , and obtain an initial analysis from the presented paper . in addition , we present some major issue , suggest future direction for the ﬁeld , and discuss the main benchmarking datasets for visual-slam and odometry algorithms evaluation . regarding future work , we will apply the propose criterion analysis to nuclear decommis- sioning scenario . the best slam algorithm shall be select after consider the variety of feature and speciﬁcities that this environment and application posse . author contributions : conceptualization , a.m.b. , m.m. , y.m. , g.c . and f.c . ; methodology , a.m.b. , m.m . and y.m . ; formal analysis , a.m.b. , m.m. , y.m . and f.c . ; investigation , a.m.b . ; writing—original draft preparation , a.m.b . ; writing—review and editing , m.m. , y.m. , g.c . and f.c . ; supervision , m.m. , y.m. , g.c . and f.c . all author have read and agree to the publish version of the manuscript . funding : this research receive no external funding . institutional review board statement : not applicable . informed consent statement : not applicable . conﬂicts of interest : the author declare no conﬂict of interest . references 1 . smith , r. ; cheeseman , p . on the representation and estimation of spatial uncertainty . int . j . robot . res . 1987 ,5 , 56–68 . [ crossref ] 2 . jinyu , l. ; bangbang , y. ; danpeng , c. ; nan , w. ; guofeng , z. ; hujun , b . survey and evaluation of monocular visual-inertial slam algorithm for augmented reality . virtual real . intell . hardw . 2019 ,1 , 386–410 . [ crossref ] 3 . covolan , j.p . ; sementille , a. ; sanches , s. a mapping of visual slam algorithm and their application in augmented reality . in proceedings of the 2020 22nd symposium on virtual and augmented reality ( svr ) , porto de galinhas , brazil , 7–10 november 2020 . [ crossref ] 4 . singandhupe , a. ; la , h. a review of slam techniques and security in autonomous driving . in proceedings of the 2019 third ieee international conference on robotic computing ( irc ) , naples , italy , 25–27 february 2019 ; pp . 602–607 . [ crossref ] 5 . dworakowski , d. ; thompson , c. ; pham-hung , m. ; nejat , g. a robot architecture using contextslam to find products in unknown crowded retail environments . robotics 2021 ,10 , 110 . [ crossref ] 6 . ruan , k. ; wu , z. ; xu , q . smart cleaner : a new autonomous indoor disinfection robot for combating the covid-19 pandemic . robotics 2021 ,10 , 87 . [ crossref ] 7 . liu , c. ; zhou , c. ; cao , w. ; li , f. ; jia , p . a novel design and implementation of autonomous robotic car based on ros in indoor scenario . robotics 2020 ,9 , 19 . [ crossref ] 8 . cadena , c. ; carlone , l. ; carrillo , h. ; latif , y. ; scaramuzza , d. ; neira , j. ; reid , i. ; leonard , j.j. past , present , and future of simultaneous localization and mapping : toward the robust-perception age . ieee trans . robot . 2016 ,32 , 1309–1332 . [ crossref ] 9 . stachniss , c. robotic mapping and exploration ; springer : berlin/heidelberg , germany , 2009 ; volume 55 . 10 . taketomi , t. ; uchiyama , h. ; ikeda , s. visual slam algorithm : a survey from 2010 to 2016 . ipsj trans . comput . vis . appl . 2017 , 9 , 1–11 . [ crossref ] 11 . kabzan , j. ; valls , m. ; reijgwart , v . ; hendrikx , h. ; ehmke , c. ; prajapat , m. ; bühler , a. ; gosala , n. ; gupta , m. ; sivanesan , r. ; et al . amz driverless : the full autonomous racing system . j . field robot . 2020 ,37 , 1267–1294 . [ crossref ] 12 . durrant-whyte , h. ; bailey , t. simultaneous localization and mapping : part i. ieee robot . autom . mag . 2006 ,13 , 99–110 . [ crossref ] 13 . bailey , t. ; durrant-whyte , h. simultaneous localization and mapping ( slam ) : part ii . ieee robot . autom . mag . 2006 ,13 , 108–117 . [ crossref ] 14 . yousif , k. ; bab-hadiashar , a. ; hoseinnezhad , r. an overview to visual odometry and visual slam : applications to mobile robotics . intell . ind . syst . 2015 ,1 , 289–311 . [ crossref ] robotics 2022 ,11 , 24 23 of 27 15 . fuentes-pacheco , j. ; ruiz-ascencio , j. ; rendón-mancha , j.m . visual simultaneous localization and mapping : a survey . artif . intell . rev . 2015 ,43 , 55–81 . [ crossref ] 16 . servières , m. ; renaudin , v . ; dupuis , a. ; antigny , n. visual and visual-inertial slam : state of the art , classiﬁcation , and experimental benchmarking . j . sensors 2021 ,2021 , 2054828 . [ crossref ] 17 . gui , j. ; gu , d. ; wang , s. ; hu , h. a review of visual inertial odometry from ﬁltering and optimisation perspective . adv . robot . 2015 ,29 , 1–13 . [ crossref ] 18 . chen , c. ; zhu , h. ; li , m. ; you , s. a review of visual-inertial simultaneous localization and mapping from filtering-based and optimization-based perspectives . robotics 2018 ,7 , 45 . [ crossref ] 19 . huang , g. visual-inertial navigation : a concise review . in proceedings of the 2019 international conference on robotics and automation ( icra ) , montreal , qc , canada , 20–24 may 2019 ; pp . 9572–9582 . [ crossref ] 20 . chen , k. ; lai , y. ; hu , s. 3d indoor scene model from rgb-d data : a survey . comput . vis . media 2015 ,1 , 267–278 . [ crossref ] 21 . zhang , s. ; zheng , l. ; tao , w. survey and evaluation of rgb-d slam . ieee access 2021 ,9 , 21367–21387 . [ crossref ] 22 . campos , c. ; elvira , r. ; rodríguez , j.j.g . ; m. montiel , j.m . ; d. tardós , j. orb-slam3 : an accurate open-source library for visual , visual–inertial , and multimap slam . ieee trans . robot . 2021 ,37 , 1874–1890 . [ crossref ] 23 . burri , m. ; nikolic , j. ; gohl , p . ; schneider , t. ; rehder , j. ; omari , s. ; achtelik , m. ; siegwart , r. the euroc micro aerial vehicle datasets . int . j . robot . res . 2016 ,35 , 1157–1163 . [ crossref ] 24 . engel , j. ; schöps , t. ; cremers , d. lsd-slam : large-scale direct monocular slam . in computer vision–eccv 2014 ; fleet , d. , pajdla , t. , schiele , b. , tuytelaars , t. , eds . ; springer international publishing : cham , switzerland , 2014 ; pp . 834–849 . 25 . bianco , s. ; ciocca , g. ; marelli , d. evaluating the performance of structure from motion pipelines . j . imaging 2018 ,4 , 98 . [ crossref ] 26 . lepetit , v . ; moreno-noguer , f. ; fua , p . epnp : an accurate o ( n ) solution to the pnp problem . int . j. comput . vis . 2008 ,81 , 155 . [ crossref ] 27 . davison , a.j . ; reid , i.d . ; molton , n.d. ; stasse , o. monoslam : real-time single camera slam . ieee trans . pattern anal . mach . intell . 2007 ,29 , 1052–1067 . [ crossref ] [ pubmed ] 28 . loo , s.y . ; amiri , a. ; mashohor , s. ; tang , s. ; zhang , h. cnn-svo : improving the mapping in semi-direct visual odometry using single-image depth prediction . in proceedings of the 2019 international conference on robotics and automation ( icra ) , montreal , qc , canada , 20–24 may 2019 . 29 . boikos , k. ; bouganis , c.s . semi-dense slam on an fpga soc . in proceedings of the 2016 26th international conference on field programmable logic and applications ( fpl ) , lausanne , switzerland , 29 august–2 september 2016 ; pp . 1–4 . [ crossref ] 30 . engel , j. ; usenko , v . ; cremers , d. a photometrically calibrated benchmark for monocular visual odometry . arxiv 2016 , arxiv:1607.02555 . 31 . engel , j. ; koltun , v . ; cremers , d. direct sparse odometry . ieee trans . pattern anal . mach . intell . 2018 ,40 , 611–625 . [ crossref ] [ pubmed ] 32 . canovas , b. ; rombaut , m. ; nègre , a. ; pellerin , d. ; olympieff , s. speed and memory efﬁcient dense rgb-d slam in dynamic scenes . in proceedings of the iros 2020—ieee/rsj international conference on intelligent robots and systems , las vegas , nv , usa , 25–29 october 2020 ; pp . 4996–5001 . [ crossref ] 33 . bresson , g. ; alsayed , z. ; yu , l. ; glaser , s. simultaneous localization and mapping : a survey of current trends in autonomous driving . ieee trans . intell . veh . 2017 ,2 , 194–220 . [ crossref ] 34 . vincke , b. ; elouardi , a. ; lambert , a . design and evaluation of an embedded system base slam application . in proceedings of the 2010 ieee/sice international symposium on system integration , sendai , japan , 21–22 december 2010 ; pp . 224–229 . [ crossref ] 35 . vincke , b. ; elouardi , a. ; lambert , a. ; merigot , a. efﬁcient implementation of ekf-slam on a multi-core embedded system . in proceedings of the iecon 2012—38th annual conference on ieee industrial electronics society , montreal , qc , canada , 25–28 october 2012 ; pp . 3049–3054 . [ crossref ] 36 . klein , g. ; murray , d. parallel tracking and mapping for small ar workspaces . in proceedings of the 2007 6th ieee and acm international symposium on mixed and augmented reality , nara , japan , 13–16 november 2007 ; pp . 225–234 . [ crossref ] 37 . serrata , a.a.j . ; yang , s. ; li , r. an intelligible implementation of fastslam2.0 on a low-power embedded architecture . eurasip j. embed . syst . 2017 ,2017 , 27 . 38 . newcombe , r.a. ; lovegrove , s.j . ; davison , a.j . dtam : dense track and map in real-time . in proceedings of the 2011 international conference on computer vision , barcelona , spain , 6–13 november 2011 ; pp . 2320–2327 . 39 . ondrúška , p . ; kohli , p . ; izadi , s. mobilefusion : real-time volumetric surface reconstruction and dense tracking on mobile phones . ieee trans . vis . comput . graph . 2015 ,21 , 1251–1258 . [ crossref ] 40 . forster , c. ; pizzoli , m. ; scaramuzza , d. svo : fast semi-direct monocular visual odometry . in proceedings of the 2014 ieee international conference on robotics and automation ( icra ) , hong kong , china , 31 may–7 june 2014 ; pp . 15–22 . [ crossref ] 41 . mur-artal , r. ; montiel , j. ; tardos , j. orb-slam : a versatile and accurate monocular slam system . ieee trans . robot . 2015 , 31 , 1147–1163 . [ crossref ] 42 . forster , c. ; zhang , z. ; gassner , m. ; werlberger , m. ; scaramuzza , d. svo : semidirect visual odometry for monocular and multicamera systems . ieee trans . robot . 2017 ,33 , 249–265 . [ crossref ] robotics 2022 ,11 , 24 24 of 27 43 . boikos , k. ; bouganis , c.s . a high-performance system-on-chip architecture for direct tracking for slam . in proceedings of the 2017 27th international conference on field programmable logic and applications ( fpl ) , gent , belgium , 4–6 september 2017 ; pp . 1–7 . [ crossref ] 44 . mur-artal , r. ; tardós , j.d . orb-slam2 : an open-source slam system for monocular , stereo , and rgb-d cameras . ieee trans . robot . 2017 ,33 , 1255–1262 . [ crossref ] 45 . zhan , z. ; jian , w. ; li , y. ; yue , y . a slam map restoration algorithm based on submaps and an undirected connected graph . ieee access 2021 ,9 , 12657–12674 . [ crossref ] 46 . abouzahir , m. ; elouardi , a. ; latif , r. ; bouaziz , s. ; tajer , a. embedding slam algorithm : has it come of age ? robot . auton . syst . 2018 ,100 , 14–26 . [ crossref ] 47 . yu , j. ; gao , f. ; cao , j. ; yu , c. ; zhang , z. ; huang , z. ; wang , y. ; yang , h. cnn-based monocular decentralized slam on embedded fpga . in proceedings of the 2020 ieee international parallel and distributed processing symposium workshops ( ipdpsw ) , new orleans , la , usa , 18–22 may 2020 ; pp . 66–73 . [ crossref ] 48 . tateno , k. ; tombari , f. ; laina , i. ; navab , n. cnn-slam : real-time dense monocular slam with learned depth prediction . in proceedings of the 2017 ieee conference on computer vision and pattern recognition ( cvpr ) , honolulu , hi , usa , 21–26 july 2017 ; pp . 6565–6574 . [ crossref ] 49 . gao , x. ; wang , r. ; demmel , n. ; cremers , d. ldso : direct sparse odometry with loop closure . in proceedings of the 2018 ieee/rsj international conference on intelligent robots and systems ( iros ) , madrid , spain , 1–5 october 2018 . 50 . davison , a.j . scenelib 1.0 . 2006 . available online : http : //www.doc.ic.ac.uk/~ajd/scene/index.html ( access on 21 january 2022 ) . 51 . klein , g. ; murray , d. parallel tracking and mapping on a camera phone . in proceedings of the 2009 8th ieee international symposium on mixed and augmented reality , orlando , fl , usa , 19–22 october 2009 ; pp . 83–86 . 52 . oxford-ptam . available online : http : //github.com/oxford-ptam/ptam-gpl ( access on 21 january 2022 ) . 53 . opendtam . available online : http : //github.com/anuranbaka/opendtam ( access on 21 january 2022 ) . 54 . svo . available online : http : //github.com/uzh-rpg/rpg_svo ( access on 21 january 2022 ) . 55 . lsd-slam : large-scale direct monocular slam . available online : http : //github.com/tum-vision/lsd_slam ( access on 21 january 2022 ) . 56 . orb-slam2 . available online : http : //github.com/raulmur/orb_slam2 ( access on 21 january 2022 ) . 57 . cnn slam . available online : http : //github.com/iitmcvg/cnn_slam ( access on 21 january 2022 ) . 58 . dso : direct sparse odometry . available online : http : //github.com/jakobengel/dso ( access on 21 january 2022 ) . 59 . piat , j. ; fillatreau , p . ; tortei , d. ; brenot , f. ; devy , m. hw/sw co-design of a visual slam application . j.-real-time image process . 2018 . [ crossref ] 60 . dpu for convolutional neural network . available online : http : //www.xilinx.com/products/intellectual-property/dpu.html # overview ( access on 21 january 2022 ) . 61 . xu , z. ; yu , j. ; yu , c. ; shen , h. ; wang , y. ; yang , h. cnn-based feature-point extraction for real-time visual slam on embedded fpga . in proceedings of the 2020 ieee 28th annual international symposium on field-programmable custom computing machines ( fccm ) , fayetteville , ar , usa , 3–6 may 2020 ; pp . 33–37 . 62 . mourikis , a.i . ; roumeliotis , s.i . a multi-state constraint kalman filter for vision-aided inertial navigation . in proceedings of the 2007 ieee international conference on robotics and automation , roma , italy , 10–14 april 2007 ; pp . 3565–3572 . 63 . sun , k. ; mohta , k. ; pfrommer , b. ; watterson , m. ; liu , s. ; mulgaonkar , y. ; taylor , c.j . ; kumar , v . robust stereo visual inertial odometry for fast autonomous flight . ieee robot . autom . lett . 2018 ,3 , 965–972 . [ crossref ] 64 . li , s.p . ; zhang , t. ; gao , x. ; wang , d. ; xian , y. semi-direct monocular visual and visual-inertial slam with loop closure detection . robot . auton . syst . 2019 ,112 , 201–210 . [ crossref ] 65 . delmerico , j. ; scaramuzza , d. a benchmark comparison of monocular visual-inertial odometry algorithms for flying robots . in proceedings of the 2018 ieee international conference on robotics and automation ( icra ) , brisbane , australia , 21–25 may 2018 ; pp . 2502–2509 . [ crossref ] 66 . li , m. ; mourikis , a.i . improving the accuracy of ekf-based visual-inertial odometry . in proceedings of the 2012 ieee international conference on robotics and automation , saint paul , mi , usa , 14–18 may 2012 ; pp . 828–835 . 67 . leutenegger , s. ; lynen , s. ; bosse , m. ; siegwart , r. ; furgale , p . keyframe-based visual-inertial odometry using nonlinear optimization . int . j . robot . res . 2014 ,34 , 314–334 . [ crossref ] 68 . nikolic , j. ; rehder , j. ; burri , m. ; gohl , p . ; leutenegger , s. ; furgale , p .t . ; siegwart , r. a synchronize visual-inertial sensor system with fpga pre-processing for accurate real-time slam . in proceedings of the 2014 ieee international conference on robotics and automation ( icra ) , hong kong , china , 31 may–7 june 2014 ; pp . 431–437 . [ crossref ] 69 . bloesch , m. ; omari , s. ; hutter , m. ; siegwart , r. robust visual inertial odometry use a direct ekf-based approach . in proceedings of the 2015 ieee/rsj international conference on intelligent robots and systems ( iros ) , hamburg , germany , 28 september–3 october 2015 ; pp . 298–304 . [ crossref ] 70 . von stumberg , l. ; usenko , v . ; cremers , d. direct sparse visual-inertial odometry using dynamic marginalization . in proceedings of the 2018 ieee international conference on robotics and automation ( icra ) , brisbane , australia , 21–25 may 2018 ; pp . 2510–2517 . [ crossref ] robotics 2022 ,11 , 24 25 of 27 71 . mur-artal , r. ; tardós , j.d . visual-inertial monocular slam with map reuse . ieee robot . autom . lett . 2017 ,2 , 796–803 . [ crossref ] 72 . silveira , o.c.b . ; de melo , j.g.o.c . ; moreira , l.a.s . ; pinto , j.b.n.g . ; rodrigues , l.r.l . ; rosa , p .f.f . evaluating a visual simultaneous localization and mapping solution on embedded platforms . in proceedings of the 2020 ieee 29th international symposium on industrial electronics ( isie ) , delft , the netherlands , 17–19 june 2020 ; pp . 530–535 . [ crossref ] 73 . qin , t. ; li , p . ; shen , s. vins-mono : a robust and versatile monocular visual-inertial state estimator . ieee trans . robot . 2018 , 34 , 1004–1020 . [ crossref ] 74 . paul , m.k . ; wu , k. ; hesch , j.a . ; nerurkar , e.d . ; roumeliotis , s.i . a comparative analysis of tightly-coupled monocular , binocular , and stereo vins . in proceedings of the 2017 ieee international conference on robotics and automation ( icra ) , singapore , 29 may–3 june 2017 ; pp . 165–172 . [ crossref ] 75 . campos , c. ; montiel , j.m . ; tardós , j.d . inertial-only optimization for visual-inertial initialization . in proceedings of the 2020 ieee international conference on robotics and automation ( icra ) , paris , france , 31 may–31 august 2020 ; pp . 51–57 . [ crossref ] 76 . seiskari , o. ; rantalankila , p . ; kannala , j. ; ylilammi , j. ; rahtu , e. ; solin , a. hybvio : pushing the limits of real-time visual-inertial odometry . in proceedings of the ieee/cvf winter conference on applications of computer vision ( wacv ) , waikoloa , hi , usa , 4–8 january 2022 ; pp . 701–710 . 77 . merzlyakov , a. ; macenski , s. a comparison of modern general-purpose visual slam approaches . in proceedings of the 2021 ieee/rsj international conference on intelligent robots and systems ( iros ) , prague , czech republic , 27 september–1 october 2021 ; pp . 9190–9197 . [ crossref ] 78. dvo . available online : http : //github.com/daniilidis-group/msckf_mono ( access on 21 january 2022 ) . 79. msckf_vio . available online : http : //github.com/kumarrobotics/msckf_vio ( access on 21 january 2022 ) . 80 . okvis . available online : http : //github.com/ethz-asl/okvis ( access on 21 january 2022 ) . 81 . rovio . available online : http : //github.com/ethz-asl/rovio ( access on 21 january 2022 ) . 82 . vins-mono . available online : http : //github.com/hkust-aerial-robotics/vins-mono ( access on 21 january 2022 ) . 83 . vi-stereo-dso . available online : http : //github.com/ronaldsun/vi-stereo-dso ( access on 21 january 2022 ) . 84 . orb-slam3 : an accurate open-source library for visual , visual-inertial and multi-map slam . available online : http : //github.com/uz-slamlab/orb_slam3 ( access on 21 january 2022 ) . 85 . aslam , m.s . ; aziz , m.i . ; naveed , k. ; uz zaman , u.k. an rplidar base slam equip with imu for autonomous navigation of wheeled mobile robot . in proceedings of the 2020 ieee 23rd international multitopic conference ( inmic ) , bahawalpur , pakistan , 5–7 november 2020 ; pp . 1–5 . [ crossref ] 86 . nguyen , t.m . ; yuan , s. ; cao , m. ; nguyen , t.h . ; xie , l. viral slam : tightly coupled camera-imu-uwb-lidar slam . arxiv 2021 , arxiv:2105.03296 . 87 . chang , l. ; niu , x. ; liu , t. gnss/imu/odo/lidar-slam integrated navigation system using imu/odo pre-integration . sensors 2020 ,20 , 4702 . [ crossref ] 88 . zuñiga-noël , d. ; moreno , f.a . ; gonzalez-jimenez , j . an analytical solution to the imu initialization problem for visual-inertial systems . ieee robot . autom . lett . 2021 ,6 , 6116–6122 . [ crossref ] 89 . petit , b. ; guillemard , r. ; gay-bellile , v . time shifted imu preintegration for temporal calibration in incremental visual-inertial initialization . in proceedings of the 2020 international conference on 3d vision ( 3dv ) , fukuoka , japan , 25–28 november 2020 ; pp . 171–179 . [ crossref ] 90 . newcombe , r.a. ; izadi , s. ; hilliges , o. ; molyneaux , d. ; kim , d. ; davison , a.j . ; kohi , p . ; shotton , j. ; hodges , s. ; fitzgibbon , a. kinectfusion : real-time dense surface mapping and tracking . in proceedings of the 2011 10th ieee international symposium on mixed and augmented reality , basel , switzerland , 26–29 october 2011 ; pp . 127–136 . 91 . jin , q. ; liu , y. ; man , y. ; li , f. visual slam with rgb-d cameras . in proceedings of the 2019 chinese control conference ( ccc ) , guangzhou , china , 27–30 july 2019 ; pp . 4072–4077 . [ crossref ] 92 . nardi , l. ; bodin , b. ; zia , m.z . ; mawer , j. ; nisbet , a. ; kelly , p .h.j . ; davison , a.j . ; luján , m. ; o ’ boyle , m.f.p . ; riley , g.d. ; et al . introducing slambench , a performance and accuracy benchmarking methodology for slam . in proceedings of the 2015 ieee international conference on robotics and automation ( icra ) , seattle , wa , usa , 26–30 may 2015 ; pp . 5783–5790 . 93 . bodin , b. ; nardi , l. ; zia , m.z . ; wagstaff , h. ; shenoy , g.s . ; emani , m. ; mawer , j. ; kotselidis , c. ; nisbet , a. ; lujan , m. ; et al . integrating algorithmic parameter into benchmarking and design space exploration in 3d scene understanding . in proceedings of the 2016 international conference on parallel architecture and compilation techniques ( pact ) , haifa , israel , 11–15 september 2016 ; pp . 57–69 . [ crossref ] 94 . salas-moreno , r.f . ; newcombe , r.a. ; strasdat , h. ; kelly , p .h . ; davison , a.j . slam++ : simultaneous localisation and mapping at the level of objects . in proceedings of the 2013 ieee conference on computer vision and pattern recognition , portland , or , usa , 23–28 june 2013 ; pp . 1352–1359 . [ crossref ] 95 . kerl , c. ; sturm , j. ; cremers , d. dense visual slam for rgb-d camera . in proceedings of the 2013 ieee/rsj international conference on intelligent robots and systems , tokyo , japan , 3–7 november 2013 ; pp . 2100–2106 . 96 . endres , f. ; hess , j. ; sturm , j. ; cremers , d. ; burgard , w. 3-d mapping with an rgb-d camera . ieee trans . robot . 2014 ,30 , 177–187 . [ crossref ] 97 . kinectfusion . available online : http : //github.com/parikagoel/kinectfusion ( access on 21 january 2022 ) . 98. rgbdslam . available online : http : //ros.org/wiki/rgbdslam ( access on 21 january 2022 ) .robotics 2022 ,11 , 24 26 of 27 99. dvo . available online : http : //github.com/tum-vision/dvo ( access on 21 january 2022 ) . 100 . belshaw , m.s . ; greenspan , m.a . a high speed iterative closest point tracker on an fpga platform . in proceedings of the 2009 ieee 12th international conference on computer vision workshops , iccv workshops , kyoto , japan , 27 september–4 october 2009 ; pp . 1449–1456 . [ crossref ] 101 . williams , b . evaluation of a soc for real-time 3d slam . doctoral dissertation , iowa state university , ames , ia , usa , 2017 . 102 . gautier , q. ; shearer , a. ; matai , j. ; richmond , d. ; meng , p . ; kastner , r. real-time 3d reconstruction for fpgas : a case study for evaluate the performance , area , and programmability trade-off of the altera opencl sdk . in proceedings of the 2014 international conference on field-programmable technology ( fpt ) , shanghai , china , 10–12 december 2014 ; pp . 326–329 . [ crossref ] 103 . zhang , t. ; zhang , h. ; li , y. ; nakamura , y. ; zhang , l. flowfusion : dynamic dense rgb-d slam based on optical flow . in proceedings of the 2020 ieee international conference on robotics and automation ( icra ) , paris , france , 31 may–31 august 2020 ; pp . 7322–7328 . [ crossref ] 104 . dai , w. ; zhang , y. ; li , p . ; fang , z. ; scherer , s. rgb-d slam in dynamic environments using point correlations . ieee trans . pattern anal . mach . intell . 2020 ,44 , 373–389 . [ crossref ] 105 . ai , y. ; rui , t. ; lu , m. ; fu , l. ; liu , s. ; wang , s. ddl-slam : a robust rgb-d slam in dynamic environments combined with deep learning . ieee access 2020 ,8 , 162335–162342 . [ crossref ] 106 . deng , x. ; zhang , z. ; sintov , a. ; huang , j. ; bretl , t. feature-constrained active visual slam for mobile robot navigation . in proceedings of the 2018 ieee international conference on robotics and automation ( icra ) , brisbane , australia , 21–25 may 2018 ; pp . 7233–7238 . [ crossref ] 107 . jaenal , a. ; zuñiga-nöel , d. ; gomez-ojeda , r. ; gonzalez-jimenez , j . improving visual slam in car-navigated urban environ- ments with appearance maps . in proceedings of the 2020 ieee/rsj international conference on intelligent robots and systems ( iros ) , las vegas , nv , usa , 25–29 october 2020 ; pp . 4679–4685 . [ crossref ] 108 . li , d. ; shi , x. ; long , q. ; liu , s. ; yang , w. ; wang , f. ; wei , q. ; qiao , f. dxslam : a robust and efﬁcient visual slam system with deep features . in proceedings of the 2020 ieee/rsj international conference on intelligent robots and systems ( iros ) , las vegas , nv , usa , 25–29 october 2020 ; pp . 4958–4965 . [ crossref ] 109 . xu , q. ; kuang , h. ; kneip , l. ; schwertfeger , s. rethinking the fourier-mellin transform : multiple depths in the camera ’ s view . remote sens . 2021 ,13 , 1000 . [ crossref ] 110 . xu , q. ; chavez , a.g. ; bülow , h. ; birk , a. ; schwertfeger , s. improved fourier mellin invariant for robust rotation estimation with omni-cameras . in proceedings of the 2019 ieee international conference on image processing ( icip ) , taipei , taiwan , 22–25 september 2019 ; pp . 320–324 . [ crossref ] 111 . scona , r. ; jaimez , m. ; petillot , y.r . ; fallon , m. ; cremers , d. staticfusion : background reconstruction for dense rgb-d slam in dynamic environments . in proceedings of the 2018 ieee international conference on robotics and automation ( icra ) , brisbane , australia , 21–25 may 2018 ; pp . 3849–3856 . [ crossref ] 112 . soares , j.c.v . ; gattass , m. ; meggiolaro , m.a . visual slam in human populated environments : exploring the trade-off between accuracy and speed of yolo and mask r-cnn . in proceedings of the 2019 19th international conference on advanced robotics ( icar ) , horizonte , brazil , 2–6 december 2019 ; pp . 135–140 . [ crossref ] 113 . soares , j.c.v . ; gattass , m. ; meggiolaro , m.a . crowd-slam : visual slam towards crowded environments use object detection . j. intell . robot . syst . 2021 ,102 , 50 . [ crossref ] 114 . van opdenbosch , d. ; aykut , t. ; alt , n. ; steinbach , e. efﬁcient map compression for collaborative visual slam . in proceedings of the 2018 ieee winter conference on applications of computer vision ( wacv ) , lake tahoe , nv , usa , 12–15 march 2018 ; pp . 992–1000 . [ crossref ] 115 . wan , z. ; yu , b. ; li , t. ; tang , j. ; wang , y. ; raychowdhury , a. ; liu , s. a survey of fpga-based robotic computing . ieee circuits syst . mag . 2021 ,21 , 48–74 . [ crossref ] 116 . li , r. ; wang , s. ; long , z. ; gu , d. undeepvo : monocular visual odometry through unsupervised deep learning . in proceedings of the 2018 ieee international conference on robotics and automation ( icra ) , brisbane , australia , 21–25 may 2018 ; pp . 7286–7291 . [ crossref ] 117 . li , r. ; wang , s. ; gu , d. deepslam : a robust monocular slam system with unsupervised deep learning . ieee trans . ind . electron . 2021 ,68 , 3577–3587 . [ crossref ] 118 . kang , r. ; shi , j. ; li , x. ; liu , y. ; liu , x. df-slam : a deep-learning enhanced visual slam system base on deep local features . arxiv 2019 , arxiv:1901.07223 119 . zhao , c. ; sun , q. ; zhang , c. ; tang , y. ; qian , f. monocular depth estimation base on deep learning : an overview . sci . china technol . sci . 2020 ,63 , 1612–1627 . [ crossref ] 120 . xiaogang , r. ; wenjing , y. ; jing , h. ; peiyuan , g. ; wei , g. monocular depth estimation based on deep learning : a survey . in proceedings of the 2020 chinese automation congress ( cac ) , shanghai , china , 6–8 november 2020 ; pp . 2436–2440 . [ crossref ] 121 . ming , y. ; meng , x. ; fan , c. ; yu , h. deep learning for monocular depth estimation : a review . neurocomputing 2021 ,438 , 14–33 . [ crossref ] 122 . doherty , k. ; fourie , d. ; leonard , j. multimodal semantic slam with probabilistic data association . in proceedings of the 2019 international conference on robotics and automation ( icra ) , montreal , qc , canada , 20–24 may 2019 ; pp . 2419–2425 . [ crossref ] robotics 2022 ,11 , 24 27 of 27 123 . cao , y. ; hu , l. ; kneip , l. representations and benchmarking of modern visual slam systems . sensors 2020 ,20 , 2572 . [ crossref ] [ pubmed ] 124 . sun , y. ; liu , m. ; meng , m.q.h . improving rgb-d slam in dynamic environment : a motion removal approach . robot . auton . syst . 2017 ,89 , 110–122 . [ crossref ] 125 . sun , y. ; liu , m. ; meng , m.q.h . motion removal for reliable rgb-d slam in dynamic environment . robot . auton . syst . 2018 , 108 , 115–128 . [ crossref ] 126 . xiao , l. ; wang , j. ; qiu , x. ; rong , z. ; zou , x. dynamic-slam : semantic monocular visual localization and mapping base on deep learning in dynamic environment . robot . auton . syst . 2019 ,117 , 1–16 . [ crossref ] 127 . bescos , b. ; campos , c. ; tardós , j.d . ; neira , j. dynaslam ii : tightly-coupled multi-object tracking and slam . ieee robot . autom . lett . 2021 ,6 , 5191–5198 . [ crossref ] 128 . sturm , j. ; engelhard , n. ; endres , f. ; burgard , w. ; cremers , d. a benchmark for the evaluation of rgb-d slam system . in proceedings of the 2012 ieee/rsj international conference on intelligent robots and systems , algarve , portugal , 7–12 october 2012 ; pp . 573–580 . [ crossref ] 129 . geiger , a. ; lenz , p . ; stiller , c. ; urtasun , r. vision meet robotics : the kitti dataset . int . j . robot . res . 2013 ,32 , 1231–1237 . [ crossref ] 130 . handa , a. ; whelan , t. ; mcdonald , j. ; davison , a.j . a benchmark for rgb-d visual odometry , 3d reconstruction and slam . in proceedings of the 2014 ieee international conference on robotics and automation ( icra ) , hong kong , china , 31 may–7 june 2014 ; pp . 1524–1531 . [ crossref ] 131 . whelan , t. ; kaess , m. ; johannsson , h. ; fallon , m. ; leonard , j.j. ; mcdonald , j. real-time large-scale dense rgb-d slam with volumetric fusion . int . j . robot . res . 2015 ,34 , 598–626 . [ crossref ] 132 . schubert , d. ; goll , t. ; demmel , n. ; usenko , v . ; stückler , j. ; cremers , d. the tum vi benchmark for evaluating visual-inertial odometry . in proceedings of the 2018 ieee/rsj international conference on intelligent robots and systems ( iros ) , madrid , spain , 1–5 october 2018 ; pp . 1680–1687 . [ crossref ] 133 . rgb-d slam dataset and benchmark . available online : http : //vision.in.tum.de/data/datasets/rgbd-dataset ( access on 21 january 2022 ) . 134 . kitti-360 . available online : http : //www.cvlibs.net/datasets/kitti/ ( access on 21 january 2022 ) . 135 . icl-nuim . available online : http : //www.doc.ic.ac.uk/~ahanda/vafric/iclnuim.html ( access on 21 january 2022 ) . 136 . the euroc mav dataset . available online : http : //projects.asl.ethz.ch/datasets/doku.php ? id=kmavvisualinertialdatasets ( access on 21 january 2022 ) . 137 . monocular visual odometry dataset . available online : http : //vision.in.tum.de/mono-dataset ( access on 21 january 2022 ) . 138 . visual-inertial dataset . available online : http : //vision.in.tum.de/data/datasets/visual-inertial-dataset ( access on 21 january 2022 ) . view publication stats","['see discussion , st at , and author pr ofiles f or this public ation at : http : //www .researchgate.ne t/public ation/358523574 a comprehensive su rvey of visual slam algorithms article in robotics · februar y 2022 doi : 10.3390/r obotics11010024 citations 266reads 7,176 5 author s , include : andr éa mac ario barr os atomic ener gy and alt ernativ e ener gi commission 6 publica tions 276 citations see profile yoann moline atomic ener gy and alt ernativ e ener gi commission 21 publica tions 310 citations see profile frédérick carr el atomic ener gy and alt ernativ e ener gi commission 139 publica tions 1,499 citations see profile maug an michel atomic ener gy and alt ernativ e ener gi commission 20 publica tions 348 citations see profile all c ontent f ollo wing this p age be uplo aded b y andr éa mac ario barr o on 11 f ebruar y 2022 . the user have r equest ed enhanc ement of the do wnlo aded file./gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045 /gid00001 /gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046citation : macario barros , a. ; michel , m. ; moline , y. ; corre , g. ; carrel , f. a comprehensive survey of visual slam algorithms . robotics 2022 ,11 , 24. http : //doi.org/10.3390/ robotics11010024 academic editor : dario richiedei received : 12 december 2021 accepted : 7 february 2022 published : 10 february 2022 publisher ’ s note : mdpi stay neutral with regard to jurisdictional claim in publish map and institutional afﬁl- iations . copyright : © 2022 by the author . licensee mdpi , basel , switzerland . this article be an open access article distribute under the term and condition of the creative commons attribution ( cc by ) license ( http : // creativecommons.org/licenses/by/ 4.0/ ) . robotics review a comprehensive survey of visual slam algorithms andréa macario barros * , maugan michel , yoann moline , gwenolé corre and frédérick carrel laboratoire capteurs et architectures électroniques ( lcae ) , laboratoire d ’ intégration des systèmes et de', 'macario barros * , maugan michel , yoann moline , gwenolé corre and frédérick carrel laboratoire capteurs et architectures électroniques ( lcae ) , laboratoire d ’ intégration des systèmes et de technologies ( list ) , commissariat à l ’ énergie atomique et aux énergies alternatives ( cea ) , 91400 saclay , france ; maugan.michel @ cea.fr ( m.m . ) ; yoann.moline @ cea.fr ( y.m . ) ; gwenole.corre @ cea.fr ( g.c . ) ; frederick.carrel @ cea.fr ( f.c . ) *correspondence : andrea.barros @ cea.fr ; tel . : +33-1-69-08-22-59 abstract : simultaneous localization and mapping ( slam ) technique be widely research , since they allow the simultaneous creation of a map and the sensor ’ pose estimation in an unknown environment . visual-based slam technique play a signiﬁcant role in this ﬁeld , a they be base on a low-cost and small sensor system , which guarantee those advantage compare to other sensor- base slam technique . the literature present different approach and method to implement visual-based slam system . among this variety of publication , a beginner in this domain may ﬁnd problem with identify and analyze the main algorithm and select the most appropriate one accord to his or her project constraint . therefore , we present the three main visual-based slam approach ( visual-only , visual-inertial , and rgb-d slam ) , provide a review of the main algorithm of each approach through diagram and ﬂowcharts , and highlight the main advantage and disadvantage of each technique . furthermore , we propose six criterion that ease the slam algorithm ’ s analysis and consider both the software and hardware level . in addition , we present some major issue and future direction on visual-slam ﬁeld , and provide a general overview of some of the exist benchmark datasets . this work aim to be the ﬁrst step for those initiate a slam project to have a good perspective of slam technique ’ main element and characteristic . keywords : embed slam ; evaluation criterion ; rgb-d slam ; visual-inertial slam ; visual-slam ; 3d', 'a slam project to have a good perspective of slam technique ’ main element and characteristic . keywords : embed slam ; evaluation criterion ; rgb-d slam ; visual-inertial slam ; visual-slam ; 3d reconstruction 1 . introduction simultaneous localization and mapping ( slam ) technology , ﬁrst propose by smith in 1986 [ 1 ] , be use in an extensive range of application , especially in the domain of augmented reality ( ar ) [ 2–4 ] and robotics [ 5–7 ] . the slam process aim at map an unknown environment and simultaneously locate a sensor system in this environment through the signal provide by the sensor ( s ) . in robotics , the construction of a map be a crucial task , since it allow the visualization of landmark , facilitate the environment ’ s visualization . in addition , it can help in the state estimation of the robot , relocate it , and decrease estimation error when re-visiting register area [ 8 ] . the map construction come with two other task : localization and path planning . according to stachniss [ 9 ] , the mapping problem may be describe by examine three question consider the robot ’ s perspective : what do the world look like ? where be i ? and how can i reach a give location ? the ﬁrst question be clariﬁed by the mapping task , which search to construct a map , i.e. , a model of the environment . to do so , it require the location of the observed landmark , i.e. , the answer for the second question , provide by the localization task . the localization task search to determine the robot ’ s pose , i.e. , it orientation and position and , consequently , locate the robot on the map . depending on the ﬁrst two task , the path planning clear up the last question , and seek to estimate a trajectory for the robot to achieve a give location . it rely on the current robot ’ s pose , provide by the localization task , and on the environment ’ s characteristic , provide by the mapping task . slam be a solution that integrate both the mapping and localization task . robotics 2022 ,11 , 24. http :', 'localization task , and on the environment ’ s characteristic , provide by the mapping task . slam be a solution that integrate both the mapping and localization task . robotics 2022 ,11 , 24. http : //doi.org/10.3390/robotics11010024 http : //www.mdpi.com/journal/roboticsrobotics 2022 ,11 , 24 2 of 27 visual-based slam algorithm can be consider especially attractive , due to their sensor conﬁguration simplicity , miniaturize size , and low cost . therefore , numerous visual-based technique be propose in the literature , which make the choice of the most suitable one accord to one ’ s project constraint difﬁcult . the visual-based approach can be divide into three main category : visual-only slam , visual-inertial ( vi ) slam , and rgb-d slam . the ﬁrst one refers to the slam technique base only on 2d image provide by a monocular or stereo camera . they present a high technical difﬁculty due to their limited visual input [ 10 ] . the robustness in the sensor ’ s track of the visual-slam algorithm may be increase by add an inertial measurement unit ( imu ) , which can be find in their miniaturize size and low cost , while achieve high accuracy , essential aspect to many application that require lightweight design , such a autonomous race car [ 11 ] . in addition , visual-based slam system may employ a depth sensor and process the depth information by apply a rgb-d-based approach . to obtain a general overview and an introduction to the slam problem , the work by durrant-white and bailey [ 12,13 ] propose a slam tutorial presenting from the problem description to the environment representation . in addition , cadena et al . [ 8 ] analyze the main open problem and future perspective of the slam . considering the review and survey of visual-based technique , yousif et al . [ 14 ] and fuentes-pacheco et al . [ 15 ] present an overview of the main concept use in the visual-only slam technique and the fundamental algorithm . yousif et al . [ 14 ] also brieﬂy describe the rgb-d-based slam problem . taketomi et al . [ 10', 'an overview of the main concept use in the visual-only slam technique and the fundamental algorithm . yousif et al . [ 14 ] also brieﬂy describe the rgb-d-based slam problem . taketomi et al . [ 10 ] and covolan et al . [ 3 ] present an overview of the main concept use in the visual-based slam approach , focus on visual-only and rgb-d-based approach and describe the main algorithm . the recent publication by servieres et al . [ 16 ] propose a classiﬁcation of the main visual-based slam algorithm and perform a historical research . gui et al . [ 17 ] and chen et al . [ 18 ] present the main concept and algorithm of visual- inertial slam and visual-inertial odometry approach , consider the ﬁltering-based and optimization-based perspective . in [ 17 ] , the technique up to 2015 be present , while in [ 18 ] , the algorithm up to 2018 be also include . an overview of the main concept and technique in visual-inertial navigation can also be find in [ 19 ] . concerning the rgb-d approach , chen et al . [ 20 ] present a global perspective from the main concept use in rgb-d modeling . a recent survey by zhang et al . [ 21 ] present an overview of the main concept and describe the principal rgb-d-based slam algorithm . as one can see , there be several review and survey in the literature about visual-based slam technique ; still , most of them be limited to just one or two of the three main approach and do not address the algorithm in detail . so , a review that address the three approach and the fundamental algorithm be necessary to help researcher and student to initiate their work on visual-slam technique to obtain an overview of this large domain . thus , this paper provide a review of the most representative visual-based slam technique and an overview of each method ’ s main advantage and disadvantage . this article present three main contribution : 1—an explanation of the most representative visual-based slam algorithm through the construction of diagram and ﬂowcharts . this approach will be helpful to the reader , a', 'three main contribution : 1—an explanation of the most representative visual-based slam algorithm through the construction of diagram and ﬂowcharts . this approach will be helpful to the reader , a it provide an overview of the slam technique when initiate a project and allow the reader to have a ﬁrst contact with the visual-based slam algorithm . 2—as far a we know , this be the ﬁrst review article that present the three main visual-based approach , perform an individual analysis of each method and a general analysis of the approach . 3—focusing on the reader initiate their study on the slam algorithm , we propose six main criterion to be observe in the different technique and implementation to be consider accord to one ’ s application . the requirement consider , from a software level , slam technique , such a loop closure , to a hardware-level approach , such a slam on soc implementation . this paper be organize a follow . section 2 present the main essential concept of the three select approach . section 3 present the six criterion that we establish to evaluate the slam algorithm and the most representative slam technique accordingrobotics 2022 ,11 , 24 3 of 27 to the present approach . section 4 present some of the recent major issue face by the visual-slam community and point out future direction to deal with these problem . section 5 provide a general overview of some of the most signiﬁcant publicly available benchmark datasets . finally , our conclusion be present in section 6 . 2 . visual-based slam concepts this section present concept relate to visual-based slam and odometry algo- rithms , and the main characteristic of the visual-based approach cover in this paper . the visual-based slam technique use one or more camera in the sensor system , receive 2d image a the source of information . in general , the visual-based slam algorithm be divide into three main thread : initialization , track , and map [ 10 ] . figure 1 show a general view of the three main part generally present in visual-based slam', 'the visual-based slam algorithm be divide into three main thread : initialization , track , and map [ 10 ] . figure 1 show a general view of the three main part generally present in visual-based slam approach . initializationtracking + mapping input data determination of the global coordinate system dense reconstructionsparse reconstruction semi-dense reconstruction2d image 2d image + imu data 2d image + depth data+ + roll pitch yaw figure 1 . general component of a visual-based slam . the depth and inertial data may be add to the 2d visual input to generate a sparse map ( generate with the orb-slam3 algorithm [ 22 ] in the mh_01 sequence [ 23 ] ) , semi-dense map ( obtain with the lsd-slam [ 24 ] in the dataset provide by the author ) , and a dense reconstruction ( reprinted from [ 25 ] ) . as one can see in the figure , in visual-slam system , the input can be a 2d image , both a 2d image and imu data , or a 2d image and depth data , depend on the used approach , i.e. , visual-only ( section 2.1 ) , visual-inertial ( section 2.2 ) , or rgb-d-based ( section 2.3 ) , respectively . the initialization determine the global coordinate and build an initial map , use to perform the two main step : tracking and mapping . the track process be responsible for the continuous estimation of the sensor ’ s pose . in general , the algorithm establish 2d–3d correspondence between the current frame and map , constitute a problem call perspective-n-points . there be several way to solve this problem , epnp be one of the most representative solution [ 26 ] . the mapping process be in charge of compute and expand the 3d structure a the camera move . the depth data computa- tion differs accord to the employ algorithm ( section 3 address individually each algorithm provide detailed explanation ) . finally , the mapping process shall result in a sparse , semi-dense , or dense 3d reconstruction , accord to the implement technique . although we mainly refer to the concept a belonging to the slam methodology , we consider , in this', 'result in a sparse , semi-dense , or dense 3d reconstruction , accord to the implement technique . although we mainly refer to the concept a belonging to the slam methodology , we consider , in this paper , both visual-slam and visual-odometry ( vo ) technique , since they be closely relate . the vo algorithm also seek to estimate a robot ’ s position through camera a a source of information . the main difference between visual-slam and vo lie in consider , or not , the global consistency of the estimate trajectory and map [ 14 ] .robotics 2022 ,11 , 24 4 of 27 while vo perform only local optimization , visual-slam algorithm also employ loop closure detection ( see section 3 ) , be capable of correct drift accumulate at the end of the robot ’ s trajectory . 2.1 . visual-only slam the visual-only slam system be base on 2d image processing . after the image ’ acquisition from more than one point of view , the system perform the initialization process to deﬁne a global coordinate system and reconstruct an initial map . in the feature-based algorithm rely on ﬁlters ( ﬁltering-based algorithm ) , the ﬁrst step consist of initialize the map point with high uncertainty , which may converge later to their actual position . this procedure be follow by track , which attempt to estimate the camera pose . simultaneously , the mapping process include new point in the 3d reconstruction a more unknown scene be observe . the visual-only slam system may use a monocular or stereo camera . the monocular camera-based slam be a well-explored domain give the small size of the sensor ( the small of all the presented approach ) , it low price , easy calibration , and reduced power consumption [ 27 ] . despite these advantage , the monocular-based system offer a high complexity in system ’ s initialization , since at least two different view be necessary to determine an initial depth , and pose estimation and problem concern drift and scale estimation . this last problem may be compensate by stereo camera , which present the main', 'view be necessary to determine an initial depth , and pose estimation and problem concern drift and scale estimation . this last problem may be compensate by stereo camera , which present the main advantage to feature the stereo view in only one frame . however , the sensor ’ s size be more signiﬁcant than a simple monocular camera . in addition , it require more processing for each frame , mainly due to the need for an image rectiﬁcation process in the stereo matching stage . the visual-only slam category can be divide into two main method : feature-based and direct . 2.1.1 . feature-based methods slam algorithm base on feature consider a certain number of point of interest , call keypoints . they can be detect in several image and match by compare their descriptor ; this process provide the camera pose estimation information . the descriptor data and keypoint location compose the feature , i.e. , the data use by the algorithm to process the tracking and mapping . as the feature-based method do not use all the frame information , they be suitable to ﬁgure in embedded implementation . however , the feature extraction may fail in a textureless environment [ 28 ] , as well a it generate a sparse map , provide less information than a dense one . 2.1.2 . direct methods in contrast with the feature-based method , the direct method use the sensor data without pre-processing , consider pixel ’ intensity , and minimize the photometric er- ror . there be many different algorithm base on this methodology , and depend on the chosen technique , the reconstruction may be dense , semi-dense , or sparse . the reconstruc- tion density be a substantial constraint to the algorithm ’ s real-time operation , since the joint optimization of both structure and camera position be more computationally expensive for dense and semi-dense reconstruction than for a sparse one [ 29 ] . figure 2 show the main difference between feature-based ( indirect ) and direct method accord to their front-end and back-end , that be , the part of the', 'reconstruction than for a sparse one [ 29 ] . figure 2 show the main difference between feature-based ( indirect ) and direct method accord to their front-end and back-end , that be , the part of the algorithm responsible for sensor ’ s data abstraction and the part responsible for the interpretation of the abstract data , respectively.robotics 2022 ,11 , 24 5 of 27 input image feature processing tracking and sparse map front-end back-end tracking and sparse map front-end back-endinput image tracking and dense map back-end figure 2 . general difference between feature-based and direct method . top : main step follow by the feature-based method , result in a sparse reconstruction ( map generate with the orb-slam3 algorithm [ 22 ] in the mh_01/euroc sequence [ 23 ] ) . bottom : main step follow by a direct method , that may result in a sparse ( generate from the reconstruction of sequence_02 /tum monovo [ 30 ] with the dso algorithm [ 31 ] ) or dense reconstruction ( reprinted from [ 25 ] ) , accord to the chosen technique . 2.2 . visual-inertial slam the vi-slam approach incorporate inertial measurement to estimate the structure and the sensor pose . the inertial data be provide by the use of an inertial measurement unit ( imu ) , which consist of a combination of gyroscope , accelerometer , and , additionally , magnetometer device . this way , the imu be capable of provide information relative to the angular rate ( gyroscope ) and acceleration ( accelerometer ) along the x- , y- , and z-axes , and , additionally , the magnetic ﬁeld around the device ( magnometer ) . while add an imu may increase the information richness of the environment and provide high accuracy , it also increase the algorithm ’ s complexity , especially during the initialization step , since , besides the initial estimation of the camera pose , the algorithm also have to estimate the imu pose . vi-slam algorithm can be divide accord to the type of fusion between the camera and imu data , which can be loosely or tightly couple . the loosely', 'the algorithm also have to estimate the imu pose . vi-slam algorithm can be divide accord to the type of fusion between the camera and imu data , which can be loosely or tightly couple . the loosely couple method do not merge the imu state to estimate the full pose : instead , the imu data be use to estimate the orientation and change in the sensor ’ s position [ 18 ] . on the other side , the tightly couple method be base on the fusion of camera and imu data into a motion equation , result in a state estimation that consider both data . in addition , vi-slam algorithms present different implementation accord to their back-end approach , which can be ﬁltering-based or optimization-based . the front-end of ﬁltering-based approach for vi-slam relies on feature extraction , while optimization- base method ( also know a keyframe-based approach ) rely on global optimization , which increase the system ’ s accuracy , as well a the algorithm ’ s computational cost.robotics 2022 ,11 , 24 6 of 27 2.3 . rgb-d slam slam system base on rgb-d data start to attract more attention with the advent of microsoft ’ s kinect in 2010 . rgb-d sensor consist of a monocular rgb camera and a depth sensor , allow slam system to directly acquire the depth information with a feasible accuracy accomplish in real-time by low-cost hardware . as the rgb-d device directly provide the depth map to the slam system , the general framework of slam base on this approach differ from the other one already present . most of the rgb-d-based system make use of the iterative closest point ( icp ) al- gorithm to locate the sensor , fuse the depth map to obtain the reconstruction of the whole structure . rgb-d system present advantage such a provide color image data and dense depth map without any pre-processing step , hence decrease the complexity of the slam initialization [ 10 ] . despite this , this approach be most suitable to indoor environment , and require large memory and power consumption [ 32 ] . 3 . visual-slam algorithms each consider approach', 'initialization [ 10 ] . despite this , this approach be most suitable to indoor environment , and require large memory and power consumption [ 32 ] . 3 . visual-slam algorithms each consider approach present in section 2 include several algorithm , make it difﬁcult to select the most suitable slam or odometry algorithm accord to one ’ s project constraint . therefore , we present the most representative algorithm of each approach , select base on literature feedback , to accomplish a brief review of each one , and a systematic analysis base on six select criterion that , in general , be present a limit factor of slam project . besides the propose criterion , it be also necessary to characterize the scene and application , since some scenario may present speciﬁc attribute that may imply speciﬁc evaluation criterion , such a the analysis present in [ 33 ] . the author consider the autonomous drive application characteristic , which imply a set of speciﬁc criterion , such a the required accuracy , scalability , dynamicity , etc . thus , consider the general approach of the slam system , we establish six criterion that inﬂuence system dimension , accuracy , and hardware implementation . they be : algorithm type , map density , global optimization , loop closure , availability , and embed implementation : • algorithm type : this criterion indicate the methodology adopt by the algorithm . for the visual-only algorithm , we divide them into feature-based , hybrid , and direct method . considering the visual-inertial algorithm , they must be ﬁltering-based or optimization-based method . lastly , the rgb-d approach can be divide concern their tracking method , which can be direct , hybrid , or feature-based . • map density : in general , dense reconstruction require more computational resource than a sparse one , have an impact on memory usage and computational cost . on the other hand , it provide a more detailed and accurate reconstruction , which may be a key factor in a slam project . • global optimization : slam', 'impact on memory usage and computational cost . on the other hand , it provide a more detailed and accurate reconstruction , which may be a key factor in a slam project . • global optimization : slam algorithm may include global map optimization , which refer to the technique that search to compensate the accumulative error introduce by the camera movement , consider the consistency of the entire structure . • loop closure : the loop closing detection refers to the capability of the slam algorithm to identify the image that be previously detect by the algorithm to estimate and correct the drift accumulate during the sensor movement . • availability : several slam algorithm be open source and make available by the author or have their implementation make available by third party , facilitate their usage and reproduction . • embedded implementation : the embedded slam implementation be an emerge ﬁeld use in several application , especially in robotics and automobile domain . this criterion depend on each algorithm ’ s hardware constraint and speciﬁcity , since there must be a trade-off between algorithm architecture in term of energy consumption , memory , and processing usage . we assemble the main publication we find present fully embed slam system in platform such a microcontrollers and fpga boards.robotics 2022 ,11 , 24 7 of 27 in the following , we present the select slam algorithms consider the most repre- sentative of each of the three present approach accord to their publication year . 3.1 . visual-only slam the select visual-only slam algorithm be present in figure 3 and explain in the following subsection . monoslam2007 ptam 2007dtam2011 svo 2014lsd-slam2014 orb-slam22017 cnn-vslam 2017dso2018 figure 3 . timeline represent the most representative visual-only slam algorithm . 3.1.1 . monoslam ( 2007 ) the ﬁrst monocular slam algorithm be monoslam , which be propose by david- son et al . [ 27 ] in 2007 . the ﬁrst step of the algorithm consist of the system ’ s initialization . then , it update the state', 'monocular slam algorithm be monoslam , which be propose by david- son et al . [ 27 ] in 2007 . the ﬁrst step of the algorithm consist of the system ’ s initialization . then , it update the state vector consider a constant velocity motion model , where the camera motion and environment structure be estimate in real-time use an extended kalman ﬁlter ( ekf ) . the algorithm be represent by figure 4 . monoslam operate in real-time and be make available by the author . moreover , since monoslam be base on ekf , an already well-covered topic , several embed implementation base on this algorithm be find in the literature . in [ 34,35 ] , vincke et al . base their implementation on the monoslam algorithm , combine multiple sensor and a multi-processor architecture to evaluate it implementation . in [ 34 ] , the author use an arm + dsp + gpu architec- ture ( omap3530 architecture ) to implement the localization , reconstruction , and feature detection . they combine this architecture with a co-processor atmega168 use for data pre-processing and robot control . in [ 35 ] , they base the architecture on a combination of multi-cpus + gpus provide by the use of an omap4430 architecture . the author implement the different task of the algorithm into both single-core and dual-core arm architecture , and compare their performance . in addition , they parallelize the matching and initialization task use the arm and neon processor provide by the omap4430 . monoslam require a known target for the initialization step , which be not always accessible . in addition , the algorithm ’ s complexity increase proportionally with the size of the environment . this algorithm neither employ global optimization technique nor loop closure detection . at last , it only reconstruct a map of landmark , which may be a drawback regard the application that require a more accurate reconstruction . initializationprediction cinematic model ekf tracking shi & tomasi depth estimation partticle filter correlationcorrection ekf noise addition figure 4 .', 'that require a more accurate reconstruction . initializationprediction cinematic model ekf tracking shi & tomasi depth estimation partticle filter correlationcorrection ekf noise addition figure 4 . diagram represent the monoslam algorithm . 3.1.2 . parallel tracking and mapping ( 2007 ) another pioneer algorithm be the parallel tracking and mapping ( ptam ) [ 36 ] algo- rithm . ptam be the ﬁrst algorithm to separate tracking and mapping into two differentrobotics 2022 ,11 , 24 8 of 27 thread and to apply the concept of keyframes to the mapping thread . first , the mapping thread perform the map initialization . new keyframes be add to the system a the camera move and the initial map be expand . triangulation between two consecutive keyframes calculate the new point ’ s depth information . the track thread compute the camera pose , and for each new frame , it estimate an initial pose for perform the projection of the map point on the image . ptam use the correspondence to compute the camera pose by minimize the reprojection error . figure 5 represent the step perform by the ptam algorithm . new frameprior pose estimationcoarsest- scale feature searchingcamera pose updatingfinal pose estimationmap points projectionfine-scale feature search new keyframe ? update keyframe data associationintegrate keyframe add new featureslocally converged ? globally converged ? local bundle adjustmentglobal bundle adjustment repeat cycleupdate data associationtracking thread m a p p i n g t h r e a dyesno yes no noyes figure 5 . diagram represent the ptam algorithm . ptam allow the map representation by a large number of feature and performs global optimization . despite these advantage , the ptam algorithm present a high com- plexity due to the bundle adjustment step . in addition , it do not count with loop closure , and the generated map be more suitable to identify landmark . furthermore , it require the user ’ s interaction to establish the initial keyframes , and it present a non-negligible power consumption , which make it', 'map be more suitable to identify landmark . furthermore , it require the user ’ s interaction to establish the initial keyframes , and it present a non-negligible power consumption , which make it unsuitable for low-cost embed system [ 37 ] . 3.1.3 . dense tracking and mapping ( 2011 ) dense track and map ( dtam ) , propose by newcombe et al . [ 38 ] , be the ﬁrst fully direct method in the literature . the algorithm be divide into two main part : dense mapping and dense tracking . the ﬁrst stage search to estimate the depth value by deﬁning data cost volume represent the average photometric error of multiple frame compute for the inverse depth of the current frame . the inverse depth that minimize the photometric error be select to integrate the reconstruction . in the dense tracking stage , dtam estimate the motion parameter by align an image from the dense model project in a virtual camera and the current frame . figure 6 show a general view of the dtam algorithm . the algorithm provide an accurate and detailed reconstruction , but this level of density reconstruction impact the computational cost to store and process the data . as a consequence , to achieve real-time operation , the algorithm require state-of-the-art gpus [ 10 ] . the author in [ 39 ] employ a cpu + gpu architecture of different iphone model to implement a fully dense algorithm base on dtam . they use the cpu for the tracking task and the gpu for depth estimation and frame fusion . dtam do not implement loop closure technique or global optimization . new frameptam ’ s tracking dense trackingdepth map estimationcost volume 3d surface figure 6 . diagram represent the dtam algorithm.robotics 2022 ,11 , 24 9 of 27 3.1.4 . semi-direct visual odometry ( 2014 ) the semi-direct visual odometry ( svo ) algorithm [ 40 ] combine the advantage of both feature-based and direct method . the algorithm be divide into two main thread : motion estimation and mapping . the ﬁrst thread search to estimate the sensor ’ s motion parameter , which consist of', 'feature-based and direct method . the algorithm be divide into two main thread : motion estimation and mapping . the ﬁrst thread search to estimate the sensor ’ s motion parameter , which consist of minimize the photometric error . the mapping thread be base on probabilistic depth ﬁlters , and it search to estimate the optimum depth value for each 2d feature . when the algorithm achieve a low uncertainty , it insert the 3d point in the reconstruction , a show in figure 7 . svo enable direct pixel correspondence and the usage of a probabilistic mapping method . in addition , the algorithm be capable of operate with a high frame rate , since it do not need to extract feature for every frame [ 41 ] , which enable it operation in a low-cost embedded system , a with the embedded platform consider by [ 40 ] that consist in an odroid-u2 . nonetheless , svo present a limited accuracy due to the short-term data association [ 22 ] . svo do not implement global optimization technique or loop closure . the author already propose an extended version of the svo , svo 2.0 [ 42 ] , in which the algorithm be capable of process stereo data and imu information . last frameimage alignementpose and structure refinementfeature alignment keyframe ? motion estimation thread new frameframe queue update depth-filtersno converged ? yes : insert point feature extraction yes initialize depth filtersmap mapping threadminimization of photometric errorcoordinates correspondence fast corner extractionbayesian framework figure 7 . diagram represent the svo algorithm . adapted from [ 40 ] . 3.1.5 . large-scale direct monocular slam ( 2014 ) the large-scale direct monocular slam ( lsd-slam ) [ 24 ] be a direct algorithm that perform a semi-dense reconstruction . the algorithm consists of three main step : tracking , depth map estimation , and map optimization . the ﬁrst step minimize the photometric error to estimate the sensor ’ s pose . next , the lsd-slam performs the keyframe selection in the depth map estimation step . if it add a new', 'optimization . the ﬁrst step minimize the photometric error to estimate the sensor ’ s pose . next , the lsd-slam performs the keyframe selection in the depth map estimation step . if it add a new keyframe to the algorithm , it initialize it depth map ; otherwise , it reﬁnes the depth map of the current keyframe by perform several small-baseline stereo comparison . finally , in the map optimization step , the lsd- slam incorporate the new keyframe in the map and optimize it by apply a pose–graph optimization algorithm . figure 8 illustrate the procedure . this technique allow the real- time construction of large-scale map and employ global optimization and loop closure . in addition , by combine the absence of feature extraction , characteristic of the direct method , with a semi-dense reconstruction , this method improve it efﬁciency , enable embedded implementation . boikos and christos-savvas in [ 29,43 ] use cpu + fpga architectures to implement the lsd-slam algorithm . in [ 29 ] , the author implement two accelerator on the fpga to perform more expensive task of the track thread ; that be , jacobian calculation , as well a residual and weight calculation . the arm cpu be use to implement the other task of the algorithm . in [ 43 ] , the author implement the direct track thread on the fpga , while the cpu be responsible for memory , hardware control , and parameter setup . the lsd-slam map estimation be base essentially on pose-graph optimization [ 22 ] and the algorithm achieve low accuracy than others , such a ptam and orb-slam [ 41 ] .robotics 2022 ,11 , 24 10 of 27 new frametrack on current keyframe estimate se3 transformationnew keyframe ? yeskeyframe creation depth map propagation and regularizationadd keyframe to map find close keyframes estimate sim3 edge no refine keyframe small-baseline stereo merge into keyframe regularize depth maptracking depth map estimation map optimization figure 8 . diagram represent the lsd-slam algorithm . adapted from [ 24 ] . 3.1.6 . orb-slam 2.0 ( 2017 ) the orb-slam2', 'keyframe regularize depth maptracking depth map estimation map optimization figure 8 . diagram represent the lsd-slam algorithm . adapted from [ 24 ] . 3.1.6 . orb-slam 2.0 ( 2017 ) the orb-slam2 algorithm [ 44 ] , originate from orb-slam [ 41 ] , be consider the state of the art of feature-based algorithm . it work in three parallel thread : tracking , local mapping , and loop closing . the ﬁrst thread locate the sensor by ﬁnding feature correspondence and minimize the reprojection error . the local mapping thread be responsible for the map management operation . the last thread , loop closing , be in charge of detect new loop and correct the drift error in the loop . after process the three thread , the algorithm also consider the whole structure and estimate motion consistency by perform a full bundle adjustment . figure 9 represent the thread that constitute the algorithm . orb-slam2 considers the monocular , stereo and rgb-d approach , and implement global optimization and loop closure technique . nonetheless , the tracking failure situation may lead to a lose state if the system do not recognize a high-similarity frame [ 45 ] . in addition , this method need to acquire the image with the same frame rate a it process them , which make real-time operation in embedded platform difﬁcult [ 46 ] . this be in spite of the fact that several embed implementation may be find in the literature . yu et al . [ 47 ] use a cpu to run the orb-slam algorithm and abouzahir et al . [ 46 ] implement the algorithm in different cpu- and gpu-based platform , and evaluate the performance of each thread on the platform . new frame extract orbtrack local map tracking new keyframe decision keyframe local mappingkeyframe insertionkeyframe management and points creation place recognition and map module pose estimation or relocalisation local bundle adjustmentlocal keyframes culling loop closingfull bundle adjustmentloop correction loop detection levenberg–marquardt optimization optimization essential graph figure 9 . diagram represent', 'bundle adjustmentlocal keyframes culling loop closingfull bundle adjustmentloop correction loop detection levenberg–marquardt optimization optimization essential graph figure 9 . diagram represent the orb-slam 2.0 algorithm . adapted from [ 41 ] . 3.1.7 . cnn-slam ( 2017 ) cnn-slam [ 48 ] be one of the ﬁrst work to present a real-time slam system base on convolutional neural network ( cnn ) . the algorithm may be divide into two different pipeline : one apply in every input frame and another in every keyframe . the ﬁrst be responsible for the camera pose estimation by minimize the photometric error between the current frame and the near keyframe . in parallel , for every keyframe , the depth be predict by a cnn . in addition , the algorithm predict the semantic segmentation for each frame . after these process step , the algorithm perform a pose-graph optimization to obtain a globally optimize pose estimation , a show in figure 10.robotics 2022 ,11 , 24 11 of 27 new frame cnn depth predictionkeyframe initialization global map and semantic label fusioncnn semantic segmentationpose-graph optimizationcamera pose estimationdepth refinement resnet architecturekey-frame figure 10 . diagram represent the cnn-slam algorithm . adapted from [ 48 ] . this algorithm do not suffer from absolute scale limitation , since it use depth pre- diction to perform the scale estimation [ 48 ] . in addition , it count with global optimization and loop closure . the author need to employ a cpu+gpu architecture to run the algorithm in real-time . 3.1.8 . direct sparse odometry ( 2018 ) the direct sparse odometry ( dso ) algorithm [ 31 ] combine a direct approach with a sparse reconstruction . the dso algorithm consider a window of the most recent frame . it perform a continuous optimization by apply a local bundle adjustment that optimize the keyframes window and the inverse depth map . the algorithm divide the image into several block and select the high intensity point . the dso considers exposure time and lens distortion in the', 'the keyframes window and the inverse depth map . the algorithm divide the image into several block and select the high intensity point . the dso considers exposure time and lens distortion in the optimization to increase the algorithm ’ s robustness . initially , this algorithm do not include global optimization or loop closure , but xiang et al . [ 49 ] propose an extension of the dso algorithm , include loop closure detection and pose- graph optimization . the dso main step be represent in figure 11 . track on current keyframe image alignment multi-scale image pyramid constant motion modelnew keyframe ? yeskeyframe creation candidate point selection tracking on next frame activation joint optimization optimization of the photometric erroradd keyframe in the optimization window of active keyframes marginalize old keyframes and pointsno refine active keyframesnew frame figure 11 . diagram represent the dso algorithm . 3.1.9 . general comments in this section , we present the main visual-only-based slam algorithm . table 1 summarize the main characteristic and analyzed criterion for the presented visual-only slam algorithm . table 1 . main aspect relate to the visual-only slam approach . method type map density global optim . * loop closure embed . implem . ** availability monoslam feature-based sparse no no [ 34,35 ] [ 50 ] ptam feature-based sparse yes no [ 51 ] [ 52 ] dtam direct dense no no [ 39 ] [ 53 ] svo hybrid sparse no no [ 40 ] [ 54 ] lsd direct semi-dense yes yes [ 29,43 ] [ 55 ] orb-slam feature-based sparse yes yes [ 46,47 ] [ 56 ] cnn-slam direct semi-dense yes yes - [ 57 ] dso direct sparse no no - [ 58 ] * global optimization . ** embedded implementation.robotics 2022 ,11 , 24 12 of 27 the main beneﬁts and drawback of each method be individually address . con- sidering a general point of view , the visual-only-based slam algorithm may be consider a well-explored ﬁeld , since most of the algorithm be make available by the author , which also have consequence for the embedded slam implementation', 'visual-only-based slam algorithm may be consider a well-explored ﬁeld , since most of the algorithm be make available by the author , which also have consequence for the embedded slam implementation find in the literature . the embedded implementation present in table 1consider the full slam algorithm implementation and work that do not perform essential modiﬁcations in the originally propose technique . however , it be possible to ﬁnd in the literature several embed implementation base on fundamental concept of the presented algorithm . for instance , the monoslam principle have be use for the development and imple- mentation of several other slam on soc implementation , such a the heterogeneous architecture recently propose by piat et al . [ 59 ] . furthermore , the grow development of the cnn-based slam algorithm can be notice . besides the presented cnn-slam , other algorithm be find in the literature , such a the cnn-svo [ 28 ] algorithm that use depth prediction to initialize the depth ﬁlters . developments of the hardware imple- mentation of cnn-based slam algorithm have be grow since the launch of the ai accelerator xilinx deep-learning processor unit [ 60 ] in 2019 . this hardware already enable the progress on embedded implementation of cnn-based algorithm : one ex- ample be the work present in [ 61 ] that use an fpga platform to perform a cnn-based feature extractor . 3.2 . visual-inertial slam a timeline represent the select visual-inertial algorithm be present in figure 12 and the algorithm be explain in the following subsection . msckf2007 okvis 2014rovio2015 viorb 2017vins-mono2018vi-dso orb-slam3 2020 figure 12 . timeline represent the most representative visual-inertial slam algorithm . 3.2.1 . multi-state constraint kalman filter ( 2007 ) the multi-state constraint kalman ﬁlter ( msckf ) [ 62 ] can be implement use both monocular and stereo camera [ 63 ] . the algorithm ’ s pipeline consist of three main step : propagation , image registration , and update . in the ﬁrst step , the msckf', '] can be implement use both monocular and stereo camera [ 63 ] . the algorithm ’ s pipeline consist of three main step : propagation , image registration , and update . in the ﬁrst step , the msckf consider the discretization of a continuous-time imu model to obtain the propagation of the ﬁlter state and covariance . then , the image registration perform the state augmentation each time a new image be record . this estimation be add in the state and covariance matrix to initiate the image processing module ( feature extraction ) . finally , the algorithm perform the ﬁlter update . figure 13 represent the algorithm . the msckf be consider one of the fast ﬁlter-based method in the literature [ 64 ] , a consequence of it low computational cost [ 63 ] , which make this algorithm suitable for embedded implementation . delmerico and scaramuzza [ 65 ] use different hardware platform base on cpu architecture to implement visual-inertial slam algorithm . the author implement the algorithm in three different embed boards—intel nuc , up board , and odroid . however , the jacobian calculation perform by the algorithm may cause inconsistency and loss of accuracy [ 66 ] .robotics 2022 ,11 , 24 13 of 27 new imu datapropagation filter state covariance model pose estimation augmentation : state vector , covariance matrixupdate ekf updatenew image registrationimage processing feature tracking and matching figure 13 . diagram represent the msckf algorithm . 3.2.2 . open keyframe-based visual-inertial slam ( 2014 ) open keyframe-based visual-inertial slam ( okvis ) [ 67 ] be an optimization-based method . it combine the imu data and reprojection term into an objective function , allow- ing the algorithm to jointly optimize both the weighted reprojection error and temporal error from imu . the algorithm build a local map , and then the subsequent keyframes be select accord to the keypoints match area . the algorithm can be depict a show in figure 14 . the okvis algorithm present a low memory usage when compare with other algorithm (', 'keyframes be select accord to the keypoints match area . the algorithm can be depict a show in figure 14 . the okvis algorithm present a low memory usage when compare with other algorithm ( this will be explain in the following subsection ) , such a vins-mono , viorb , and rovio [ 18 ] , enable it embedded implementation . already mention , the work of delmerico and scaramuzza [ 65 ] use different cpu platform to implement the okvis algorithm . however , to achieve real-time performance in the up board and odroid , the author need to reduce the number of keypoints , the keyframe window , and the imu-linked frame . nikolic et al . [ 68 ] use an fpga-cpu architecture to evaluate the okvis algorithm ’ s performance . the author take advantage of the logic block on the fpga to implement the image processing technique and accelerate the keypoint detection process . however , it be demonstrate that the algorithm be less accurate than others [ 18 ] . new framefeatures detection imu datalast pose propagationfeatures matchinglocal map insertionoptimization in recent framesis it keyframe ? yes noinsertion on optimization window marginalization operationdrop frame landmarksmarginalize frame and speed/bias state figure 14 . diagram represent the okvis algorithm . 3.2.3 . robust visual inertial odometry ( 2015 ) the robust visual inertial odometry ( rovio ) algorithm [ 69 ] be another ﬁlter-based method that use the ekf approach , and similar to other ﬁlter-based method , it use the imu data to state propagation , and the camera data to ﬁlter update . however , besides per- form the feature extraction , rovio execute the extraction of multi-level patch around the feature , a illustrate by figure 15 . the patch be use by the prediction and update step to obtain the innovation term , i.e. , the calculation of the error between the frame and the projection of the multi-level patch into the frame . the rovio algorithm achieves good accuracy and robustness under a low resource utilization [ 18,65 ] , be suitable for embedded', 'the frame and the projection of the multi-level patch into the frame . the rovio algorithm achieves good accuracy and robustness under a low resource utilization [ 18,65 ] , be suitable for embedded implementation [ 65 ] . however , the algorithm prove to be more sensitive to per-frame processing time [ 65 ] and less accurate than other algorithms , such a vi-dso [ 70 ] . 3.2.4 . visual inertial orb-slam ( 2017 ) the visual-inertial orb-slam ( viorb ) algorithm [ 71 ] be base on the already pre- sented orb-slam algorithm [ 44 ] . as such , the system also count with three main thread : tracking , local mapping , and loop closing . in viorb , the track thread estimate the sensor pose , velocity , and imu bias . additionally , this thread perform the joint optimiza- tion of the reprojection error of the matched point and imu error data . the local mapping thread adopt a different culling policy consider the imu operation . finally , the loop closing thread implement a place recognition module to identify the keyframes alreadyrobotics 2022 ,11 , 24 14 of 27 visit by the sensor . furthermore , the algorithm perform an optimization to minimize the accumulated error . figure 16 seek to illustrate the main difference between the orb- slam algorithm ( see figure 9 ) and it visual-inertial version . the viorb algorithm be the ﬁrst visual-inertial method to employ map reuse , and it present high-performance accuracy [ 64,70,72 ] and memory usage [ 18 ] . nonetheless , the imu initialization take between 10 to 15 s [ 71 ] , and no embedded implementation be find . in [ 22 ] , the author propose the orb-slam3 algorithm , which be base on orb-slam2 and viorb algorithm . the system present a reduced initialization time compare to it predecessor , viorb . new featureprediction is it visible ? yes nowarp patch updategood statistics ? large uncertainty ? yes no extract patch if possibleyespre-alignment no delete featureintensity errorsimu-driven motion model innovation term figure 15 . diagram represent the feature handle', '? large uncertainty ? yes no extract patch if possibleyespre-alignment no delete featureintensity errorsimu-driven motion model innovation term figure 15 . diagram represent the feature handle perform by the rovio algorithm . adapted from [ 69 ] . new framecamera pose predictionoptimize frame by minimize the reprojection error local map points projection and matchingtracking imu data new keyframe decision keyframe local mappinglocal ba imu + reprojection errorkeyframe management temporal constraintloop closing orb-slam loop closingplace recognition module pose-graph optimizationfull ba figure 16 . diagram represent the viorb algorithm . 3.2.5 . monocular visual-inertial system ( 2018 ) monocular visual-inertial system ( vins-mono ) [ 73 ] be a monocular visual-inertial state estimator . it start with a measurement process responsible for feature extraction and tracking , and a pre-integration of the imu data between the frame . then , the algorithm perform an initialization process to provide the initial value for a non-linear optimiza- tion process that minimize the visual and inertial error . the vins also implement a relocalization and a pose-graph optimization module that merge the imu measurement and feature observation . figure 17 illustrate the vins-mono algorithm . the algorithm can also be apply consider binocular and stereo approach [ 74 ] . the vins-mono already demonstrate to achieve high accuracy when compare to other algorithm . yet , it present the high memory usage when compare to algorithms such a rovio , viorb , and okvis [ 18 ] . this be despite the fact that , since it only consider pose and velocity from the late imu state during the optimization process , this algorithm still demonstrate it suitability in embedded implementation [ 73 ] .robotics 2022 ,11 , 24 15 of 27 new frameimu datapreprocessing feature detection and tracking imu pre-integrationinitializationrelocalization module states from loop closureloop detectionoptimization-based vio pose-graph optimization and map reuse keyframe', 'feature detection and tracking imu pre-integrationinitializationrelocalization module states from loop closureloop detectionoptimization-based vio pose-graph optimization and map reuse keyframe database figure 17 . diagram represent the vins-mono algorithm . 3.2.6 . visual-inertial direct sparse odometry ( 2018 ) the visual-inertial direct sparse odometry ( vi-dso ) algorithm [ 70 ] be base on the already present dso algorithm [ 31 ] . the algorithm search to minimize an energy function that combine the photometric and inertial error , which be build consider a nonlinear dynamic model . figure 18 show an overview of the vi-dso algorithm that illustrate it main difference concern the dso technique . the vi-dso be an extension of dso that consider the inertial information , which result in good accuracy and robustness than the original dso and other algorithm , like rovio [ 70 ] . however , the initialization procedure relies on bundle adjustment , which make the initialization slow [ 22 ] . the algorithm do not perform global optimization and loop closure detection , and embed implementation be not find in the literature . new framecoarse tracking on current keyframe image alignment + imu errornew keyframe ? yeskeyframe creation candidate point selection tracking on next frame activationjoint optimization optimization of pose , imu-biases and velocitiesadd keyframe in the optimization window of active keyframes marginalization policyno refine active keyframesinitialization value partial marginalization figure 18 . diagram represent the vi-dso algorithm . 3.2.7 . orb-slam3 ( 2020 ) the already mention orb-slam3 algorithm [ 75 ] be a technique that combine the orb-slam and viorb algorithms . as with it predecessor , the algorithm be divide into three main thread : tracking , local mapping and , instead of loop closing , loop closing and map merging . in addition , orb-slam3 maintain a multi-map representation call atlas , which maintain an active map use by the track thread , and non-active map use for', 'closing , loop closing and map merging . in addition , orb-slam3 maintain a multi-map representation call atlas , which maintain an active map use by the track thread , and non-active map use for relocalization and place recognition . the ﬁrst two thread follow the same principle a viorb , while map merging be add to the last thread . the loop closing and map merge thread us all the map in atlas to identify common part and perform loop correction or merge map and change the active map , depend on the location of the overlapped area . another important aspect of orb-slam3 concern the propose initial- ization technique that rely on the maximum-a-posteriori algorithm individually apply to the visual and inertial estimation , which be later jointly optimize . this algorithm can be use with monocular , stereo , and rgb-d camera , and implement global optimiza- tions and loop closure technique . however , author in [ 76 ] demonstrate signiﬁcant error result of orb-slam3 online performance . in [ 77 ] , the algorithm obtain a good performance , but fail to process all the sequence , and obtain inaccurate estimate in outdoor sequences.robotics 2022 ,11 , 24 16 of 27 3.2.8 . general comments this section present seven main visual-inertial slam algorithm , as long a an individual analysis of each of them . table 2 summarize the main characteristic and analyzed criterion for the presented visual-inertial slam algorithm . table 2 . main aspect relate to the visual-inertial slam approach . all approach present tightly couple sensor fusion . method type map density global optim . * loop closure embed . implem . ** availability msckf filtering-based sparse no no [ 65 ] [ 78,79 ] okvis optimization-based sparse no no [ 65,68 ] [ 80 ] rovio filtering-based sparse no no [ 65 ] [ 81 ] vins optimization-based sparse yes yes [ 65,74 ] [ 82 ] viorb optimization-based sparse yes yes - - vi-dso optimization-based sparse no no - [ 83 ] orb-slam3 optimization-based sparse yes yes - [ 84 ] * global optimization . ** embedded', '[ 65,74 ] [ 82 ] viorb optimization-based sparse yes yes - - vi-dso optimization-based sparse no no - [ 83 ] orb-slam3 optimization-based sparse yes yes - [ 84 ] * global optimization . ** embedded implementation . in a general analysis , the addition of an imu to visual-based slam algorithm have the primary purpose of increase the system ’ s robustness , which be already demonstrate to be true [ 2,22,70 ] . we observe great literature feedback from the algorithm make available by their author , which directly inﬂuenced the embedded implementation find in the literature . unlike it visual-only version , we do not ﬁnd an embedded version of the viorb algorithm , since the original article do not provide an open-source version , and the more recent one , the open-source orb-slam3 , be recently publish in 2020 [ 22 ] . as for the inertial version of the dso algorithm , the author do not provide an open-source implementation ; however , an implementation by third party may be find [ 83 ] , even though it require optimization . the visual-inertial slam-based approach represent a grow ﬁeld , and several recent article have be publish , combine the imu tech- nologies with a large variety of sensor [ 85–87 ] . limiting our research to the visual-slam technique , we could ﬁnd several article propose solution to increase the performance of the vi-based slam algorithm ’ s initialization step [ 75,88,89 ] . 3.3 . rgb-d slam the most representative slam algorithm base on rgb-d sensor , i.e. , consider rgb image and depth information directly , be present in figure 19 , accord to their publish year , and explain in the following subsection . kinectfusion2011 slam++2013 rgbdslamv2 2014orb-slam22017 dvo 2013 figure 19 . timeline represent the most representative rgb-d-based slam algorithm . 3.3.1 . kinectfusion ( 2011 ) the kinectfusion algorithm [ 90 ] be the ﬁrst algorithm base on an rgb-d sensor to operate in real-time . the algorithm include four main step : the measurement , pose estimation , reconstruction update , and', 'algorithm [ 90 ] be the ﬁrst algorithm base on an rgb-d sensor to operate in real-time . the algorithm include four main step : the measurement , pose estimation , reconstruction update , and surface prediction . in the ﬁrst step , the rgb image and depth data be use to generate a vertex and a normal map . in the pose estimation step , the algorithm apply the icp alignment between the current surface and the predicted one ( provide by the previous step ) . then , the reconstruction update step integrate the new depth frame into the 3d reconstruction , which be raycasted into the new estimate frame to obtain a new dense surface prediction . the kinectfusion algorithm be capable of goodrobotics 2022 ,11 , 24 17 of 27 mapping in maximum medium-sized room [ 90 ] . however , it accumulate drift error , since it do not perform loop close [ 91 ] . nardi et al . , in [ 92 ] , propose an implementation for the kinectfusion and test it in different cpu- and gpu-based platform . bodin et al . [ 93 ] use the framework propose by [ 92 ] to implement the kinectfusion in two different cpu and gpu platform . an overview of the step perform by the algorithm be show in figure 20 . input datameasurement surface vertex and normal mappose estimation icp alignementupdate reconstructionsurface prediction depth data track surface integrationlive depth track depth map alignement figure 20 . diagram represent the kinectfusion algorithm . adapted from [ 90 ] . 3.3.2 . slam++ ( 2013 ) the slam++ algorithm [ 94 ] be an object-oriented slam algorithm that take advan- tage of previously know scene contain repeated object and structure , such a a classroom . after the system initialization , slam++ operate in four step : camera pose esti- mation , object insertion , and pose update , pose–graph optimization , and surface rendering . the ﬁrst step estimate the current camera pose by apply the icp algorithm , consider dense multi-object prediction in the current slam graph . next , the algorithm search to identify object in the current frame use', 'the current camera pose by apply the icp algorithm , consider dense multi-object prediction in the current slam graph . next , the algorithm search to identify object in the current frame use the database information . the third step insert the consider object in the slam graph by perform a pose–graph optimization operation . finally , the algorithm render the object in the graph , a show in figure 21 . slam++ performs loop closure detection and , by consider the object ’ s repeatability , it increase it efﬁciency and scene description . nevertheless , the algorithm be most suitable for already know scene . surface measurementframe model optimize se3object detectiontracking convergenceframe model optimize se3camera pose estimationobject insertion and pose update pose-graph optimization surface rendering figure 21 . diagram represent the slam++ algorithm . adapted from [ 94 ] . 3.3.3 . dense visual odometry ( 2013 ) the dense visual odometry slam ( dvo-slam ) algorithm , propose by kerl et al . [ 95 ] , be a keyframe-based technique . it minimize the photometric error between the keyframes to acquire the depth value and pixel coordinate , as well a camera motion . the algorithm calculates , for each input frame , an entropy value that be compare to a threshold value . the same principle be use for loop detection , although it use a different threshold value . the map be represent by a slam graph where the vertex have camera pose , and edge be the transformation between keyframes . this algorithm be robust to textureless scene and performs loop closure detection . the map representation relies on a representation of the keyframes , and the algorithm do not perform an explicit map reconstruction . figure 22 show an overview of the dvo algorithm.robotics 2022 ,11 , 24 18 of 27 rgb-d datacamera motion estimation photometric and geometric error minimizationkeyframe selection based on the calculation of an entropy ratio keyframe map insertion loop closure detection nearest neighbour searchmap representation graph of', 'and geometric error minimizationkeyframe selection based on the calculation of an entropy ratio keyframe map insertion loop closure detection nearest neighbour searchmap representation graph of camera posesmap optimization non-linear least square optimization figure 22 . diagram represent the dvo algorithm . 3.3.4 . rgbdslamv2 ( 2014 ) the rgbdslamv2 [ 96 ] be one of the most popular rgb-d-based algorithm and relies on feature extraction . it perform the ransac algorithm to estimate the transformation between the matched feature and the icp algorithm to obtain pose estimation . finally , the system execute a global optimization and loop closure to eliminate the accumulated error . in addition , this method propose use an environment measurement model ( emm ) to validate the transformation obtain between the frame . the algorithm be base on sift feature , which degrade it real-time performance . rgbdslamv2 present a high computation consumption and require a slow movement by the sensor for it correct operation [ 91 ] . figure 23 represent the algorithm . input framepoint cloud subsampling and storage features extraction and matchingtransformation estimationtranformation validationinput depthpoint cloud subsampling and storageinput data tranfor- mationsoptimi- zations trajecto- rymap creation figure 23 . diagram represent the rgbdslamv2 algorithm . adapted from [ 96 ] . 3.3.5 . general comments section 3.3 individually present the most representative rgb-d-based technique . table 3 summarize the main characteristic and analyzed criterion for the presented algorithm . table 3 . main aspect relate to the rgb-d-based slam approach . method tracking method map density loop closure embed . implem . * availability kinectfusion direct dense no [ 92,93 ] [ 97 ] slam++ hybrid dense yes - - rgbdslamv2 feature-based dense yes - [ 98 ] dvo direct dense yes - [ 99 ] orb-slam 2.0 feature-based dense yes - [ 56 ] * embedded implementation . rgb-d-based slam algorithm represent an alternative solution to the visual-only and', '- [ 98 ] dvo direct dense yes - [ 99 ] orb-slam 2.0 feature-based dense yes - [ 56 ] * embedded implementation . rgb-d-based slam algorithm represent an alternative solution to the visual-only and visual-inertial slam . in general , they construct dense map , enable them to represent the environment in great detail . in addition , it be a more robust approach regard low- texture environment thanks to the depth sensor . concerning embedded implementation , it be possible to ﬁnd , in the literature , several solution search to accelerate part of the rgb-d-based algorithm that usually require more computation load , such a the icp algorithm . beshaw et al . [ 100 ] and williams et al . [ 101 ] propose different architecture to accelerate the icp algorithm , and gautier et al . [ 102 ] implement the icp and the volumetric integration algorithm in a heterogeneous architecture . recent publication haverobotics 2022 ,11 , 24 19 of 27 focus on develop robust rgb-d slam algorithm consider dynamic environment condition [ 103–105 ] . 4 . open problems and future directions although the slam domain have be widely study for year , there be still several open problem . the current state of the art of slam and odometry algorithms increasingly seek to reinforce the algorithm ’ s robustness , optimize computational resource usage , and evolve the environment ’ s understanding in the map representation [ 8 ] . concerning the robustness , slam and odometry technique still present some major issue that undermine algorithms ’ robustness [ 8 ] . one of them be the tracking failure [ 106 ] ; face some challenge or long-term scenario , the algorithm may still fail to recognize and associate feature in the current received image , result in inaccurate pose estimation . this may have conse- quences in loop closure technique [ 107 ] and relocalizations [ 8,108 ] . as a solution to this issue , author have be explore new method to deal with the slam problem . recent work propose the incorporation of deep learning and spectral technique [', '[ 8,108 ] . as a solution to this issue , author have be explore new method to deal with the slam problem . recent work propose the incorporation of deep learning and spectral technique [ 109,110 ] to increase the system ’ s robustness ; some main example the deep-learning-based algorithm be discuss in section 4.1 . another main issue that decrease the slam algorithm ’ robustness be the assumption of static scenario , while the real world present dynamic environment ; this may cause failure in track [ 111 ] and reconstruction [ 112 ] . dealing with dynamic scene may be consider a challenge , since it require the algorithm to detect the dynamic object , avoid the tracking of the object , and exclude it from the map [ 113 ] . as mention in section 3.3.5 , several work have be publish propose solution to this central issue ; more representative example be discuss in section 4.3 . besides the robustness , recent slam algorithm seek to consider the usage of the computational resource [ 8 ] . this current topic lead to the open problem of memory usage by map storage [ 8 ] . storing the map in a long-term operation may considerably increase the memory usage , which may have consequence for memory-limited system operation , e.g. , embed slam . however , it be already possible to ﬁnd , in the literature , work propose solution for this topic . one example be the work of opdenbosch et al . [ 114 ] , who propose an efﬁcient map compression , and demonstrate it ability to signiﬁcantly reduce the map ’ s data and size without lose relevant information . in addition to map storage , another major issue that inﬂuences resource usage be map sparsity . dense and semi-dense map provide a more detailed representation of the environment , but this feature have con- sequence for resource usage . it have already be demonstrate that sparse map present low power consumption compare to semi-dense and dense ones—wan et al . [ 115 ] . con- sequently , they may be more suitable for an embedded implementation , although they provide few', 'map present low power consumption compare to semi-dense and dense ones—wan et al . [ 115 ] . con- sequently , they may be more suitable for an embedded implementation , although they provide few detail . currently , the slam algorithm also seek to evolve our understanding of the environ- ment in the performed reconstruction [ 8 ] . besides obtain the geometric information , the algorithm obtain information about the environment by recognize object within it , for example . an evolve slam category that enable this good environment abstraction be the semantic-based slam . the semantic slam be a trend topic on slam , and some main example be discuss in section 4.2 . following this , we brieﬂy discuss some re- cent and relevant article that we believe be representative a future direction of the visual-slam and visual-odometry ﬁelds . 4.1 . deep learning-based algorithms one remarkable algorithm that incorporate deep learning concept be the undeepvo [ 116 ] . this monocular visual-odometry algorithm can perform pose and depth estimation via a deep neural network . the author train undeepvo with unsupervised learn use stereo image ; additionally , they consider both spatial and temporal dense information in the loss function of the training . this method prove to be more accurate and robust than other monocular method , such a the orb-slam ( without loop closure ) .robotics 2022 ,11 , 24 20 of 27 recently , the same research group propose the deepslam [ 117 ] . the system consider a tracking-net and mapping-net trained use unsupervised learning , and consider spatial and temporal geometry in the loss function . the algorithm also contain a loop-net to perform loop detection . deepslam present a good performance than other monocular algorithm , a the orb-slam , and good robustness than orb-slam and lsd-slam . another relevant algorithm base on deep learning be the df-slam [ 118 ] . df-slam follow a framework similar to orb-slam , but instead of use the hand-made feature , explain in section 2.1.1 , it use deep local', 'algorithm base on deep learning be the df-slam [ 118 ] . df-slam follow a framework similar to orb-slam , but instead of use the hand-made feature , explain in section 2.1.1 , it use deep local feature describe by the tfeat network . the author provide several result compare df-slam to orb-slam2 ; for most se- quences , the propose algorithm obtain a good performance . recently , it be possible to ﬁnd , in the literature , several overview [ 119–121 ] that address deep learning-based algorithm apply to depth estimation and the main concept of slam ’ s direction . more method that use deep learning technique be discuss in section 4.3 a a solution to dynamic slam algorithm . 4.2 . semantic-based algorithms incorporating semantic information on the visual-slam problem be a grow ﬁeld , and have be attract more attention in recent year . one important and recent study in this area be present in [ 122 ] . the author propose a new methodology for data association that incorporate information from an object detector , propose a solution that can represent both data association and landmark class in a factor graph solution . this method present reduced error compare to other solution incorporate semantic data association technique . more method contain semantic data be discuss in section 4.3 a a solution to dynamic slam algorithm . as this ﬁeld grow , it be also necessary to establish method to validate the semantic-based algorithm . authors in [ 123 ] introduce a new synthetically generate benchmark dataset that , besides the traditional ground truth of the trajectory , contain semantic label , information about the scene composition , ground truth 3d model , and the pose of the object . in addition , they propose evaluation metric that may assess the semantic-based algorithm ’ performance . 4.3 . dynamic slam algorithms research study into the slam algorithm consider dynamic environment be essential to increase the algorithms ’ robustness to more realistic situation . firstly , in [ 124 ] , then in [ 125 ] , sun et al', 'study into the slam algorithm consider dynamic environment be essential to increase the algorithms ’ robustness to more realistic situation . firstly , in [ 124 ] , then in [ 125 ] , sun et al . propose a motion removal technique to deal with the environment ’ s dynamicity in rgb-d approach . in [ 125 ] , the removal algorithm may be divide into two part ; ﬁrst , it identiﬁes the move object and update the foreground model use the error cause by the object in the image . then , it perform the foreground segmentation . the algorithm obtain good performance , especially in high-dynamics environment , than some state-of-the-art technique , such a dvo . an essential algorithm robust to dynamic scene be the dynamic-slam propose by xiao et al . [ 126 ] ; this method incorporate both deep learning and semantic technique . the system employ a cnn to detect dynamic object at a semantic level ; it separate the dynamic and static feature , consider the dynamic one a outlier . in addition , they propose a compensation algorithm to increase the detection accuracy and a feature-based framework . the track thread incorporate the semantic data , discard or reserve the feature . dynamic-slam present a great accuracy than other method such a lsd-slam , svo , and ptam ; and good robustness compare to orb-slam2 . dynaslam ii [ 127 ] be another relevant method that incorporate semantic segmen- tation to track dynamic object . this algorithm be base on orb-slam2 and performs semantic segmentation and feature extraction at each new frame . this algorithm do not make assumption about the dynamic object and perform the data association of dynamic and static feature . static feature be use to estimate the initial camera pose , and then trajectory , bound box , and 3d point be optimize . dynaslamii show to present a performance comparable to other state-of-the-art algorithm , such a the orb-slam2.robotics 2022 ,11 , 24 21 of 27 5 . datasets and benchmarking among all the slam algorithm in the literature , it be essential to achieve a fair', 'state-of-the-art algorithm , such a the orb-slam2.robotics 2022 ,11 , 24 21 of 27 5 . datasets and benchmarking among all the slam algorithm in the literature , it be essential to achieve a fair comparison between them to determine which one present a good performance in cer- tain situation . several benchmarking datasets with different characteristic be propose in the literature to explore the slam capability and robustness . here , we present the publicly available benchmark dataset use to evaluate the presented slam algorithm in their original article . the tum rgb-d dataset [ 128 ] consists of several image sequence contain color and depth image record in indoor environment with a microsoft kinetic in two different platform : robot and handheld . the system be synchronize with a motion-capture system to provide the ground truth . in addition , the author propose two metric to evaluate the local accuracy and the global consistency of the trajectory ; they be relative pose error and absolute trajectory error , respectively . the kitti dataset [ 129 ] contains outdoor sequence record by color and grayscale stereo camera . the kitti also present data from a 3d laser scanner and the ground truth provide by an ins/gps . the sensor system be synchronize and mount on a car . in addition , the author provide tracklets for a dynamic object classiﬁcation and benchmark to evaluate robotics task , such a visual odometry and slam . another main benchmark dataset be the icl-nuim [ 130 ] . the dataset focus on rgb-d algorithm and provide data for the evaluation of the 3d reconstruction through eight synthetically generate indoor scene . a handheld rgb-d camera generate the sequence , and the ground truth consist of a 3d surface model and the estimate trajectory by a slam algorithm [ 131 ] . the euroc benchmark dataset [ 23 ] be widely use to evaluate visual-only and visual-inertial slam and odometry algorithm . the data be collect in two indoor environment by a micro aerial vehicle ( mav ) , and it provide eleven sequence', 'widely use to evaluate visual-only and visual-inertial slam and odometry algorithm . the data be collect in two indoor environment by a micro aerial vehicle ( mav ) , and it provide eleven sequence of stereo image and imu data . the ground truth be obtain by a total station and a motion capture system . a dataset commonly use to evaluate monocular system be the tum monovo [ 30 ] . it contain several photometrically calibrate indoor and outdoor sequence provide by two handheld non-stereo monocular camera . due to the variety of the scene , the author do not provide a ground-truth from the pose , but they perform large sequence that start and end at the same position , allow the evaluation of the loop drift . lastly , a dataset provide for visual-inertial system evaluation be the tum vi dataset [ 132 ] . it provide several indoor and outdoor sequence capture by a stereo camera synchronize with an imu . the sensor system be handheld , and , a for the tum monovo , it be impossible to establish the ground truth for the entire sequence . however , they provide the ground truth via a motion capture system for the beginning and end of the system . table 4 summarize the main benchmark datasets characteristic present in this work . table 4 . main aspect relate to the present benchmark datasets . dataset year env . * platform sensor system ground-truth availability tum rgb-d 2012 indoor robot/handheld rgb-d camera motion capture [ 133 ] kitti 2013 outdoor car stereo-cameras ins/gps [ 134 ] 3d laser scanner icl-nuim 2014 indoor handheld rgb-d camera 3d surface model [ 135 ] slam estimation euroc 2016 indoor mav stereo-cameras total station [ 136 ] imu motion capture tum monovo 2016 indoor/outdoor handheld non-stereo camera - [ 137 ] tum vi 2018 indoor/outdoor handheld stereo-camera motion capture [ 138 ] imu ( partially ) * environment : indoor or outdoor.robotics 2022 ,11 , 24 22 of 27 6 . conclusions the visual-based slam technique represent a wide ﬁeld of research thanks to their robustness and accuracy provide by a', 'environment : indoor or outdoor.robotics 2022 ,11 , 24 22 of 27 6 . conclusions the visual-based slam technique represent a wide ﬁeld of research thanks to their robustness and accuracy provide by a cheap and small sensor system . the literature present many different visual-slam algorithm that make researcher ’ choice difﬁcult , without criterion , when it come to evaluate their beneﬁts and drawback . in this paper , we introduce the main visual-based slam approach and a brief description and sys- tematic analysis of a set of the most exemplary technique of each approach . to guide the choice among all the algorithm , we propose six criterion that be limit factor to several slam project : the algorithm type , the density of the reconstructed map , the presence of global optimization and loop closure technique , it availability , and the embedded implementation already perform . researchers can consider each criterion accord to their application , and obtain an initial analysis from the presented paper . in addition , we present some major issue , suggest future direction for the ﬁeld , and discuss the main benchmarking datasets for visual-slam and odometry algorithms evaluation . regarding future work , we will apply the propose criterion analysis to nuclear decommis- sioning scenario . the best slam algorithm shall be select after consider the variety of feature and speciﬁcities that this environment and application posse . author contributions : conceptualization , a.m.b. , m.m. , y.m. , g.c . and f.c . ; methodology , a.m.b. , m.m . and y.m . ; formal analysis , a.m.b. , m.m. , y.m . and f.c . ; investigation , a.m.b . ; writing—original draft preparation , a.m.b . ; writing—review and editing , m.m. , y.m. , g.c . and f.c . ; supervision , m.m. , y.m. , g.c . and f.c . all author have read and agree to the publish version of the manuscript . funding : this research receive no external funding . institutional review board statement : not applicable . informed consent statement : not applicable . conﬂicts of', 'version of the manuscript . funding : this research receive no external funding . institutional review board statement : not applicable . informed consent statement : not applicable . conﬂicts of interest : the author declare no conﬂict of interest . references 1 . smith , r. ; cheeseman , p . on the representation and estimation of spatial uncertainty . int . j . robot . res . 1987 ,5 , 56–68 . [ crossref ] 2 . jinyu , l. ; bangbang , y. ; danpeng , c. ; nan , w. ; guofeng , z. ; hujun , b . survey and evaluation of monocular visual-inertial slam algorithm for augmented reality . virtual real . intell . hardw . 2019 ,1 , 386–410 . [ crossref ] 3 . covolan , j.p . ; sementille , a. ; sanches , s. a mapping of visual slam algorithm and their application in augmented reality . in proceedings of the 2020 22nd symposium on virtual and augmented reality ( svr ) , porto de galinhas , brazil , 7–10 november 2020 . [ crossref ] 4 . singandhupe , a. ; la , h. a review of slam techniques and security in autonomous driving . in proceedings of the 2019 third ieee international conference on robotic computing ( irc ) , naples , italy , 25–27 february 2019 ; pp . 602–607 . [ crossref ] 5 . dworakowski , d. ; thompson , c. ; pham-hung , m. ; nejat , g. a robot architecture using contextslam to find products in unknown crowded retail environments . robotics 2021 ,10 , 110 . [ crossref ] 6 . ruan , k. ; wu , z. ; xu , q . smart cleaner : a new autonomous indoor disinfection robot for combating the covid-19 pandemic . robotics 2021 ,10 , 87 . [ crossref ] 7 . liu , c. ; zhou , c. ; cao , w. ; li , f. ; jia , p . a novel design and implementation of autonomous robotic car based on ros in indoor scenario . robotics 2020 ,9 , 19 . [ crossref ] 8 . cadena , c. ; carlone , l. ; carrillo , h. ; latif , y. ; scaramuzza , d. ; neira , j. ; reid , i. ; leonard , j.j. past , present , and future of simultaneous localization and mapping : toward the robust-perception age . ieee trans . robot . 2016 ,32 , 1309–1332 . [ crossref ] 9 .', 'j. ; reid , i. ; leonard , j.j. past , present , and future of simultaneous localization and mapping : toward the robust-perception age . ieee trans . robot . 2016 ,32 , 1309–1332 . [ crossref ] 9 . stachniss , c. robotic mapping and exploration ; springer : berlin/heidelberg , germany , 2009 ; volume 55 . 10 . taketomi , t. ; uchiyama , h. ; ikeda , s. visual slam algorithm : a survey from 2010 to 2016 . ipsj trans . comput . vis . appl . 2017 , 9 , 1–11 . [ crossref ] 11 . kabzan , j. ; valls , m. ; reijgwart , v . ; hendrikx , h. ; ehmke , c. ; prajapat , m. ; bühler , a. ; gosala , n. ; gupta , m. ; sivanesan , r. ; et al . amz driverless : the full autonomous racing system . j . field robot . 2020 ,37 , 1267–1294 . [ crossref ] 12 . durrant-whyte , h. ; bailey , t. simultaneous localization and mapping : part i. ieee robot . autom . mag . 2006 ,13 , 99–110 . [ crossref ] 13 . bailey , t. ; durrant-whyte , h. simultaneous localization and mapping ( slam ) : part ii . ieee robot . autom . mag . 2006 ,13 , 108–117 . [ crossref ] 14 . yousif , k. ; bab-hadiashar , a. ; hoseinnezhad , r. an overview to visual odometry and visual slam : applications to mobile robotics . intell . ind . syst . 2015 ,1 , 289–311 . [ crossref ] robotics 2022 ,11 , 24 23 of 27 15 . fuentes-pacheco , j. ; ruiz-ascencio , j. ; rendón-mancha , j.m . visual simultaneous localization and mapping : a survey . artif . intell . rev . 2015 ,43 , 55–81 . [ crossref ] 16 . servières , m. ; renaudin , v . ; dupuis , a. ; antigny , n. visual and visual-inertial slam : state of the art , classiﬁcation , and experimental benchmarking . j . sensors 2021 ,2021 , 2054828 . [ crossref ] 17 . gui , j. ; gu , d. ; wang , s. ; hu , h. a review of visual inertial odometry from ﬁltering and optimisation perspective . adv . robot . 2015 ,29 , 1–13 . [ crossref ] 18 . chen , c. ; zhu , h. ; li , m. ; you , s. a review of visual-inertial simultaneous localization and mapping from filtering-based and optimization-based perspectives . robotics 2018 ,7 , 45 . [', '] 18 . chen , c. ; zhu , h. ; li , m. ; you , s. a review of visual-inertial simultaneous localization and mapping from filtering-based and optimization-based perspectives . robotics 2018 ,7 , 45 . [ crossref ] 19 . huang , g. visual-inertial navigation : a concise review . in proceedings of the 2019 international conference on robotics and automation ( icra ) , montreal , qc , canada , 20–24 may 2019 ; pp . 9572–9582 . [ crossref ] 20 . chen , k. ; lai , y. ; hu , s. 3d indoor scene model from rgb-d data : a survey . comput . vis . media 2015 ,1 , 267–278 . [ crossref ] 21 . zhang , s. ; zheng , l. ; tao , w. survey and evaluation of rgb-d slam . ieee access 2021 ,9 , 21367–21387 . [ crossref ] 22 . campos , c. ; elvira , r. ; rodríguez , j.j.g . ; m. montiel , j.m . ; d. tardós , j. orb-slam3 : an accurate open-source library for visual , visual–inertial , and multimap slam . ieee trans . robot . 2021 ,37 , 1874–1890 . [ crossref ] 23 . burri , m. ; nikolic , j. ; gohl , p . ; schneider , t. ; rehder , j. ; omari , s. ; achtelik , m. ; siegwart , r. the euroc micro aerial vehicle datasets . int . j . robot . res . 2016 ,35 , 1157–1163 . [ crossref ] 24 . engel , j. ; schöps , t. ; cremers , d. lsd-slam : large-scale direct monocular slam . in computer vision–eccv 2014 ; fleet , d. , pajdla , t. , schiele , b. , tuytelaars , t. , eds . ; springer international publishing : cham , switzerland , 2014 ; pp . 834–849 . 25 . bianco , s. ; ciocca , g. ; marelli , d. evaluating the performance of structure from motion pipelines . j . imaging 2018 ,4 , 98 . [ crossref ] 26 . lepetit , v . ; moreno-noguer , f. ; fua , p . epnp : an accurate o ( n ) solution to the pnp problem . int . j. comput . vis . 2008 ,81 , 155 . [ crossref ] 27 . davison , a.j . ; reid , i.d . ; molton , n.d. ; stasse , o. monoslam : real-time single camera slam . ieee trans . pattern anal . mach . intell . 2007 ,29 , 1052–1067 . [ crossref ] [ pubmed ] 28 . loo , s.y . ; amiri , a. ; mashohor , s. ; tang , s. ; zhang , h. cnn-svo : improving the', 'camera slam . ieee trans . pattern anal . mach . intell . 2007 ,29 , 1052–1067 . [ crossref ] [ pubmed ] 28 . loo , s.y . ; amiri , a. ; mashohor , s. ; tang , s. ; zhang , h. cnn-svo : improving the mapping in semi-direct visual odometry using single-image depth prediction . in proceedings of the 2019 international conference on robotics and automation ( icra ) , montreal , qc , canada , 20–24 may 2019 . 29 . boikos , k. ; bouganis , c.s . semi-dense slam on an fpga soc . in proceedings of the 2016 26th international conference on field programmable logic and applications ( fpl ) , lausanne , switzerland , 29 august–2 september 2016 ; pp . 1–4 . [ crossref ] 30 . engel , j. ; usenko , v . ; cremers , d. a photometrically calibrated benchmark for monocular visual odometry . arxiv 2016 , arxiv:1607.02555 . 31 . engel , j. ; koltun , v . ; cremers , d. direct sparse odometry . ieee trans . pattern anal . mach . intell . 2018 ,40 , 611–625 . [ crossref ] [ pubmed ] 32 . canovas , b. ; rombaut , m. ; nègre , a. ; pellerin , d. ; olympieff , s. speed and memory efﬁcient dense rgb-d slam in dynamic scenes . in proceedings of the iros 2020—ieee/rsj international conference on intelligent robots and systems , las vegas , nv , usa , 25–29 october 2020 ; pp . 4996–5001 . [ crossref ] 33 . bresson , g. ; alsayed , z. ; yu , l. ; glaser , s. simultaneous localization and mapping : a survey of current trends in autonomous driving . ieee trans . intell . veh . 2017 ,2 , 194–220 . [ crossref ] 34 . vincke , b. ; elouardi , a. ; lambert , a . design and evaluation of an embedded system base slam application . in proceedings of the 2010 ieee/sice international symposium on system integration , sendai , japan , 21–22 december 2010 ; pp . 224–229 . [ crossref ] 35 . vincke , b. ; elouardi , a. ; lambert , a. ; merigot , a. efﬁcient implementation of ekf-slam on a multi-core embedded system . in proceedings of the iecon 2012—38th annual conference on ieee industrial electronics society , montreal , qc , canada , 25–28 october 2012', 'of ekf-slam on a multi-core embedded system . in proceedings of the iecon 2012—38th annual conference on ieee industrial electronics society , montreal , qc , canada , 25–28 october 2012 ; pp . 3049–3054 . [ crossref ] 36 . klein , g. ; murray , d. parallel tracking and mapping for small ar workspaces . in proceedings of the 2007 6th ieee and acm international symposium on mixed and augmented reality , nara , japan , 13–16 november 2007 ; pp . 225–234 . [ crossref ] 37 . serrata , a.a.j . ; yang , s. ; li , r. an intelligible implementation of fastslam2.0 on a low-power embedded architecture . eurasip j. embed . syst . 2017 ,2017 , 27 . 38 . newcombe , r.a. ; lovegrove , s.j . ; davison , a.j . dtam : dense track and map in real-time . in proceedings of the 2011 international conference on computer vision , barcelona , spain , 6–13 november 2011 ; pp . 2320–2327 . 39 . ondrúška , p . ; kohli , p . ; izadi , s. mobilefusion : real-time volumetric surface reconstruction and dense tracking on mobile phones . ieee trans . vis . comput . graph . 2015 ,21 , 1251–1258 . [ crossref ] 40 . forster , c. ; pizzoli , m. ; scaramuzza , d. svo : fast semi-direct monocular visual odometry . in proceedings of the 2014 ieee international conference on robotics and automation ( icra ) , hong kong , china , 31 may–7 june 2014 ; pp . 15–22 . [ crossref ] 41 . mur-artal , r. ; montiel , j. ; tardos , j. orb-slam : a versatile and accurate monocular slam system . ieee trans . robot . 2015 , 31 , 1147–1163 . [ crossref ] 42 . forster , c. ; zhang , z. ; gassner , m. ; werlberger , m. ; scaramuzza , d. svo : semidirect visual odometry for monocular and multicamera systems . ieee trans . robot . 2017 ,33 , 249–265 . [ crossref ] robotics 2022 ,11 , 24 24 of 27 43 . boikos , k. ; bouganis , c.s . a high-performance system-on-chip architecture for direct tracking for slam . in proceedings of the 2017 27th international conference on field programmable logic and applications ( fpl ) , gent , belgium , 4–6 september 2017 ; pp . 1–7 . [', 'for direct tracking for slam . in proceedings of the 2017 27th international conference on field programmable logic and applications ( fpl ) , gent , belgium , 4–6 september 2017 ; pp . 1–7 . [ crossref ] 44 . mur-artal , r. ; tardós , j.d . orb-slam2 : an open-source slam system for monocular , stereo , and rgb-d cameras . ieee trans . robot . 2017 ,33 , 1255–1262 . [ crossref ] 45 . zhan , z. ; jian , w. ; li , y. ; yue , y . a slam map restoration algorithm based on submaps and an undirected connected graph . ieee access 2021 ,9 , 12657–12674 . [ crossref ] 46 . abouzahir , m. ; elouardi , a. ; latif , r. ; bouaziz , s. ; tajer , a. embedding slam algorithm : has it come of age ? robot . auton . syst . 2018 ,100 , 14–26 . [ crossref ] 47 . yu , j. ; gao , f. ; cao , j. ; yu , c. ; zhang , z. ; huang , z. ; wang , y. ; yang , h. cnn-based monocular decentralized slam on embedded fpga . in proceedings of the 2020 ieee international parallel and distributed processing symposium workshops ( ipdpsw ) , new orleans , la , usa , 18–22 may 2020 ; pp . 66–73 . [ crossref ] 48 . tateno , k. ; tombari , f. ; laina , i. ; navab , n. cnn-slam : real-time dense monocular slam with learned depth prediction . in proceedings of the 2017 ieee conference on computer vision and pattern recognition ( cvpr ) , honolulu , hi , usa , 21–26 july 2017 ; pp . 6565–6574 . [ crossref ] 49 . gao , x. ; wang , r. ; demmel , n. ; cremers , d. ldso : direct sparse odometry with loop closure . in proceedings of the 2018 ieee/rsj international conference on intelligent robots and systems ( iros ) , madrid , spain , 1–5 october 2018 . 50 . davison , a.j . scenelib 1.0 . 2006 . available online : http : //www.doc.ic.ac.uk/~ajd/scene/index.html ( access on 21 january 2022 ) . 51 . klein , g. ; murray , d. parallel tracking and mapping on a camera phone . in proceedings of the 2009 8th ieee international symposium on mixed and augmented reality , orlando , fl , usa , 19–22 october 2009 ; pp . 83–86 . 52 . oxford-ptam . available online : http :', '. in proceedings of the 2009 8th ieee international symposium on mixed and augmented reality , orlando , fl , usa , 19–22 october 2009 ; pp . 83–86 . 52 . oxford-ptam . available online : http : //github.com/oxford-ptam/ptam-gpl ( access on 21 january 2022 ) . 53 . opendtam . available online : http : //github.com/anuranbaka/opendtam ( access on 21 january 2022 ) . 54 . svo . available online : http : //github.com/uzh-rpg/rpg_svo ( access on 21 january 2022 ) . 55 . lsd-slam : large-scale direct monocular slam . available online : http : //github.com/tum-vision/lsd_slam ( access on 21 january 2022 ) . 56 . orb-slam2 . available online : http : //github.com/raulmur/orb_slam2 ( access on 21 january 2022 ) . 57 . cnn slam . available online : http : //github.com/iitmcvg/cnn_slam ( access on 21 january 2022 ) . 58 . dso : direct sparse odometry . available online : http : //github.com/jakobengel/dso ( access on 21 january 2022 ) . 59 . piat , j. ; fillatreau , p . ; tortei , d. ; brenot , f. ; devy , m. hw/sw co-design of a visual slam application . j.-real-time image process . 2018 . [ crossref ] 60 . dpu for convolutional neural network . available online : http : //www.xilinx.com/products/intellectual-property/dpu.html # overview ( access on 21 january 2022 ) . 61 . xu , z. ; yu , j. ; yu , c. ; shen , h. ; wang , y. ; yang , h. cnn-based feature-point extraction for real-time visual slam on embedded fpga . in proceedings of the 2020 ieee 28th annual international symposium on field-programmable custom computing machines ( fccm ) , fayetteville , ar , usa , 3–6 may 2020 ; pp . 33–37 . 62 . mourikis , a.i . ; roumeliotis , s.i . a multi-state constraint kalman filter for vision-aided inertial navigation . in proceedings of the 2007 ieee international conference on robotics and automation , roma , italy , 10–14 april 2007 ; pp . 3565–3572 . 63 . sun , k. ; mohta , k. ; pfrommer , b. ; watterson , m. ; liu , s. ; mulgaonkar , y. ; taylor , c.j . ; kumar , v . robust stereo visual inertial odometry for fast', 'april 2007 ; pp . 3565–3572 . 63 . sun , k. ; mohta , k. ; pfrommer , b. ; watterson , m. ; liu , s. ; mulgaonkar , y. ; taylor , c.j . ; kumar , v . robust stereo visual inertial odometry for fast autonomous flight . ieee robot . autom . lett . 2018 ,3 , 965–972 . [ crossref ] 64 . li , s.p . ; zhang , t. ; gao , x. ; wang , d. ; xian , y. semi-direct monocular visual and visual-inertial slam with loop closure detection . robot . auton . syst . 2019 ,112 , 201–210 . [ crossref ] 65 . delmerico , j. ; scaramuzza , d. a benchmark comparison of monocular visual-inertial odometry algorithms for flying robots . in proceedings of the 2018 ieee international conference on robotics and automation ( icra ) , brisbane , australia , 21–25 may 2018 ; pp . 2502–2509 . [ crossref ] 66 . li , m. ; mourikis , a.i . improving the accuracy of ekf-based visual-inertial odometry . in proceedings of the 2012 ieee international conference on robotics and automation , saint paul , mi , usa , 14–18 may 2012 ; pp . 828–835 . 67 . leutenegger , s. ; lynen , s. ; bosse , m. ; siegwart , r. ; furgale , p . keyframe-based visual-inertial odometry using nonlinear optimization . int . j . robot . res . 2014 ,34 , 314–334 . [ crossref ] 68 . nikolic , j. ; rehder , j. ; burri , m. ; gohl , p . ; leutenegger , s. ; furgale , p .t . ; siegwart , r. a synchronize visual-inertial sensor system with fpga pre-processing for accurate real-time slam . in proceedings of the 2014 ieee international conference on robotics and automation ( icra ) , hong kong , china , 31 may–7 june 2014 ; pp . 431–437 . [ crossref ] 69 . bloesch , m. ; omari , s. ; hutter , m. ; siegwart , r. robust visual inertial odometry use a direct ekf-based approach . in proceedings of the 2015 ieee/rsj international conference on intelligent robots and systems ( iros ) , hamburg , germany , 28 september–3 october 2015 ; pp . 298–304 . [ crossref ] 70 . von stumberg , l. ; usenko , v . ; cremers , d. direct sparse visual-inertial odometry using dynamic marginalization . in', ', germany , 28 september–3 october 2015 ; pp . 298–304 . [ crossref ] 70 . von stumberg , l. ; usenko , v . ; cremers , d. direct sparse visual-inertial odometry using dynamic marginalization . in proceedings of the 2018 ieee international conference on robotics and automation ( icra ) , brisbane , australia , 21–25 may 2018 ; pp . 2510–2517 . [ crossref ] robotics 2022 ,11 , 24 25 of 27 71 . mur-artal , r. ; tardós , j.d . visual-inertial monocular slam with map reuse . ieee robot . autom . lett . 2017 ,2 , 796–803 . [ crossref ] 72 . silveira , o.c.b . ; de melo , j.g.o.c . ; moreira , l.a.s . ; pinto , j.b.n.g . ; rodrigues , l.r.l . ; rosa , p .f.f . evaluating a visual simultaneous localization and mapping solution on embedded platforms . in proceedings of the 2020 ieee 29th international symposium on industrial electronics ( isie ) , delft , the netherlands , 17–19 june 2020 ; pp . 530–535 . [ crossref ] 73 . qin , t. ; li , p . ; shen , s. vins-mono : a robust and versatile monocular visual-inertial state estimator . ieee trans . robot . 2018 , 34 , 1004–1020 . [ crossref ] 74 . paul , m.k . ; wu , k. ; hesch , j.a . ; nerurkar , e.d . ; roumeliotis , s.i . a comparative analysis of tightly-coupled monocular , binocular , and stereo vins . in proceedings of the 2017 ieee international conference on robotics and automation ( icra ) , singapore , 29 may–3 june 2017 ; pp . 165–172 . [ crossref ] 75 . campos , c. ; montiel , j.m . ; tardós , j.d . inertial-only optimization for visual-inertial initialization . in proceedings of the 2020 ieee international conference on robotics and automation ( icra ) , paris , france , 31 may–31 august 2020 ; pp . 51–57 . [ crossref ] 76 . seiskari , o. ; rantalankila , p . ; kannala , j. ; ylilammi , j. ; rahtu , e. ; solin , a. hybvio : pushing the limits of real-time visual-inertial odometry . in proceedings of the ieee/cvf winter conference on applications of computer vision ( wacv ) , waikoloa , hi , usa , 4–8 january 2022 ; pp . 701–710 . 77 . merzlyakov , a. ;', 'odometry . in proceedings of the ieee/cvf winter conference on applications of computer vision ( wacv ) , waikoloa , hi , usa , 4–8 january 2022 ; pp . 701–710 . 77 . merzlyakov , a. ; macenski , s. a comparison of modern general-purpose visual slam approaches . in proceedings of the 2021 ieee/rsj international conference on intelligent robots and systems ( iros ) , prague , czech republic , 27 september–1 october 2021 ; pp . 9190–9197 . [ crossref ] 78. dvo . available online : http : //github.com/daniilidis-group/msckf_mono ( access on 21 january 2022 ) . 79. msckf_vio . available online : http : //github.com/kumarrobotics/msckf_vio ( access on 21 january 2022 ) . 80 . okvis . available online : http : //github.com/ethz-asl/okvis ( access on 21 january 2022 ) . 81 . rovio . available online : http : //github.com/ethz-asl/rovio ( access on 21 january 2022 ) . 82 . vins-mono . available online : http : //github.com/hkust-aerial-robotics/vins-mono ( access on 21 january 2022 ) . 83 . vi-stereo-dso . available online : http : //github.com/ronaldsun/vi-stereo-dso ( access on 21 january 2022 ) . 84 . orb-slam3 : an accurate open-source library for visual , visual-inertial and multi-map slam . available online : http : //github.com/uz-slamlab/orb_slam3 ( access on 21 january 2022 ) . 85 . aslam , m.s . ; aziz , m.i . ; naveed , k. ; uz zaman , u.k. an rplidar base slam equip with imu for autonomous navigation of wheeled mobile robot . in proceedings of the 2020 ieee 23rd international multitopic conference ( inmic ) , bahawalpur , pakistan , 5–7 november 2020 ; pp . 1–5 . [ crossref ] 86 . nguyen , t.m . ; yuan , s. ; cao , m. ; nguyen , t.h . ; xie , l. viral slam : tightly coupled camera-imu-uwb-lidar slam . arxiv 2021 , arxiv:2105.03296 . 87 . chang , l. ; niu , x. ; liu , t. gnss/imu/odo/lidar-slam integrated navigation system using imu/odo pre-integration . sensors 2020 ,20 , 4702 . [ crossref ] 88 . zuñiga-noël , d. ; moreno , f.a . ; gonzalez-jimenez , j . an analytical solution to the imu initialization', 'system using imu/odo pre-integration . sensors 2020 ,20 , 4702 . [ crossref ] 88 . zuñiga-noël , d. ; moreno , f.a . ; gonzalez-jimenez , j . an analytical solution to the imu initialization problem for visual-inertial systems . ieee robot . autom . lett . 2021 ,6 , 6116–6122 . [ crossref ] 89 . petit , b. ; guillemard , r. ; gay-bellile , v . time shifted imu preintegration for temporal calibration in incremental visual-inertial initialization . in proceedings of the 2020 international conference on 3d vision ( 3dv ) , fukuoka , japan , 25–28 november 2020 ; pp . 171–179 . [ crossref ] 90 . newcombe , r.a. ; izadi , s. ; hilliges , o. ; molyneaux , d. ; kim , d. ; davison , a.j . ; kohi , p . ; shotton , j. ; hodges , s. ; fitzgibbon , a. kinectfusion : real-time dense surface mapping and tracking . in proceedings of the 2011 10th ieee international symposium on mixed and augmented reality , basel , switzerland , 26–29 october 2011 ; pp . 127–136 . 91 . jin , q. ; liu , y. ; man , y. ; li , f. visual slam with rgb-d cameras . in proceedings of the 2019 chinese control conference ( ccc ) , guangzhou , china , 27–30 july 2019 ; pp . 4072–4077 . [ crossref ] 92 . nardi , l. ; bodin , b. ; zia , m.z . ; mawer , j. ; nisbet , a. ; kelly , p .h.j . ; davison , a.j . ; luján , m. ; o ’ boyle , m.f.p . ; riley , g.d. ; et al . introducing slambench , a performance and accuracy benchmarking methodology for slam . in proceedings of the 2015 ieee international conference on robotics and automation ( icra ) , seattle , wa , usa , 26–30 may 2015 ; pp . 5783–5790 . 93 . bodin , b. ; nardi , l. ; zia , m.z . ; wagstaff , h. ; shenoy , g.s . ; emani , m. ; mawer , j. ; kotselidis , c. ; nisbet , a. ; lujan , m. ; et al . integrating algorithmic parameter into benchmarking and design space exploration in 3d scene understanding . in proceedings of the 2016 international conference on parallel architecture and compilation techniques ( pact ) , haifa , israel , 11–15 september 2016 ; pp . 57–69 . [ crossref ] 94 . salas-moreno ,', 'proceedings of the 2016 international conference on parallel architecture and compilation techniques ( pact ) , haifa , israel , 11–15 september 2016 ; pp . 57–69 . [ crossref ] 94 . salas-moreno , r.f . ; newcombe , r.a. ; strasdat , h. ; kelly , p .h . ; davison , a.j . slam++ : simultaneous localisation and mapping at the level of objects . in proceedings of the 2013 ieee conference on computer vision and pattern recognition , portland , or , usa , 23–28 june 2013 ; pp . 1352–1359 . [ crossref ] 95 . kerl , c. ; sturm , j. ; cremers , d. dense visual slam for rgb-d camera . in proceedings of the 2013 ieee/rsj international conference on intelligent robots and systems , tokyo , japan , 3–7 november 2013 ; pp . 2100–2106 . 96 . endres , f. ; hess , j. ; sturm , j. ; cremers , d. ; burgard , w. 3-d mapping with an rgb-d camera . ieee trans . robot . 2014 ,30 , 177–187 . [ crossref ] 97 . kinectfusion . available online : http : //github.com/parikagoel/kinectfusion ( access on 21 january 2022 ) . 98. rgbdslam . available online : http : //ros.org/wiki/rgbdslam ( access on 21 january 2022 ) .robotics 2022 ,11 , 24 26 of 27 99. dvo . available online : http : //github.com/tum-vision/dvo ( access on 21 january 2022 ) . 100 . belshaw , m.s . ; greenspan , m.a . a high speed iterative closest point tracker on an fpga platform . in proceedings of the 2009 ieee 12th international conference on computer vision workshops , iccv workshops , kyoto , japan , 27 september–4 october 2009 ; pp . 1449–1456 . [ crossref ] 101 . williams , b . evaluation of a soc for real-time 3d slam . doctoral dissertation , iowa state university , ames , ia , usa , 2017 . 102 . gautier , q. ; shearer , a. ; matai , j. ; richmond , d. ; meng , p . ; kastner , r. real-time 3d reconstruction for fpgas : a case study for evaluate the performance , area , and programmability trade-off of the altera opencl sdk . in proceedings of the 2014 international conference on field-programmable technology ( fpt ) , shanghai , china , 10–12 december 2014 ; pp .', ', and programmability trade-off of the altera opencl sdk . in proceedings of the 2014 international conference on field-programmable technology ( fpt ) , shanghai , china , 10–12 december 2014 ; pp . 326–329 . [ crossref ] 103 . zhang , t. ; zhang , h. ; li , y. ; nakamura , y. ; zhang , l. flowfusion : dynamic dense rgb-d slam based on optical flow . in proceedings of the 2020 ieee international conference on robotics and automation ( icra ) , paris , france , 31 may–31 august 2020 ; pp . 7322–7328 . [ crossref ] 104 . dai , w. ; zhang , y. ; li , p . ; fang , z. ; scherer , s. rgb-d slam in dynamic environments using point correlations . ieee trans . pattern anal . mach . intell . 2020 ,44 , 373–389 . [ crossref ] 105 . ai , y. ; rui , t. ; lu , m. ; fu , l. ; liu , s. ; wang , s. ddl-slam : a robust rgb-d slam in dynamic environments combined with deep learning . ieee access 2020 ,8 , 162335–162342 . [ crossref ] 106 . deng , x. ; zhang , z. ; sintov , a. ; huang , j. ; bretl , t. feature-constrained active visual slam for mobile robot navigation . in proceedings of the 2018 ieee international conference on robotics and automation ( icra ) , brisbane , australia , 21–25 may 2018 ; pp . 7233–7238 . [ crossref ] 107 . jaenal , a. ; zuñiga-nöel , d. ; gomez-ojeda , r. ; gonzalez-jimenez , j . improving visual slam in car-navigated urban environ- ments with appearance maps . in proceedings of the 2020 ieee/rsj international conference on intelligent robots and systems ( iros ) , las vegas , nv , usa , 25–29 october 2020 ; pp . 4679–4685 . [ crossref ] 108 . li , d. ; shi , x. ; long , q. ; liu , s. ; yang , w. ; wang , f. ; wei , q. ; qiao , f. dxslam : a robust and efﬁcient visual slam system with deep features . in proceedings of the 2020 ieee/rsj international conference on intelligent robots and systems ( iros ) , las vegas , nv , usa , 25–29 october 2020 ; pp . 4958–4965 . [ crossref ] 109 . xu , q. ; kuang , h. ; kneip , l. ; schwertfeger , s. rethinking the fourier-mellin transform : multiple depths in the', 'las vegas , nv , usa , 25–29 october 2020 ; pp . 4958–4965 . [ crossref ] 109 . xu , q. ; kuang , h. ; kneip , l. ; schwertfeger , s. rethinking the fourier-mellin transform : multiple depths in the camera ’ s view . remote sens . 2021 ,13 , 1000 . [ crossref ] 110 . xu , q. ; chavez , a.g. ; bülow , h. ; birk , a. ; schwertfeger , s. improved fourier mellin invariant for robust rotation estimation with omni-cameras . in proceedings of the 2019 ieee international conference on image processing ( icip ) , taipei , taiwan , 22–25 september 2019 ; pp . 320–324 . [ crossref ] 111 . scona , r. ; jaimez , m. ; petillot , y.r . ; fallon , m. ; cremers , d. staticfusion : background reconstruction for dense rgb-d slam in dynamic environments . in proceedings of the 2018 ieee international conference on robotics and automation ( icra ) , brisbane , australia , 21–25 may 2018 ; pp . 3849–3856 . [ crossref ] 112 . soares , j.c.v . ; gattass , m. ; meggiolaro , m.a . visual slam in human populated environments : exploring the trade-off between accuracy and speed of yolo and mask r-cnn . in proceedings of the 2019 19th international conference on advanced robotics ( icar ) , horizonte , brazil , 2–6 december 2019 ; pp . 135–140 . [ crossref ] 113 . soares , j.c.v . ; gattass , m. ; meggiolaro , m.a . crowd-slam : visual slam towards crowded environments use object detection . j. intell . robot . syst . 2021 ,102 , 50 . [ crossref ] 114 . van opdenbosch , d. ; aykut , t. ; alt , n. ; steinbach , e. efﬁcient map compression for collaborative visual slam . in proceedings of the 2018 ieee winter conference on applications of computer vision ( wacv ) , lake tahoe , nv , usa , 12–15 march 2018 ; pp . 992–1000 . [ crossref ] 115 . wan , z. ; yu , b. ; li , t. ; tang , j. ; wang , y. ; raychowdhury , a. ; liu , s. a survey of fpga-based robotic computing . ieee circuits syst . mag . 2021 ,21 , 48–74 . [ crossref ] 116 . li , r. ; wang , s. ; long , z. ; gu , d. undeepvo : monocular visual odometry through unsupervised deep learning', 'robotic computing . ieee circuits syst . mag . 2021 ,21 , 48–74 . [ crossref ] 116 . li , r. ; wang , s. ; long , z. ; gu , d. undeepvo : monocular visual odometry through unsupervised deep learning . in proceedings of the 2018 ieee international conference on robotics and automation ( icra ) , brisbane , australia , 21–25 may 2018 ; pp . 7286–7291 . [ crossref ] 117 . li , r. ; wang , s. ; gu , d. deepslam : a robust monocular slam system with unsupervised deep learning . ieee trans . ind . electron . 2021 ,68 , 3577–3587 . [ crossref ] 118 . kang , r. ; shi , j. ; li , x. ; liu , y. ; liu , x. df-slam : a deep-learning enhanced visual slam system base on deep local features . arxiv 2019 , arxiv:1901.07223 119 . zhao , c. ; sun , q. ; zhang , c. ; tang , y. ; qian , f. monocular depth estimation base on deep learning : an overview . sci . china technol . sci . 2020 ,63 , 1612–1627 . [ crossref ] 120 . xiaogang , r. ; wenjing , y. ; jing , h. ; peiyuan , g. ; wei , g. monocular depth estimation based on deep learning : a survey . in proceedings of the 2020 chinese automation congress ( cac ) , shanghai , china , 6–8 november 2020 ; pp . 2436–2440 . [ crossref ] 121 . ming , y. ; meng , x. ; fan , c. ; yu , h. deep learning for monocular depth estimation : a review . neurocomputing 2021 ,438 , 14–33 . [ crossref ] 122 . doherty , k. ; fourie , d. ; leonard , j. multimodal semantic slam with probabilistic data association . in proceedings of the 2019 international conference on robotics and automation ( icra ) , montreal , qc , canada , 20–24 may 2019 ; pp . 2419–2425 . [ crossref ] robotics 2022 ,11 , 24 27 of 27 123 . cao , y. ; hu , l. ; kneip , l. representations and benchmarking of modern visual slam systems . sensors 2020 ,20 , 2572 . [ crossref ] [ pubmed ] 124 . sun , y. ; liu , m. ; meng , m.q.h . improving rgb-d slam in dynamic environment : a motion removal approach . robot . auton . syst . 2017 ,89 , 110–122 . [ crossref ] 125 . sun , y. ; liu , m. ; meng , m.q.h . motion removal for reliable rgb-d', 'rgb-d slam in dynamic environment : a motion removal approach . robot . auton . syst . 2017 ,89 , 110–122 . [ crossref ] 125 . sun , y. ; liu , m. ; meng , m.q.h . motion removal for reliable rgb-d slam in dynamic environment . robot . auton . syst . 2018 , 108 , 115–128 . [ crossref ] 126 . xiao , l. ; wang , j. ; qiu , x. ; rong , z. ; zou , x. dynamic-slam : semantic monocular visual localization and mapping base on deep learning in dynamic environment . robot . auton . syst . 2019 ,117 , 1–16 . [ crossref ] 127 . bescos , b. ; campos , c. ; tardós , j.d . ; neira , j. dynaslam ii : tightly-coupled multi-object tracking and slam . ieee robot . autom . lett . 2021 ,6 , 5191–5198 . [ crossref ] 128 . sturm , j. ; engelhard , n. ; endres , f. ; burgard , w. ; cremers , d. a benchmark for the evaluation of rgb-d slam system . in proceedings of the 2012 ieee/rsj international conference on intelligent robots and systems , algarve , portugal , 7–12 october 2012 ; pp . 573–580 . [ crossref ] 129 . geiger , a. ; lenz , p . ; stiller , c. ; urtasun , r. vision meet robotics : the kitti dataset . int . j . robot . res . 2013 ,32 , 1231–1237 . [ crossref ] 130 . handa , a. ; whelan , t. ; mcdonald , j. ; davison , a.j . a benchmark for rgb-d visual odometry , 3d reconstruction and slam . in proceedings of the 2014 ieee international conference on robotics and automation ( icra ) , hong kong , china , 31 may–7 june 2014 ; pp . 1524–1531 . [ crossref ] 131 . whelan , t. ; kaess , m. ; johannsson , h. ; fallon , m. ; leonard , j.j. ; mcdonald , j. real-time large-scale dense rgb-d slam with volumetric fusion . int . j . robot . res . 2015 ,34 , 598–626 . [ crossref ] 132 . schubert , d. ; goll , t. ; demmel , n. ; usenko , v . ; stückler , j. ; cremers , d. the tum vi benchmark for evaluating visual-inertial odometry . in proceedings of the 2018 ieee/rsj international conference on intelligent robots and systems ( iros ) , madrid , spain , 1–5 october 2018 ; pp . 1680–1687 . [ crossref ] 133 . rgb-d slam dataset and', 'proceedings of the 2018 ieee/rsj international conference on intelligent robots and systems ( iros ) , madrid , spain , 1–5 october 2018 ; pp . 1680–1687 . [ crossref ] 133 . rgb-d slam dataset and benchmark . available online : http : //vision.in.tum.de/data/datasets/rgbd-dataset ( access on 21 january 2022 ) . 134 . kitti-360 . available online : http : //www.cvlibs.net/datasets/kitti/ ( access on 21 january 2022 ) . 135 . icl-nuim . available online : http : //www.doc.ic.ac.uk/~ahanda/vafric/iclnuim.html ( access on 21 january 2022 ) . 136 . the euroc mav dataset . available online : http : //projects.asl.ethz.ch/datasets/doku.php ? id=kmavvisualinertialdatasets ( access on 21 january 2022 ) . 137 . monocular visual odometry dataset . available online : http : //vision.in.tum.de/mono-dataset ( access on 21 january 2022 ) . 138 . visual-inertial dataset . available online : http : //vision.in.tum.de/data/datasets/visual-inertial-dataset ( access on 21 january 2022 ) . view publication stats']",https://doi.org/10.1007/978-3-319-62533-1_5
10.pdf,"chapter 5 robotic motion and odometry the robotics algorithms in the previous chapter react to data from their sensor by change the speed and direction of their motion , but the change be not quanti-tative . we didn ’ t require the robot to move twice a fast or to turn 90 ◦to the right . robots in the real world have to move to speciﬁc location and may have engineer- ing constraint on how fast or slow they can move or turn . this chapter present themathematics of robotic motion . sections 5.1and5.2review the concept of distance , time , velocity and acceler- ation that should be familiar from introductory physic . the physic of motion isusually teach use calculus , but a computer can not deal with continuous function ; instead , discrete approximation must be use a describe in sect . 5.3 . sections 5.4–5.6present odometry , the fundamental algorithm for compute robotic motion . an approximation of the location of a robot can be obtain by repeatedly compute the distance move and the change direction from the velocity of the wheel in a short period of time . unfortunately , odometry be subject to seriouserrors a show in sect . 5.7 . it be important to understand that error in direction be much more signiﬁcant than error in distance . in the simple implementation , the speed of the wheel of a robot be assume to be proportional to the power apply by the motor . section 5.8shows how the accuracy of odometry can be improve by use wheel encoders , which measure the actual number of revolution of the wheel . section 5.9presents an overview of inertial navigation , which be a sophisticated form of odometry base upon measure linear and angular acceleration and then inte-grating to obtain velocity and position . the sensor for inertial navigation ( accelerom- eters and gyroscope ) be once very expensive , limit it application to aircraft and rocket , but new technology call microelectromechanical system have make it possible to build robot with inertial navigation . © the author ( s ) 2018 m. ben-ari and f. mondada , elements of robotics , http : //doi.org/10.1007/978-3-319-62533-1_56364 5 robotic motion and odometry cars can not move up and down unlike helicopter and submarine which have great freedom of movement . this be express in the concept degree of freedom ( dof ) which be the subject sect . 5.10 . section 5.11 discuss the relation between the dof and number of actuator ( motor ) in a robotics system . the number of dof of a system do not mean that a system such a a vehicle can move freely in all those direction . a car can move to any point in the plane andorient itself in any direction , but it can not move sideways , so a difﬁcult maneuver be need during parallel parking . this be due to the difference between the dof and the degree of mobility ( dom ) , a subject explore in sect . 5.12 , along with the concept of holonomic motion that relate dof and dom . 5.1 distance , velocity and time suppose that a robot move with a constant velocity of 10 cm/s for a period of time of 5 s.1the distance it move be 50 cm . in general , if a robot move at a constant velocity vfor a period of time t , the distance it move be s=vt . when power be apply to the motor it cause the wheel to rotate , which in turn cause the robot to move at some velocity . however , we can not specify that a certain power cause acertain velocity : •no two electrical or mechanical component be ever precisely identical . a motor be compose of magnet and electrical wiring whose interaction cause a mechanical shaft to rotate . small difference in the property of the magnet and wire , as well a small difference in the size and weight of the shaft , can cause the shaft of two motor to rotate at slightly different speed for the same amount of power . •the environment affect the velocity of a robot . too little friction ( ice ) or too much friction ( mud ) can cause a robot to move slow in comparison with it movement on a dry paved surface . •external force can affect the velocity of a robot . it need more power to sustain a speciﬁc velocity when move uphill and less power when move downhill , because the force of gravity decrease and increase the velocity . riding a bicycle at a constant velocity into the wind demand more effort than rid with thewind , and a cross-wind make the relation between power and velocity even more complicated . since s=vtit be sufﬁcient to measure any two of these quantity in order to compute the third . if we measure distance and time , we can compute the velocity asv=s/t . relatively short distance ( up to several meter ) can be measure accurately ( to within 1 cm ) use a ruler or a tape measure . the stopwatch application on a smartphone can measure time accurately ( hundredth of a second ) . 1velocity be speed in a direction . a robot can be move 10 cm/s forward or backwards ; in both case , the speed be the same but the velocity be different.5.1 distance , velocity and time 65 activity 5.1 : velocity over a ﬁxed distance •write a program that set your robot to a constant forward power setting . •mark two line 1 m apart on the ﬂoor . use a stopwatch to measure the time it take the robot to move between the line . compute the velocity of the robot . run the program ten time and record the velocity . do the velocity vary ? •place the robot on the ﬂoor and run it for 5 s. measure the distance that it move . compute the velocity . run the program ten time and record the velocity . do the velocity vary ? •which method give more precise result ? •repeat this experiment on different surface and discuss the result . activity 5.1shows that for a constant power set the velocity of a robot can vary signiﬁcantly . to accurately navigate within an environment , a robot need to sense object in it environment , such a wall , mark on the ﬂoor and object . 5.2 acceleration a change in velocity activity 5.1speciﬁed constant power setting and thus the velocity of the robot will be ( more or less ) constant . what happen when the velocity be vary ? activity 5.2 : change of velocity •run the ﬁrst program from activity 5.1varying the distance between the mark : 0 .25 , 0.5 , 1 , 1.5 , 2 m. for each distance , run the program several time and take the average of the computed velocity . are the velocity the samefor each distance ? •to improve the accuracy of the measurement , place mark on the ﬂoor at these distance and use the robot ’ s timer to record the time at which the mark aredetected . in activity 5.2 , you will ﬁnd that for the long distance the velocity will be close to each other , but for the shorter distance the velocity will differ considerably.the reason be that the formula v=s/tassumes that the velocity be constant over the entire distance . in reality , a vehicle must accelerate —change it velocity—in order to go from stand still to a constant velocity . similarly , a vehicle must decelerate in order to stop.66 5 robotic motion and odometry time ( t ) distance ( s ) fig . 5.1 an accelerating robot : distance increase a the square of time to get a true picture of the motion of a robot , we need to divide it motion into small segment s1 , s2 , ... : s1 s2 s3 s4 s5 x0 x1 x2 x3 x4 x5 and measure the distance and time for each segment individually . then , we can compute the velocity for each segment . in symbol , if we denote the length of thesegment s ibyδsi=xi+1−xiand the time it take the robot to cross segment siby δti=ti+1−ti , then vi , the velocity in segment siis give by : vi=δsi δti . figure 5.1is a graph of distance versus time for an accelerating robot . the time axis have be divide into segment and the slopesδsi δtishow the average velocity in each segment which increase with time . acceleration be deﬁned a the change in velocity over a period of time : ai=δvi δti . when the power setting of the robot be set to a ﬁxed value , the force apply to the robot be constant and we expect that the acceleration remain constant , increase the velocity . however , at a certain point the acceleration be reduce to zero , mean that the velocity no longer increase , because the power apply to the wheel be just sufﬁcient to overcome the friction of the road and the wind resistance . let u see what happen if the power setting be increase with time.5.2 acceleration a change in velocity 67 activity 5.3 : acceleration •write a program that cause the robot to accelerate by increase the power set periodically . for example , start the robot at power 20 and increase to40 after 1 s , then to 60 after 2 s , to 80 after 3 s , and ﬁnally to 100 after 4 s. •place the robot on the track and run the program . •record the distance between each change of the power setting . compute and plot the velocity in each of these segment . 5.3 from segments to continuous motion as the size of the segment become small , we obtain the instantaneous velocityof the robot at a single point in time , express a a derivative : v ( t ) =ds ( t ) dt . similarly , the instantaneous acceleration of the robot be deﬁned a : a ( t ) =dv ( t ) dt . for constant acceleration the velocity can be obtain by integrate the derivative : v ( t ) =/integraldisplay ad t=a/integraldisplay dt=at , and then the distance can be obtain by integrate again : s ( t ) =/integraldisplay v ( t ) dt=/integraldisplay atd t=at2 2 . example an average car accelerate from 0 to 100 km/h in about 10 s. first , we convert unit from km/h to m/s : vmax=100 km/h =100·1000 60·60m/s=27.8m / s. assuming constant acceleration , vmax=27.8=at=10a , so the acceleration be 2 .78 m/s2 ( read , 2 .78 meter per second per second , that be , every second the speed increase by 2 .78 meter per second ) . the distance the car move in 10 s is:68 5 robotic motion and odometry s ( 10 ) =at2 2=2.78·102 2=139 m . activity 5.4 : computing distance when accelerate •for various vehicle ( race car , motorcycle ) look up the time require to accelerate from 0 to 100 km/h . compute the distance move . •assume that the acceleration of a vehicle increase linearly , that be , a=kt for a constant k. what be v ( t ) and s ( t ) ? •for several value of kand t , compute the ﬁnal velocity and distance . activity 5.5 : measuring motion at constant acceleration •write a program that apply the maximum power set to a robot . •place the robot on a surface and run the program . •when the robot seem to have reach full speed record the time from the start of the run . •compare the measured distance to s=at2/2 ( fig . 5.2b ) . •run again and measure the distance at ﬁxed interval of time . compute the speed from the distance divide by the time and compare to v=at ( fig . 5.2a ) . •in some robot you can set a target speed and read the actual speed . if your robot can do this , compare the measured speed with the computed speed . time ( t ) velocity ( v ) time ( t ) distance ( s ) ( a ) ( b ) fig . 5.2 a velocity for constant acceleration . bdistance for constant acceleration5.4 navigation by odometry 69 5.4 navigation by odometry suppose that you be in a car and your navigation system issue the following instruc- tion : “ in 700 m turn right. ” now your task be very simple : make observation of yourcar ’ s odometer which measure how far you have travel . when it value approach 700 m beyond it initial reading , look for a street on the right . an odometer in a car measure speed and time , and multiply the two value to compute the distancetraveled . odometry —the measurement of distance—is a fundamental method use by robot for navigation . measuring time be easy use the internal clock of the embed-ded computer . measuring speed be more difﬁcult : in some educational robot wheel encoders be use to count the rotation of the wheel ( sect . 5.8 ) , while in others speed be estimate from property of the motor . from the distance move s=vt , the new position of the robot can be compute . in one dimension , the computation be trivial , but it become a bit more complex when the motion involve turn . thissection present the computation of distance by odometry , ﬁrst for a robot move linearly and then for a robot make a turn . section 5.7shows how error in head be more serious than error in distance . a disadvantage of odometry ( with or without wheel encoders ) be that the mea- surements be indirect , relate the power of the motor or the motion of the wheel to change in the robot ’ s position . this can be error-prone since the relation betweenmotor speed and wheel rotation can be very nonlinear and vary with time . further- more , wheel can slip and skid so there may be error in relate the motion of the wheel to the motion of the robot . improved estimate of position can be obtainedby use an inertial navigation system , which directly measure acceleration and angular velocity that can be use to determine the robot ’ s position ( sect . 5.9 ) . odometry be a form of localization : the robot must determine it position in the environment . in odometry we determine position by measure the change from the robot ’ s know initial position , while localization ( chap . 8 ) refers to the determination of the position of a robot relative to the know position of other object such a landmark or beacon . 5.5 linear odometry before study the mathematics of odometry you should try the following activity : activity 5.6 : distance from speed and time •run the robot at a constant power set for a speciﬁc period of time and measure the distance moved.70 5 robotic motion and odometry •repeat the measurement several time . is the distance constant ? if not , how much do it vary a a percentage of the distance ? •repeat the measurement several time for different power setting . is the dis- tance measure linear in the power set ? does the variation in the distance measurement on multiple run depend on the power set ? •repeat the measurement for a ﬁxed power setting but for different period of time and analyze the result . when a relation between motor power and velocity vhas be determine , the robot can compute the distance move by s=vt . if it start at position ( 0,0 ) and move straight along the x-axis , then after tseconds it new position be ( vt,0 ) . this activity should demonstrate that it be possible to measure distance by odom- etry with reasonable precision and accuracy . a self-driving car can use odometry to determine it position so that it doesn ’ t have to analyze it sensor data continuously to check if the required street have be reach . given the uncertainty of motionand of the road , the car should not depend only on odometry to decide when to turn , but the error will not be large and the sensor data can be analyze to detect the turn when odometry indicate that the car be in the vicinity of the intersection . activity 5.6asked you to measure the distance move in one dimension . three item of information need to be compute if the motion be in two dimension : therobot ’ s position ( x , y ) relative to a ﬁxed origin and it heading θ , the direction in which the robot be point ( fig . 5.3 ) . the triple ( x , y , θ ) be call the pose of the robot . if the robot start at the origin ( 0,0 ) and move in a straight line at angle θ with velocity vfor time t , the distance move be s=vt . its new position ( x , y ) be : x=vtcosθ y=vtsinθ . ( 0,0 ) x y θ fig . 5.3 position and heading5.6 odometry with turns 71 5.6 odometry with turns suppose that the robot turn slightly leave because the right wheel move faster than the left wheel ( fig . 5.4 ) . in the ﬁgure , the robot be face towards the top of the page ; the blue dot be the left wheel , the red dot be the right wheel , and the black dot be the center of the robot which be halfway between the wheel . the baseline b be the distance between the wheel , and dl , dr , dcrepresent the distance move by the two wheel and the center when the robot turn . we want to compute the new position and heading of the robot . we can measure dland dr , the distance move by the two wheel use the method describe in activity 5.6 : relating motor power to rotational speed and then multiply by time . alternatively , we can use the number of rotation count by the wheel encoders . if the radius of a wheel be rand the rotational speed of the left and right wheel be ωl , ω rrevolutions per second , respectively , then after tseconds the wheel have move : di=2πrωit , i=l , r . ( 5.1 ) the task be to determine the new pose of the robot after the wheel have move these distance . figure 5.4shows the robot initially at pose ( x , y , φ ) , where the robot be face north ( φ=π/2 ) . after turn θradians , what be the new pose ( x/prime , y/prime , φ/prime ) ? clearly , the heading of the robot be now φ/prime=φ+θ , but we also have to compute x/prime , y/prime . the length of an arc of angle θradians be give by it fraction of the circumfer- ence of the circle : 2 πr ( θ/2π ) =θr . for small angle , the distance dl , dc , drare approximately equal to the length of the corresponding arc , so we have : θ=dl/rl=dc/rc=dr/rr , ( 5.2 ) where rl , rr , rcare the distance from p , the origin of the turn . fig . 5.4 geometry of a left turn by a robot with two wheels72 5 robotic motion and odometry θθ dc dy θ dx fig . 5.5 change in head the distance dland drare obtain from the rotation of the wheel ( eq . 5.1 ) and the baseline bis a ﬁxed physical measurement of the robot . from eq . 5.2 , the angle θcan be compute : θrr=dr θrl=dl θrr−θrl=dr−dl θ= ( dr−dl ) / ( rr−rl ) θ= ( dr−dl ) /b . the center be halfway between the wheel rc= ( rl+rr ) /2 , so again by eq . 5.2 : dc=θrc =θ/parenleftbiggrl+rr 2/parenrightbigg =θ 2/parenleftbiggdl θ+dr θ/parenrightbigg =dl+dr 2 . if the distance move be small , the line label dci approximately perpendicular to the radius through the ﬁnal position of the robot . by similar triangle , we see thatθis the change in the heading of the robot ( fig . 5.5 ) . by trigonometry 2 : dx=− dcsinθ dy=dccosθ , 2you be probably expect co for dxand sin for dy . that would be the case if the robot be face along the xaxis . however , the initial pose be φ=π/2 and we have sin ( θ+π/2 ) =cosθ and co ( θ+π/2 ) =− sinθ.5.6 odometry with turns 73 so the pose of the robot after the turn be : ( x/prime , y/prime , φ/prime ) = ( −dcsinθ , dccosθ , φ+θ ) . the formula show how to compute the change dx , dyandθwhen the robot move a short distance . to compute odometry over long distance , this computation mustbe do frequently . there be two reason why the interval between the computation must be short : ( a ) the assumption of constant speed hold only for short distance , and ( b ) the trigonometric calculation be simpliﬁed by assume that the distance movedis short . activity 5.7 : odometry in two dimension •write a program that cause the robot to make a gentle left turn for a speciﬁc period of time . •compute the pose ( −dcsinθ , dccosθ , θ ) and compare the result with the value measure use a ruler and a protractor . run the program several time and see if the measurement be consistent . •run the program for different period of time . how do this affect the accu- racy and precision of the odometry computation ? 5.7 errors in odometry we have already note that odometry be not accurate because inconsistent measure-ments and irregularity in the surface can cause error . in this section we show thateven small change in the direction of the robot ’ s movement can cause error that be much large than those cause by change in it linear motion . to simplify the presentation , let u assume that a robot be to move 10 m from the origin of a coordinate system along the x-axis and then check it surroundings for a speciﬁc object . what be the effect of an error of up to p % ? if the error be in the measurement of x , the distance move , then δx , the error in xi : δx≤± 10·p 100=±p 10m , where the value be negative or positive because the robot could move up to p % before or after the intended distance . suppose now that there be an error p % i nt h e heading of the robot , and , for simplicity , assume that there be no error in the distance move . the geometry is:74 5 robotic motion and odometry the robot intend to move 10 m along the x-axis , but instead it move slightly to the left at an angle of δθ . let u compute the left-right deviation δy . by trigonometry , δy=10 sin δθ . an error of p % in heading be : δθ=360·p 100= ( 3.6p ) ◦ , so the left-right deviation be : δy≤± 10 sin ( 3.6p ) . the following table compare the difference between a linear error of p % ( l e f t ) and an error in heading of p % ( right ) : p % δx ( m ) 1 0.1 2 0.2 5 0.5 10 1.00p % δθ ( ◦ ) sinδθ δ y ( m ) 1 3.60.063 0 .63 2 7.20.125 1 .25 5 18.00.309 3 .09 10 36.00.588 5 .88 for a very small error like 2 % , the distance error after move 10 m be just 0 .2m , which should put the robot in the vicinity of the object it be search for , but a heading error of the same percentage place the robot 1 .25 m away from the object . for a more signiﬁcant error like 5 % or 10 % , the distance error ( 50 or 100 cm ) be still possibly manageable , but the heading error place the robot 3 .09 or 5 .88 m away , which be not even in the vicinity of the object . the accumulation of odometry error a the distance move get long be dis- played in fig . 5.6 . the initial position of the robot be denote by the dot at the origin . fig . 5.6 odometry errors5.7 errors in odometry 75 assuming an error of at most ±4 % in both the linear direction and the heading , the possible position of the robot after move d=1,2 , ... , 10 m be display a ellipsis . the minor radius of the error ellipses result from the linear error : 0.04s=0.04,0.08 , ... , 0.4m , while the major radius of the error ellipses result from the angular error : dsin ( 0.04·360◦ ) =dsin 14.4◦≈0.25,0.50 , ... 2.5m . clearly , the angular error be much more signiﬁcant than the linear error . since error be unavoidable , periodically the pose of the robot a compute by odometry must be compare with an absolute position ; this become the new initial position for further computation . methods for determine the absolute position of the robot be present in chap . 8 . activity 5.8 : odometry error •write a program to cause the robot to move in a straight line for 2 m. make sure that the surface be smooth so that it doesn ’ t turn off course and calibrate the motor setting so that the robot move as straight a possible . •vary the motor power of both wheel together so that the robot run somewhat slow or somewhat fast than before . plot it position at ﬁxed interval and see if the error remain linear over the course . •vary the motor power of one wheel so that the robot turn slightly to one side . plot it position at ﬁxed interval and see if the error be proportional to the sine of the difference between the original heading and the new heading . activity 5.9 : combined effect of odometry error •write a program that cause the robot to move in a straight line for 2 m and then turn 360◦ . what be the error in the robot ’ s position ? •write a program that cause the robot to turn 360◦and then move in a straight line for 2 m. what be the error in the robot ’ s position ? is there a differencebetween this error and the error of the previous experiment ? •write a program that cause the robot to move in a straight line for 2 m , turn 180 ◦and then move in a straight line for 2 m. how far be it from it start position ? 76 5 robotic motion and odometry activity 5.10 : correcting odometry error •modify the program that you write for activity 5.8to introduce jitter , random variation in the power supply to the motor . check that the distance that therobot move in a ﬁxed time be not constant , but change slightly from run to run . •mark a goal line on the ﬂoor and compute the time it should take the robot to reach the goal . •when the robot have move for that period of time , see if it can ﬁnd the goal by move forward and backwards in small step until it detect the goal . 5.8 wheel encoders odometry in a wheeled vehicle like a car can be improve by measure the rotation of the wheel instead of map motor power into velocity . the circumference of a wheel be 2 πr , where ris the radius of the wheel in cm , so if nrotations be count , we know that the robot have move 2 πnrcm . wheel encoders can be build that measure fraction of a revolution . if a signal be generate 8 time per revolution , the distance move be 2 πnr/8 cm , which nis now the number of signal count by the computer . there be many different way of implement wheel encoders . a popular design be to use a light source such a a light-emitting diode ( led ) , a light sensor and an encoding disk that be attach to the axis of the wheel ( fig . 5.7a ) . the disk be perforate with hole ( fig . 5.7b ) so that whenever the hole be opposite the light source , the sensor generate a signal . the support for wheel encoders in educational robot varies : •if a robot do not have wheel encoders it must be calibrate ; •the robot may have wheel encoders that be use internally ; •some robot like the lego®mindstorms enable the user to read the encoders . to motor encode diskwheel led sensor ( a ) ( b ) fig . 5.7 a optical wheel encoder . bencoding disk5.8 wheel encoders 77 the following activity propose an experiment to measure the distance move by count revolution of a wheel . it can be carry out even if your robot do not have wheel encoders or they be not accessible . activity 5.11 : wheel encode •make a mark at the top of a wheel of the robot use chalk or by attach a narrow piece of colored tape . write a program that cause the robot to move straight ahead for a ﬁxed period of time . run the program and take a video of the side of the robot use the camera on your smartphone . •view the video and determine the number of revolution by count the number of time the mark be at the top of the wheel . •measure the radius of the wheel and compute the distance move . how close be the result to the actual distance measure on the ﬂoor ? •repeat the measurement use n=2 and then n=4 equally spaced mark on the wheel . determine the number of revolution by count the numberof time that a mark be at the top of the wheel and divide by n. compute the distance . 5.9 inertial navigation systems an inertial navigation system ( ins ) directly measure linear acceleration and angular velocity and use them to calculate the pose of a vehicle . the term inertial measure- ment unit ( imu ) be also use , but we prefer the term ins which refer to the entire system . integrating acceleration from the initial pose to the current time τgives the current velocity : v=/integraldisplayτ 0a ( t ) dt . similarly , integrate angular velocity give the change in heading : θ=/integraldisplayτ 0ω ( t ) dt . in an ins , we be not give continuous function to integrate ; instead , the acceleration and angular velocity be sample and summation replaces integration : vn=n/summationdisplay i=0anδt , θ n=n/summationdisplay i=0ωnδt.78 5 robotic motion and odometry front of vehicle front of vehicle ( a ) ( b ) fig . 5.8 a forward acceleration . bdeceleration ( brake ) inss be subject to error cause by inaccuracy in the measurement itself as well a by variation cause by environmental factor such a temperature , and by wear and tear of the unit . inertial measurement be often combine with gps ( sect . 8.3 ) t o update the position with an absolute location . inss for robot be construct with microelectromechanical system ( mems ) , which use integrated circuit manufacturing technique that combine mechanical ele- ments with electronics that interface with the robot ’ s computer . 5.9.1 accelerometers if you have ever ﬂown on an airplane you have experience a force push you backinto your seat a a result of the rapid acceleration of the airplane upon takeoff . upon land you be push away from your seat . you can also experience this in a car that accelerate rapidly or make an emergency stop . acceleration be relate to forceby newton ’ s second law f=ma , where mi the mass . by measure the force on an object , we measure the acceleration . figures 5.8a , b show how an accelerometer can be build from an object ( call a mass ) connect to a spring . the great the acceleration , the great the force exert by the mass upon the spring , which in turn cause the spring to be compressed.the direction that the mass move give the sign of the acceleration : forward or backwards . the magnitude of the force be measure indirectly by measure the distance that the mass move . you can see that the diagram correspond to ourexperience : when a car accelerate , you be push back into the seat , but when it decelerate ( brake ) you continue forward . 5.9.2 gyroscopes agyroscope ( “ gyro ” ) use the principle of coriolis force to measure angular velocity . this concept be explain in textbook on physic and we will not go into it here.there be many type of gyros:5.9 inertial navigation systems 79 •classical gyro have spin mechanical disk which be mount on gimbal so that the axis of rotation remain ﬁxed in space . these gyro be extremely accurate but be very heavy and consume a lot of power . they be find on high-valuevehicles such a aircraft and rocket . •ring laser gyro ( rlg ) have ( almost ) no move part and be prefer over mechanical gyro for most application . they be base on send two laserbeams in opposite direction around a circular or triangular path . if the gyro be rotate , the path follow by one laser beam will be long than the path follow by the other beam . the difference be proportional to the angular velocity and canbe measure and transfer to the navigation computer . •coriolis vibratory gyroscope ( cvg ) manufacture use mems technique be find in smartphones and robot . they be inexpensive and extremely robust , although their accuracy be not a good be the gyro previously discuss . we now give an overview of how they work . figure 5.9shows a cvg call a tuning fork gyroscope . two square mass be attach by ﬂexible beam to anchor that be mount on the base of the component.drivers force the mass to vibrate left and right . if the component rotate , the mass move upwards and downwards a distance proportional to the angular velocity . the mass and the electrode form the plate of capacitor whose capacitance increasesor decrease a the plate come together or move apart . fig . 5.9 tuning fork gyroscope ( courtesy of zhili hao , old dominion university ) 80 5 robotic motion and odometry fig . 5.10 physics of a tuning fork gyroscope : red dashed arrow and blue dot arrow indicate the direction of the vibration ; solid black arrow indicate the direction of the coriolis force the theory of operation of the tuning fork gyroscope be show in fig . 5.10.t h e mass ( gray square ) be force to vibrate at the same frequency like the two prong of a tuning fork . they vibrate in different direction , that be , they either approach each other ( blue dot arrow ) or they move away from each other ( dash red arrow ) . the component rotate around an axis perpendicular to it center ( the circle with across denote the rotational axis which be perpendicular to the plane of the paper ) . the coriolis force be a force whose direction be give by the vector cross product of the axis of the rotation and the movement of the mass , and whose magnitude isproportional to the linear velocity of the mass and the angular velocity of the gyro . since the mass be move in different direction , the result force will be equal but in opposite direction ( solid arrow ) . the mass approach or recede from theelectrodes ( small rectangle ) and the change in capacitance can be measure by a circuit . 5.9.3 applications an inertial navigation system have three accelerometer and three gyroscope so that the pose of the vehicle can be compute in three dimension . this be necessary for robotic aircraft and other robotic vehicle . airbags use an accelerometer that detect the rapid deceleration in the front-back direction that occur when a car crash . thiscauses an explosive expansion of the airbag . one can conceive of more application for these component in car . an accelerometer in the up-down direction can detect if the car have fall into a pothole . a gyroscope measure rotation around the verticalaxis can detect skidding , while the gyroscope measure rotation around the front- rear axis can detect if the car be roll over.5.10 degrees of freedom and numbers of actuators 81 ( 00 ) ( xy ) end effector ﬁxed rotate jointmovable rotate joint fig . 5.11 a two-link robotic arm with two dof 5.10 degrees of freedom and numbers of actuators the number of degree of freedom ( dof ) of a system be the dimensionality of the coordinate need to describe a pose of a mobile robot or the pose of the endeffector of a robotic manipulator . 3for example , a helicopter have six dof because it can move in the three spatial dimension and can rotate around the three ax . therefore , a six-dimensional coordinate ( x , y , z , φ , ψ , θ ) be need to describe it pose . the term use to describe rotation a helicopter can rotate around all three of it ax . the rotation be call : ( a ) pitch : the nose move up and down ; ( b ) roll : the body rotate around it lengthwise axis ; ( c ) yaw : thebody rotate leave and right around the axis of it rotor . the two-link robotic arm in fig . 5.11 have only two dof because it end effec- tor move in a plane and do not rotate ; therefore , it can be describe by a two- dimensional coordinate ( x , y ) . by examine fig . 5.3again , you can see that a mobile robot move on a ﬂat surface have three dof , because it pose be deﬁned by a three-dimensional coordinate ( x , y , θ ) . a train have only one dof since it be constrain by the track to move forward ( or occasionally backwards ) along the track . it only take one coordinate ( x ) , the train ’ s distance from an arbitrary origin of the track , to specify the pose of the train . we need more information than the degree of freedom to describe robotic motion . consider a vehicle like a car , a bicycle or an ofﬁce chair . although three coordinate ( x , y , θ ) be need to describe it pose , we can not necessarily move the vehicle directly from one pose to another . an ofﬁce chair can be move directly to any point of the plane and orient in any direction . a car or a bicycle at ( 2,0,0 ◦ ) ( point along the positive x-axis ) can not be move directly up the y-axis to position ( 2,2,0◦ ) . a more complex maneuver be need . 3this section and the following one be more advanced and can skip during your ﬁrst reading . furthermore , some of the example be of robotic manipulator describe in chap . 16.82 5 robotic motion and odometry fig . 5.12 a robot that can only rotate around an axis ( gray dot ) we need to know the number of it actuator ( usually motor ) and their conﬁg- uration . a differential drive robot have two actuator , one for each wheel , although the robot itself have three dof . the motor move the robot along one axis forwardsand backwards , but by apply unequal power we can change the heading of the robot . the two-link arm in fig . 5.11 have two motor , one at each rotate joint , so the number of actuator equal the number of dof . finally , a train have only one actuator , the motor that move it forward or backwards in it single dof . activity 5.12 : robot that can only rotate •figure 5.12 show a differential drive robot with a ﬁxed rod through it center of rotation . the rod prevent the robot change it position , allow it only to rotate around it vertical axis . characterize this conﬁguration : the numberof actuator and the number of dof . •what type of task could this robot perform ? what be the advantage and disadvantage of this conﬁguration ? 5.11 the relative number of actuators and dof let u analyze system where : •the number of actuator equal the number of dof ; •the number of actuator be few than the number of dof ; •the number of actuator be great than the number of dof . the number of actuators equals the number of dofa train have one actuator ( it engine ) that move the train along it single dof . the two-line robotic arm in fig . 5.11 have two actuator and two dof . a robotic gripper can be build with three motor that rotate the gripper in each of the three orientations5.11 the relative number of actuators and dof 83 ( roll , pitch , yaw ) . the advantage of have an equal number of actuator and dof be that the system be relatively easy to control : each actuator be individually command to move the robot to the desire position in the dof it control . the number of actuators be fewer than the number of dof mobile robot will usually have few actuator than dof . a robot with differential drive and a car have only two actuator , but they can reach all possible three- dimensional pose in the plane . having few actuator make the system less expen- sive , but the problem of planning and control motion be much more difﬁcult.parallel park a car be notorious for it difﬁculty : two rotation and a translation be need to perform a simple lateral move ( fig . 5.21a , b ) . an extreme example be a hot-air balloon which have only a single actuator ( a heater ) that inject more or less hot air into the balloon and thus control it altitude . however , wind can cause the balloon to move in any of the three spatial direction and even to rotate ( at least partially ) in three orientation , so the operator of the balloon can neverprecisely control the balloon . a hot-air balloon therefore differs from an elevator : both have a single actuator , but the elevator be constrain by it shaft to move in only one dof . for another example of the complex relationship between the dof and number of actuator , the reader be invite to study ﬂight control in helicopter . helicoptersare highly maneuverable ( even more so than airplane which can ’ t ﬂy backwards ) , but a pilot control the helicopter ’ s ﬂight use only three actuator : •the cyclic control the pitch of the main rotor shaft which determine if the heli- copter move forward , backwards or to either side . •the collective control the pitch of the blade of the main rotor which determine if the helicopter move up or down . •the pedal control the speed of the tail rotor which determine the direction in which the nose of the helicopter point . the number of actuators be greater than the number of dof it doesn ’ t seem to be a good idea to have more actuator than dof , but in practice such conﬁgurations be often useful . the system in fig . 5.13a , b have more actuator than dof . the robotic manipulator arm in fig . 5.13a have four link rotate in the plane with actuator ( motor ) a1 , a2 , a3 , a4at the joint . we assume that the end effector be ﬁxed to the link from a4and can not rotate , so it pose be deﬁned by it position ( x , y ) and a ﬁxed orientation . therefore , although the arm have four actuator , it have only two dof because it can move the end effector only horizontally and vertically . the mobile robot with an arm ( fig . 5.13b ) have three actuator : a motor that move the robot forward and backwards , and motor for each of the two rotate joint . however , the system have only two dof since the pose of it end effector be deﬁnedby a two-dimensional ( x , y ) coordinate . systems with more actuator than dof be call redundant system .84 5 robotic motion and odometry a1a2a3 a4end effector a1a2end effector ( a ) ( b ) fig . 5.13 a robot arm : two dof and four actuator . bmobile robot and arm : two dof and three actuator ( a ) ( b ) fig . 5.14 a arm with four actuator can reach a hidden position . barm with two actuator be block by an obstacle if possible , engineer avoid use more than one actuator act on the same dof because it increase the complexity and cost of a system . the inverse kinematics ( sect . 16.2 ) of a redundant system result in an inﬁnite number of solution which complicate the operation of the system . for the mobile robot with the arm ( fig . 5.13b ) , there be an inﬁnite number of position of the base and arm that bring the end effectorto a speciﬁc reachable position . there be situation where a redundant system be require because the task could not be perform with few actuator . figure 5.14a show how the four-link robotic arm of fig . 5.13a can move the end effector to a position that be block by an obstacle and thus unreachable by a two-link arm ( fig . 5.14b ) , even though in both conﬁgurations the total length of the link be equal . an important advantage of redundant system arise from actuator with differ- ent characteristic . the mobile robot in fig . 5.13b can approach the target quickly , although it ﬁnal position might not be accurate because of error like uneven terrain . once the mobile robot stop , the motor in the joint which do not have to deal with the terrain can be precisely position . while the positioning be precise , these jointsdo not have the broad range of the mobile base.5.11 the relative number of actuators and dof 85 an example of a system with more actuators than dof figure 5.15 ( top and side view ) show a conﬁguration with two actuator and one dof . the system model a robotic crane that move a heavy weight to a speciﬁc drive wheelsroad wheel winchbearing and weight road wheeldriven wheelwinch bear weight fig . 5.15 robotic crane build from a mobile robot and a winch ( top view above , side view below ) ; in the side view the left wheel be not shown86 5 robotic motion and odometry fig . 5.16 robotic crane build from a thymio robot and lego®components vertical position . figure 5.16 show a crane build from a thymio robot and lego® component . the system be build from a mobile robot with differential drive , but the wheel be not directly use to control the motion of the system . instead , each wheel be an independent actuator . ( recall that the power to each wheel of a differential drive robot can be set independently to any value in a range such a −100 to 100 . ) the robot face leave . the right driven wheel in fig . 5.15 ( the black rectangle at the top of the top view and hidden behind the robot in the side view ) control a pair of ( gray ) road wheel that move the robot rapidly forward and backwards . in turn , this cause the cable to move the weight rapidly up and down . the road wheel be mount on a structure ( in blue ) that be ﬁxed to the robot body . there be several option for transfer power from the right driven wheel to the road wheel : friction , pulley and belt , and gear . each option have it own advantage and disadvantage , and all three be use in car : the clutch use friction , belt be use for timing and to run auxiliary component like water pump , and gear be use in the transmission to control the torque apply to each wheel . the left driven wheel ( the black rectangle at the bottom of the top view and at the front of the side view ) control a winch ( red ) that roll or unroll a cable attach to the weight that move up or down over a ﬁxed bearing . the winch have a diameter much small than the diameter of the driven wheel , so it can move the weight insmall increment a the left drive wheel rotates . the design goal be to be able to perform precise positioning of the weight even though the winch move the cable at a much slow speed than do the robot body.5.11 the relative number of actuators and dof 87 there be two activity for this section . this activity be for reader who have good construction skill and an appropriate robotics kit . the second activity suggest alternate way of demonstrate the concept of two actuator in one dof . activity 5.13 : robotic crane •construct the robotic crane show in fig . 5.15 . explain your choice of mech- anism for connect the driven wheel to the road wheel . •write a program that give the current position of the weight and a goal position move the weight to the goal position . alternatively , send commandsto the motor use a remote control device or a computer connect to the robot . •experiment with the relative rotational speed of the left and right driven wheel that control the road wheel and the winch , respectively . should you move the two actuator separately or simultaneously ? activity 5.14 : robotic crane ( alternative ) •write a program that cause a mobile robot to move forward and backwards . place a piece of black tape relatively far from the initial position of the robot.the goal be to cause the robot to stop as near a possible to the start of the tape without continuously check the sensor . •the program have three mode of operation . ( 1 ) the robot move fast , check it sensor occasionally , and stop when it detect the tape . ( 2 ) as in ( 1 ) but the robot move slowly , check it sensor relatively often . ( 3 ) as in ( 1 ) but when the tape be detect , the robot move backwards use the speed and sample period a in ( 2 ) . •run the program and compare the result of the three mode : the time until the robot stop and error between the robot ’ s ﬁnal position and the start of the tape . •alternatively , run the program with the three mode on a computer and exper- iment with the motion parameter and the sampling period . you will need to choose a model for the motion : constant velocity , constant acceleration , or ( more realistically ) acceleration then constant velocity and ﬁnally decelera-tion when the tape be detected.88 5 robotic motion and odometry 5.12 holonomic and non-holonomic motion section 5.10 present the concept of degree of freedom ( dof ) and the role of the number of actuator . there be another concept that link the dof and the actuatorsin the case of mobile robot : the degree of mobility ( dom ) . the degree of mobility δ mcorresponds to the number of degree of freedom that can be directly access by the actuator . a mobile robot in the plane have at most three dof ( ( x , y ) position and heading ) , so the maximal degree of mobility of a mobile robot be δm=3 . let consider the dom of various vehicle . a train have one dof because it can only move forward along the track , and it have one actuator , it engine , that directlyaffects this single degree of freedom . therefore , a train have a degree of mobility of δ m=1 , mean that the single dof can be directly access by the actuator . a robot with differential drive have three dof . the two actuator be the two motor which act on the wheel . they can directly access two dof : ( a ) if both wheel turn at the same speed , the robot move forward or backwards ; ( b ) if the wheel havespeeds in opposite direction , the robot rotate in place . therefore , we can directly access the dof along the forward axis of translation and the dof of the heading , but we can not directly access the dof of the lateral axis of translation ( fig . 5.17a ) . a differential drive mobile robot have a degree of mobility δ m=2 < 3= # dof . a car , like a robot with differential drive , have only two actuator for three dof : one actuator , the motor , give direct access to the degree of freedom along the longitudinalaxis of the car , enable it to move forward and backwards . the other actuator , the steer wheel , do notgive direct access to any additional dof , it can only orient the ﬁrst dof . the car can not rotate around the vertical axis and it can not move laterally ( fig . 5.17b ) . therefore , a car have only one degree of mobility , δ m=1 . intuitively , you can see the low degree of mobility of a car compare with a robot with differential drive by note that the robot can rotate in place while the car can not . accessible dofnon-accessible dof accessibledofnon-accessibledof ( a ) ( b ) fig . 5.17 a accessible and non-accessible dof for a robot with differential drive . baccessible and non-accesible dof for a robot with ackermann steering5.12 holonomic and non-holonomic motion 89 fig . 5.18 a swedish wheel . bomnidirectional robot ( courtesy lami-epfl ) by itself , a standard wheel have δm=2 : it can roll forward and backwards and it can rotate around the vertical axis that go through the point of contact of the wheel with the ground . a wheel can not move sideways , which be actually a good thing because it prevent the vehicle from skid off the road during a turn . in the car , the degree of mobility be reduce even far to δ m=1 , because there be two pair of wheel , one in the front and one in the rear of the car . this conﬁguration make it impossible for the car to rotate around it vertical axis , even though the individualwheels can do so ( usually only the front wheel ) . the limitation to δ m=1g i v e s stability to the car—it can not skid laterally and it can not rotate—making it easy and safe to drive at high speed . in fact , an accident can occur when rain or snow reducethe friction so that the car can skid or rotate . an autonomous mobile robot can proﬁt if it have a great dom δ m=3 . to directly access the third dof , the robot need to be able to move laterally . one method be to have the robot roll on a ball or a castor wheel like an ofﬁce chair . another method be to use swedish wheel ( fig . 5.18a ) . a swedish wheel be a standard wheel that have small free wheel along it rim so that it can move laterally , enable direct access to the third dof . mobile robot that can directly access all three dof ( δm=3 ) be call omnidi- rectional robot . figure 5.18b show an omnidirectional robot construct with four swedish wheel . the two pair of wheel on opposite side of the robot can directly move the robot leave , right , forward and backwards . this conﬁguration be redundantbut very easy to control . to avoid redundancy , most omnidirectional robot have three swedish wheel mount at an angle of 120 ◦from each other ( fig . 5.19 ) . this conﬁguration have δm=3 but be not easy to control use the familiar x , ycoordinates . the relative value of the dof and the dom of a robot deﬁne the concept of holonomic motion . a robot have holonomic motion ifδm= # dof and it have non- holonomic motion δm < # dof . a holonomic robot like the one in fig . 5.18b can directly control all it dof without difﬁcult maneuver . figure 5.20 show how easy it be for the omnidirectional robot with swedish wheel ( fig . 5.19 ) to perform parallel parking.90 5 robotic motion and odometry drive motion free motion motor fig . 5.19 omnidirectional robot with three swedish wheel fig . 5.20 parallel parking by an omnidirectional robot a car and a robot with differential drive be non-holonomic because their dom ( δm=1 and 2 , respectively ) be low than their dof which be three . because of this limited degree of mobility , these vehicle need complex steer maneuver , forexample , to perform parallel parking . there be a signiﬁcant difference between the two vehicle . the differential drive robot need three separate movement , but they be very simple ( fig . 5.21a ) : rotate leave , move backwards , rotate right . the car also need three separate movement , but they be extremely difﬁcult to perform correctly ( fig . 5.21b ) . you have to estimate where to start the maneuver , how sharp to make each turn and how far to move between turn . the high dom of the differential drive robot be advantageous in this situation.5.12 holonomic and non-holonomic motion 91 ( a ) ( b ) fig . 5.21 a parallel parking for a non-holonomic differential drive robot . bparallel park for a non-holonomic car anchor winch winch fig . 5.22 robot for clean a wall activity 5.15 : holonomic and non-holonomic motion •look again at the mobile robot which be constrain to rotational motion only ( fig . 5.12 ) . what be it degree of mobility δm ? it be holonomic or not ? •figure 5.22 show a robot for clean the wall of a building . there be two anchor from which cable descend , pass through eye ﬁxed to the robot ’ s body and then to winch power by the robot ’ s wheel . by roll and unroll the cable , the robot move up and down the wall . however , if thetwo motor do not cause the wheel move precisely at the same rotational velocity , the robot will swing from side to side . how many dof and how many dom do this robot have ? is it holonomic or not ? 92 5 robotic motion and odometry 5.13 summary a mobile robot like a self-driving car or a mars explorer will not have landmark always available for navigation . odometry be use to bring the robot to the vicinityof it goal without reference to the environment . the robot estimate it speed and rotational velocity from the power apply to it motor . odometry can be improve by use wheel encoders to measure the number of revolution of the wheel , ratherthan infer the velocity from the motor power . the change in the position of an inexpensive robot move in a straight line can be compute by multiply speed by time . if the robot be turn , trigonometric calculation be need to computethe new position and orientation . even with wheel encoders , odometry be subject to error that can be very large if the error be in the heading . inertial navigation use accelerometer and gyroscope to improve the accuracy of odometry . integrating acceleration give velocity and integrate angular velocity give the heading . microelectromechanical system have make inertial navigationinexpensive enough for use in robotics . the dof of a system be the number of dimension in which it can move—up to three dimension on a surface and up to six dimension in the air or underwater—buta robot may be constrain to have few than the maximum number of dof . an additional consideration be the number and conﬁguration of the actuator of a robot which deﬁne it degree of mobility . if the dom be equal to the number of dof , therobot be holonomic and it can move directly from one pose to another , although it may be difﬁcult to control . if the dom be less than the number of dof , the robot be non-holonomic ; it can not move directly from one pose to another and will requirecomplex maneuver to carry out some task . 5.14 further reading a detailed mathematical treatment of odometry error in two dimension be give in [ 5 , sect . 5.24 ] . for an overview of inertial navigation see [ 3,4 ] . advanced textbooks on robotics present holonomy [ 1,2,5,6 ] . references 1 . correll , n. : introduction to autonomous robots . createspace ( 2014 ) . http : //github.com/ correll/introduction-to-autonomous-robots/releases/download/v1.9/book.pdf 2 . craig , j.j. : introduction to robotics : mechanics and control , 3rd edn . pearson , boston ( 2005 ) 3 . king , a. : inertial navigation—forty year of evolution . gec rev . 13 ( 3 ) , 140–149 ( 1998 ) . http : // www.imar-navigation.de/downloads/papers/inertial_navigation_introduction.pdf 4 . oxford technical solutions : what be an ( ins ) inertial navigation system ? http : //www.oxts.com/ what-is-inertial-navigation-guide/references 93 5 . siegwart , r. , nourbakhsh , i.r. , scaramuzza , d. : introduction to autonomous mobile robots , 2nd edn . mit press , cambridge ( 2011 ) 6 . spong , m.w. , hutchinson , s. , vidyasagar , m. : robot modeling and control . wiley , new york ( 2005 ) open access this chapter be license under the term of the creative commons attribution 4.0 international license ( http : //creativecommons.org/licenses/by/4.0/ ) , which permit use , share , adaptation , distribution and reproduction in any medium or format , as long a you give appropriate credit to the original author ( s ) and the source , provide a link to the creative commons license and indicate if change be make . the image or other third party material in this chapter be include in the chapter ’ s creative commons license , unless indicate otherwise in a credit line to the material . if material be notincluded in the chapter ’ s creative commons license and your intend use be not permit by statutory regulation or exceed the permitted use , you will need to obtain permission directly from the copyright holder .","['chapter 5 robotic motion and odometry the robotics algorithms in the previous chapter react to data from their sensor by change the speed and direction of their motion , but the change be not quanti-tative . we didn ’ t require the robot to move twice a fast or to turn 90 ◦to the right . robots in the real world have to move to speciﬁc location and may have engineer- ing constraint on how fast or slow they can move or turn . this chapter present themathematics of robotic motion . sections 5.1and5.2review the concept of distance , time , velocity and acceler- ation that should be familiar from introductory physic . the physic of motion isusually teach use calculus , but a computer can not deal with continuous function ; instead , discrete approximation must be use a describe in sect . 5.3 . sections 5.4–5.6present odometry , the fundamental algorithm for compute robotic motion . an approximation of the location of a robot can be obtain by repeatedly compute the distance move and the change direction from the velocity of the wheel in a short period of time . unfortunately , odometry be subject to seriouserrors a show in sect . 5.7 . it be important to understand that error in direction be much more signiﬁcant than error in distance . in the simple implementation , the speed of the wheel of a robot be assume to be proportional to the power apply by the motor . section 5.8shows how the accuracy of odometry can be improve by use wheel encoders , which measure the actual number of revolution of the wheel . section 5.9presents an overview of inertial navigation , which be a sophisticated form of odometry base upon measure linear and angular acceleration and then inte-grating to obtain velocity and position . the sensor for inertial navigation ( accelerom- eters and gyroscope ) be once very expensive , limit it application to aircraft and rocket , but new technology call microelectromechanical system have make it possible to build robot with inertial navigation . © the author ( s ) 2018 m. ben-ari and f. mondada ,', 'to aircraft and rocket , but new technology call microelectromechanical system have make it possible to build robot with inertial navigation . © the author ( s ) 2018 m. ben-ari and f. mondada , elements of robotics , http : //doi.org/10.1007/978-3-319-62533-1_56364 5 robotic motion and odometry cars can not move up and down unlike helicopter and submarine which have great freedom of movement . this be express in the concept degree of freedom ( dof ) which be the subject sect . 5.10 . section 5.11 discuss the relation between the dof and number of actuator ( motor ) in a robotics system . the number of dof of a system do not mean that a system such a a vehicle can move freely in all those direction . a car can move to any point in the plane andorient itself in any direction , but it can not move sideways , so a difﬁcult maneuver be need during parallel parking . this be due to the difference between the dof and the degree of mobility ( dom ) , a subject explore in sect . 5.12 , along with the concept of holonomic motion that relate dof and dom . 5.1 distance , velocity and time suppose that a robot move with a constant velocity of 10 cm/s for a period of time of 5 s.1the distance it move be 50 cm . in general , if a robot move at a constant velocity vfor a period of time t , the distance it move be s=vt . when power be apply to the motor it cause the wheel to rotate , which in turn cause the robot to move at some velocity . however , we can not specify that a certain power cause acertain velocity : •no two electrical or mechanical component be ever precisely identical . a motor be compose of magnet and electrical wiring whose interaction cause a mechanical shaft to rotate . small difference in the property of the magnet and wire , as well a small difference in the size and weight of the shaft , can cause the shaft of two motor to rotate at slightly different speed for the same amount of power . •the environment affect the velocity of a robot . too little friction ( ice ) or too much friction ( mud ) can cause a', 'two motor to rotate at slightly different speed for the same amount of power . •the environment affect the velocity of a robot . too little friction ( ice ) or too much friction ( mud ) can cause a robot to move slow in comparison with it movement on a dry paved surface . •external force can affect the velocity of a robot . it need more power to sustain a speciﬁc velocity when move uphill and less power when move downhill , because the force of gravity decrease and increase the velocity . riding a bicycle at a constant velocity into the wind demand more effort than rid with thewind , and a cross-wind make the relation between power and velocity even more complicated . since s=vtit be sufﬁcient to measure any two of these quantity in order to compute the third . if we measure distance and time , we can compute the velocity asv=s/t . relatively short distance ( up to several meter ) can be measure accurately ( to within 1 cm ) use a ruler or a tape measure . the stopwatch application on a smartphone can measure time accurately ( hundredth of a second ) . 1velocity be speed in a direction . a robot can be move 10 cm/s forward or backwards ; in both case , the speed be the same but the velocity be different.5.1 distance , velocity and time 65 activity 5.1 : velocity over a ﬁxed distance •write a program that set your robot to a constant forward power setting . •mark two line 1 m apart on the ﬂoor . use a stopwatch to measure the time it take the robot to move between the line . compute the velocity of the robot . run the program ten time and record the velocity . do the velocity vary ? •place the robot on the ﬂoor and run it for 5 s. measure the distance that it move . compute the velocity . run the program ten time and record the velocity . do the velocity vary ? •which method give more precise result ? •repeat this experiment on different surface and discuss the result . activity 5.1shows that for a constant power set the velocity of a robot can vary signiﬁcantly . to accurately navigate within an environment , a', 'on different surface and discuss the result . activity 5.1shows that for a constant power set the velocity of a robot can vary signiﬁcantly . to accurately navigate within an environment , a robot need to sense object in it environment , such a wall , mark on the ﬂoor and object . 5.2 acceleration a change in velocity activity 5.1speciﬁed constant power setting and thus the velocity of the robot will be ( more or less ) constant . what happen when the velocity be vary ? activity 5.2 : change of velocity •run the ﬁrst program from activity 5.1varying the distance between the mark : 0 .25 , 0.5 , 1 , 1.5 , 2 m. for each distance , run the program several time and take the average of the computed velocity . are the velocity the samefor each distance ? •to improve the accuracy of the measurement , place mark on the ﬂoor at these distance and use the robot ’ s timer to record the time at which the mark aredetected . in activity 5.2 , you will ﬁnd that for the long distance the velocity will be close to each other , but for the shorter distance the velocity will differ considerably.the reason be that the formula v=s/tassumes that the velocity be constant over the entire distance . in reality , a vehicle must accelerate —change it velocity—in order to go from stand still to a constant velocity . similarly , a vehicle must decelerate in order to stop.66 5 robotic motion and odometry time ( t ) distance ( s ) fig . 5.1 an accelerating robot : distance increase a the square of time to get a true picture of the motion of a robot , we need to divide it motion into small segment s1 , s2 , ... : s1 s2 s3 s4 s5 x0 x1 x2 x3 x4 x5 and measure the distance and time for each segment individually . then , we can compute the velocity for each segment . in symbol , if we denote the length of thesegment s ibyδsi=xi+1−xiand the time it take the robot to cross segment siby δti=ti+1−ti , then vi , the velocity in segment siis give by : vi=δsi δti . figure 5.1is a graph of distance versus time for an accelerating robot . the time axis', 'take the robot to cross segment siby δti=ti+1−ti , then vi , the velocity in segment siis give by : vi=δsi δti . figure 5.1is a graph of distance versus time for an accelerating robot . the time axis have be divide into segment and the slopesδsi δtishow the average velocity in each segment which increase with time . acceleration be deﬁned a the change in velocity over a period of time : ai=δvi δti . when the power setting of the robot be set to a ﬁxed value , the force apply to the robot be constant and we expect that the acceleration remain constant , increase the velocity . however , at a certain point the acceleration be reduce to zero , mean that the velocity no longer increase , because the power apply to the wheel be just sufﬁcient to overcome the friction of the road and the wind resistance . let u see what happen if the power setting be increase with time.5.2 acceleration a change in velocity 67 activity 5.3 : acceleration •write a program that cause the robot to accelerate by increase the power set periodically . for example , start the robot at power 20 and increase to40 after 1 s , then to 60 after 2 s , to 80 after 3 s , and ﬁnally to 100 after 4 s. •place the robot on the track and run the program . •record the distance between each change of the power setting . compute and plot the velocity in each of these segment . 5.3 from segments to continuous motion as the size of the segment become small , we obtain the instantaneous velocityof the robot at a single point in time , express a a derivative : v ( t ) =ds ( t ) dt . similarly , the instantaneous acceleration of the robot be deﬁned a : a ( t ) =dv ( t ) dt . for constant acceleration the velocity can be obtain by integrate the derivative : v ( t ) =/integraldisplay ad t=a/integraldisplay dt=at , and then the distance can be obtain by integrate again : s ( t ) =/integraldisplay v ( t ) dt=/integraldisplay atd t=at2 2 . example an average car accelerate from 0 to 100 km/h in about 10 s. first , we convert unit from km/h to m/s : vmax=100 km/h', ': s ( t ) =/integraldisplay v ( t ) dt=/integraldisplay atd t=at2 2 . example an average car accelerate from 0 to 100 km/h in about 10 s. first , we convert unit from km/h to m/s : vmax=100 km/h =100·1000 60·60m/s=27.8m / s. assuming constant acceleration , vmax=27.8=at=10a , so the acceleration be 2 .78 m/s2 ( read , 2 .78 meter per second per second , that be , every second the speed increase by 2 .78 meter per second ) . the distance the car move in 10 s is:68 5 robotic motion and odometry s ( 10 ) =at2 2=2.78·102 2=139 m . activity 5.4 : computing distance when accelerate •for various vehicle ( race car , motorcycle ) look up the time require to accelerate from 0 to 100 km/h . compute the distance move . •assume that the acceleration of a vehicle increase linearly , that be , a=kt for a constant k. what be v ( t ) and s ( t ) ? •for several value of kand t , compute the ﬁnal velocity and distance . activity 5.5 : measuring motion at constant acceleration •write a program that apply the maximum power set to a robot . •place the robot on a surface and run the program . •when the robot seem to have reach full speed record the time from the start of the run . •compare the measured distance to s=at2/2 ( fig . 5.2b ) . •run again and measure the distance at ﬁxed interval of time . compute the speed from the distance divide by the time and compare to v=at ( fig . 5.2a ) . •in some robot you can set a target speed and read the actual speed . if your robot can do this , compare the measured speed with the computed speed . time ( t ) velocity ( v ) time ( t ) distance ( s ) ( a ) ( b ) fig . 5.2 a velocity for constant acceleration . bdistance for constant acceleration5.4 navigation by odometry 69 5.4 navigation by odometry suppose that you be in a car and your navigation system issue the following instruc- tion : “ in 700 m turn right. ” now your task be very simple : make observation of yourcar ’ s odometer which measure how far you have travel . when it value approach 700 m beyond it initial reading , look for a', '700 m turn right. ” now your task be very simple : make observation of yourcar ’ s odometer which measure how far you have travel . when it value approach 700 m beyond it initial reading , look for a street on the right . an odometer in a car measure speed and time , and multiply the two value to compute the distancetraveled . odometry —the measurement of distance—is a fundamental method use by robot for navigation . measuring time be easy use the internal clock of the embed-ded computer . measuring speed be more difﬁcult : in some educational robot wheel encoders be use to count the rotation of the wheel ( sect . 5.8 ) , while in others speed be estimate from property of the motor . from the distance move s=vt , the new position of the robot can be compute . in one dimension , the computation be trivial , but it become a bit more complex when the motion involve turn . thissection present the computation of distance by odometry , ﬁrst for a robot move linearly and then for a robot make a turn . section 5.7shows how error in head be more serious than error in distance . a disadvantage of odometry ( with or without wheel encoders ) be that the mea- surements be indirect , relate the power of the motor or the motion of the wheel to change in the robot ’ s position . this can be error-prone since the relation betweenmotor speed and wheel rotation can be very nonlinear and vary with time . further- more , wheel can slip and skid so there may be error in relate the motion of the wheel to the motion of the robot . improved estimate of position can be obtainedby use an inertial navigation system , which directly measure acceleration and angular velocity that can be use to determine the robot ’ s position ( sect . 5.9 ) . odometry be a form of localization : the robot must determine it position in the environment . in odometry we determine position by measure the change from the robot ’ s know initial position , while localization ( chap . 8 ) refers to the determination of the position of a robot relative to the know', 'we determine position by measure the change from the robot ’ s know initial position , while localization ( chap . 8 ) refers to the determination of the position of a robot relative to the know position of other object such a landmark or beacon . 5.5 linear odometry before study the mathematics of odometry you should try the following activity : activity 5.6 : distance from speed and time •run the robot at a constant power set for a speciﬁc period of time and measure the distance moved.70 5 robotic motion and odometry •repeat the measurement several time . is the distance constant ? if not , how much do it vary a a percentage of the distance ? •repeat the measurement several time for different power setting . is the dis- tance measure linear in the power set ? does the variation in the distance measurement on multiple run depend on the power set ? •repeat the measurement for a ﬁxed power setting but for different period of time and analyze the result . when a relation between motor power and velocity vhas be determine , the robot can compute the distance move by s=vt . if it start at position ( 0,0 ) and move straight along the x-axis , then after tseconds it new position be ( vt,0 ) . this activity should demonstrate that it be possible to measure distance by odom- etry with reasonable precision and accuracy . a self-driving car can use odometry to determine it position so that it doesn ’ t have to analyze it sensor data continuously to check if the required street have be reach . given the uncertainty of motionand of the road , the car should not depend only on odometry to decide when to turn , but the error will not be large and the sensor data can be analyze to detect the turn when odometry indicate that the car be in the vicinity of the intersection . activity 5.6asked you to measure the distance move in one dimension . three item of information need to be compute if the motion be in two dimension : therobot ’ s position ( x , y ) relative to a ﬁxed origin and it heading θ , the direction in which the', 'dimension . three item of information need to be compute if the motion be in two dimension : therobot ’ s position ( x , y ) relative to a ﬁxed origin and it heading θ , the direction in which the robot be point ( fig . 5.3 ) . the triple ( x , y , θ ) be call the pose of the robot . if the robot start at the origin ( 0,0 ) and move in a straight line at angle θ with velocity vfor time t , the distance move be s=vt . its new position ( x , y ) be : x=vtcosθ y=vtsinθ . ( 0,0 ) x y θ fig . 5.3 position and heading5.6 odometry with turns 71 5.6 odometry with turns suppose that the robot turn slightly leave because the right wheel move faster than the left wheel ( fig . 5.4 ) . in the ﬁgure , the robot be face towards the top of the page ; the blue dot be the left wheel , the red dot be the right wheel , and the black dot be the center of the robot which be halfway between the wheel . the baseline b be the distance between the wheel , and dl , dr , dcrepresent the distance move by the two wheel and the center when the robot turn . we want to compute the new position and heading of the robot . we can measure dland dr , the distance move by the two wheel use the method describe in activity 5.6 : relating motor power to rotational speed and then multiply by time . alternatively , we can use the number of rotation count by the wheel encoders . if the radius of a wheel be rand the rotational speed of the left and right wheel be ωl , ω rrevolutions per second , respectively , then after tseconds the wheel have move : di=2πrωit , i=l , r . ( 5.1 ) the task be to determine the new pose of the robot after the wheel have move these distance . figure 5.4shows the robot initially at pose ( x , y , φ ) , where the robot be face north ( φ=π/2 ) . after turn θradians , what be the new pose ( x/prime , y/prime , φ/prime ) ? clearly , the heading of the robot be now φ/prime=φ+θ , but we also have to compute x/prime , y/prime . the length of an arc of angle θradians be give by it fraction of the circumfer- ence of the circle : 2 πr (', 'heading of the robot be now φ/prime=φ+θ , but we also have to compute x/prime , y/prime . the length of an arc of angle θradians be give by it fraction of the circumfer- ence of the circle : 2 πr ( θ/2π ) =θr . for small angle , the distance dl , dc , drare approximately equal to the length of the corresponding arc , so we have : θ=dl/rl=dc/rc=dr/rr , ( 5.2 ) where rl , rr , rcare the distance from p , the origin of the turn . fig . 5.4 geometry of a left turn by a robot with two wheels72 5 robotic motion and odometry θθ dc dy θ dx fig . 5.5 change in head the distance dland drare obtain from the rotation of the wheel ( eq . 5.1 ) and the baseline bis a ﬁxed physical measurement of the robot . from eq . 5.2 , the angle θcan be compute : θrr=dr θrl=dl θrr−θrl=dr−dl θ= ( dr−dl ) / ( rr−rl ) θ= ( dr−dl ) /b . the center be halfway between the wheel rc= ( rl+rr ) /2 , so again by eq . 5.2 : dc=θrc =θ/parenleftbiggrl+rr 2/parenrightbigg =θ 2/parenleftbiggdl θ+dr θ/parenrightbigg =dl+dr 2 . if the distance move be small , the line label dci approximately perpendicular to the radius through the ﬁnal position of the robot . by similar triangle , we see thatθis the change in the heading of the robot ( fig . 5.5 ) . by trigonometry 2 : dx=− dcsinθ dy=dccosθ , 2you be probably expect co for dxand sin for dy . that would be the case if the robot be face along the xaxis . however , the initial pose be φ=π/2 and we have sin ( θ+π/2 ) =cosθ and co ( θ+π/2 ) =− sinθ.5.6 odometry with turns 73 so the pose of the robot after the turn be : ( x/prime , y/prime , φ/prime ) = ( −dcsinθ , dccosθ , φ+θ ) . the formula show how to compute the change dx , dyandθwhen the robot move a short distance . to compute odometry over long distance , this computation mustbe do frequently . there be two reason why the interval between the computation must be short : ( a ) the assumption of constant speed hold only for short distance , and ( b ) the trigonometric calculation be simpliﬁed by assume that the distance movedis short . activity 5.7 :', 'must be short : ( a ) the assumption of constant speed hold only for short distance , and ( b ) the trigonometric calculation be simpliﬁed by assume that the distance movedis short . activity 5.7 : odometry in two dimension •write a program that cause the robot to make a gentle left turn for a speciﬁc period of time . •compute the pose ( −dcsinθ , dccosθ , θ ) and compare the result with the value measure use a ruler and a protractor . run the program several time and see if the measurement be consistent . •run the program for different period of time . how do this affect the accu- racy and precision of the odometry computation ? 5.7 errors in odometry we have already note that odometry be not accurate because inconsistent measure-ments and irregularity in the surface can cause error . in this section we show thateven small change in the direction of the robot ’ s movement can cause error that be much large than those cause by change in it linear motion . to simplify the presentation , let u assume that a robot be to move 10 m from the origin of a coordinate system along the x-axis and then check it surroundings for a speciﬁc object . what be the effect of an error of up to p % ? if the error be in the measurement of x , the distance move , then δx , the error in xi : δx≤± 10·p 100=±p 10m , where the value be negative or positive because the robot could move up to p % before or after the intended distance . suppose now that there be an error p % i nt h e heading of the robot , and , for simplicity , assume that there be no error in the distance move . the geometry is:74 5 robotic motion and odometry the robot intend to move 10 m along the x-axis , but instead it move slightly to the left at an angle of δθ . let u compute the left-right deviation δy . by trigonometry , δy=10 sin δθ . an error of p % in heading be : δθ=360·p 100= ( 3.6p ) ◦ , so the left-right deviation be : δy≤± 10 sin ( 3.6p ) . the following table compare the difference between a linear error of p % ( l e f t ) and an error in heading of p % (', '100= ( 3.6p ) ◦ , so the left-right deviation be : δy≤± 10 sin ( 3.6p ) . the following table compare the difference between a linear error of p % ( l e f t ) and an error in heading of p % ( right ) : p % δx ( m ) 1 0.1 2 0.2 5 0.5 10 1.00p % δθ ( ◦ ) sinδθ δ y ( m ) 1 3.60.063 0 .63 2 7.20.125 1 .25 5 18.00.309 3 .09 10 36.00.588 5 .88 for a very small error like 2 % , the distance error after move 10 m be just 0 .2m , which should put the robot in the vicinity of the object it be search for , but a heading error of the same percentage place the robot 1 .25 m away from the object . for a more signiﬁcant error like 5 % or 10 % , the distance error ( 50 or 100 cm ) be still possibly manageable , but the heading error place the robot 3 .09 or 5 .88 m away , which be not even in the vicinity of the object . the accumulation of odometry error a the distance move get long be dis- played in fig . 5.6 . the initial position of the robot be denote by the dot at the origin . fig . 5.6 odometry errors5.7 errors in odometry 75 assuming an error of at most ±4 % in both the linear direction and the heading , the possible position of the robot after move d=1,2 , ... , 10 m be display a ellipsis . the minor radius of the error ellipses result from the linear error : 0.04s=0.04,0.08 , ... , 0.4m , while the major radius of the error ellipses result from the angular error : dsin ( 0.04·360◦ ) =dsin 14.4◦≈0.25,0.50 , ... 2.5m . clearly , the angular error be much more signiﬁcant than the linear error . since error be unavoidable , periodically the pose of the robot a compute by odometry must be compare with an absolute position ; this become the new initial position for further computation . methods for determine the absolute position of the robot be present in chap . 8 . activity 5.8 : odometry error •write a program to cause the robot to move in a straight line for 2 m. make sure that the surface be smooth so that it doesn ’ t turn off course and calibrate the motor setting so that the robot move as straight a possible . •vary', 'to move in a straight line for 2 m. make sure that the surface be smooth so that it doesn ’ t turn off course and calibrate the motor setting so that the robot move as straight a possible . •vary the motor power of both wheel together so that the robot run somewhat slow or somewhat fast than before . plot it position at ﬁxed interval and see if the error remain linear over the course . •vary the motor power of one wheel so that the robot turn slightly to one side . plot it position at ﬁxed interval and see if the error be proportional to the sine of the difference between the original heading and the new heading . activity 5.9 : combined effect of odometry error •write a program that cause the robot to move in a straight line for 2 m and then turn 360◦ . what be the error in the robot ’ s position ? •write a program that cause the robot to turn 360◦and then move in a straight line for 2 m. what be the error in the robot ’ s position ? is there a differencebetween this error and the error of the previous experiment ? •write a program that cause the robot to move in a straight line for 2 m , turn 180 ◦and then move in a straight line for 2 m. how far be it from it start position ? 76 5 robotic motion and odometry activity 5.10 : correcting odometry error •modify the program that you write for activity 5.8to introduce jitter , random variation in the power supply to the motor . check that the distance that therobot move in a ﬁxed time be not constant , but change slightly from run to run . •mark a goal line on the ﬂoor and compute the time it should take the robot to reach the goal . •when the robot have move for that period of time , see if it can ﬁnd the goal by move forward and backwards in small step until it detect the goal . 5.8 wheel encoders odometry in a wheeled vehicle like a car can be improve by measure the rotation of the wheel instead of map motor power into velocity . the circumference of a wheel be 2 πr , where ris the radius of the wheel in cm , so if nrotations be count , we know that the robot', 'rotation of the wheel instead of map motor power into velocity . the circumference of a wheel be 2 πr , where ris the radius of the wheel in cm , so if nrotations be count , we know that the robot have move 2 πnrcm . wheel encoders can be build that measure fraction of a revolution . if a signal be generate 8 time per revolution , the distance move be 2 πnr/8 cm , which nis now the number of signal count by the computer . there be many different way of implement wheel encoders . a popular design be to use a light source such a a light-emitting diode ( led ) , a light sensor and an encoding disk that be attach to the axis of the wheel ( fig . 5.7a ) . the disk be perforate with hole ( fig . 5.7b ) so that whenever the hole be opposite the light source , the sensor generate a signal . the support for wheel encoders in educational robot varies : •if a robot do not have wheel encoders it must be calibrate ; •the robot may have wheel encoders that be use internally ; •some robot like the lego®mindstorms enable the user to read the encoders . to motor encode diskwheel led sensor ( a ) ( b ) fig . 5.7 a optical wheel encoder . bencoding disk5.8 wheel encoders 77 the following activity propose an experiment to measure the distance move by count revolution of a wheel . it can be carry out even if your robot do not have wheel encoders or they be not accessible . activity 5.11 : wheel encode •make a mark at the top of a wheel of the robot use chalk or by attach a narrow piece of colored tape . write a program that cause the robot to move straight ahead for a ﬁxed period of time . run the program and take a video of the side of the robot use the camera on your smartphone . •view the video and determine the number of revolution by count the number of time the mark be at the top of the wheel . •measure the radius of the wheel and compute the distance move . how close be the result to the actual distance measure on the ﬂoor ? •repeat the measurement use n=2 and then n=4 equally spaced mark on the wheel . determine the number', 'compute the distance move . how close be the result to the actual distance measure on the ﬂoor ? •repeat the measurement use n=2 and then n=4 equally spaced mark on the wheel . determine the number of revolution by count the numberof time that a mark be at the top of the wheel and divide by n. compute the distance . 5.9 inertial navigation systems an inertial navigation system ( ins ) directly measure linear acceleration and angular velocity and use them to calculate the pose of a vehicle . the term inertial measure- ment unit ( imu ) be also use , but we prefer the term ins which refer to the entire system . integrating acceleration from the initial pose to the current time τgives the current velocity : v=/integraldisplayτ 0a ( t ) dt . similarly , integrate angular velocity give the change in heading : θ=/integraldisplayτ 0ω ( t ) dt . in an ins , we be not give continuous function to integrate ; instead , the acceleration and angular velocity be sample and summation replaces integration : vn=n/summationdisplay i=0anδt , θ n=n/summationdisplay i=0ωnδt.78 5 robotic motion and odometry front of vehicle front of vehicle ( a ) ( b ) fig . 5.8 a forward acceleration . bdeceleration ( brake ) inss be subject to error cause by inaccuracy in the measurement itself as well a by variation cause by environmental factor such a temperature , and by wear and tear of the unit . inertial measurement be often combine with gps ( sect . 8.3 ) t o update the position with an absolute location . inss for robot be construct with microelectromechanical system ( mems ) , which use integrated circuit manufacturing technique that combine mechanical ele- ments with electronics that interface with the robot ’ s computer . 5.9.1 accelerometers if you have ever ﬂown on an airplane you have experience a force push you backinto your seat a a result of the rapid acceleration of the airplane upon takeoff . upon land you be push away from your seat . you can also experience this in a car that accelerate rapidly or make an emergency stop .', 'a result of the rapid acceleration of the airplane upon takeoff . upon land you be push away from your seat . you can also experience this in a car that accelerate rapidly or make an emergency stop . acceleration be relate to forceby newton ’ s second law f=ma , where mi the mass . by measure the force on an object , we measure the acceleration . figures 5.8a , b show how an accelerometer can be build from an object ( call a mass ) connect to a spring . the great the acceleration , the great the force exert by the mass upon the spring , which in turn cause the spring to be compressed.the direction that the mass move give the sign of the acceleration : forward or backwards . the magnitude of the force be measure indirectly by measure the distance that the mass move . you can see that the diagram correspond to ourexperience : when a car accelerate , you be push back into the seat , but when it decelerate ( brake ) you continue forward . 5.9.2 gyroscopes agyroscope ( “ gyro ” ) use the principle of coriolis force to measure angular velocity . this concept be explain in textbook on physic and we will not go into it here.there be many type of gyros:5.9 inertial navigation systems 79 •classical gyro have spin mechanical disk which be mount on gimbal so that the axis of rotation remain ﬁxed in space . these gyro be extremely accurate but be very heavy and consume a lot of power . they be find on high-valuevehicles such a aircraft and rocket . •ring laser gyro ( rlg ) have ( almost ) no move part and be prefer over mechanical gyro for most application . they be base on send two laserbeams in opposite direction around a circular or triangular path . if the gyro be rotate , the path follow by one laser beam will be long than the path follow by the other beam . the difference be proportional to the angular velocity and canbe measure and transfer to the navigation computer . •coriolis vibratory gyroscope ( cvg ) manufacture use mems technique be find in smartphones and robot . they be inexpensive and extremely robust ,', 'measure and transfer to the navigation computer . •coriolis vibratory gyroscope ( cvg ) manufacture use mems technique be find in smartphones and robot . they be inexpensive and extremely robust , although their accuracy be not a good be the gyro previously discuss . we now give an overview of how they work . figure 5.9shows a cvg call a tuning fork gyroscope . two square mass be attach by ﬂexible beam to anchor that be mount on the base of the component.drivers force the mass to vibrate left and right . if the component rotate , the mass move upwards and downwards a distance proportional to the angular velocity . the mass and the electrode form the plate of capacitor whose capacitance increasesor decrease a the plate come together or move apart . fig . 5.9 tuning fork gyroscope ( courtesy of zhili hao , old dominion university ) 80 5 robotic motion and odometry fig . 5.10 physics of a tuning fork gyroscope : red dashed arrow and blue dot arrow indicate the direction of the vibration ; solid black arrow indicate the direction of the coriolis force the theory of operation of the tuning fork gyroscope be show in fig . 5.10.t h e mass ( gray square ) be force to vibrate at the same frequency like the two prong of a tuning fork . they vibrate in different direction , that be , they either approach each other ( blue dot arrow ) or they move away from each other ( dash red arrow ) . the component rotate around an axis perpendicular to it center ( the circle with across denote the rotational axis which be perpendicular to the plane of the paper ) . the coriolis force be a force whose direction be give by the vector cross product of the axis of the rotation and the movement of the mass , and whose magnitude isproportional to the linear velocity of the mass and the angular velocity of the gyro . since the mass be move in different direction , the result force will be equal but in opposite direction ( solid arrow ) . the mass approach or recede from theelectrodes ( small rectangle ) and the change in capacitance can be', 'direction , the result force will be equal but in opposite direction ( solid arrow ) . the mass approach or recede from theelectrodes ( small rectangle ) and the change in capacitance can be measure by a circuit . 5.9.3 applications an inertial navigation system have three accelerometer and three gyroscope so that the pose of the vehicle can be compute in three dimension . this be necessary for robotic aircraft and other robotic vehicle . airbags use an accelerometer that detect the rapid deceleration in the front-back direction that occur when a car crash . thiscauses an explosive expansion of the airbag . one can conceive of more application for these component in car . an accelerometer in the up-down direction can detect if the car have fall into a pothole . a gyroscope measure rotation around the verticalaxis can detect skidding , while the gyroscope measure rotation around the front- rear axis can detect if the car be roll over.5.10 degrees of freedom and numbers of actuators 81 ( 00 ) ( xy ) end effector ﬁxed rotate jointmovable rotate joint fig . 5.11 a two-link robotic arm with two dof 5.10 degrees of freedom and numbers of actuators the number of degree of freedom ( dof ) of a system be the dimensionality of the coordinate need to describe a pose of a mobile robot or the pose of the endeffector of a robotic manipulator . 3for example , a helicopter have six dof because it can move in the three spatial dimension and can rotate around the three ax . therefore , a six-dimensional coordinate ( x , y , z , φ , ψ , θ ) be need to describe it pose . the term use to describe rotation a helicopter can rotate around all three of it ax . the rotation be call : ( a ) pitch : the nose move up and down ; ( b ) roll : the body rotate around it lengthwise axis ; ( c ) yaw : thebody rotate leave and right around the axis of it rotor . the two-link robotic arm in fig . 5.11 have only two dof because it end effec- tor move in a plane and do not rotate ; therefore , it can be describe by a two- dimensional coordinate ( x ,', 'it rotor . the two-link robotic arm in fig . 5.11 have only two dof because it end effec- tor move in a plane and do not rotate ; therefore , it can be describe by a two- dimensional coordinate ( x , y ) . by examine fig . 5.3again , you can see that a mobile robot move on a ﬂat surface have three dof , because it pose be deﬁned by a three-dimensional coordinate ( x , y , θ ) . a train have only one dof since it be constrain by the track to move forward ( or occasionally backwards ) along the track . it only take one coordinate ( x ) , the train ’ s distance from an arbitrary origin of the track , to specify the pose of the train . we need more information than the degree of freedom to describe robotic motion . consider a vehicle like a car , a bicycle or an ofﬁce chair . although three coordinate ( x , y , θ ) be need to describe it pose , we can not necessarily move the vehicle directly from one pose to another . an ofﬁce chair can be move directly to any point of the plane and orient in any direction . a car or a bicycle at ( 2,0,0 ◦ ) ( point along the positive x-axis ) can not be move directly up the y-axis to position ( 2,2,0◦ ) . a more complex maneuver be need . 3this section and the following one be more advanced and can skip during your ﬁrst reading . furthermore , some of the example be of robotic manipulator describe in chap . 16.82 5 robotic motion and odometry fig . 5.12 a robot that can only rotate around an axis ( gray dot ) we need to know the number of it actuator ( usually motor ) and their conﬁg- uration . a differential drive robot have two actuator , one for each wheel , although the robot itself have three dof . the motor move the robot along one axis forwardsand backwards , but by apply unequal power we can change the heading of the robot . the two-link arm in fig . 5.11 have two motor , one at each rotate joint , so the number of actuator equal the number of dof . finally , a train have only one actuator , the motor that move it forward or backwards in it single dof . activity 5.12 :', 'at each rotate joint , so the number of actuator equal the number of dof . finally , a train have only one actuator , the motor that move it forward or backwards in it single dof . activity 5.12 : robot that can only rotate •figure 5.12 show a differential drive robot with a ﬁxed rod through it center of rotation . the rod prevent the robot change it position , allow it only to rotate around it vertical axis . characterize this conﬁguration : the numberof actuator and the number of dof . •what type of task could this robot perform ? what be the advantage and disadvantage of this conﬁguration ? 5.11 the relative number of actuators and dof let u analyze system where : •the number of actuator equal the number of dof ; •the number of actuator be few than the number of dof ; •the number of actuator be great than the number of dof . the number of actuators equals the number of dofa train have one actuator ( it engine ) that move the train along it single dof . the two-line robotic arm in fig . 5.11 have two actuator and two dof . a robotic gripper can be build with three motor that rotate the gripper in each of the three orientations5.11 the relative number of actuators and dof 83 ( roll , pitch , yaw ) . the advantage of have an equal number of actuator and dof be that the system be relatively easy to control : each actuator be individually command to move the robot to the desire position in the dof it control . the number of actuators be fewer than the number of dof mobile robot will usually have few actuator than dof . a robot with differential drive and a car have only two actuator , but they can reach all possible three- dimensional pose in the plane . having few actuator make the system less expen- sive , but the problem of planning and control motion be much more difﬁcult.parallel park a car be notorious for it difﬁculty : two rotation and a translation be need to perform a simple lateral move ( fig . 5.21a , b ) . an extreme example be a hot-air balloon which have only a single actuator ( a heater ) that', 'difﬁculty : two rotation and a translation be need to perform a simple lateral move ( fig . 5.21a , b ) . an extreme example be a hot-air balloon which have only a single actuator ( a heater ) that inject more or less hot air into the balloon and thus control it altitude . however , wind can cause the balloon to move in any of the three spatial direction and even to rotate ( at least partially ) in three orientation , so the operator of the balloon can neverprecisely control the balloon . a hot-air balloon therefore differs from an elevator : both have a single actuator , but the elevator be constrain by it shaft to move in only one dof . for another example of the complex relationship between the dof and number of actuator , the reader be invite to study ﬂight control in helicopter . helicoptersare highly maneuverable ( even more so than airplane which can ’ t ﬂy backwards ) , but a pilot control the helicopter ’ s ﬂight use only three actuator : •the cyclic control the pitch of the main rotor shaft which determine if the heli- copter move forward , backwards or to either side . •the collective control the pitch of the blade of the main rotor which determine if the helicopter move up or down . •the pedal control the speed of the tail rotor which determine the direction in which the nose of the helicopter point . the number of actuators be greater than the number of dof it doesn ’ t seem to be a good idea to have more actuator than dof , but in practice such conﬁgurations be often useful . the system in fig . 5.13a , b have more actuator than dof . the robotic manipulator arm in fig . 5.13a have four link rotate in the plane with actuator ( motor ) a1 , a2 , a3 , a4at the joint . we assume that the end effector be ﬁxed to the link from a4and can not rotate , so it pose be deﬁned by it position ( x , y ) and a ﬁxed orientation . therefore , although the arm have four actuator , it have only two dof because it can move the end effector only horizontally and vertically . the mobile robot with an arm ( fig . 5.13b )', '. therefore , although the arm have four actuator , it have only two dof because it can move the end effector only horizontally and vertically . the mobile robot with an arm ( fig . 5.13b ) have three actuator : a motor that move the robot forward and backwards , and motor for each of the two rotate joint . however , the system have only two dof since the pose of it end effector be deﬁnedby a two-dimensional ( x , y ) coordinate . systems with more actuator than dof be call redundant system .84 5 robotic motion and odometry a1a2a3 a4end effector a1a2end effector ( a ) ( b ) fig . 5.13 a robot arm : two dof and four actuator . bmobile robot and arm : two dof and three actuator ( a ) ( b ) fig . 5.14 a arm with four actuator can reach a hidden position . barm with two actuator be block by an obstacle if possible , engineer avoid use more than one actuator act on the same dof because it increase the complexity and cost of a system . the inverse kinematics ( sect . 16.2 ) of a redundant system result in an inﬁnite number of solution which complicate the operation of the system . for the mobile robot with the arm ( fig . 5.13b ) , there be an inﬁnite number of position of the base and arm that bring the end effectorto a speciﬁc reachable position . there be situation where a redundant system be require because the task could not be perform with few actuator . figure 5.14a show how the four-link robotic arm of fig . 5.13a can move the end effector to a position that be block by an obstacle and thus unreachable by a two-link arm ( fig . 5.14b ) , even though in both conﬁgurations the total length of the link be equal . an important advantage of redundant system arise from actuator with differ- ent characteristic . the mobile robot in fig . 5.13b can approach the target quickly , although it ﬁnal position might not be accurate because of error like uneven terrain . once the mobile robot stop , the motor in the joint which do not have to deal with the terrain can be precisely position . while the positioning be precise ,', 'because of error like uneven terrain . once the mobile robot stop , the motor in the joint which do not have to deal with the terrain can be precisely position . while the positioning be precise , these jointsdo not have the broad range of the mobile base.5.11 the relative number of actuators and dof 85 an example of a system with more actuators than dof figure 5.15 ( top and side view ) show a conﬁguration with two actuator and one dof . the system model a robotic crane that move a heavy weight to a speciﬁc drive wheelsroad wheel winchbearing and weight road wheeldriven wheelwinch bear weight fig . 5.15 robotic crane build from a mobile robot and a winch ( top view above , side view below ) ; in the side view the left wheel be not shown86 5 robotic motion and odometry fig . 5.16 robotic crane build from a thymio robot and lego®components vertical position . figure 5.16 show a crane build from a thymio robot and lego® component . the system be build from a mobile robot with differential drive , but the wheel be not directly use to control the motion of the system . instead , each wheel be an independent actuator . ( recall that the power to each wheel of a differential drive robot can be set independently to any value in a range such a −100 to 100 . ) the robot face leave . the right driven wheel in fig . 5.15 ( the black rectangle at the top of the top view and hidden behind the robot in the side view ) control a pair of ( gray ) road wheel that move the robot rapidly forward and backwards . in turn , this cause the cable to move the weight rapidly up and down . the road wheel be mount on a structure ( in blue ) that be ﬁxed to the robot body . there be several option for transfer power from the right driven wheel to the road wheel : friction , pulley and belt , and gear . each option have it own advantage and disadvantage , and all three be use in car : the clutch use friction , belt be use for timing and to run auxiliary component like water pump , and gear be use in the transmission to control the torque', ', and all three be use in car : the clutch use friction , belt be use for timing and to run auxiliary component like water pump , and gear be use in the transmission to control the torque apply to each wheel . the left driven wheel ( the black rectangle at the bottom of the top view and at the front of the side view ) control a winch ( red ) that roll or unroll a cable attach to the weight that move up or down over a ﬁxed bearing . the winch have a diameter much small than the diameter of the driven wheel , so it can move the weight insmall increment a the left drive wheel rotates . the design goal be to be able to perform precise positioning of the weight even though the winch move the cable at a much slow speed than do the robot body.5.11 the relative number of actuators and dof 87 there be two activity for this section . this activity be for reader who have good construction skill and an appropriate robotics kit . the second activity suggest alternate way of demonstrate the concept of two actuator in one dof . activity 5.13 : robotic crane •construct the robotic crane show in fig . 5.15 . explain your choice of mech- anism for connect the driven wheel to the road wheel . •write a program that give the current position of the weight and a goal position move the weight to the goal position . alternatively , send commandsto the motor use a remote control device or a computer connect to the robot . •experiment with the relative rotational speed of the left and right driven wheel that control the road wheel and the winch , respectively . should you move the two actuator separately or simultaneously ? activity 5.14 : robotic crane ( alternative ) •write a program that cause a mobile robot to move forward and backwards . place a piece of black tape relatively far from the initial position of the robot.the goal be to cause the robot to stop as near a possible to the start of the tape without continuously check the sensor . •the program have three mode of operation . ( 1 ) the robot move fast , check it sensor', 'to cause the robot to stop as near a possible to the start of the tape without continuously check the sensor . •the program have three mode of operation . ( 1 ) the robot move fast , check it sensor occasionally , and stop when it detect the tape . ( 2 ) as in ( 1 ) but the robot move slowly , check it sensor relatively often . ( 3 ) as in ( 1 ) but when the tape be detect , the robot move backwards use the speed and sample period a in ( 2 ) . •run the program and compare the result of the three mode : the time until the robot stop and error between the robot ’ s ﬁnal position and the start of the tape . •alternatively , run the program with the three mode on a computer and exper- iment with the motion parameter and the sampling period . you will need to choose a model for the motion : constant velocity , constant acceleration , or ( more realistically ) acceleration then constant velocity and ﬁnally decelera-tion when the tape be detected.88 5 robotic motion and odometry 5.12 holonomic and non-holonomic motion section 5.10 present the concept of degree of freedom ( dof ) and the role of the number of actuator . there be another concept that link the dof and the actuatorsin the case of mobile robot : the degree of mobility ( dom ) . the degree of mobility δ mcorresponds to the number of degree of freedom that can be directly access by the actuator . a mobile robot in the plane have at most three dof ( ( x , y ) position and heading ) , so the maximal degree of mobility of a mobile robot be δm=3 . let consider the dom of various vehicle . a train have one dof because it can only move forward along the track , and it have one actuator , it engine , that directlyaffects this single degree of freedom . therefore , a train have a degree of mobility of δ m=1 , mean that the single dof can be directly access by the actuator . a robot with differential drive have three dof . the two actuator be the two motor which act on the wheel . they can directly access two dof : ( a ) if both wheel turn at the same speed , the', '. a robot with differential drive have three dof . the two actuator be the two motor which act on the wheel . they can directly access two dof : ( a ) if both wheel turn at the same speed , the robot move forward or backwards ; ( b ) if the wheel havespeeds in opposite direction , the robot rotate in place . therefore , we can directly access the dof along the forward axis of translation and the dof of the heading , but we can not directly access the dof of the lateral axis of translation ( fig . 5.17a ) . a differential drive mobile robot have a degree of mobility δ m=2 < 3= # dof . a car , like a robot with differential drive , have only two actuator for three dof : one actuator , the motor , give direct access to the degree of freedom along the longitudinalaxis of the car , enable it to move forward and backwards . the other actuator , the steer wheel , do notgive direct access to any additional dof , it can only orient the ﬁrst dof . the car can not rotate around the vertical axis and it can not move laterally ( fig . 5.17b ) . therefore , a car have only one degree of mobility , δ m=1 . intuitively , you can see the low degree of mobility of a car compare with a robot with differential drive by note that the robot can rotate in place while the car can not . accessible dofnon-accessible dof accessibledofnon-accessibledof ( a ) ( b ) fig . 5.17 a accessible and non-accessible dof for a robot with differential drive . baccessible and non-accesible dof for a robot with ackermann steering5.12 holonomic and non-holonomic motion 89 fig . 5.18 a swedish wheel . bomnidirectional robot ( courtesy lami-epfl ) by itself , a standard wheel have δm=2 : it can roll forward and backwards and it can rotate around the vertical axis that go through the point of contact of the wheel with the ground . a wheel can not move sideways , which be actually a good thing because it prevent the vehicle from skid off the road during a turn . in the car , the degree of mobility be reduce even far to δ m=1 , because there be two pair of', ', which be actually a good thing because it prevent the vehicle from skid off the road during a turn . in the car , the degree of mobility be reduce even far to δ m=1 , because there be two pair of wheel , one in the front and one in the rear of the car . this conﬁguration make it impossible for the car to rotate around it vertical axis , even though the individualwheels can do so ( usually only the front wheel ) . the limitation to δ m=1g i v e s stability to the car—it can not skid laterally and it can not rotate—making it easy and safe to drive at high speed . in fact , an accident can occur when rain or snow reducethe friction so that the car can skid or rotate . an autonomous mobile robot can proﬁt if it have a great dom δ m=3 . to directly access the third dof , the robot need to be able to move laterally . one method be to have the robot roll on a ball or a castor wheel like an ofﬁce chair . another method be to use swedish wheel ( fig . 5.18a ) . a swedish wheel be a standard wheel that have small free wheel along it rim so that it can move laterally , enable direct access to the third dof . mobile robot that can directly access all three dof ( δm=3 ) be call omnidi- rectional robot . figure 5.18b show an omnidirectional robot construct with four swedish wheel . the two pair of wheel on opposite side of the robot can directly move the robot leave , right , forward and backwards . this conﬁguration be redundantbut very easy to control . to avoid redundancy , most omnidirectional robot have three swedish wheel mount at an angle of 120 ◦from each other ( fig . 5.19 ) . this conﬁguration have δm=3 but be not easy to control use the familiar x , ycoordinates . the relative value of the dof and the dom of a robot deﬁne the concept of holonomic motion . a robot have holonomic motion ifδm= # dof and it have non- holonomic motion δm < # dof . a holonomic robot like the one in fig . 5.18b can directly control all it dof without difﬁcult maneuver . figure 5.20 show how easy it be for the omnidirectional robot with', 'motion δm < # dof . a holonomic robot like the one in fig . 5.18b can directly control all it dof without difﬁcult maneuver . figure 5.20 show how easy it be for the omnidirectional robot with swedish wheel ( fig . 5.19 ) to perform parallel parking.90 5 robotic motion and odometry drive motion free motion motor fig . 5.19 omnidirectional robot with three swedish wheel fig . 5.20 parallel parking by an omnidirectional robot a car and a robot with differential drive be non-holonomic because their dom ( δm=1 and 2 , respectively ) be low than their dof which be three . because of this limited degree of mobility , these vehicle need complex steer maneuver , forexample , to perform parallel parking . there be a signiﬁcant difference between the two vehicle . the differential drive robot need three separate movement , but they be very simple ( fig . 5.21a ) : rotate leave , move backwards , rotate right . the car also need three separate movement , but they be extremely difﬁcult to perform correctly ( fig . 5.21b ) . you have to estimate where to start the maneuver , how sharp to make each turn and how far to move between turn . the high dom of the differential drive robot be advantageous in this situation.5.12 holonomic and non-holonomic motion 91 ( a ) ( b ) fig . 5.21 a parallel parking for a non-holonomic differential drive robot . bparallel park for a non-holonomic car anchor winch winch fig . 5.22 robot for clean a wall activity 5.15 : holonomic and non-holonomic motion •look again at the mobile robot which be constrain to rotational motion only ( fig . 5.12 ) . what be it degree of mobility δm ? it be holonomic or not ? •figure 5.22 show a robot for clean the wall of a building . there be two anchor from which cable descend , pass through eye ﬁxed to the robot ’ s body and then to winch power by the robot ’ s wheel . by roll and unroll the cable , the robot move up and down the wall . however , if thetwo motor do not cause the wheel move precisely at the same rotational velocity , the robot will swing from', '. by roll and unroll the cable , the robot move up and down the wall . however , if thetwo motor do not cause the wheel move precisely at the same rotational velocity , the robot will swing from side to side . how many dof and how many dom do this robot have ? is it holonomic or not ? 92 5 robotic motion and odometry 5.13 summary a mobile robot like a self-driving car or a mars explorer will not have landmark always available for navigation . odometry be use to bring the robot to the vicinityof it goal without reference to the environment . the robot estimate it speed and rotational velocity from the power apply to it motor . odometry can be improve by use wheel encoders to measure the number of revolution of the wheel , ratherthan infer the velocity from the motor power . the change in the position of an inexpensive robot move in a straight line can be compute by multiply speed by time . if the robot be turn , trigonometric calculation be need to computethe new position and orientation . even with wheel encoders , odometry be subject to error that can be very large if the error be in the heading . inertial navigation use accelerometer and gyroscope to improve the accuracy of odometry . integrating acceleration give velocity and integrate angular velocity give the heading . microelectromechanical system have make inertial navigationinexpensive enough for use in robotics . the dof of a system be the number of dimension in which it can move—up to three dimension on a surface and up to six dimension in the air or underwater—buta robot may be constrain to have few than the maximum number of dof . an additional consideration be the number and conﬁguration of the actuator of a robot which deﬁne it degree of mobility . if the dom be equal to the number of dof , therobot be holonomic and it can move directly from one pose to another , although it may be difﬁcult to control . if the dom be less than the number of dof , the robot be non-holonomic ; it can not move directly from one pose to another and will requirecomplex', 'another , although it may be difﬁcult to control . if the dom be less than the number of dof , the robot be non-holonomic ; it can not move directly from one pose to another and will requirecomplex maneuver to carry out some task . 5.14 further reading a detailed mathematical treatment of odometry error in two dimension be give in [ 5 , sect . 5.24 ] . for an overview of inertial navigation see [ 3,4 ] . advanced textbooks on robotics present holonomy [ 1,2,5,6 ] . references 1 . correll , n. : introduction to autonomous robots . createspace ( 2014 ) . http : //github.com/ correll/introduction-to-autonomous-robots/releases/download/v1.9/book.pdf 2 . craig , j.j. : introduction to robotics : mechanics and control , 3rd edn . pearson , boston ( 2005 ) 3 . king , a. : inertial navigation—forty year of evolution . gec rev . 13 ( 3 ) , 140–149 ( 1998 ) . http : // www.imar-navigation.de/downloads/papers/inertial_navigation_introduction.pdf 4 . oxford technical solutions : what be an ( ins ) inertial navigation system ? http : //www.oxts.com/ what-is-inertial-navigation-guide/references 93 5 . siegwart , r. , nourbakhsh , i.r. , scaramuzza , d. : introduction to autonomous mobile robots , 2nd edn . mit press , cambridge ( 2011 ) 6 . spong , m.w. , hutchinson , s. , vidyasagar , m. : robot modeling and control . wiley , new york ( 2005 ) open access this chapter be license under the term of the creative commons attribution 4.0 international license ( http : //creativecommons.org/licenses/by/4.0/ ) , which permit use , share , adaptation , distribution and reproduction in any medium or format , as long a you give appropriate credit to the original author ( s ) and the source , provide a link to the creative commons license and indicate if change be make . the image or other third party material in this chapter be include in the chapter ’ s creative commons license , unless indicate otherwise in a credit line to the material . if material be notincluded in the chapter ’ s creative commons license and your intend use be', 'the chapter ’ s creative commons license , unless indicate otherwise in a credit line to the material . if material be notincluded in the chapter ’ s creative commons license and your intend use be not permit by statutory regulation or exceed the permitted use , you will need to obtain permission directly from the copyright holder .']",https://doi.org/10.48550/arXiv.2007.00258
13.pdf,"visual odometry part i : the first 30 years and fundamentals by davide scaramuzza and friedrich fraundorfer visual odometry ( vo ) be the process of estimate the egomotion of an agent ( e.g. , vehicle , human , and robot ) use only the input of a single or multiple camera attach to it . application domains include robotics , wearable computing , augment reality , and automotive . the term vowas coin in 2004 by nis- ter in his landmark paper [ 1 ] . the term be choose for it similarity to wheel odometry , which incrementally esti-mates the motion of a vehicle by integrate the number of turn of it wheel over time . likewise , vo operate by incrementally estimate the pose of the vehicle throughexamination of the change that motion induces on the image of it onboard camera . for vo to work effec- tively , there should be sufficient illumination in the envi-ronment and a static scene with enough texture to allow apparent motion to be extract . furthermore , consecu- tive frame should be capture by ensure that they havesufficient scene overlap.the advantage of vo with respect to wheel odometry be that vo be not affect by wheel slip in uneven terrain orother adverse condition . it have be demonstrate that compare to wheel odometry , vo provide more accurate trajectory estimate , with relative position error rangingfrom 0.1 to 2 % . this capability make vo an interesting supplement to wheel odometry and , additionally , to other navigation system such a global positioning system ( gps ) , inertial measurement unit ( imus ) , and laser odometry ( similar to vo , laser odometry estimate the egomotion of a vehicle by scan-matching of consecutivelaser scan ) . in gps-denied environment , such a under- water and aerial , vo have utmost importance . this two-part tutorial and survey provide a broad introduction to vo and the research that have be under- take from 1980 to 2011 . although the first two decade witness many offline implementation , only in the thirddecade do real-time work system flourish , which have lead vo to be use on another planet by two mars-explora- tion rover for the first time . part i ( this tutorial ) present ahistorical review of the first 30 year of research in this field and it fundamental . after a brief discussion on camera 80ieee robotics & automation magazine december 2011 1070-9932/11/ $ 26.00 ª2011 ieeedigital object identifier 10.1109/mra.2011.943233 date of publication : 8 december 2011 © digital visionmodeling and calibration , it describe the main motion- estimation pipeline for both monocular and binocular scheme , outline pro and con of each implementation.part ii will deal with feature matching , robustness , and application . it will review the main point-feature detector use in vo and the different outlier-rejection scheme . par-ticular emphasis will be give to the random sample consen- sus ( ransac ) , and the distinct trick devise to speed it up will be discuss . other topic cover will be error model-ing , location recognition ( or loop-closure detection ) , and bundle adjustment . this tutorial provide both the experienced and non- expert user with guideline and reference to algorithms to build a complete vo system . since an ideal and unique vo solution for every possible working environmentdoes not exist , the optimal solution should be choose carefully accord to the specific navigation environ- ment and the give computational resource . history of visual odometry the problem of recover relative camera pose andthree-dimensional ( 3-d ) structure from a set of camera image ( calibrate or non calibrate ) be know in the computer vision community a structure from motion ( sfm ) . its origin can be date back to work such a [ 2 ] and [ 3 ] . vo be a particular case of sfm . sfm be more gen- eral and tackle the problem of 3-d reconstruction ofboth the structure and camera pose from sequentially order or unordered image set . the final structure and camera pose be typically refine with an offline optimi-zation ( i.e. , bundle adjustment ) , whose computation time grow with the number of image [ 4 ] . conversely , vo focus on estimate the 3-d motion of the camerasequentially —as a new frame arrive —and in real time . bundle adjustment can be use to refine the local estimate of the trajectory . the problem of estimate a vehicle ’ s egomotion from visual input alone start in the early 1980s and be describe by moravec [ 5 ] . it be interest to observe thatmost of the early research in vo [ 5 ] – [ 9 ] be do for planetary rover and be motivate by the nasa mars exploration program in the endeavor to provide all-terrainrovers with the capability to measure their 6-degree-of- freedom ( dof ) motion in the presence of wheel slippage in uneven and rough terrain . the work of moravec stand out not only for present- ing the first motion-estimation pipeline —whose main functioning block be still use today —but also for describe one of the early corner detector ( after the first one propose in 1974 by hannah [ 10 ] ) which be know today a the moravec corner detector [ 11 ] , a prede- cessor of the one propose by forstner [ 12 ] and harris and stephens [ 3 ] , [ 82 ] . moravec test his work on a planetary rover equip with what he term a slider stereo : a single camera slide on a rail . the robot move in a stop-and-go fashion , digitize and analyze image at every location . at each stop , the camera slide horizontally take nine picture at equidistant interval . corners be detect in an imageusing his operator and match along the epipolar line of the other eight frame use normalized cross correlation . potential match at the next robotlocations be find again by correla- tion use a coarse-to-fine strategy to account for large-scale change . out-liers be subsequently remove by check for depth inconsistency in the eight stereo pair . finally , motionwas compute a the rigid body transformation to align the triangu- lated 3-d point see at two consecu-tive robot position . the system of equation be solve via a weighted least square , where the weight wereinversely proportional to the dis- tance from the 3-d point . although moravec use a single sliding camera , his work belongs to the class of stereo vo algorithm . this terminology account for the fact that the relative 3-d position of the feature be directly measure by triangula-tion at every robot location and use to derive the relative motion . trinocular method belong to the same class of algorithm . the alternative to stereo vision be to use asingle camera . in this case , only bear information be available . the disadvantage be that motion can only be recover up to a scale factor . the absolute scale can thenbe determine from direct measurement ( e.g. , measure the size of an element in the scene ) , motion constraint , or from the integration with other sensor , such a imu , air-pressure , and range sensor . the interest in monocular method be due to the observation that stereo vo can degenerate to the monocular case when the distance to thescene be much large than the stereo baseline ( i.e. , the dis- tance between the two camera ) . in this case , stereo vision become ineffective and monocular method must be used.over the year , monocular and stereo vos have almost progress a two independent line of research . in the remainder of this section , we have survey the relatedwork in these field . stereo vo most of the research do in vo have be produce use stereo camera . building upon moravec ’ s work , matthies and shafer [ 6 ] , [ 7 ] use a binocular system and moravec ’ sprocedure for detect and track corner . instead of use a scalar representation of the uncertainty a moravec do , they take advantage of the error covariance matrix ofthe triangulate feature and incorporate it into the motion estimation step . compared to moravec , they dem- onstrated superior result in trajectory recovery for aplanetary rover , with 2 % relative error on a 5.5-m path . olson et al . [ 9 ] , [ 13 ] later extend that work by• the advantage of vo with respect to wheel odometry be that vo be not affect by wheel slip in uneven terrain or other adverse condition . • december 2011 ieee robotics & automation magazine 81•introducing an absolute orientation sensor ( e.g. , compass or omnidirectional camera ) and use the forstner corner detector , which be significantly faster to compute than moravec ’ s operator . they show that the use of camera egomotion estimate alone result in accumulation errorswith superlinear growth in the distance travel , lead to increase orienta- tion error . conversely , when an abso-lute orientation sensor be incorporate , the error growth can be reduce to a linear function of the distance traveled.this lead them to a relative position error of 1 :2 % on a 20-m path . lacroix et al . [ 8 ] implement a stereo vo approach for planetary rover similar to those explain earlier . the difference lie in the selection of key point . instead of use the forstner detector , theyused dense stereo and , then , select the candidate key point by analyze the correlation function around it peaks—an approach that be later exploit in [ 14 ] , [ 15 ] , and other work . this choice be base on the observa- tion that there be a strong correlation between the shape of the correlation curve and the standard deviation of thefeature depth . this observation be later use by cheng et al . [ 16 ] , [ 17 ] in their final vo implementation onboard the mars rover . they improve on the early implemen-tation by olson et al . [ 9 ] , [ 13 ] in two area . first , after use the harris corner detector , they utilize the curva- ture of the correlation function around the feature —as propose by lacroix et al . —to define the error covariance matrix of the image point . second , a propose by nister et al . [ 1 ] , they use the random sample consensus ( ran-sac ) ransac [ 18 ] in the least-squares motion estima- tion step for outlier rejection . a different approach to motion estimation and outlier removal for an all-terrain rover be propose by milella and siegwart [ 14 ] . they use the shi-tomasi approach [ 19 ] for corner detection , and similar to lacroix , theyretained those point with high confidence in the stereo disparity map . motion estimation be then solve by first use least square , a in the method earlier , and then theiterative close point ( icp ) algorithm [ 20 ] —an algorithm popular for 3-d registration of laser scan —for pose refinement . for robustness , an outlier removal stage wasincorporated into the icp . the work mention so far have in common that the 3-d point be triangulate for every stereo pair , and therelative motion be solve a a 3-d-to-3-d point registration ( alignment ) problem . a completely different approach be propose in 2004 by nister et al . [ 1 ] . their paper be knownnot only for coin the term vo but also for provide the first real-time long-run implementation with a robust outlier rejection scheme . nister et al . improve the earlierimplementations in several area . first , contrary to all previous work , they do not track feature among framesbut detect feature ( harris corner ) independently in all frame and only allow match between feature . this have the benefit of avoid feature drift during cross-corre-lation-based tracking . second , they do not compute the relative motion a a 3-d-to-3-d point registration problem but a a 3-d-to-two-dimensional ( 2-d ) camera-pose estima-tion problem ( these method be describe in the “ motion estimation ” section ) . finally , they incorporate ransac outlier rejection into the motion estimation step . a different motion estimation scheme be introduce by comport et al . [ 21 ] . instead of use 3-d-to-3-d point registration or 3-d-to-2-d camera-pose estimation tech-niques , they rely on the quadrifocal tensor , which allow motion to be compute from 2-d-to-2-d image match without have to triangulate 3-d point in any ofthe stereo pair . the benefit of use directly raw 2-d point in lieu of triangulated 3-d point lay in a more accurate motion computation . monocular vo the difference from the stereo scheme be that in themonocular vo , both the relative motion and 3-d structure must be compute from 2-d bearing data . since the abso- lute scale be unknown , the distance between the first twocamera pose be usually set to one . as a new image arrive , the relative scale and camera pose with respect to the first two frame be determine use either the knowledge of3-d structure or the trifocal tensor [ 22 ] . successful result with a single camera over long distan- ce ( up to several kilometer ) have be obtain in thelast decade use both perspective and omnidirectional camera [ 23 ] – [ 29 ] . related work can be divide into three category : feature-based method , appearance-based meth-ods , and hybrid method . feature-based method be base on salient and repeatable feature that be track over the frame ; appearance-based method use the intensity infor-mation of all the pixel in the image or subregions of it ; and hybrid method use a combination of the previous two . in the first category be the work by the author in [ 1 ] , [ 24 ] , [ 25 ] , [ 27 ] , and [ 30 ] – [ 32 ] . the first real-time , large- scale vo with a single camera be present by nister et al . [ 1 ] . they use ransac for outlier rejection and 3-d-to-2-d camera-pose estimation to compute the new upcoming camera pose . the novelty of their paper be the use of a five-point minimal solver [ 33 ] to calculate themotion hypothesis in ransac . after that paper , five- point ransac become very popular in vo and be use in several other work [ 23 ] , [ 25 ] , [ 27 ] . corke et al . [ 24 ] provide an approach for monocular vo base on omni- directional imagery from a catadioptric camera and optical flow . lhuillier [ 25 ] and mouragnon et al . [ 30 ] present anapproach base on local windowed-bundle adjustment to recover both the motion and the 3-d map ( this mean that bundle adjustment be perform over a window of the last mframes ) . again , they use the five-point ransac in [ 33 ] to remove the outlier . tardif et al . [ 27 ] present an• keyframe selection be av e r yi m p o r t a n t t e p in vo and should always be do before update the motion . • 82ieee robotics & automation magazine december 2011•approach for vo on a car over a very long run ( 2.5 km ) without bundle adjustment . contrary to the previous work , they decouple the rotation and translation estimation . therotation be estimate by use point at infinity and the translation from the recover 3-d map . erroneous corre- spondences be remove with five-point ransac . among the appearance-based or hybrid approach be the work by the author in [ 26 ] , [ 28 ] , and [ 29 ] . goecke et al . [ 26 ] use the fourier –mellin transform for register perspective image of the ground plane take from a car . milford and wyeth [ 28 ] present a method to extract approximate rotational and translational velocity informa-tion from a single perspective camera mount on a car , which be then use in a ratslam scheme [ 34 ] . they use template tracking on the center of the scene . a major draw-back with appearance-based approach be that they be not robust to occlusion . for this reason , scaramuzza and sieg- wart [ 29 ] use image appearance to estimate the rotationof the car and feature from the ground plane to estimate the translation and the absolute scale . the feature-based approach be also use to detect failure of the appearance-based method . all the approach mention early be design for unconstrained motion in 6 dof . however , several voworks have be specifically design for vehicle with motion constraint . the advantage be decrease computa- tion time and improved motion accuracy . for instance , liang and pears [ 35 ] , ke and kanade [ 36 ] , wang et al . [ 37 ] , and guerrero et al . [ 38 ] take advantage of homogra- phies for estimate the egomotion on a dominant groundplane . scaramuzza et al . [ 31 ] , [ 39 ] introduce a one-point ransac outlier rejection base on the vehicle nonholo- nomic constraint to speed up egomotion estimation to400 hz . in the follow-up work , they show that nonholo- nomic constraint allow the absolute scale to be recover from a single camera whenever the vehicle make a turn [ 40 ] . following that work , vehicle nonholonomic con- straints have also be use by pretto et al . [ 32 ] for improv- ing feature track and by fraundorfer et al . [ 41 ] forwindowed bundle adjustment ( see the following section ) . reducing the drift since vo work by compute the camera path incre- mentally ( pose after pose ) , the error introduce by each new frame-to-frame motion accumulate over time . thisgenerates a drift of the estimated trajectory from the real path . for some application , it be of utmost importance to keep drift as small a possible , which can be donethrough local optimization over the last mcamera pose . this approach —called slide window bundle adjust- ment orwindowed bundle adjustment —has be use in several work , such a [ 41 ] – [ 44 ] . in particular , on a 10-km vo experiment , konolige et al . [ 43 ] demonstrate that windowed-bundle adjustment can decrease the finalposition error by a factor of 2 –5 . obviously , the vo drift can also be reduce through combination with othersensors , such a gps and laser , or even with only an imu [ 43 ] , [ 45 ] , [ 46 ] . v-slam although this tutorial focus on vo , it be worth mention- ing the parallel line of research undertake by visual simul-taneous localization and mapping ( v-slam ) . for an in- depth study of the slam problem , the reader be refer to two tutorial on this topic by durrant-whyte and bailey [ 47 ] , [ 48 ] . two methodology have become predominant in v-slam : 1 ) filter method fuse the informationfrom all the image with a probability distribution [ 49 ] and 2 ) nonfiltering method ( also call keyframe meth- od ) retain the optimization of global bundle adjustment to select key- frame [ 50 ] . the main advantage ofeither approach have be evaluate and summarize in [ 51 ] . in the last few year , successful result have be obtain use both single and stereo camera [ 49 ] , [ 52 ] – [ 62 ] . most of these work have be limit to small , indoor workspace and only a few of them have recently be design for large-scale area [ 54 ] , [ 60 ] , [ 62 ] . some of the early work in real-time v-slam be present bychiuso et al . [ 52 ] , deans [ 53 ] , and davison [ 49 ] use a full-covariance kalman approach . the advantage of davi- son ’ s work be to account for repeatable localization afteran arbitrary amount of time . later , handa et al . [ 59 ] improve on that work use an active matching technique base on a probabilistic framework . civera et al . [ 60 ] builtupon that work by propose a combination of one-point ransac within the kalman filter that use the available prior probabilistic information from the filter in the ran-sac model-hypothesis stage . finally , strasdat et al . [ 61 ] present a new framework for large-scale v-slam that take advantage of the keyframe optimization approach [ 50 ] while take into account the special character of slam . vo versus v-slam in this section , the relationship of vo with v-slam be analyze . the goal of slam in general ( and v-slam in particular ) be to obtain a global , consistent estimate of therobot path . this imply keep a track of a map of the environment ( even in the case where the map be not need per se ) because it be need to realize when therobot return to a previously visit area . ( this be call loop closure . when a loop closure be detect , this informa- tion be use to reduce the drift in both the map and camerapath . understanding when a loop closure occur and effi- ciently integrate this new constraint into the current map be two of the main issue in slam . ) conversely , voaims at recover the path incrementally , pose after pose , and potentially optimize only over the last n pose of the• vo be only concerned with the local consistency of the trajectory , whereas slam with the global consistency . • december 2011 ieee robotics & automation magazine 83•path ( this be also call windowed bundle adjustment ) . this slide window optimization can be consider equivalent to build a local map in slam ; however , the philosophyis different : in vo , we only care about local consistency of the trajectory and the local map be use to obtain a more accurate estimate of the local trajectory ( for example , inbundle adjustment ) , whereas slam be concern with the global map consistency . vo can be use a a building block for a complete slam algorithm to recover the incremental motion of the camera ; however , to make a complete slam method , onemust also add some way to detect loop closing and possibly a global optimiza- tion step to obtain a metrically consist-ent map ( without this step , the map be still topologically consistent ) . if the user be only interested in the camera path and not in the environ- ment map , there be still the possibility of use a complete v-slam methodinstead of one of the vo technique describe in this tutorial . a v-slam method be potentially much more precise , because it enforce many more con-straints on the path , but not necessarily more robust ( e.g. , outlier in loop closing can severely affect the map consis- tency ) . in addition , it be more complex and computation-ally expensive . in the end , the choice between vo and v-slam depend on the tradeoff between performance and con-sistency , and simplicity in implementation . although the global consistency of the camera path be sometimes desir- able , vo trade off consistency for real-time perform-ance , without the need to keep track of all the previous history of the camera.formulation of the vo problem an agent be move through an environment and take image with a rigidly attach camera system at discretetime instant k. in case of a monocular system , the set of image take at time kis denote by i 0 : n¼fi0 , ... , ing . in case of a stereo system , there be a left and a right imageat every time instant , denote by i l,0 : n¼fil,0 , ... , il , ng andir,0 : n¼fir,0 , ... , ir , ng . figure 1 show an illustration of this setting . for simplicity , the camera coordinate frame be assume to be also the agent ’ s coordinate frame . in case of a stereo system , without loss of generality , the coordinate system ofthe left camera can be use a the origin . two camera position at adjacent time instant k/c01 and kare relate by the rigid body transformation t k , k/c012r434of the following form : tk , k/c01¼rk , k/c01tk , k/c01 01/c20/c21 , ( 1 ) where rk , k/c012so ( 3 ) be the rotation matrix , and tk , k/c012r331the translation vector . the set t1 : n¼ ft1 , 0 , ... , tn , n/c01gcontains all subsequent motion . to simplify the notation , from now on , tkwill be use instead of tk , k/c01 . finally , the set of camera pose c0 : n¼ fc0 , ... , cngcontains the transformation of the camera with respect to the initial coordinate frame at k¼0 . the current pose cncan be compute by concatenate all the transformation tk ( k¼1 ... n ) , and , therefore , cn¼ cn/c01tn , w i t h c0being the camera pose at the instant k¼0 , which can be set arbitrarily by the user . the main task in vo be to compute the relative transfor- mations tkfrom the image ikandik/c01and then to concate- nate the transformation to recover the full trajectory c0 : nof the camera . this mean that vo recover the path incremen- tally , pose after pose . an iter ative refinement over the last m pose can be perform after this step to obtain a more accu- rate estimate of the local trajectory . this iterative refinement work by minimize the sum of the squared reprojectionerrors of the reconstruct 3-d point ( i.e. , the 3-d map ) over the last mi m a g e s ( t h i si sc a l l e d windowed-bundle adjustment , because it be perform on a window of mframes . bundle adjustment will be describe in part ii of this tutorial ) . the 3-d point be obtain by triangulation of the image point ( see the “ triangulation and keyframe selection ” section ) . as mention in the “ monocular vo ” section , there be two main approach to compute the relative motion t k : appearance-based ( or global ) method , which use theintensity information of all the pixel in the two input image , and feature-based method , which only use salient and repeatable feature extract ( or track ) across theimages . global method be less accurate than feature- base method and be computationally more expensive . ( as observe in the “ history of vo ” section , most appear-ance-based method have be apply to monocular vo . this be due to ease of implementation compare with the ck+1 ck–1tk , k–1tk+1 , k ck figure 1 . an illustration of the visual odometry problem . the relative pose tk ; k/c01of adjacent camera position ( or position of a camera system ) be compute from visual feature andconcatenated to get the absolute pose c kwith respect to the initial coordinate frame at k¼0.• in the motion estimation step , the camera motion between the current and the previous image be compute . • 84ieee robotics & automation magazine december 2011•stereo camera case . ) feature-based method require the ability to robustly match ( or track ) feature across frame but be fast and more accurate than global methods.therefore , most vo implementation be feature base . the vo pipeline be summarize in figure 2 . for every new image i k ( or image pair in the case of a stereo camera ) , the first two step consist of detect and match 2-d feature with those from the previous frame . two-dimen- sional feature that be the reprojection of the same 3-dfeature across different frame be call image correspond- ences . ( as will be explain in part ii of this tutorial , we distinguish between feature matching and feature tracking.the first one consist of detect feature independently in all the image and then match them base on some similarity metric ; the second one consist of find fea-tures in one image and then track them in the next image use a local search technique , such a correlation . ) the third step consist of compute the relative motiont kbetween the time instant k/c01 and k. depending on whether the correspondence be specify in three or two dimension , there be three distinct approach to tacklethis problem ( see the “ motion estimation ” section ) . the camera pose c ki then compute by concatenation of tk with the previous pose . finally , an iterative refinement ( bundle adjustment ) can be do over the last mframes to obtain a more accurate estimate of the local trajectory . motion estimation be explain in this tutorial ( see “ motion estimation ” section ) . feature detection and matching and bundle adjustment will be describe in part ii . also , notice that for an accurate motion computation , feature correspondence should not contain outlier ( i.e. , wrong data association ) . ensuring accurate motion esti- mation in the presence of outlier be the task of robust esti-mation , which will be describe in part ii of this tutorial . most vo implementation assume that the camera be cali- brated . to this end , the next section review the standardmodels and calibration procedure for perspective and omnidirectional camera . camera modeling and calibration vo can be do use both perspective and omnidirec- tional camera . in this section , we review the main model . perspective camera model the most used model for perspective camera assume a pin-hole projection system : the image be form by the intersec- tion of the light ray from the object through the center of the lens ( projection center ) , with the focal plane [ figure3 ( a ) ] . let x¼½x , y , z/c138 > be a scene point in the camera refer- ence frame and p¼½u , v/c138 > it projection on the image plane measure in pixel . the mapping from the 3-d world to the2-d image be give by the perspective projection equation : ku v 12 43 5¼kx¼a u0u0 0avv0 0012 43 5x y z2 43 5 , ( 2 ) where kis the depth factor , auandavthe focal length , and u0 , v0the image coordinate of the projection center . these parameter be call intrinsic parameter . when the field of view of the camera be large than 45 /c176 , the effect of the radial distortion may become visible and can be model use a second- ( or high ) -order polynomial . the deriva-tion of the complete model can be find in computer vision textbook , such a [ 22 ] and [ 63 ] . let ~p¼½~u , ~v,1/c138 > ¼ k/c01½u , v,1/c138 > be the normalized image coordinate . nor- malized coordinate will be use throughout in the follow- ing section . ( u0 , v0 ) u pvx x yccz y x x v u x yz c x ( a ) ( b ) ( c ) z figure 3 . ( a ) perspective projection , ( b ) catadioptric projection , and ( c ) a spherical model for perspective and omnidirectionalcameras . image point be represent a direction to the view point normalize on the unit sphere.image sequence feature detection feature matching ( or tracking ) local optimization ( bundle adjustment ) motion estimation 2-d-to-2-d 3-d-to-3-d 3-d-to-2-d figure 2 . a block diagram show the main component of a vo system . december 2011 ieee robotics & automation magazine 85•omnidirectional camera model omnidirectional camera be cameras with wide field of view ( even more than 180 /c176 ) and can be build use fish-eye lens or by combine standard camera with mirror [ the latter be call catadioptric camera , figure 3 ( b ) ] . typical mirror shape in catadioptric camera be quadratic surface of revolution ( e.g. , paraboloid or hyper-boloid ) , because they guarantee a sin- gle projection center , which make it possible to use the motion estima-tion theory present in the “ motion estimation ” section . currently , there be two accepted model for omnidir- ectional camera . the first one propose by geyer and daniilidis [ 64 ] be for general catadioptric camera ( para- bolic or hyperbolic ) , while the second one propose byscaramuzza et al . [ 65 ] be a unified model for both fish-eye and catadioptric camera . a survey of these two model can be find in [ 66 ] and [ 67 ] . the projection equation ofthe unified model be a follow : ku v a 0þa1qþ/c1/c1/c1þ an/c01qn/c012 43 5¼x y z2 43 5 , ( 3 ) where q¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ u2þv2p and a0 , a1 , ... , anare intrinsic parameter that depend on the type of mirror or fish-eyelens . as show in [ 65 ] , n¼4 be a reasonable choice for a large variety of mirror and fish-eye lens . finally , this model assume that the image plane satisfy the idealproperty that the ax of symmetry of the camera and mir- ror be align . although this assumption hold for most catadioptric and fish-eye camera , misalignment can bemodeled by introduce a perspective projection between the ideal and real-image plane [ 66 ] . spherical model as mention earlier , it be desirable that the camera possess a single projection center ( also call single effec- tive viewpoint ) . in a catadioptric camera , this happen when the ray reflect by the mirror intersect all in a sin- gle point ( namely c ) . the existence of this point allow u to model any omnidirectional projection a a mapping from the single viewpoint to a sphere . for convenience , a unit sphere be usually adopt . it be important to notice that the spherical model applies not only to omnidirectional camera but also to perspec- tive camera . if the camera be calibrate , any point in theperspective or omnidirectional image can be map into a vector on the unit sphere . as can be observe in figure 3 ( c ) , these unit vector represent the direction to theviewed scene point . these vector be call normalized image point on the unit sphere .camera calibration the goal of calibration be to accurately measure the intrinsic and extrinsic parameter of the camera system.in a multicamera system ( e.g. , stereo and trinocular ) , the extrinsic parameter describe the mutual position and orientation between each camera pair . the most popularmethod use a planar checkerboard-like pattern . the position of the square on the board be know . to com- pute the calibration parameter accurately , the user musttake several picture of the board show at different posi- tions and orientation by ensure that the field of view of the camera be fill as much a possible . the intrinsic andextrinsic parameter be then find through a least-square minimization method . the input data be the 2-d position of the corner of the square of the board and their corre-sponding pixel coordinate in each image . many camera calibration toolbox have be devise for matlab and c. an up-to-date list can be find in [ 68 ] . among these , the most popular one for matlab be give in [ 69 ] and [ 70 ] – [ 72 ] —for perspective and omni- directional camera , respectively . a c implementation ofcamera calibration for perspective camera can be find in opencv [ 73 ] , the open-source computer vision library . motion estimation motion estimation be the core computation step per- form for every image in a vo system . more precisely , in the motion estimation step , the camera motion between the current image and the previous image be compute . by concatenation of all these single move-ments , the full trajectory of the camera and the agent ( assume that the camera be rigidly mount ) can be recover . this section explain how the transformationt kbetween two image ik/c01andikcan be compute from two set of correspond feature fk/c01 , fkat time instant k/c01a n d k , respectively . depending on whether the fea- ture correspondence be specify in two or three dimension , there be three different method . l2-d-to-2-d : in this case , both fk/c01andfkare specify in 2-d image coordinate . l3-d-to-3-d : in this case , both fk/c01andfkare specify in 3-d. to do this , it be necessary to triangulate 3-d pointsat each time instant ; for instance , by use a stereo camera system . l3-d-to-2-d : in this case , fk/c01are specify in 3-d and fk be their corresponding 2-d reprojections on the image ik . in the monocular case , the 3-d structure need to be triangulate from two adjacent camera view ( e.g. , ik/c02 andik/c01 ) and then match to 2-d image feature in a third view ( e.g. , ik ) . in the monocular scheme , match over at least three view be necessary . notice that feature can be point or line . in general , due to the lack of line in unstructured scene , point feature be use in vo . an in-depth review of these three approach forboth point and line feature can be find in [ 74 ] . the formu- lation give in this tutorial be for point feature only.• in gps-denied environment , vo become of utmost importance . • 86ieee robotics & automation magazine december 2011•2-d to 2-d : motion from image feature correspondences estimating the essential matrix the geometric relation between two image ikandik/c01of a calibrated camera be describe by the so-called essentialmatrix e.econtains the camera motion parameters up to an unknown scale factor for the translation in the follow- ing form : e k ’ ^tkrk , ( 4 ) where tk¼½tx , ty , tz/c138 > and ^tk¼0/c0tzty tz 0/c0tx /c0tytx 02 43 5 : ( 5 ) the symbol ’ be use to denote that the equivalence be valid up to a multiplicative scalar . the essential matrix can be compute from 2-d-to-2-d feature correspondence , and rotation and translation can directly be extract from e. the main property of 2-d-to- 2-d-based motion estimation be the epipolar constraint , which determine the line on which the correspond fea- ture point ~p 0of~plies in the other image ( figure 4 ) . this con- straint can be formulate by ~p0 > e~p¼0 , where ~p0is a feature location in one image ( e.g. , ik ) a n d ~pis the location of it corresponding feature in another image ( e.g. , ik/c01 ) .~pand~p0 be normalize image coordinate . for the sake of simplic- ity , throughout the following section , normalize coordi- nates in the form ~p¼½~u , ~v,1/c138 > will be use ( see the perspective camera model ” section ) . however , very similarequations can also be derive for normalized coordinate on the unit sphere ( see the “ spherical model ” section ) . the essential matrix can be compute from 2-d-to-2- d feature correspondence use the epipolar constraint . the minimal case solution involve five 2-d-to-2-d cor- respondences [ 75 ] and an efficient implementation pro-posed by nister in [ 76 ] . nister ’ s five-point algorithm have become the standard for 2-d-to-2-d motion estimation in the presence of outlier ( the problem of robust estima-tion will be tackle in part ii of this tutorial ) . a simple and straightforward solution for n/c218 noncoplanar point be the longuet-higgins ’ eight-point algorithm [ 2 ] , which be summarize here . each feature match give a con- straint of the following form : ~u~u 0~u0~v~u0~u~v0~v~v0~v0~u~v1 ½/c138 e¼0 , ( 6 ) where e¼½e1e2e3e4e5e6e7e8e9/c138 > : stacking the constraint from eight point give the lin- ear equation system ae¼0 , and by solve the system , the parameter of ecan be compute . this homogeneous equation system can easily be solve use singular valuedecomposition ( svd ) [ 2 ] . having more than eight point lead to an overdetermined system to solve in the least-squares sense and provide a degree of robustness to noise . the svd of ahas the form a¼usv > , and the least- square estimate of ewithjjejj ¼ 1 can be find a the last column of v. however , this linear estimation of edoes not fulfill the inner constraint of an essential matrix , which come from the multiplication of the rotation matrixrand the skew-symmetric translation matrix ^t . these con- straints be visible in the singular value of the essential matrix . a valid essential matrix after svd be e¼usv > and have diag ( s ) ¼fs , s,0g , which mean that the first and second singular value be equal and the third one be zero . to get a valid ethat fulfill the constraint , the solution need to be project onto the space of valid essential matri- ce . the projected essential matrix be /c22e¼udiagf1 , 1 , 0 gv > . observe that the solution of the eight-point algorithm be degenerate when the 3-d point be coplanar . conversely , the five-point algorithm work also for coplanar point . finally , observe that the eight-point algorithm work forboth calibrate ( perspective or omnidirectional ) and unca- librate ( only perspective ) camera , whereas the five-point algorithm assume that the camera ( perspective or omni-directional ) be calibrate . extracting rand tfrom e from the estimate of /c22e , the rotation and translation part can be extract . in general , there be four different solu- tions for r , tfor one essential matrix ; however , by triangu- lation of a single point , the correct r , tpair can be identify . the four solution be r¼u ( /c6w > ) v > , ^t¼u ( /c6w ) su > , where w > ¼0/c610 /c7100 00 12 43 5 : ( 7 ) an efficient decomposition of einto rand ti describe in [ 76 ] . ˜px epipolar plane˜p′ ck–1ck tk , k–1epipolar line epipolar line figure 4 . an illustration of the epipolar constraint . december 2011 ieee robotics & automation magazine 87•after select the correct solution by triangulation of a point and choose the solution where the point be in front of both camera , a nonlinear optimization of the rotationand translation parameter should be perform use the estimate r , tas initial value . the function to minimize be the reprojection error define in ( 10 ) . computing the relative scale to recover the trajectory of an image sequence , the differ-ent transformation t 0 : nhave to be concatenate . to do this , the proper relative scale need to be compute a the absolute scale ofthe translation can not be compute from two image . however , it be possi- ble to compute relative scale for thesubsequent transformation . one way of do this be to triangulate 3-d point x k/c01and xkfrom two subse- quent image pair . from the corre- sponding 3-d point , the relative distance between any combination of two 3-d point can becomputed . the proper scale can then be determine from the distance ratio rbetween a point pair in x k/c01and a pair in xk . r¼jjxk/c01 , i/c0xk/c01 , jjj jjxk , i/c0xk , jjj : ( 8 ) for robustness , the scale ratio for many point pair be com- puted and the mean ( or in presence of outlier , the median ) be use . the translation vector ti then scale with this distance ratio . observe that the relative-scale computation require fea- tures to be match ( or track ) over multiple frame ( at least three ) . instead of perform exp licit triangulation of the 3-d point , the scale can also be recover by exploit the trifocal constraint between three-view match of 2-d feature [ 22 ] . the vo algorithm with the 2-d-to-2-d correspond- ences be summarize in algorithm 1 . 3d-to-3d : motion from 3-d structure correspondences for the case of correspond 3-d-to-3-d feature , thecamera motion t kcan be compute by determine the align transformation of the two 3-d feature sets.corresponding 3-d-to-3-d feature be available in the stereo vision case . the general solution consist of find the tkthat min- imizes the l2distance between the two 3-d feature set arg min tkx ijj~xi k/c0tk~xi k/c01jj , ( 9 ) where the superscript idenotes the ith feature , and ~xk , ~xk/c01are the homogeneous coordinate of the 3-d point , i.e. , ~x¼½x , y , z,1/c138 > . as show in [ 77 ] , the minimal case solution involve three 3-d-to-3-d noncollinear correspondence , which can be use for robust estimation in the presence of out- liers ( part ii of this tutorial ) . for the case of n/c213 corre- spondences , one possible solution ( accord to arun et al . [ 78 ] ) be to compute the translation part a the difference of the centroid of the 3-d feature set and the rotation partusing svd . the translation be give by t k¼xk/c0rxk/c01 , where /c22/c1stands for the arithmetic mean value . the rotation can be efficiently compute use svd a rk¼vu > , where usv > ¼svd ( ( xk/c01/c0xk/c01 ) ( xk/c0xk ) > ) and xk/c01 andxkare set of correspond 3-d point . if the measurement uncertainty of the 3-d point be know , they can be add a weight into the estimation a describe by maimone et al . [ 17 ] . the computed transfor- mations have absolute scale , and thus , the trajectory of asequence can be compute by directly concatenate the transformation . the vo algorithm with the 3-d-to-3-d correspond- ences be summarize in algorithm 2 . to compute the transformation , it be also possible to avoid the triangulation of the 3-d point in the stereo camera and use quadrifocal constraint instead . this method be point out by comport et al . [ 21 ] . the quadrifocal tensorallows compute the transformation directly from 2-d- to-2-d stereo correspondences.• algorithm 1 . vo from 2-d-to-2-d correspondence . 1 ) capture new frame ik 2 ) extract and match feature between ik/c01and ik 3 ) compute essential matrix for image pair ik/c01 , ik 4 ) decompose essential matrix into rkand tk , a n df o r m tk 5 ) compute relative scale and rescale tkaccordingly 6 ) concatenate transformation by compute ck¼ck/c01tk 7 ) repeat from 1 ) .• algorithm 2 . vo from 3-d-to-3-d correspondence . 1 ) capture two stereo image pair il ; k/c01 , ir ; k/c01and il ; k , ir ; k 2 ) extract and match feature between il ; k/c01and il ; k 3 ) triangulate match feature for each stereo pair 4 ) compute tkfrom 3-d feature xk/c01and xk 5 ) concatenate transformation by compute ck¼ck/c01tk 6 ) repeat from 1 ) .• bundle adjustment c a nb eu s e dt or e f i n e the local estimate of the trajectory . • 88ieee robotics & automation magazine december 2011•3-d-to-2-d : motion from 3-d structure and image feature correspondences as point out by nister et al . [ 1 ] , motion estimation from3-d-to-2-d correspondence be more accurate than from 3-d-to-3-d correspondence because it minimize the image reprojection error ( 10 ) instead of the 3-d-to-3-dfeature position error ( 9 ) . the transformation t kis com- put from the 3-d-to-2-d correspondence xk/c01andpk : xk/c01can be estimate from stereo data or , in the monocu- lar case , from triangulation of the image measurement pk/c01and pk/c02 . the latter , however , require image corre- spondences across three view . the general formulation in this case be to find tkthat minimize the image reprojection error arg min tkx ikpi k/c0^pi k/c01k2 , ( 10 ) where ^pi k/c01is the reprojection of the 3-d point xi k/c01into image ikaccording to the transformation tk . this problem be know a perspective from n point ( pnp ) ( or resection ) , and there be many different solution to it in the literature [ 79 ] . as show in [ 18 ] , the minimal case involve three 3- d-to-2-d correspondence . this be call perspective from three point ( p3p ) and return four solution that can be disambiguate use one or more additional point . ( a fast implementation of p3p be describe in [ 80 ] , and c code can be freely download from the author ’ web page . ) inthe 3-d-to-2-d case , p3p be the standard method for robust motion estimation in the presence of outlier [ 18 ] . robust estimation will be describe in part ii of this tutorial . a simple and straightforward solution to the p np prob- lem for n/c216 point be the direct linear transformation algorithm [ 22 ] . one 3-d-to-2-d point correspondenceprovides two constraint of the following form for the entry of p k¼½rjt/c138 . 0000 /c0x/c0y/c0z/c01x~vy ~vz ~v~v xyz 10 0 0 0 /c0x~u/c0y~u/c0z~u/c0~u/c20/c21 p1 p2 p32 43 5¼0 , ( 11 ) where each pj > be a four vector ( the jth row of pk ) and x , y , zare the coordinate of the 3-d point xk/c01 . stacking the constraint of six-point correspondence give a linear system of equation of the form ap¼0 . the entry of pcan be compute from the nullvector of a , e.g. , by use svd . the rotation and translation part can easily be extract from pk¼½rjt/c138 . the result rotation ris not necessarily orthonormal . however , this be not a prob- lem since both randtcan be refine by nonlinear optimi- zation of the reprojection error a define in ( 10 ) . the 3-d-to-2-d motion estimation assume that the 2- d image point only come from one camera . this mean that for the case of a stereo camera , the 2-d image pointsare those of either the left or the right camera . obviously , it be desirable to make use of the image point of bothcameras at the same time . a generalized version of the 3- d-to-2-d motion estimation algorithm for nonconcurrent ray ( i.e. , 2-d image point from multiple camera ) wasproposed by nister in [ 81 ] for extrinsically calibrate cam- era ( i.e. , the mutual position and orientation between the camera be know ) . for the monocular case , it be necessary to triangulate 3- d point and estimate the pose from 3-d-to-2-d match in an alternating fashion . this alternate scheme be oftenreferred to a sfm . starting from two view , the initial set of 3-d point and the first transformation be computedfrom 2-d-to-2-d feature match . subsequent transformation be then compute from 3-d-to-2-d featurematches . to do this , feature need to be match ( or track ) over multiple frame ( at least three ) . new 3-d fea-tures be again triangulate when a new transformation be compute and add to the set of 3-d feature . the main challenge of thismethod be to maintain a consistent and accurate set of tri- angulated 3-d feature and to create 3-d-to-2-d feature match for at least three adjacent frame . the vo algorithm with 3-d-to-2-d correspondence be summarize in algorithm 3 . triangulation and keyframe selection some of the previous motion estimation method require triangulation of 3-d point ( structure ) from 2-d imagecorrespondences . structure computation be also need by bundle adjustment ( part ii of this tutorial ) to compute a more accurate estimate of the local trajectory . triangulated 3-d point be determine by intersect back-projected ray from 2-d image correspondence of at least two image frame . in perfect condition , these rayswould intersect in a single 3-d point . however , because of image noise , camera model and calibration error , and• algorithm 3 . vo from 3-d-to-2-d correspondences . 1 ) do only once : 1.1 ) capture two frame ik/c02 , ik/c01 1.2 ) extract and match feature between them 1.3 ) triangulate feature from ik/c02 , ik/c01 2 ) do at each iteration : 2.1 ) capture new frame ik 2.2 ) extract feature and match with previous frame ik/c01 2.3 ) compute camera pose ( pnp ) from 3-d-to-2-d match 2.4 ) triangulate all new feature match between ikand ik/c01 2.5 ) iterate from 2.1 ) .• 2-d-to-2-d and 3-d-to-2-d method a r em o r ea c c u r a t e than 3-d-to-3-d method . • december 2011 ieee robotics & automation magazine 89•feature match uncertainty , they never intersect . there- fore , the point at a minimal distance , in the least-squares sense , from all intersect ray can betaken a an estimate of the 3-d point position . notice that the standard de- viation of the distance of the triangu-lated 3-d point from all ray give an idea of the quality of the 3-d point . three-dimensional point with largeuncertainty will be throw out . this happen especially when frame be take at very nearby interval com- par with the distance to the scene point . when this occur , 3-d point exhibit very large uncertainty . one way to avoid this consist of skip frame until the averageuncertainty of the 3-d point decrease below a certain threshold . the select frame be call keyframes . key- frame selection be a very important step in vo and shouldalways be do before update the motion . discussion according to nister et al . [ 1 ] , there be an advantage in use the 2-d-to-2-d and 3-d-to-2-d method compare to the 3-d-to-3-d method for motion computation . nister com-pared the vo performance of the 3-d-to-3-d case to that of the 3-d-to-2-d case for a stereo camera system and find the latter be greatly superior to the former . thereason be due to the triangulate 3-d point be much more uncertain in the depth direction . when 3-d-to-3-d feature correspondence be use in motion computation , their uncertainty may have a devastating effect on the motion estimate . in fact , in the 3-d-to-3-d case , the 3-d position error , ( 9 ) , be minimize whereas in the 3-d-to-2-d case the image reprojection error , ( 10 ) . in the monocular scheme , t he 2-d-to-2-d method be preferable compare to the 3-d-to-2-d case since itavoids point triangulation . h owever , in practice , the 3-d- to-2-d method be use more often than the 2-d-to-2-d method . the reason lie in it faster data association . aswill be describe in part ii of this tutorial , for accurate motion computation , it be of utmost importance that the input data do not contain outlier . outlier rejection be avery delicate step , and the computation time of this oper- ation be strictly link to the minimum number of point necessary to estimate the motion . as mentioned previ-ously , the 2-d-to-2-d case require a minimum of five- point correspondence ( see th e five-point algorithm ) ; however , only three correspondence be necessary in the3-d-to-2-d motion case ( see p3p ) . as will be show in part ii of this tutorial , this low number of point result in a much fast motion estimation . an advantage of the stereo camera scheme compare to the monocular one , besides the property that 3-d feature be compute directly in the absolute scale , be that matchesneed to be compute only between two view instead of three view a in the monocular scheme . additionally , sincethe 3-d structure be compute directly from a single stereo pair rather than from adjacent frame a in the monocular case , the stereo scheme exhibit less drift than the monocu-lar one in case of small motion . monocular method be interest because stereo vo degenerate into the monocu- lar case when the distance to the scene be much large thanthe stereo baseline ( i.e. , the distance between the two cam- era ) . in this case , stereo vision become ineffective and monocular method must be use . regardless of the chosen motion computation method , local bundle adjustment ( over the last mframes ) should always be perform to compute a more accu-rate estimate of the trajectory . after bundle adjustment , the effect of the motion estimation method be much more alleviated . conclusions this tutorial have describe the history of vo , the problemformulation , and the distinct approach to motion com- putation . vo be a well-understood and establish part of robotics . part ii of this tutorial will summarize theremaining build block of the vo pipeline : how to detect and match salient and repeatable feature across frame , robust estimation in the presence of outlier , andbundle adjustment . in addition , error propagation , appli- cation , and link to free-to-download code will be include . acknowledgments the author thank konstantinos derpanis , oleg narodit-sky , carolin baez , and andrea censi for their fruitful com- ments and suggestion . references [ 1 ] d. nister , o. naroditsky , and j. bergen , “ visual odometry , ” in proc . int . conf . computer vision and pattern recognition , 2004 , pp . 652 –659 . [ 2 ] h. longuet-higgins , “ a computer algorithm for reconstruct a scene from two projection , ” nature , vol . 293 , no . 10 , pp . 133 –135 , 1981 . [ 3 ] c. harris and j. pike , “ 3d positional integration from imagesequences , ” in proc . alvey vision conf . , 1988 , pp . 87 –90 . [ 4 ] j.-m. frahm , p. georgel , d. gallup , t. johnson , r. raguram , c. wu , y.-h. jen , e. dunn , b. clipp , s. lazebnik , and m. pollefeys , “ building rome on a cloudless day , ” in proc . european conf . computer vision , 2010 , pp . 368 –381 . [ 5 ] h. moravec , “ obstacle avoidance and navigation in the real world by a see robot rover , ” ph.d. dissertation , stanford univ. , stanford , ca , 1980 . [ 6 ] l. matthies and s. shafer , “ error model in stereo navigation , ” ieee j . robot . automat . , vol . 3 , no . 3 , pp . 239 –248 , 1987 . [ 7 ] l. matthies , “ dynamic stereo vision , ” ph.d. dissertation , carnegie mel- lon univ. , pittsburgh , pa , 1989 . [ 8 ] s. lacroix , a. mallet , r. chatila , and l. gallo , “ rover self localization in planetary-like environment , ” in proc . int . symp . articial intelligence , robotics , and automation for space ( i-sairas ) , 1999 , pp . 433 –440 . [ 9 ] c. olson , l. matthies , m. schoppers , and m. w. maimone , “ robust stereo ego-motion for long distance navigation , ” in proc . ieee conf . computer vision and pattern recognition , 2000 , pp . 453 –458.• vo trade off consistency for real-time performance . • 90ieee robotics & automation magazine december 2011• [ 10 ] m. hannah , “ computer matching of area in stereo image , ” ph.d. dissertation , stanford univ. , stanford , ca , 1974 . [ 11 ] h. moravec , “ towards automatic visual obstacle avoidance , ” in proc . 5th int . joint conf . artificial intelligence , aug. 1977 , p. 584 . [ 12 ] w. forstner , “ a feature base correspondence algorithm for image matching , ” int . arch . photogrammetry , vol . 26 , no . 3 , pp . 150 –166 , 1986 . [ 13 ] c. olson , l. matthies , m. schoppers , and m. maimone , “ rover navi- gation use stereo ego-motion , ” robot . autonom . syst . , vol . 43 , no . 4 , pp . 215 –229 , 2003 . [ 14 ] a. milella and r. siegwart , “ stereo-based ego-motion estimation use pixel tracking and iterative closest point , ” in p r o c .i e e ei n t .c o n f . vision systems , pp . 21 –24 , 2006 . [ 15 ] a. howard , “ real-time stereo visual odometry for autonomous ground vehicle , ” in proc . ieee/rsj int . conf . intelligent robots and sys- tems , 2008 , pp . 3946 –3952 . [ 16 ] y. cheng , m. w. maimone , and l. matthies , “ visual odometry on the mar exploration rover , ” ieee robot . automat . mag . , vol . 13 , no . 2 , pp . 54–62 , 2006 . [ 17 ] m. maimone , y. cheng , and l. matthies , “ two year of visual odom- etry on the mar exploration rover : field report , ” j . field robot . , vol . 24 , no . 3 , pp . 169 –186 , 2007 . [ 18 ] m. a. fischler and r. c. bolles , “ random sample consensus : a para- digm for model fit with application to image analysis and automate cartography , ” commun . acm , vol . 24 , no . 6 , pp . 381 –395 , 1981 . [ 19 ] c. tomasi and j. shi , “ good feature to track , ” in proc . computer vision and pattern recognition ( cvpr ’ 94 ) , 1994 , pp . 593 –600 . [ 20 ] p. besl and n. mckay , “ a method for registration of 3-d shape , ” ieee trans . pattern anal . machine intell . , vol . 14 , no . 2 , pp . 239 –256 , 1992 . [ 21 ] a. comport , e. malis , and p. rives , “ accurate quadrifocal trackingfor robust 3d visual odometry , ” in proc . ieee int . conf . robotics and automation , 2007 , pp . 40 –45 . [ 22 ] r. hartley and a. zisserman , multiple view geometry in computer vision , 2nd ed . cambridge u.k. : cambridge univ . press , 2004 . [ 23 ] d. nister , o. naroditsky , and j. bergen , “ visual odometry for ground vehicle application , ” j . field robot . , vol . 23 , no . 1 , pp . 3 –20 , 2006 . [ 24 ] p. i. corke , d. strelow , and s. singh , “ omnidirectional visual odom- etry for a planetary rover , ” in proc . ieee/rsj int . conf . intelligent robots and systems , 2005 , pp . 4007 –4012 . [ 25 ] m. lhuillier , “ automatic structure and motion use a catadioptric camera , ” in proc . ieee workshop omnidirectional vision , 2005 , pp . 1 –8 . [ 26 ] r. goecke , a. asthana , n. pettersson , and l. petersson , “ visual vehi-cle egomotion estimation use the fourier-mellin transform , ” in proc . ieee intelligent vehicles symp . , 2007 , pp . 450 –455 . [ 27 ] j. tardif , y. pavlidis , and k. daniilidis , “ monocular visual odometry in urban environment use an omnidirectional camera , ” in proc . ieee/ rsj int . conf . intelligent robots and systems , 2008 , pp . 2531 –2538 . [ 28 ] m. j. milford and g. wyeth , “ single camera vision-only slam on a suburban road network , ” in proc . ieee int . conf . robotics and automa- tion ( icra ’ 08 ) , 2008 , pp . 3684 –3689 . [ 29 ] d. scaramuzza and r. siegwart , “ appearance-guided monocular omnidirectional visual odometry for outdoor ground vehicle , ” ieee trans . robot . ( special issue on visual slam ) , vol . 24 , no . 5 , pp . 1015 – 1026 , oct. 2008 . [ 30 ] e. mouragnon , m. lhuillier , m. dhome , f. dekeyser , and p. sayd , “ real time localization and 3d reconstruction , ” in proc . int . conf . computer vision and pattern recognition , 2006 , pp . 363 –370 . [ 31 ] d. scaramuzza , f. fraundorfer , and r. siegwart , “ real-time monoc- ular visual odometry for on-road vehicle with 1-point ransac , ” in proc . ieee int . conf . robotics and automation ( icra ’ 09 ) , 2009 , pp . 4293 –4299 . [ 32 ] a. pretto , e. menegatti , and e. pagello , “ omnidirectional dense large-scale mapping and navigation base on meaningful triangulation , ” inproc . ieee int . conf . robotics and automation , 2011 , pp . 3289 –3296 . [ 33 ] d. nister , “ an efficient solution to the five-point relative pose prob- lem , ” in proc . int . conf . computer vision and pattern recognition , 2003 , pp . 195 –202 . [ 34 ] m. milford , g. wyeth , and d. prasser , “ ratslam : a hippocampal model for simultaneous localization and mapping , ” in proc . ieee int . conf . robotics and automation ( icra ’ 04 ) , 2004 , pp . 403 –408 . [ 35 ] b. liang and n. pears , “ visual navigation use planar homo- graphies , ” in proc . ieee int . conf . robotics and automation ( icra ’ 02 ) , 2002 , pp . 205 –210 . [ 36 ] q. ke and t. kanade , “ transforming camera geometry to a virtual downward-looking camera : robust ego-motion estimation and ground-layer detection , ” in proc . computer vision and pattern recognition ( cvpr ) , june 2003 , pp . 390 –397 . [ 37 ] h. wang , k. yuan , w. zou , and q. zhou , “ visual odometry basedon locally planar ground assumption , ” in proc . ieee int . conf . informa- tion acquisition , 2005 , pp . 59 –64 . [ 38 ] j. guerrero , r. martinez-cantin , and c. sagues , “ visual map-less navigation base on homographies , ” j . robot . syst . , vol . 22 , no . 10 , pp . 569 –581 , 2005 . [ 39 ] d. scaramuzza , “ 1-point-ransac structure from motion for vehi- cle-mounted camera by exploit non-holonomic constraint , ” int . j. comput . vis . , vol . 95 , no . 1 , pp . 74 –85 , 2011 . [ 40 ] d. scaramuzza , f. fraundorfer , m. pollefeys , and r. siegwart , “ absolute scale in structure from motion from a single vehicle mount camera by exploit nonholonomic constraint , ” in proc . ieee int . conf . computer vision ( iccv ) , kyoto , oct. 2009 , pp . 1413 –1419 . [ 41 ] f. fraundorfer , d. scaramuzza , and m. pollefeys , “ a constricted bundle adjustment parameterization for relative scale estimation in visualodometry , ” in proc . ieee int . conf . robotics and automation , 2010 , pp . 1899 –1904 . [ 42 ] n. sunderhauf , k. konolige , s. lacroix , and p. protzel , “ visualodometry use sparse bundle adjustment on an autonomous outdoor vehicle , ” in tagungsband autonome mobile systeme , reihe informatik aktuell . levi , schanz , lafrenz , and avrutin , eds . berlin , springer-ver- lag , 2005 , pp . 157 –163 . [ 43 ] k. konolige , m. agrawal , and j. sol , “ large scale visual odometry for rough terrain , ” in proc . int . symp . robotics research , 2007 . [ 44 ] j. tardif , m. g. m. laverne , a. kelly , and m. laverne , “ a new approach to vision-aided inertial navigation , ” in proc . ieee/rsj int . conf . intelligent robots and systems , 2010 , pp . 4161 –4168 . [ 45 ] a. i. mourikis and s. roumeliotis , “ a multi-state constraint kalman filter for vision-aided inertial navigation , ” in proc . ieee int . conf . robotics and automation ,2 0 0 7 , p p .3 5 6 5 –3572 . [ 46 ] e. jones and s. soatto , “ visual-inertial navigation , mapping and localization : a scalable real-time causal approach , ” int . j . robot . res . , vol . 30 , no . 4 , pp . 407 –430 , 2010 . [ 47 ] h. durrant-whyte and t. bailey , “ simultaneous localization and mapping ( slam ) : part i . the essential algorithm , ” robot . automat . mag . , vol . 13 , no . 2 , pp . 99 –110 , 2006 . december 2011 ieee robotics & automation magazine 91• [ 48 ] t. bailey and h. durrant-whyte , “ simultaneous localisation and mapping ( slam ) : part ii . state of the art , ” robot . automat . mag . , vol . 13 , no . 3 , pp . 108 –117 , 2006 . [ 49 ] a. davison , “ real-time simultaneous localisation and mapping with a single camera , ” in proc . int . conf . computer vision , 2003 , pp . 1403 – 1410 . [ 50 ] g. klein and d. murray , “ parallel tracking and mapping for small ar workspace , ” in proc . int . symp . mixed and augmented reality , 2007 , pp . 225 –234 . [ 51 ] h. strasdat , j. montiel , and a. davison , “ real time monocular slam : why filter ? ” in proc . ieee int . conf . robotics and automation , 2010 , pp . 2657 –2664 . [ 52 ] a. chiuso , p. favaro , h. jin , and s. soatto , “ 3-d motion and struc- ture from 2-d motion causally integrate over time : implementation , ” in proc . european conf . computer vision , 2000 , pp . 734 –750 . [ 53 ] m. c. deans , “ bearing-only localization and mapping , ” ph.d. disser- tation , carnegie mellon univ. , pittsburgh , 2002 . [ 54 ] l. a. clemente , a. j. davison , i. reid , j. neira , and j. d. tardos , “ mapping large loop with a single hand-held camera , ” in proc . robotics science and systems , 2007 . [ 55 ] t. lemaire and s. lacroix , “ vision-based slam : stereo and monocu-lar approach , ” int . j . computer vision , vol . 74 , no . 3 , pp . 343 –364 , 2006 . [ 56 ] e. eade and t. drummond , “ monocular slam a a graph of coa- lesced observation , ” in proc . ieee int . conf . computer vision , 2007 , pp . 1–8 . [ 57 ] g. klein and d. murray , “ improving the agility of keyframe-basedslam , ” in proc . european conf . computer vision , 2008 , pp . 802 –815 . [ 58 ] k. konolige and m. agrawal , “ frameslam : from bundle adjust- ment to real-time visual mappping , ” ieee trans . robot . , vol . 24 , no . 5 , pp . 1066 –1077 , 2008 . [ 59 ] a. handa , m. chli , h. strasdat , and a. j. davison , “ scalable active matching , ” in proc . int . conf . computer vision and pattern recognition , 2010 , pp . 1546 –1553 . [ 60 ] j. civera , o. grasa , a. davison , and j. montiel , “ 1-point ransac for ekf filtering : application to real-time structure from motion andvisual odometry , ” j . field robot . , vol . 27 , no . 5 , pp . 609 –631 , 2010 . [ 61 ] h. strasdat , j. montiel , and a. j. davison , “ scale drift-aware large scale monocular slam , ” in proc . robotics science and systems , 2010 . [ 62 ] c. mei , g. sibley , m. cummins , p. newman , and i. reid , “ rslam : a system for large-scale mapping in constant-time use stereo , ” int . j . computer vision , vol . 94 , no . 2 , pp . 198 –214 , 2010 . [ 63 ] y. ma , s. soatto , j. kosecka , and s. sastry , an invitation to 3d vision , from images to models . berlin : springer-verlag , 2003 . [ 64 ] c. geyer and k. daniilidis , “ a unifying theory for central panoramic system and practical application , ” in proc . european conf . computer vision , 2000 , pp . 445 –461 . [ 65 ] d. scaramuzza , a. martinelli , and r. siegwart , “ a flexible technique for accurate omnidirectional camera calibration and structure from motion , ” in proc . ieee int . conf . computer vision systems ( icvs ) 2006 , jan. 2006 , pp . 45 –53 . [ 66 ] d. scaramuzza , “ omnidirectional vision : from calibration to robot motion estimation ” ph.d. dissertation , eth zurich , 2008 . [ 67 ] d. scaramuzza , “ omnidirectional camera , ” in encyclopedia of computer vision , k. ikeuchi , ed . berlin : springer-verlag , 2012 . [ 68 ] j. bouguet ( 2 011 ) . list of camera calibration toolbox . [ online ] . available : http : //www.visi on.caltech.edu/bo uguetj/calib . doc/htmls/links.html [ 69 ] j. bouguet . camera calibration toolbox for matlab . [ online ] . available : http : //www.vision.caltech.edu/bouguetj/calib.doc/index.html [ 70 ] d. scaramuzza . ( 2006 ) . ocamcalib toolbox : omnidirectional cameracalibration toolbox for matlab ( use planar grid ) google for “ ocamcalib . ” [ online ] . available : http : //sites.google.com/site/scarabotix/ocamcalib- toolbox [ 71 ] c. mei . ( 2 006 ) . omnidirectional camera calibration toolbox for matlab ( use planar grid ) . [ online ] . available : http : //homepage- s.laas.fr/~cmei/index.php/toolbox [ 72 ] j. barreto . omnidirectional camera calibration toolbox for matlab ( use line ) . [ online ] . available : http : //www.isr.uc.pt/~jpbar/catpack/ pag1.htm [ 73 ] opencv : open-source computer vision library . [ online ] . available : http : //opencv.willowgarage.com/wiki/ [ 74 ] t. huang and a. netravalli , “ motion and structure from featurecorrespondences : a review , ” proc . ieee , v o l .8 2 , n o .2 , p p .2 5 2 –268 , 1994 . [ 75 ] e. kruppa , “ zur ermittlung eines objektes aus zwei perspektiven mitinnerer orientierung , ” sitzungsberichte der akademie der wissenschaften , wien , mathematisch-naturwissenschaftlichen klasse , abteilung iia , vol . 122 , pp . 1939 –1948 , 1913 . [ 76 ] d. nister , “ an efficient solution to the five-point relative pose prob- lem , ” in proc . computer vision and pattern recognition ( cvpr ’ 03 ) , 2003 , pp . ii : 195 –202 . [ 77 ] h. goldstein , classical mechanics .r e a d i n g , m a : a d d i s o n - w e s l e y ,1 9 8 1 . [ 78 ] k. s. arun , t. s. huang , and s. d. blostein , “ least-squares fit oftwo 3-d point set , ” ieee trans . pattern anal . machine intell . , v o l .9 , n o .5 , pp . 698 –700 , 1987 . [ 79 ] f. moreno-noguer , v. lepetit , and p. fua , “ accurate non-iterativeo ( n ) solution to the pnp problem , ” in proc . ieee int . conf . computer vision , 2007 , pp . 1 –8 . [ 80 ] l. kneip , d. scaramuzza , and r. siegwart , “ a novel parameterizationof the perspective-three-point problem for a direct computation of abso- lute camera position and orientation , ” in proc . ieee conf . computer vision and pattern recognition , 2011 , pp . 2969 –2976 . [ 81 ] d. nister , “ a minimal solution to the generalise 3-point pose prob- lem , ” in proc . ieee conf . computer vision and pattern recognition , 2004 , pp . 560 –567 . [ 82 ] c. harris and m. stephens , “ a combined corner and edge detector , ” inproc . alvey vision conf . , 1988 , pp . 147 –151 . davide scaramuzza , grasp lab , department of com- puter and information science , school of engineering andapplied science , university of pennsylvania , philadelphia . e-mail : davide.scaramuzza @ ieee.org . friedrich fraundorfer , computer vision and geometry lab , institute of visual computing , department of computer science , eth zurich , switzerland . e-mail : fraundorfer @ inf.ethz.ch . 92ieee robotics & automation magazine december 2011•","['visual odometry part i : the first 30 years and fundamentals by davide scaramuzza and friedrich fraundorfer visual odometry ( vo ) be the process of estimate the egomotion of an agent ( e.g. , vehicle , human , and robot ) use only the input of a single or multiple camera attach to it . application domains include robotics , wearable computing , augment reality , and automotive . the term vowas coin in 2004 by nis- ter in his landmark paper [ 1 ] . the term be choose for it similarity to wheel odometry , which incrementally esti-mates the motion of a vehicle by integrate the number of turn of it wheel over time . likewise , vo operate by incrementally estimate the pose of the vehicle throughexamination of the change that motion induces on the image of it onboard camera . for vo to work effec- tively , there should be sufficient illumination in the envi-ronment and a static scene with enough texture to allow apparent motion to be extract . furthermore , consecu- tive frame should be capture by ensure that they havesufficient scene overlap.the advantage of vo with respect to wheel odometry be that vo be not affect by wheel slip in uneven terrain orother adverse condition . it have be demonstrate that compare to wheel odometry , vo provide more accurate trajectory estimate , with relative position error rangingfrom 0.1 to 2 % . this capability make vo an interesting supplement to wheel odometry and , additionally , to other navigation system such a global positioning system ( gps ) , inertial measurement unit ( imus ) , and laser odometry ( similar to vo , laser odometry estimate the egomotion of a vehicle by scan-matching of consecutivelaser scan ) . in gps-denied environment , such a under- water and aerial , vo have utmost importance . this two-part tutorial and survey provide a broad introduction to vo and the research that have be under- take from 1980 to 2011 . although the first two decade witness many offline implementation , only in the thirddecade do real-time work system flourish , which have lead vo to', 'that have be under- take from 1980 to 2011 . although the first two decade witness many offline implementation , only in the thirddecade do real-time work system flourish , which have lead vo to be use on another planet by two mars-explora- tion rover for the first time . part i ( this tutorial ) present ahistorical review of the first 30 year of research in this field and it fundamental . after a brief discussion on camera 80\\x81ieee robotics & automation magazine \\x81december 2011 1070-9932/11/ $ 26.00 ª2011 ieeedigital object identifier 10.1109/mra.2011.943233 date of publication : 8 december 2011 © digital visionmodeling and calibration , it describe the main motion- estimation pipeline for both monocular and binocular scheme , outline pro and con of each implementation.part ii will deal with feature matching , robustness , and application . it will review the main point-feature detector use in vo and the different outlier-rejection scheme . par-ticular emphasis will be give to the random sample consen- sus ( ransac ) , and the distinct trick devise to speed it up will be discuss . other topic cover will be error model-ing , location recognition ( or loop-closure detection ) , and bundle adjustment . this tutorial provide both the experienced and non- expert user with guideline and reference to algorithms to build a complete vo system . since an ideal and unique vo solution for every possible working environmentdoes not exist , the optimal solution should be choose carefully accord to the specific navigation environ- ment and the give computational resource . history of visual odometry the problem of recover relative camera pose andthree-dimensional ( 3-d ) structure from a set of camera image ( calibrate or non calibrate ) be know in the computer vision community a structure from motion ( sfm ) . its origin can be date back to work such a [ 2 ] and [ 3 ] . vo be a particular case of sfm . sfm be more gen- eral and tackle the problem of 3-d reconstruction ofboth the structure and camera pose from sequentially', 'date back to work such a [ 2 ] and [ 3 ] . vo be a particular case of sfm . sfm be more gen- eral and tackle the problem of 3-d reconstruction ofboth the structure and camera pose from sequentially order or unordered image set . the final structure and camera pose be typically refine with an offline optimi-zation ( i.e. , bundle adjustment ) , whose computation time grow with the number of image [ 4 ] . conversely , vo focus on estimate the 3-d motion of the camerasequentially —as a new frame arrive —and in real time . bundle adjustment can be use to refine the local estimate of the trajectory . the problem of estimate a vehicle ’ s egomotion from visual input alone start in the early 1980s and be describe by moravec [ 5 ] . it be interest to observe thatmost of the early research in vo [ 5 ] – [ 9 ] be do for planetary rover and be motivate by the nasa mars exploration program in the endeavor to provide all-terrainrovers with the capability to measure their 6-degree-of- freedom ( dof ) motion in the presence of wheel slippage in uneven and rough terrain . the work of moravec stand out not only for present- ing the first motion-estimation pipeline —whose main functioning block be still use today —but also for describe one of the early corner detector ( after the first one propose in 1974 by hannah [ 10 ] ) which be know today a the moravec corner detector [ 11 ] , a prede- cessor of the one propose by forstner [ 12 ] and harris and stephens [ 3 ] , [ 82 ] . moravec test his work on a planetary rover equip with what he term a slider stereo : a single camera slide on a rail . the robot move in a stop-and-go fashion , digitize and analyze image at every location . at each stop , the camera slide horizontally take nine picture at equidistant interval . corners be detect in an imageusing his operator and match along the epipolar line of the other eight frame use normalized cross correlation . potential match at the next robotlocations be find again by correla- tion use a coarse-to-fine strategy to account for', 'the epipolar line of the other eight frame use normalized cross correlation . potential match at the next robotlocations be find again by correla- tion use a coarse-to-fine strategy to account for large-scale change . out-liers be subsequently remove by check for depth inconsistency in the eight stereo pair . finally , motionwas compute a the rigid body transformation to align the triangu- lated 3-d point see at two consecu-tive robot position . the system of equation be solve via a weighted least square , where the weight wereinversely proportional to the dis- tance from the 3-d point . although moravec use a single sliding camera , his work belongs to the class of stereo vo algorithm . this terminology account for the fact that the relative 3-d position of the feature be directly measure by triangula-tion at every robot location and use to derive the relative motion . trinocular method belong to the same class of algorithm . the alternative to stereo vision be to use asingle camera . in this case , only bear information be available . the disadvantage be that motion can only be recover up to a scale factor . the absolute scale can thenbe determine from direct measurement ( e.g. , measure the size of an element in the scene ) , motion constraint , or from the integration with other sensor , such a imu , air-pressure , and range sensor . the interest in monocular method be due to the observation that stereo vo can degenerate to the monocular case when the distance to thescene be much large than the stereo baseline ( i.e. , the dis- tance between the two camera ) . in this case , stereo vision become ineffective and monocular method must be used.over the year , monocular and stereo vos have almost progress a two independent line of research . in the remainder of this section , we have survey the relatedwork in these field . stereo vo most of the research do in vo have be produce use stereo camera . building upon moravec ’ s work , matthies and shafer [ 6 ] , [ 7 ] use a binocular system and moravec ’ sprocedure', 'field . stereo vo most of the research do in vo have be produce use stereo camera . building upon moravec ’ s work , matthies and shafer [ 6 ] , [ 7 ] use a binocular system and moravec ’ sprocedure for detect and track corner . instead of use a scalar representation of the uncertainty a moravec do , they take advantage of the error covariance matrix ofthe triangulate feature and incorporate it into the motion estimation step . compared to moravec , they dem- onstrated superior result in trajectory recovery for aplanetary rover , with 2 % relative error on a 5.5-m path . olson et al . [ 9 ] , [ 13 ] later extend that work by• the advantage of vo with respect to wheel odometry be that vo be not affect by wheel slip in uneven terrain or other adverse condition . • december 2011 \\x81ieee robotics & automation magazine \\x8181•introducing an absolute orientation sensor ( e.g. , compass or omnidirectional camera ) and use the forstner corner detector , which be significantly faster to compute than moravec ’ s operator . they show that the use of camera egomotion estimate alone result in accumulation errorswith superlinear growth in the distance travel , lead to increase orienta- tion error . conversely , when an abso-lute orientation sensor be incorporate , the error growth can be reduce to a linear function of the distance traveled.this lead them to a relative position error of 1 :2 % on a 20-m path . lacroix et al . [ 8 ] implement a stereo vo approach for planetary rover similar to those explain earlier . the difference lie in the selection of key point . instead of use the forstner detector , theyused dense stereo and , then , select the candidate key point by analyze the correlation function around it peaks—an approach that be later exploit in [ 14 ] , [ 15 ] , and other work . this choice be base on the observa- tion that there be a strong correlation between the shape of the correlation curve and the standard deviation of thefeature depth . this observation be later use by cheng et al . [ 16 ] , [ 17 ] in their final', 'there be a strong correlation between the shape of the correlation curve and the standard deviation of thefeature depth . this observation be later use by cheng et al . [ 16 ] , [ 17 ] in their final vo implementation onboard the mars rover . they improve on the early implemen-tation by olson et al . [ 9 ] , [ 13 ] in two area . first , after use the harris corner detector , they utilize the curva- ture of the correlation function around the feature —as propose by lacroix et al . —to define the error covariance matrix of the image point . second , a propose by nister et al . [ 1 ] , they use the random sample consensus ( ran-sac ) ransac [ 18 ] in the least-squares motion estima- tion step for outlier rejection . a different approach to motion estimation and outlier removal for an all-terrain rover be propose by milella and siegwart [ 14 ] . they use the shi-tomasi approach [ 19 ] for corner detection , and similar to lacroix , theyretained those point with high confidence in the stereo disparity map . motion estimation be then solve by first use least square , a in the method earlier , and then theiterative close point ( icp ) algorithm [ 20 ] —an algorithm popular for 3-d registration of laser scan —for pose refinement . for robustness , an outlier removal stage wasincorporated into the icp . the work mention so far have in common that the 3-d point be triangulate for every stereo pair , and therelative motion be solve a a 3-d-to-3-d point registration ( alignment ) problem . a completely different approach be propose in 2004 by nister et al . [ 1 ] . their paper be knownnot only for coin the term vo but also for provide the first real-time long-run implementation with a robust outlier rejection scheme . nister et al . improve the earlierimplementations in several area . first , contrary to all previous work , they do not track feature among framesbut detect feature ( harris corner ) independently in all frame and only allow match between feature . this have the benefit of avoid feature drift during', 'work , they do not track feature among framesbut detect feature ( harris corner ) independently in all frame and only allow match between feature . this have the benefit of avoid feature drift during cross-corre-lation-based tracking . second , they do not compute the relative motion a a 3-d-to-3-d point registration problem but a a 3-d-to-two-dimensional ( 2-d ) camera-pose estima-tion problem ( these method be describe in the “ motion estimation ” section ) . finally , they incorporate ransac outlier rejection into the motion estimation step . a different motion estimation scheme be introduce by comport et al . [ 21 ] . instead of use 3-d-to-3-d point registration or 3-d-to-2-d camera-pose estimation tech-niques , they rely on the quadrifocal tensor , which allow motion to be compute from 2-d-to-2-d image match without have to triangulate 3-d point in any ofthe stereo pair . the benefit of use directly raw 2-d point in lieu of triangulated 3-d point lay in a more accurate motion computation . monocular vo the difference from the stereo scheme be that in themonocular vo , both the relative motion and 3-d structure must be compute from 2-d bearing data . since the abso- lute scale be unknown , the distance between the first twocamera pose be usually set to one . as a new image arrive , the relative scale and camera pose with respect to the first two frame be determine use either the knowledge of3-d structure or the trifocal tensor [ 22 ] . successful result with a single camera over long distan- ce ( up to several kilometer ) have be obtain in thelast decade use both perspective and omnidirectional camera [ 23 ] – [ 29 ] . related work can be divide into three category : feature-based method , appearance-based meth-ods , and hybrid method . feature-based method be base on salient and repeatable feature that be track over the frame ; appearance-based method use the intensity infor-mation of all the pixel in the image or subregions of it ; and hybrid method use a combination of the previous two . in the first', 'over the frame ; appearance-based method use the intensity infor-mation of all the pixel in the image or subregions of it ; and hybrid method use a combination of the previous two . in the first category be the work by the author in [ 1 ] , [ 24 ] , [ 25 ] , [ 27 ] , and [ 30 ] – [ 32 ] . the first real-time , large- scale vo with a single camera be present by nister et al . [ 1 ] . they use ransac for outlier rejection and 3-d-to-2-d camera-pose estimation to compute the new upcoming camera pose . the novelty of their paper be the use of a five-point minimal solver [ 33 ] to calculate themotion hypothesis in ransac . after that paper , five- point ransac become very popular in vo and be use in several other work [ 23 ] , [ 25 ] , [ 27 ] . corke et al . [ 24 ] provide an approach for monocular vo base on omni- directional imagery from a catadioptric camera and optical flow . lhuillier [ 25 ] and mouragnon et al . [ 30 ] present anapproach base on local windowed-bundle adjustment to recover both the motion and the 3-d map ( this mean that bundle adjustment be perform over a window of the last mframes ) . again , they use the five-point ransac in [ 33 ] to remove the outlier . tardif et al . [ 27 ] present an• keyframe selection be av e r yi m p o r t a n t t e p in vo and should always be do before update the motion . • 82\\x81ieee robotics & automation magazine \\x81december 2011•approach for vo on a car over a very long run ( 2.5 km ) without bundle adjustment . contrary to the previous work , they decouple the rotation and translation estimation . therotation be estimate by use point at infinity and the translation from the recover 3-d map . erroneous corre- spondences be remove with five-point ransac . among the appearance-based or hybrid approach be the work by the author in [ 26 ] , [ 28 ] , and [ 29 ] . goecke et al . [ 26 ] use the fourier –mellin transform for register perspective image of the ground plane take from a car . milford and wyeth [ 28 ] present a method to extract approximate rotational and', 'et al . [ 26 ] use the fourier –mellin transform for register perspective image of the ground plane take from a car . milford and wyeth [ 28 ] present a method to extract approximate rotational and translational velocity informa-tion from a single perspective camera mount on a car , which be then use in a ratslam scheme [ 34 ] . they use template tracking on the center of the scene . a major draw-back with appearance-based approach be that they be not robust to occlusion . for this reason , scaramuzza and sieg- wart [ 29 ] use image appearance to estimate the rotationof the car and feature from the ground plane to estimate the translation and the absolute scale . the feature-based approach be also use to detect failure of the appearance-based method . all the approach mention early be design for unconstrained motion in 6 dof . however , several voworks have be specifically design for vehicle with motion constraint . the advantage be decrease computa- tion time and improved motion accuracy . for instance , liang and pears [ 35 ] , ke and kanade [ 36 ] , wang et al . [ 37 ] , and guerrero et al . [ 38 ] take advantage of homogra- phies for estimate the egomotion on a dominant groundplane . scaramuzza et al . [ 31 ] , [ 39 ] introduce a one-point ransac outlier rejection base on the vehicle nonholo- nomic constraint to speed up egomotion estimation to400 hz . in the follow-up work , they show that nonholo- nomic constraint allow the absolute scale to be recover from a single camera whenever the vehicle make a turn [ 40 ] . following that work , vehicle nonholonomic con- straints have also be use by pretto et al . [ 32 ] for improv- ing feature track and by fraundorfer et al . [ 41 ] forwindowed bundle adjustment ( see the following section ) . reducing the drift since vo work by compute the camera path incre- mentally ( pose after pose ) , the error introduce by each new frame-to-frame motion accumulate over time . thisgenerates a drift of the estimated trajectory from the real path . for some application , it be', '( pose after pose ) , the error introduce by each new frame-to-frame motion accumulate over time . thisgenerates a drift of the estimated trajectory from the real path . for some application , it be of utmost importance to keep drift as small a possible , which can be donethrough local optimization over the last mcamera pose . this approach —called slide window bundle adjust- ment orwindowed bundle adjustment —has be use in several work , such a [ 41 ] – [ 44 ] . in particular , on a 10-km vo experiment , konolige et al . [ 43 ] demonstrate that windowed-bundle adjustment can decrease the finalposition error by a factor of 2 –5 . obviously , the vo drift can also be reduce through combination with othersensors , such a gps and laser , or even with only an imu [ 43 ] , [ 45 ] , [ 46 ] . v-slam although this tutorial focus on vo , it be worth mention- ing the parallel line of research undertake by visual simul-taneous localization and mapping ( v-slam ) . for an in- depth study of the slam problem , the reader be refer to two tutorial on this topic by durrant-whyte and bailey [ 47 ] , [ 48 ] . two methodology have become predominant in v-slam : 1 ) filter method fuse the informationfrom all the image with a probability distribution [ 49 ] and 2 ) nonfiltering method ( also call keyframe meth- od ) retain the optimization of global bundle adjustment to select key- frame [ 50 ] . the main advantage ofeither approach have be evaluate and summarize in [ 51 ] . in the last few year , successful result have be obtain use both single and stereo camera [ 49 ] , [ 52 ] – [ 62 ] . most of these work have be limit to small , indoor workspace and only a few of them have recently be design for large-scale area [ 54 ] , [ 60 ] , [ 62 ] . some of the early work in real-time v-slam be present bychiuso et al . [ 52 ] , deans [ 53 ] , and davison [ 49 ] use a full-covariance kalman approach . the advantage of davi- son ’ s work be to account for repeatable localization afteran arbitrary amount of time . later , handa et al . [ 59 ]', 'davison [ 49 ] use a full-covariance kalman approach . the advantage of davi- son ’ s work be to account for repeatable localization afteran arbitrary amount of time . later , handa et al . [ 59 ] improve on that work use an active matching technique base on a probabilistic framework . civera et al . [ 60 ] builtupon that work by propose a combination of one-point ransac within the kalman filter that use the available prior probabilistic information from the filter in the ran-sac model-hypothesis stage . finally , strasdat et al . [ 61 ] present a new framework for large-scale v-slam that take advantage of the keyframe optimization approach [ 50 ] while take into account the special character of slam . vo versus v-slam in this section , the relationship of vo with v-slam be analyze . the goal of slam in general ( and v-slam in particular ) be to obtain a global , consistent estimate of therobot path . this imply keep a track of a map of the environment ( even in the case where the map be not need per se ) because it be need to realize when therobot return to a previously visit area . ( this be call loop closure . when a loop closure be detect , this informa- tion be use to reduce the drift in both the map and camerapath . understanding when a loop closure occur and effi- ciently integrate this new constraint into the current map be two of the main issue in slam . ) conversely , voaims at recover the path incrementally , pose after pose , and potentially optimize only over the last n pose of the• vo be only concerned with the local consistency of the trajectory , whereas slam with the global consistency . • december 2011 \\x81ieee robotics & automation magazine \\x8183•path ( this be also call windowed bundle adjustment ) . this slide window optimization can be consider equivalent to build a local map in slam ; however , the philosophyis different : in vo , we only care about local consistency of the trajectory and the local map be use to obtain a more accurate estimate of the local trajectory ( for example , inbundle', 'philosophyis different : in vo , we only care about local consistency of the trajectory and the local map be use to obtain a more accurate estimate of the local trajectory ( for example , inbundle adjustment ) , whereas slam be concern with the global map consistency . vo can be use a a building block for a complete slam algorithm to recover the incremental motion of the camera ; however , to make a complete slam method , onemust also add some way to detect loop closing and possibly a global optimiza- tion step to obtain a metrically consist-ent map ( without this step , the map be still topologically consistent ) . if the user be only interested in the camera path and not in the environ- ment map , there be still the possibility of use a complete v-slam methodinstead of one of the vo technique describe in this tutorial . a v-slam method be potentially much more precise , because it enforce many more con-straints on the path , but not necessarily more robust ( e.g. , outlier in loop closing can severely affect the map consis- tency ) . in addition , it be more complex and computation-ally expensive . in the end , the choice between vo and v-slam depend on the tradeoff between performance and con-sistency , and simplicity in implementation . although the global consistency of the camera path be sometimes desir- able , vo trade off consistency for real-time perform-ance , without the need to keep track of all the previous history of the camera.formulation of the vo problem an agent be move through an environment and take image with a rigidly attach camera system at discretetime instant k. in case of a monocular system , the set of image take at time kis denote by i 0 : n¼fi0 , ... , ing . in case of a stereo system , there be a left and a right imageat every time instant , denote by i l,0 : n¼fil,0 , ... , il , ng andir,0 : n¼fir,0 , ... , ir , ng . figure 1 show an illustration of this setting . for simplicity , the camera coordinate frame be assume to be also the agent ’ s coordinate frame . in case of a stereo', ': n¼fir,0 , ... , ir , ng . figure 1 show an illustration of this setting . for simplicity , the camera coordinate frame be assume to be also the agent ’ s coordinate frame . in case of a stereo system , without loss of generality , the coordinate system ofthe left camera can be use a the origin . two camera position at adjacent time instant k/c01 and kare relate by the rigid body transformation t k , k/c012r434of the following form : tk , k/c01¼rk , k/c01tk , k/c01 01/c20/c21 , ( 1 ) where rk , k/c012so ( 3 ) be the rotation matrix , and tk , k/c012r331the translation vector . the set t1 : n¼ ft1 , 0 , ... , tn , n/c01gcontains all subsequent motion . to simplify the notation , from now on , tkwill be use instead of tk , k/c01 . finally , the set of camera pose c0 : n¼ fc0 , ... , cngcontains the transformation of the camera with respect to the initial coordinate frame at k¼0 . the current pose cncan be compute by concatenate all the transformation tk ( k¼1 ... n ) , and , therefore , cn¼ cn/c01tn , w i t h c0being the camera pose at the instant k¼0 , which can be set arbitrarily by the user . the main task in vo be to compute the relative transfor- mations tkfrom the image ikandik/c01and then to concate- nate the transformation to recover the full trajectory c0 : nof the camera . this mean that vo recover the path incremen- tally , pose after pose . an iter ative refinement over the last m pose can be perform after this step to obtain a more accu- rate estimate of the local trajectory . this iterative refinement work by minimize the sum of the squared reprojectionerrors of the reconstruct 3-d point ( i.e. , the 3-d map ) over the last mi m a g e s ( t h i si sc a l l e d windowed-bundle adjustment , because it be perform on a window of mframes . bundle adjustment will be describe in part ii of this tutorial ) . the 3-d point be obtain by triangulation of the image point ( see the “ triangulation and keyframe selection ” section ) . as mention in the “ monocular vo ” section , there be two main approach to', '. the 3-d point be obtain by triangulation of the image point ( see the “ triangulation and keyframe selection ” section ) . as mention in the “ monocular vo ” section , there be two main approach to compute the relative motion t k : appearance-based ( or global ) method , which use theintensity information of all the pixel in the two input image , and feature-based method , which only use salient and repeatable feature extract ( or track ) across theimages . global method be less accurate than feature- base method and be computationally more expensive . ( as observe in the “ history of vo ” section , most appear-ance-based method have be apply to monocular vo . this be due to ease of implementation compare with the ck+1 ck–1tk , k–1tk+1 , k ck figure 1 . an illustration of the visual odometry problem . the relative pose tk ; k/c01of adjacent camera position ( or position of a camera system ) be compute from visual feature andconcatenated to get the absolute pose c kwith respect to the initial coordinate frame at k¼0.• in the motion estimation step , the camera motion between the current and the previous image be compute . • 84\\x81ieee robotics & automation magazine \\x81december 2011•stereo camera case . ) feature-based method require the ability to robustly match ( or track ) feature across frame but be fast and more accurate than global methods.therefore , most vo implementation be feature base . the vo pipeline be summarize in figure 2 . for every new image i k ( or image pair in the case of a stereo camera ) , the first two step consist of detect and match 2-d feature with those from the previous frame . two-dimen- sional feature that be the reprojection of the same 3-dfeature across different frame be call image correspond- ences . ( as will be explain in part ii of this tutorial , we distinguish between feature matching and feature tracking.the first one consist of detect feature independently in all the image and then match them base on some similarity metric ; the second one consist of find fea-tures in one', 'and feature tracking.the first one consist of detect feature independently in all the image and then match them base on some similarity metric ; the second one consist of find fea-tures in one image and then track them in the next image use a local search technique , such a correlation . ) the third step consist of compute the relative motiont kbetween the time instant k/c01 and k. depending on whether the correspondence be specify in three or two dimension , there be three distinct approach to tacklethis problem ( see the “ motion estimation ” section ) . the camera pose c ki then compute by concatenation of tk with the previous pose . finally , an iterative refinement ( bundle adjustment ) can be do over the last mframes to obtain a more accurate estimate of the local trajectory . motion estimation be explain in this tutorial ( see “ motion estimation ” section ) . feature detection and matching and bundle adjustment will be describe in part ii . also , notice that for an accurate motion computation , feature correspondence should not contain outlier ( i.e. , wrong data association ) . ensuring accurate motion esti- mation in the presence of outlier be the task of robust esti-mation , which will be describe in part ii of this tutorial . most vo implementation assume that the camera be cali- brated . to this end , the next section review the standardmodels and calibration procedure for perspective and omnidirectional camera . camera modeling and calibration vo can be do use both perspective and omnidirec- tional camera . in this section , we review the main model . perspective camera model the most used model for perspective camera assume a pin-hole projection system : the image be form by the intersec- tion of the light ray from the object through the center of the lens ( projection center ) , with the focal plane [ figure3 ( a ) ] . let x¼½x , y , z/c138 > be a scene point in the camera refer- ence frame and p¼½u , v/c138 > it projection on the image plane measure in pixel . the mapping from the 3-d world to', '[ figure3 ( a ) ] . let x¼½x , y , z/c138 > be a scene point in the camera refer- ence frame and p¼½u , v/c138 > it projection on the image plane measure in pixel . the mapping from the 3-d world to the2-d image be give by the perspective projection equation : ku v 12 43 5¼kx¼a u0u0 0avv0 0012 43 5x y z2 43 5 , ( 2 ) where kis the depth factor , auandavthe focal length , and u0 , v0the image coordinate of the projection center . these parameter be call intrinsic parameter . when the field of view of the camera be large than 45 /c176 , the effect of the radial distortion may become visible and can be model use a second- ( or high ) -order polynomial . the deriva-tion of the complete model can be find in computer vision textbook , such a [ 22 ] and [ 63 ] . let ~p¼½~u , ~v,1/c138 > ¼ k/c01½u , v,1/c138 > be the normalized image coordinate . nor- malized coordinate will be use throughout in the follow- ing section . ( u0 , v0 ) u pvx x yccz y x x v u x yz c x ( a ) ( b ) ( c ) z figure 3 . ( a ) perspective projection , ( b ) catadioptric projection , and ( c ) a spherical model for perspective and omnidirectionalcameras . image point be represent a direction to the view point normalize on the unit sphere.image sequence feature detection feature matching ( or tracking ) local optimization ( bundle adjustment ) motion estimation 2-d-to-2-d 3-d-to-3-d 3-d-to-2-d figure 2 . a block diagram show the main component of a vo system . december 2011 \\x81ieee robotics & automation magazine \\x8185•omnidirectional camera model omnidirectional camera be cameras with wide field of view ( even more than 180 /c176 ) and can be build use fish-eye lens or by combine standard camera with mirror [ the latter be call catadioptric camera , figure 3 ( b ) ] . typical mirror shape in catadioptric camera be quadratic surface of revolution ( e.g. , paraboloid or hyper-boloid ) , because they guarantee a sin- gle projection center , which make it possible to use the motion estima-tion theory present in the “ motion estimation ” section . currently', 'or hyper-boloid ) , because they guarantee a sin- gle projection center , which make it possible to use the motion estima-tion theory present in the “ motion estimation ” section . currently , there be two accepted model for omnidir- ectional camera . the first one propose by geyer and daniilidis [ 64 ] be for general catadioptric camera ( para- bolic or hyperbolic ) , while the second one propose byscaramuzza et al . [ 65 ] be a unified model for both fish-eye and catadioptric camera . a survey of these two model can be find in [ 66 ] and [ 67 ] . the projection equation ofthe unified model be a follow : ku v a 0þa1qþ/c1/c1/c1þ an/c01qn/c012 43 5¼x y z2 43 5 , ( 3 ) where q¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ u2þv2p and a0 , a1 , ... , anare intrinsic parameter that depend on the type of mirror or fish-eyelens . as show in [ 65 ] , n¼4 be a reasonable choice for a large variety of mirror and fish-eye lens . finally , this model assume that the image plane satisfy the idealproperty that the ax of symmetry of the camera and mir- ror be align . although this assumption hold for most catadioptric and fish-eye camera , misalignment can bemodeled by introduce a perspective projection between the ideal and real-image plane [ 66 ] . spherical model as mention earlier , it be desirable that the camera possess a single projection center ( also call single effec- tive viewpoint ) . in a catadioptric camera , this happen when the ray reflect by the mirror intersect all in a sin- gle point ( namely c ) . the existence of this point allow u to model any omnidirectional projection a a mapping from the single viewpoint to a sphere . for convenience , a unit sphere be usually adopt . it be important to notice that the spherical model applies not only to omnidirectional camera but also to perspec- tive camera . if the camera be calibrate , any point in theperspective or omnidirectional image can be map into a vector on the unit sphere . as can be observe in figure 3 ( c ) , these unit vector represent the direction to theviewed scene point . these', 'theperspective or omnidirectional image can be map into a vector on the unit sphere . as can be observe in figure 3 ( c ) , these unit vector represent the direction to theviewed scene point . these vector be call normalized image point on the unit sphere .camera calibration the goal of calibration be to accurately measure the intrinsic and extrinsic parameter of the camera system.in a multicamera system ( e.g. , stereo and trinocular ) , the extrinsic parameter describe the mutual position and orientation between each camera pair . the most popularmethod use a planar checkerboard-like pattern . the position of the square on the board be know . to com- pute the calibration parameter accurately , the user musttake several picture of the board show at different posi- tions and orientation by ensure that the field of view of the camera be fill as much a possible . the intrinsic andextrinsic parameter be then find through a least-square minimization method . the input data be the 2-d position of the corner of the square of the board and their corre-sponding pixel coordinate in each image . many camera calibration toolbox have be devise for matlab and c. an up-to-date list can be find in [ 68 ] . among these , the most popular one for matlab be give in [ 69 ] and [ 70 ] – [ 72 ] —for perspective and omni- directional camera , respectively . a c implementation ofcamera calibration for perspective camera can be find in opencv [ 73 ] , the open-source computer vision library . motion estimation motion estimation be the core computation step per- form for every image in a vo system . more precisely , in the motion estimation step , the camera motion between the current image and the previous image be compute . by concatenation of all these single move-ments , the full trajectory of the camera and the agent ( assume that the camera be rigidly mount ) can be recover . this section explain how the transformationt kbetween two image ik/c01andikcan be compute from two set of correspond feature fk/c01 , fkat time instant', 'the camera be rigidly mount ) can be recover . this section explain how the transformationt kbetween two image ik/c01andikcan be compute from two set of correspond feature fk/c01 , fkat time instant k/c01a n d k , respectively . depending on whether the fea- ture correspondence be specify in two or three dimension , there be three different method . l2-d-to-2-d : in this case , both fk/c01andfkare specify in 2-d image coordinate . l3-d-to-3-d : in this case , both fk/c01andfkare specify in 3-d. to do this , it be necessary to triangulate 3-d pointsat each time instant ; for instance , by use a stereo camera system . l3-d-to-2-d : in this case , fk/c01are specify in 3-d and fk be their corresponding 2-d reprojections on the image ik . in the monocular case , the 3-d structure need to be triangulate from two adjacent camera view ( e.g. , ik/c02 andik/c01 ) and then match to 2-d image feature in a third view ( e.g. , ik ) . in the monocular scheme , match over at least three view be necessary . notice that feature can be point or line . in general , due to the lack of line in unstructured scene , point feature be use in vo . an in-depth review of these three approach forboth point and line feature can be find in [ 74 ] . the formu- lation give in this tutorial be for point feature only.• in gps-denied environment , vo become of utmost importance . • 86\\x81ieee robotics & automation magazine \\x81december 2011•2-d to 2-d : motion from image feature correspondences estimating the essential matrix the geometric relation between two image ikandik/c01of a calibrated camera be describe by the so-called essentialmatrix e.econtains the camera motion parameters up to an unknown scale factor for the translation in the follow- ing form : e k ’ ^tkrk , ( 4 ) where tk¼½tx , ty , tz/c138 > and ^tk¼0/c0tzty tz 0/c0tx /c0tytx 02 43 5 : ( 5 ) the symbol ’ be use to denote that the equivalence be valid up to a multiplicative scalar . the essential matrix can be compute from 2-d-to-2-d feature correspondence , and rotation and translation', ') the symbol ’ be use to denote that the equivalence be valid up to a multiplicative scalar . the essential matrix can be compute from 2-d-to-2-d feature correspondence , and rotation and translation can directly be extract from e. the main property of 2-d-to- 2-d-based motion estimation be the epipolar constraint , which determine the line on which the correspond fea- ture point ~p 0of~plies in the other image ( figure 4 ) . this con- straint can be formulate by ~p0 > e~p¼0 , where ~p0is a feature location in one image ( e.g. , ik ) a n d ~pis the location of it corresponding feature in another image ( e.g. , ik/c01 ) .~pand~p0 be normalize image coordinate . for the sake of simplic- ity , throughout the following section , normalize coordi- nates in the form ~p¼½~u , ~v,1/c138 > will be use ( see the perspective camera model ” section ) . however , very similarequations can also be derive for normalized coordinate on the unit sphere ( see the “ spherical model ” section ) . the essential matrix can be compute from 2-d-to-2- d feature correspondence use the epipolar constraint . the minimal case solution involve five 2-d-to-2-d cor- respondences [ 75 ] and an efficient implementation pro-posed by nister in [ 76 ] . nister ’ s five-point algorithm have become the standard for 2-d-to-2-d motion estimation in the presence of outlier ( the problem of robust estima-tion will be tackle in part ii of this tutorial ) . a simple and straightforward solution for n/c218 noncoplanar point be the longuet-higgins ’ eight-point algorithm [ 2 ] , which be summarize here . each feature match give a con- straint of the following form : ~u~u 0~u0~v~u0~u~v0~v~v0~v0~u~v1 ½/c138 e¼0 , ( 6 ) where e¼½e1e2e3e4e5e6e7e8e9/c138 > : stacking the constraint from eight point give the lin- ear equation system ae¼0 , and by solve the system , the parameter of ecan be compute . this homogeneous equation system can easily be solve use singular valuedecomposition ( svd ) [ 2 ] . having more than eight point lead to an overdetermined system to', 'parameter of ecan be compute . this homogeneous equation system can easily be solve use singular valuedecomposition ( svd ) [ 2 ] . having more than eight point lead to an overdetermined system to solve in the least-squares sense and provide a degree of robustness to noise . the svd of ahas the form a¼usv > , and the least- square estimate of ewithjjejj ¼ 1 can be find a the last column of v. however , this linear estimation of edoes not fulfill the inner constraint of an essential matrix , which come from the multiplication of the rotation matrixrand the skew-symmetric translation matrix ^t . these con- straints be visible in the singular value of the essential matrix . a valid essential matrix after svd be e¼usv > and have diag ( s ) ¼fs , s,0g , which mean that the first and second singular value be equal and the third one be zero . to get a valid ethat fulfill the constraint , the solution need to be project onto the space of valid essential matri- ce . the projected essential matrix be /c22e¼udiagf1 , 1 , 0 gv > . observe that the solution of the eight-point algorithm be degenerate when the 3-d point be coplanar . conversely , the five-point algorithm work also for coplanar point . finally , observe that the eight-point algorithm work forboth calibrate ( perspective or omnidirectional ) and unca- librate ( only perspective ) camera , whereas the five-point algorithm assume that the camera ( perspective or omni-directional ) be calibrate . extracting rand tfrom e from the estimate of /c22e , the rotation and translation part can be extract . in general , there be four different solu- tions for r , tfor one essential matrix ; however , by triangu- lation of a single point , the correct r , tpair can be identify . the four solution be r¼u ( /c6w > ) v > , ^t¼u ( /c6w ) su > , where w > ¼0/c610 /c7100 00 12 43 5 : ( 7 ) an efficient decomposition of einto rand ti describe in [ 76 ] . ˜px epipolar plane˜p′ ck–1ck tk , k–1epipolar line epipolar line figure 4 . an illustration of the epipolar constraint . december', '7 ) an efficient decomposition of einto rand ti describe in [ 76 ] . ˜px epipolar plane˜p′ ck–1ck tk , k–1epipolar line epipolar line figure 4 . an illustration of the epipolar constraint . december 2011 \\x81ieee robotics & automation magazine \\x8187•after select the correct solution by triangulation of a point and choose the solution where the point be in front of both camera , a nonlinear optimization of the rotationand translation parameter should be perform use the estimate r , tas initial value . the function to minimize be the reprojection error define in ( 10 ) . computing the relative scale to recover the trajectory of an image sequence , the differ-ent transformation t 0 : nhave to be concatenate . to do this , the proper relative scale need to be compute a the absolute scale ofthe translation can not be compute from two image . however , it be possi- ble to compute relative scale for thesubsequent transformation . one way of do this be to triangulate 3-d point x k/c01and xkfrom two subse- quent image pair . from the corre- sponding 3-d point , the relative distance between any combination of two 3-d point can becomputed . the proper scale can then be determine from the distance ratio rbetween a point pair in x k/c01and a pair in xk . r¼jjxk/c01 , i/c0xk/c01 , jjj jjxk , i/c0xk , jjj : ( 8 ) for robustness , the scale ratio for many point pair be com- puted and the mean ( or in presence of outlier , the median ) be use . the translation vector ti then scale with this distance ratio . observe that the relative-scale computation require fea- tures to be match ( or track ) over multiple frame ( at least three ) . instead of perform exp licit triangulation of the 3-d point , the scale can also be recover by exploit the trifocal constraint between three-view match of 2-d feature [ 22 ] . the vo algorithm with the 2-d-to-2-d correspond- ences be summarize in algorithm 1 . 3d-to-3d : motion from 3-d structure correspondences for the case of correspond 3-d-to-3-d feature , thecamera motion t kcan be compute by', '2-d-to-2-d correspond- ences be summarize in algorithm 1 . 3d-to-3d : motion from 3-d structure correspondences for the case of correspond 3-d-to-3-d feature , thecamera motion t kcan be compute by determine the align transformation of the two 3-d feature sets.corresponding 3-d-to-3-d feature be available in the stereo vision case . the general solution consist of find the tkthat min- imizes the l2distance between the two 3-d feature set arg min tkx ijj~xi k/c0tk~xi k/c01jj , ( 9 ) where the superscript idenotes the ith feature , and ~xk , ~xk/c01are the homogeneous coordinate of the 3-d point , i.e. , ~x¼½x , y , z,1/c138 > . as show in [ 77 ] , the minimal case solution involve three 3-d-to-3-d noncollinear correspondence , which can be use for robust estimation in the presence of out- liers ( part ii of this tutorial ) . for the case of n/c213 corre- spondences , one possible solution ( accord to arun et al . [ 78 ] ) be to compute the translation part a the difference of the centroid of the 3-d feature set and the rotation partusing svd . the translation be give by t k¼xk/c0rxk/c01 , where /c22/c1stands for the arithmetic mean value . the rotation can be efficiently compute use svd a rk¼vu > , where usv > ¼svd ( ( xk/c01/c0xk/c01 ) ( xk/c0xk ) > ) and xk/c01 andxkare set of correspond 3-d point . if the measurement uncertainty of the 3-d point be know , they can be add a weight into the estimation a describe by maimone et al . [ 17 ] . the computed transfor- mations have absolute scale , and thus , the trajectory of asequence can be compute by directly concatenate the transformation . the vo algorithm with the 3-d-to-3-d correspond- ences be summarize in algorithm 2 . to compute the transformation , it be also possible to avoid the triangulation of the 3-d point in the stereo camera and use quadrifocal constraint instead . this method be point out by comport et al . [ 21 ] . the quadrifocal tensorallows compute the transformation directly from 2-d- to-2-d stereo correspondences.• algorithm 1 . vo from', 'instead . this method be point out by comport et al . [ 21 ] . the quadrifocal tensorallows compute the transformation directly from 2-d- to-2-d stereo correspondences.• algorithm 1 . vo from 2-d-to-2-d correspondence . 1 ) capture new frame ik 2 ) extract and match feature between ik/c01and ik 3 ) compute essential matrix for image pair ik/c01 , ik 4 ) decompose essential matrix into rkand tk , a n df o r m tk 5 ) compute relative scale and rescale tkaccordingly 6 ) concatenate transformation by compute ck¼ck/c01tk 7 ) repeat from 1 ) .• algorithm 2 . vo from 3-d-to-3-d correspondence . 1 ) capture two stereo image pair il ; k/c01 , ir ; k/c01and il ; k , ir ; k 2 ) extract and match feature between il ; k/c01and il ; k 3 ) triangulate match feature for each stereo pair 4 ) compute tkfrom 3-d feature xk/c01and xk 5 ) concatenate transformation by compute ck¼ck/c01tk 6 ) repeat from 1 ) .• bundle adjustment c a nb eu s e dt or e f i n e the local estimate of the trajectory . • 88\\x81ieee robotics & automation magazine \\x81december 2011•3-d-to-2-d : motion from 3-d structure and image feature correspondences as point out by nister et al . [ 1 ] , motion estimation from3-d-to-2-d correspondence be more accurate than from 3-d-to-3-d correspondence because it minimize the image reprojection error ( 10 ) instead of the 3-d-to-3-dfeature position error ( 9 ) . the transformation t kis com- put from the 3-d-to-2-d correspondence xk/c01andpk : xk/c01can be estimate from stereo data or , in the monocu- lar case , from triangulation of the image measurement pk/c01and pk/c02 . the latter , however , require image corre- spondences across three view . the general formulation in this case be to find tkthat minimize the image reprojection error arg min tkx ikpi k/c0^pi k/c01k2 , ( 10 ) where ^pi k/c01is the reprojection of the 3-d point xi k/c01into image ikaccording to the transformation tk . this problem be know a perspective from n point ( pnp ) ( or resection ) , and there be many different solution to it in the literature [ 79', 'xi k/c01into image ikaccording to the transformation tk . this problem be know a perspective from n point ( pnp ) ( or resection ) , and there be many different solution to it in the literature [ 79 ] . as show in [ 18 ] , the minimal case involve three 3- d-to-2-d correspondence . this be call perspective from three point ( p3p ) and return four solution that can be disambiguate use one or more additional point . ( a fast implementation of p3p be describe in [ 80 ] , and c code can be freely download from the author ’ web page . ) inthe 3-d-to-2-d case , p3p be the standard method for robust motion estimation in the presence of outlier [ 18 ] . robust estimation will be describe in part ii of this tutorial . a simple and straightforward solution to the p np prob- lem for n/c216 point be the direct linear transformation algorithm [ 22 ] . one 3-d-to-2-d point correspondenceprovides two constraint of the following form for the entry of p k¼½rjt/c138 . 0000 /c0x/c0y/c0z/c01x~vy ~vz ~v~v xyz 10 0 0 0 /c0x~u/c0y~u/c0z~u/c0~u/c20/c21 p1 p2 p32 43 5¼0 , ( 11 ) where each pj > be a four vector ( the jth row of pk ) and x , y , zare the coordinate of the 3-d point xk/c01 . stacking the constraint of six-point correspondence give a linear system of equation of the form ap¼0 . the entry of pcan be compute from the nullvector of a , e.g. , by use svd . the rotation and translation part can easily be extract from pk¼½rjt/c138 . the result rotation ris not necessarily orthonormal . however , this be not a prob- lem since both randtcan be refine by nonlinear optimi- zation of the reprojection error a define in ( 10 ) . the 3-d-to-2-d motion estimation assume that the 2- d image point only come from one camera . this mean that for the case of a stereo camera , the 2-d image pointsare those of either the left or the right camera . obviously , it be desirable to make use of the image point of bothcameras at the same time . a generalized version of the 3- d-to-2-d motion estimation algorithm for nonconcurrent ray ( i.e. , 2-d', '. obviously , it be desirable to make use of the image point of bothcameras at the same time . a generalized version of the 3- d-to-2-d motion estimation algorithm for nonconcurrent ray ( i.e. , 2-d image point from multiple camera ) wasproposed by nister in [ 81 ] for extrinsically calibrate cam- era ( i.e. , the mutual position and orientation between the camera be know ) . for the monocular case , it be necessary to triangulate 3- d point and estimate the pose from 3-d-to-2-d match in an alternating fashion . this alternate scheme be oftenreferred to a sfm . starting from two view , the initial set of 3-d point and the first transformation be computedfrom 2-d-to-2-d feature match . subsequent transformation be then compute from 3-d-to-2-d featurematches . to do this , feature need to be match ( or track ) over multiple frame ( at least three ) . new 3-d fea-tures be again triangulate when a new transformation be compute and add to the set of 3-d feature . the main challenge of thismethod be to maintain a consistent and accurate set of tri- angulated 3-d feature and to create 3-d-to-2-d feature match for at least three adjacent frame . the vo algorithm with 3-d-to-2-d correspondence be summarize in algorithm 3 . triangulation and keyframe selection some of the previous motion estimation method require triangulation of 3-d point ( structure ) from 2-d imagecorrespondences . structure computation be also need by bundle adjustment ( part ii of this tutorial ) to compute a more accurate estimate of the local trajectory . triangulated 3-d point be determine by intersect back-projected ray from 2-d image correspondence of at least two image frame . in perfect condition , these rayswould intersect in a single 3-d point . however , because of image noise , camera model and calibration error , and• algorithm 3 . vo from 3-d-to-2-d correspondences . 1 ) do only once : 1.1 ) capture two frame ik/c02 , ik/c01 1.2 ) extract and match feature between them 1.3 ) triangulate feature from ik/c02 , ik/c01 2 ) do at each', 'from 3-d-to-2-d correspondences . 1 ) do only once : 1.1 ) capture two frame ik/c02 , ik/c01 1.2 ) extract and match feature between them 1.3 ) triangulate feature from ik/c02 , ik/c01 2 ) do at each iteration : 2.1 ) capture new frame ik 2.2 ) extract feature and match with previous frame ik/c01 2.3 ) compute camera pose ( pnp ) from 3-d-to-2-d match 2.4 ) triangulate all new feature match between ikand ik/c01 2.5 ) iterate from 2.1 ) .• 2-d-to-2-d and 3-d-to-2-d method a r em o r ea c c u r a t e than 3-d-to-3-d method . • december 2011 \\x81ieee robotics & automation magazine \\x8189•feature match uncertainty , they never intersect . there- fore , the point at a minimal distance , in the least-squares sense , from all intersect ray can betaken a an estimate of the 3-d point position . notice that the standard de- viation of the distance of the triangu-lated 3-d point from all ray give an idea of the quality of the 3-d point . three-dimensional point with largeuncertainty will be throw out . this happen especially when frame be take at very nearby interval com- par with the distance to the scene point . when this occur , 3-d point exhibit very large uncertainty . one way to avoid this consist of skip frame until the averageuncertainty of the 3-d point decrease below a certain threshold . the select frame be call keyframes . key- frame selection be a very important step in vo and shouldalways be do before update the motion . discussion according to nister et al . [ 1 ] , there be an advantage in use the 2-d-to-2-d and 3-d-to-2-d method compare to the 3-d-to-3-d method for motion computation . nister com-pared the vo performance of the 3-d-to-3-d case to that of the 3-d-to-2-d case for a stereo camera system and find the latter be greatly superior to the former . thereason be due to the triangulate 3-d point be much more uncertain in the depth direction . when 3-d-to-3-d feature correspondence be use in motion computation , their uncertainty may have a devastating effect on the motion estimate . in fact , in the', 'more uncertain in the depth direction . when 3-d-to-3-d feature correspondence be use in motion computation , their uncertainty may have a devastating effect on the motion estimate . in fact , in the 3-d-to-3-d case , the 3-d position error , ( 9 ) , be minimize whereas in the 3-d-to-2-d case the image reprojection error , ( 10 ) . in the monocular scheme , t he 2-d-to-2-d method be preferable compare to the 3-d-to-2-d case since itavoids point triangulation . h owever , in practice , the 3-d- to-2-d method be use more often than the 2-d-to-2-d method . the reason lie in it faster data association . aswill be describe in part ii of this tutorial , for accurate motion computation , it be of utmost importance that the input data do not contain outlier . outlier rejection be avery delicate step , and the computation time of this oper- ation be strictly link to the minimum number of point necessary to estimate the motion . as mentioned previ-ously , the 2-d-to-2-d case require a minimum of five- point correspondence ( see th e five-point algorithm ) ; however , only three correspondence be necessary in the3-d-to-2-d motion case ( see p3p ) . as will be show in part ii of this tutorial , this low number of point result in a much fast motion estimation . an advantage of the stereo camera scheme compare to the monocular one , besides the property that 3-d feature be compute directly in the absolute scale , be that matchesneed to be compute only between two view instead of three view a in the monocular scheme . additionally , sincethe 3-d structure be compute directly from a single stereo pair rather than from adjacent frame a in the monocular case , the stereo scheme exhibit less drift than the monocu-lar one in case of small motion . monocular method be interest because stereo vo degenerate into the monocu- lar case when the distance to the scene be much large thanthe stereo baseline ( i.e. , the distance between the two cam- era ) . in this case , stereo vision become ineffective and monocular method must be use .', 'the distance to the scene be much large thanthe stereo baseline ( i.e. , the distance between the two cam- era ) . in this case , stereo vision become ineffective and monocular method must be use . regardless of the chosen motion computation method , local bundle adjustment ( over the last mframes ) should always be perform to compute a more accu-rate estimate of the trajectory . after bundle adjustment , the effect of the motion estimation method be much more alleviated . conclusions this tutorial have describe the history of vo , the problemformulation , and the distinct approach to motion com- putation . vo be a well-understood and establish part of robotics . part ii of this tutorial will summarize theremaining build block of the vo pipeline : how to detect and match salient and repeatable feature across frame , robust estimation in the presence of outlier , andbundle adjustment . in addition , error propagation , appli- cation , and link to free-to-download code will be include . acknowledgments the author thank konstantinos derpanis , oleg narodit-sky , carolin baez , and andrea censi for their fruitful com- ments and suggestion . references [ 1 ] d. nister , o. naroditsky , and j. bergen , “ visual odometry , ” in proc . int . conf . computer vision and pattern recognition , 2004 , pp . 652 –659 . [ 2 ] h. longuet-higgins , “ a computer algorithm for reconstruct a scene from two projection , ” nature , vol . 293 , no . 10 , pp . 133 –135 , 1981 . [ 3 ] c. harris and j. pike , “ 3d positional integration from imagesequences , ” in proc . alvey vision conf . , 1988 , pp . 87 –90 . [ 4 ] j.-m. frahm , p. georgel , d. gallup , t. johnson , r. raguram , c. wu , y.-h. jen , e. dunn , b. clipp , s. lazebnik , and m. pollefeys , “ building rome on a cloudless day , ” in proc . european conf . computer vision , 2010 , pp . 368 –381 . [ 5 ] h. moravec , “ obstacle avoidance and navigation in the real world by a see robot rover , ” ph.d. dissertation , stanford univ. , stanford , ca , 1980 . [ 6 ] l. matthies and s.', 'pp . 368 –381 . [ 5 ] h. moravec , “ obstacle avoidance and navigation in the real world by a see robot rover , ” ph.d. dissertation , stanford univ. , stanford , ca , 1980 . [ 6 ] l. matthies and s. shafer , “ error model in stereo navigation , ” ieee j . robot . automat . , vol . 3 , no . 3 , pp . 239 –248 , 1987 . [ 7 ] l. matthies , “ dynamic stereo vision , ” ph.d. dissertation , carnegie mel- lon univ. , pittsburgh , pa , 1989 . [ 8 ] s. lacroix , a. mallet , r. chatila , and l. gallo , “ rover self localization in planetary-like environment , ” in proc . int . symp . articial intelligence , robotics , and automation for space ( i-sairas ) , 1999 , pp . 433 –440 . [ 9 ] c. olson , l. matthies , m. schoppers , and m. w. maimone , “ robust stereo ego-motion for long distance navigation , ” in proc . ieee conf . computer vision and pattern recognition , 2000 , pp . 453 –458.• vo trade off consistency for real-time performance . • 90\\x81ieee robotics & automation magazine \\x81december 2011• [ 10 ] m. hannah , “ computer matching of area in stereo image , ” ph.d. dissertation , stanford univ. , stanford , ca , 1974 . [ 11 ] h. moravec , “ towards automatic visual obstacle avoidance , ” in proc . 5th int . joint conf . artificial intelligence , aug. 1977 , p. 584 . [ 12 ] w. forstner , “ a feature base correspondence algorithm for image matching , ” int . arch . photogrammetry , vol . 26 , no . 3 , pp . 150 –166 , 1986 . [ 13 ] c. olson , l. matthies , m. schoppers , and m. maimone , “ rover navi- gation use stereo ego-motion , ” robot . autonom . syst . , vol . 43 , no . 4 , pp . 215 –229 , 2003 . [ 14 ] a. milella and r. siegwart , “ stereo-based ego-motion estimation use pixel tracking and iterative closest point , ” in p r o c .i e e ei n t .c o n f . vision systems , pp . 21 –24 , 2006 . [ 15 ] a. howard , “ real-time stereo visual odometry for autonomous ground vehicle , ” in proc . ieee/rsj int . conf . intelligent robots and sys- tems , 2008 , pp . 3946 –3952 . [ 16 ] y. cheng , m. w. maimone , and l. matthies', 'stereo visual odometry for autonomous ground vehicle , ” in proc . ieee/rsj int . conf . intelligent robots and sys- tems , 2008 , pp . 3946 –3952 . [ 16 ] y. cheng , m. w. maimone , and l. matthies , “ visual odometry on the mar exploration rover , ” ieee robot . automat . mag . , vol . 13 , no . 2 , pp . 54–62 , 2006 . [ 17 ] m. maimone , y. cheng , and l. matthies , “ two year of visual odom- etry on the mar exploration rover : field report , ” j . field robot . , vol . 24 , no . 3 , pp . 169 –186 , 2007 . [ 18 ] m. a. fischler and r. c. bolles , “ random sample consensus : a para- digm for model fit with application to image analysis and automate cartography , ” commun . acm , vol . 24 , no . 6 , pp . 381 –395 , 1981 . [ 19 ] c. tomasi and j. shi , “ good feature to track , ” in proc . computer vision and pattern recognition ( cvpr ’ 94 ) , 1994 , pp . 593 –600 . [ 20 ] p. besl and n. mckay , “ a method for registration of 3-d shape , ” ieee trans . pattern anal . machine intell . , vol . 14 , no . 2 , pp . 239 –256 , 1992 . [ 21 ] a. comport , e. malis , and p. rives , “ accurate quadrifocal trackingfor robust 3d visual odometry , ” in proc . ieee int . conf . robotics and automation , 2007 , pp . 40 –45 . [ 22 ] r. hartley and a. zisserman , multiple view geometry in computer vision , 2nd ed . cambridge u.k. : cambridge univ . press , 2004 . [ 23 ] d. nister , o. naroditsky , and j. bergen , “ visual odometry for ground vehicle application , ” j . field robot . , vol . 23 , no . 1 , pp . 3 –20 , 2006 . [ 24 ] p. i. corke , d. strelow , and s. singh , “ omnidirectional visual odom- etry for a planetary rover , ” in proc . ieee/rsj int . conf . intelligent robots and systems , 2005 , pp . 4007 –4012 . [ 25 ] m. lhuillier , “ automatic structure and motion use a catadioptric camera , ” in proc . ieee workshop omnidirectional vision , 2005 , pp . 1 –8 . [ 26 ] r. goecke , a. asthana , n. pettersson , and l. petersson , “ visual vehi-cle egomotion estimation use the fourier-mellin transform , ” in proc . ieee', 'vision , 2005 , pp . 1 –8 . [ 26 ] r. goecke , a. asthana , n. pettersson , and l. petersson , “ visual vehi-cle egomotion estimation use the fourier-mellin transform , ” in proc . ieee intelligent vehicles symp . , 2007 , pp . 450 –455 . [ 27 ] j. tardif , y. pavlidis , and k. daniilidis , “ monocular visual odometry in urban environment use an omnidirectional camera , ” in proc . ieee/ rsj int . conf . intelligent robots and systems , 2008 , pp . 2531 –2538 . [ 28 ] m. j. milford and g. wyeth , “ single camera vision-only slam on a suburban road network , ” in proc . ieee int . conf . robotics and automa- tion ( icra ’ 08 ) , 2008 , pp . 3684 –3689 . [ 29 ] d. scaramuzza and r. siegwart , “ appearance-guided monocular omnidirectional visual odometry for outdoor ground vehicle , ” ieee trans . robot . ( special issue on visual slam ) , vol . 24 , no . 5 , pp . 1015 – 1026 , oct. 2008 . [ 30 ] e. mouragnon , m. lhuillier , m. dhome , f. dekeyser , and p. sayd , “ real time localization and 3d reconstruction , ” in proc . int . conf . computer vision and pattern recognition , 2006 , pp . 363 –370 . [ 31 ] d. scaramuzza , f. fraundorfer , and r. siegwart , “ real-time monoc- ular visual odometry for on-road vehicle with 1-point ransac , ” in proc . ieee int . conf . robotics and automation ( icra ’ 09 ) , 2009 , pp . 4293 –4299 . [ 32 ] a. pretto , e. menegatti , and e. pagello , “ omnidirectional dense large-scale mapping and navigation base on meaningful triangulation , ” inproc . ieee int . conf . robotics and automation , 2011 , pp . 3289 –3296 . [ 33 ] d. nister , “ an efficient solution to the five-point relative pose prob- lem , ” in proc . int . conf . computer vision and pattern recognition , 2003 , pp . 195 –202 . [ 34 ] m. milford , g. wyeth , and d. prasser , “ ratslam : a hippocampal model for simultaneous localization and mapping , ” in proc . ieee int . conf . robotics and automation ( icra ’ 04 ) , 2004 , pp . 403 –408 . [ 35 ] b. liang and n. pears , “ visual navigation use planar homo- graphies ,', 'localization and mapping , ” in proc . ieee int . conf . robotics and automation ( icra ’ 04 ) , 2004 , pp . 403 –408 . [ 35 ] b. liang and n. pears , “ visual navigation use planar homo- graphies , ” in proc . ieee int . conf . robotics and automation ( icra ’ 02 ) , 2002 , pp . 205 –210 . [ 36 ] q. ke and t. kanade , “ transforming camera geometry to a virtual downward-looking camera : robust ego-motion estimation and ground-layer detection , ” in proc . computer vision and pattern recognition ( cvpr ) , june 2003 , pp . 390 –397 . [ 37 ] h. wang , k. yuan , w. zou , and q. zhou , “ visual odometry basedon locally planar ground assumption , ” in proc . ieee int . conf . informa- tion acquisition , 2005 , pp . 59 –64 . [ 38 ] j. guerrero , r. martinez-cantin , and c. sagues , “ visual map-less navigation base on homographies , ” j . robot . syst . , vol . 22 , no . 10 , pp . 569 –581 , 2005 . [ 39 ] d. scaramuzza , “ 1-point-ransac structure from motion for vehi- cle-mounted camera by exploit non-holonomic constraint , ” int . j. comput . vis . , vol . 95 , no . 1 , pp . 74 –85 , 2011 . [ 40 ] d. scaramuzza , f. fraundorfer , m. pollefeys , and r. siegwart , “ absolute scale in structure from motion from a single vehicle mount camera by exploit nonholonomic constraint , ” in proc . ieee int . conf . computer vision ( iccv ) , kyoto , oct. 2009 , pp . 1413 –1419 . [ 41 ] f. fraundorfer , d. scaramuzza , and m. pollefeys , “ a constricted bundle adjustment parameterization for relative scale estimation in visualodometry , ” in proc . ieee int . conf . robotics and automation , 2010 , pp . 1899 –1904 . [ 42 ] n. sunderhauf , k. konolige , s. lacroix , and p. protzel , “ visualodometry use sparse bundle adjustment on an autonomous outdoor vehicle , ” in tagungsband autonome mobile systeme , reihe informatik aktuell . levi , schanz , lafrenz , and avrutin , eds . berlin , springer-ver- lag , 2005 , pp . 157 –163 . [ 43 ] k. konolige , m. agrawal , and j. sol , “ large scale visual odometry for rough terrain , ” in', 'levi , schanz , lafrenz , and avrutin , eds . berlin , springer-ver- lag , 2005 , pp . 157 –163 . [ 43 ] k. konolige , m. agrawal , and j. sol , “ large scale visual odometry for rough terrain , ” in proc . int . symp . robotics research , 2007 . [ 44 ] j. tardif , m. g. m. laverne , a. kelly , and m. laverne , “ a new approach to vision-aided inertial navigation , ” in proc . ieee/rsj int . conf . intelligent robots and systems , 2010 , pp . 4161 –4168 . [ 45 ] a. i. mourikis and s. roumeliotis , “ a multi-state constraint kalman filter for vision-aided inertial navigation , ” in proc . ieee int . conf . robotics and automation ,2 0 0 7 , p p .3 5 6 5 –3572 . [ 46 ] e. jones and s. soatto , “ visual-inertial navigation , mapping and localization : a scalable real-time causal approach , ” int . j . robot . res . , vol . 30 , no . 4 , pp . 407 –430 , 2010 . [ 47 ] h. durrant-whyte and t. bailey , “ simultaneous localization and mapping ( slam ) : part i . the essential algorithm , ” robot . automat . mag . , vol . 13 , no . 2 , pp . 99 –110 , 2006 . december 2011 \\x81ieee robotics & automation magazine \\x8191• [ 48 ] t. bailey and h. durrant-whyte , “ simultaneous localisation and mapping ( slam ) : part ii . state of the art , ” robot . automat . mag . , vol . 13 , no . 3 , pp . 108 –117 , 2006 . [ 49 ] a. davison , “ real-time simultaneous localisation and mapping with a single camera , ” in proc . int . conf . computer vision , 2003 , pp . 1403 – 1410 . [ 50 ] g. klein and d. murray , “ parallel tracking and mapping for small ar workspace , ” in proc . int . symp . mixed and augmented reality , 2007 , pp . 225 –234 . [ 51 ] h. strasdat , j. montiel , and a. davison , “ real time monocular slam : why filter ? ” in proc . ieee int . conf . robotics and automation , 2010 , pp . 2657 –2664 . [ 52 ] a. chiuso , p. favaro , h. jin , and s. soatto , “ 3-d motion and struc- ture from 2-d motion causally integrate over time : implementation , ” in proc . european conf . computer vision , 2000 , pp . 734 –750 . [ 53 ] m. c.', 'h. jin , and s. soatto , “ 3-d motion and struc- ture from 2-d motion causally integrate over time : implementation , ” in proc . european conf . computer vision , 2000 , pp . 734 –750 . [ 53 ] m. c. deans , “ bearing-only localization and mapping , ” ph.d. disser- tation , carnegie mellon univ. , pittsburgh , 2002 . [ 54 ] l. a. clemente , a. j. davison , i. reid , j. neira , and j. d. tardos , “ mapping large loop with a single hand-held camera , ” in proc . robotics science and systems , 2007 . [ 55 ] t. lemaire and s. lacroix , “ vision-based slam : stereo and monocu-lar approach , ” int . j . computer vision , vol . 74 , no . 3 , pp . 343 –364 , 2006 . [ 56 ] e. eade and t. drummond , “ monocular slam a a graph of coa- lesced observation , ” in proc . ieee int . conf . computer vision , 2007 , pp . 1–8 . [ 57 ] g. klein and d. murray , “ improving the agility of keyframe-basedslam , ” in proc . european conf . computer vision , 2008 , pp . 802 –815 . [ 58 ] k. konolige and m. agrawal , “ frameslam : from bundle adjust- ment to real-time visual mappping , ” ieee trans . robot . , vol . 24 , no . 5 , pp . 1066 –1077 , 2008 . [ 59 ] a. handa , m. chli , h. strasdat , and a. j. davison , “ scalable active matching , ” in proc . int . conf . computer vision and pattern recognition , 2010 , pp . 1546 –1553 . [ 60 ] j. civera , o. grasa , a. davison , and j. montiel , “ 1-point ransac for ekf filtering : application to real-time structure from motion andvisual odometry , ” j . field robot . , vol . 27 , no . 5 , pp . 609 –631 , 2010 . [ 61 ] h. strasdat , j. montiel , and a. j. davison , “ scale drift-aware large scale monocular slam , ” in proc . robotics science and systems , 2010 . [ 62 ] c. mei , g. sibley , m. cummins , p. newman , and i. reid , “ rslam : a system for large-scale mapping in constant-time use stereo , ” int . j . computer vision , vol . 94 , no . 2 , pp . 198 –214 , 2010 . [ 63 ] y. ma , s. soatto , j. kosecka , and s. sastry , an invitation to 3d vision , from images to models . berlin :', 'stereo , ” int . j . computer vision , vol . 94 , no . 2 , pp . 198 –214 , 2010 . [ 63 ] y. ma , s. soatto , j. kosecka , and s. sastry , an invitation to 3d vision , from images to models . berlin : springer-verlag , 2003 . [ 64 ] c. geyer and k. daniilidis , “ a unifying theory for central panoramic system and practical application , ” in proc . european conf . computer vision , 2000 , pp . 445 –461 . [ 65 ] d. scaramuzza , a. martinelli , and r. siegwart , “ a flexible technique for accurate omnidirectional camera calibration and structure from motion , ” in proc . ieee int . conf . computer vision systems ( icvs ) 2006 , jan. 2006 , pp . 45 –53 . [ 66 ] d. scaramuzza , “ omnidirectional vision : from calibration to robot motion estimation ” ph.d. dissertation , eth zurich , 2008 . [ 67 ] d. scaramuzza , “ omnidirectional camera , ” in encyclopedia of computer vision , k. ikeuchi , ed . berlin : springer-verlag , 2012 . [ 68 ] j. bouguet ( 2 011 ) . list of camera calibration toolbox . [ online ] . available : http : //www.visi on.caltech.edu/bo uguetj/calib . doc/htmls/links.html [ 69 ] j. bouguet . camera calibration toolbox for matlab . [ online ] . available : http : //www.vision.caltech.edu/bouguetj/calib.doc/index.html [ 70 ] d. scaramuzza . ( 2006 ) . ocamcalib toolbox : omnidirectional cameracalibration toolbox for matlab ( use planar grid ) google for “ ocamcalib . ” [ online ] . available : http : //sites.google.com/site/scarabotix/ocamcalib- toolbox [ 71 ] c. mei . ( 2 006 ) . omnidirectional camera calibration toolbox for matlab ( use planar grid ) . [ online ] . available : http : //homepage- s.laas.fr/~cmei/index.php/toolbox [ 72 ] j. barreto . omnidirectional camera calibration toolbox for matlab ( use line ) . [ online ] . available : http : //www.isr.uc.pt/~jpbar/catpack/ pag1.htm [ 73 ] opencv : open-source computer vision library . [ online ] . available : http : //opencv.willowgarage.com/wiki/ [ 74 ] t. huang and a. netravalli , “ motion and structure from featurecorrespondences : a review', 'open-source computer vision library . [ online ] . available : http : //opencv.willowgarage.com/wiki/ [ 74 ] t. huang and a. netravalli , “ motion and structure from featurecorrespondences : a review , ” proc . ieee , v o l .8 2 , n o .2 , p p .2 5 2 –268 , 1994 . [ 75 ] e. kruppa , “ zur ermittlung eines objektes aus zwei perspektiven mitinnerer orientierung , ” sitzungsberichte der akademie der wissenschaften , wien , mathematisch-naturwissenschaftlichen klasse , abteilung iia , vol . 122 , pp . 1939 –1948 , 1913 . [ 76 ] d. nister , “ an efficient solution to the five-point relative pose prob- lem , ” in proc . computer vision and pattern recognition ( cvpr ’ 03 ) , 2003 , pp . ii : 195 –202 . [ 77 ] h. goldstein , classical mechanics .r e a d i n g , m a : a d d i s o n - w e s l e y ,1 9 8 1 . [ 78 ] k. s. arun , t. s. huang , and s. d. blostein , “ least-squares fit oftwo 3-d point set , ” ieee trans . pattern anal . machine intell . , v o l .9 , n o .5 , pp . 698 –700 , 1987 . [ 79 ] f. moreno-noguer , v. lepetit , and p. fua , “ accurate non-iterativeo ( n ) solution to the pnp problem , ” in proc . ieee int . conf . computer vision , 2007 , pp . 1 –8 . [ 80 ] l. kneip , d. scaramuzza , and r. siegwart , “ a novel parameterizationof the perspective-three-point problem for a direct computation of abso- lute camera position and orientation , ” in proc . ieee conf . computer vision and pattern recognition , 2011 , pp . 2969 –2976 . [ 81 ] d. nister , “ a minimal solution to the generalise 3-point pose prob- lem , ” in proc . ieee conf . computer vision and pattern recognition , 2004 , pp . 560 –567 . [ 82 ] c. harris and m. stephens , “ a combined corner and edge detector , ” inproc . alvey vision conf . , 1988 , pp . 147 –151 . davide scaramuzza , grasp lab , department of com- puter and information science , school of engineering andapplied science , university of pennsylvania , philadelphia . e-mail : davide.scaramuzza @ ieee.org . friedrich fraundorfer , computer vision and geometry lab , institute of', 'school of engineering andapplied science , university of pennsylvania , philadelphia . e-mail : davide.scaramuzza @ ieee.org . friedrich fraundorfer , computer vision and geometry lab , institute of visual computing , department of computer science , eth zurich , switzerland . e-mail : fraundorfer @ inf.ethz.ch . 92\\x81ieee robotics & automation magazine \\x81december 2011•']",http://dx.doi.org/10.1007/978-3-030-60337-3_22
11.pdf,"see discussion , st at , and author pr ofiles f or this public ation at : http : //www .researchgate.ne t/public ation/342623264 lio-sam : t ightly-cou plead lidar inertial odometry via smoothing and mapping preprint · july 2020 doi : 10.48550/ arxiv .2007.00258 citation 1reads 5,526 6 author s , include : tixiao shan 34 publica tions 4,228 citations see profile brendan englot stevens instit ute of t echnolog y 115 publica tions 6,038 citations see profile drew me yers massachuse tt instit ute of t echnolog y 7 publica tions 1,552 citations see profile wei w ang univ ersity of wisc onsin–madison 79 publica tions 2,762 citations see profile all c ontent f ollo wing this p age be uplo aded b y tixiao shan on 12 july 2020 . the user have r equest ed enhanc ement of the do wnlo aded file.lio-sam : tightly-coupled lidar inertial odometry via smoothing and mapping tixiao shan , brendan englot , drew meyers , wei wang , carlo ratti , and daniela rus abstract — we propose a framework for tightly-coupled lidar inertial odometry via smoothing and mapping , lio-sam , that achieve highly accurate , real-time mobile robot trajectory es- timation and map-building . lio-sam formulates lidar-inertial odometry atop a factor graph , allow a multitude of relative and absolute measurement , include loop closure , to be incorporate from different source a factor into the system . the estimated motion from inertial measurement unit ( imu ) pre-integration de-skews point cloud and produce an initial guess for lidar odometry optimization . the obtained lidar odometry solution be use to estimate the bias of the imu . to ensure high performance in real-time , we marginalize old lidar scan for pose optimization , rather than match lidar scan to a global map . scan-matching at a local scale instead of a global scale signiﬁcantly improve the real-time performance of the system , a do the selective introduction of keyframes , and an efﬁcient slide window approach that register a new keyframe to a ﬁxed-size set of prior “ sub-keyframes. ” the propose method be extensively evaluate on datasets gather from three platform over various scale and environment . i. i ntroduction state estimation , localization and mapping be fundamen- tal prerequisite for a successful intelligent mobile robot , require for feedback control , obstacle avoidance , and plan- ning , among many other capability . using vision-based and lidar-based sensing , great effort have be devote to achieve high-performance real-time simultaneous lo- calization and mapping ( slam ) that can support a mobile robot ’ s six degree-of-freedom state estimation . vision-based method typically use a monocular or stereo camera and triangulate feature across successive image to determine the camera motion . although vision-based method be es- pecially suitable for place recognition , their sensitivity to initialization , illumination , and range make them unreliable when they alone be use to support an autonomous navi- gation system . on the other hand , lidar-based method be largely invariant to illumination change . especially with the recent availability of long-range , high-resolution 3d lidar , such a the velodyne vls-128 and ouster os1-128 , lidar become more suitable to directly capture the ﬁne detail of an environment in 3d space . therefore , this paper focus on lidar-based state estimation and mapping method . many lidar-based state estimation and mapping method have be propose in the last two decade . among them , the t. shan , d. meyers , w. wang , and c. ratti be with the department of urban studies and planning , massachusetts institute of technology , usa , fshant , drewm , wweiwang , ratti g @ mit.edu . b. englot be with the department of mechanical engineering , stevens institute of technology , usa , benglot @ stevens.edu . t. shan , w. wang , and d. rus be with the computer science & artiﬁcial intelligence laboratory , massachusetts institute of technology , usa , fshant , wweiwang , rusg @ mit.edu .lidar odometry and mapping ( loam ) method propose in [ 1 ] for low-drift and real-time state estimation and mapping be among the most widely use . loam , which use a lidar and an inertial measurement unit ( imu ) , achieve state-of- the-art performance and have be rank a the top lidar- base method since it release on the kitti odometry benchmark site [ 2 ] . despite it success , loam present some limitation - by save it data in a global voxel map , it be often difﬁcult to perform loop closure detection and incorporate other absolute measurement , e.g. , gps , for pose correction . its online optimization process become less efﬁcient when this voxel map become dense in a feature-rich environment . loam also suffer from drift in large-scale test , a it be a scan-matching base method at it core . in this paper , we propose a framework for tightly-coupled lidar inertial odometry via smoothing and mapping , lio- sam , to address the aforementioned problem . we assume a nonlinear motion model for point cloud de-skew , estimate the sensor motion during a lidar scan use raw imu measurement . in addition to de-skewing point cloud , the estimate motion serve a an initial guess for lidar odometry optimization . the obtained lidar odometry solution be then use to estimate the bias of the imu in the factor graph . by introduce a global factor graph for robot trajectory estimation , we can efﬁciently perform sensor fusion use lidar and imu measurement , incorporate place recognition among robot pose , and introduce absolute measurement , such a gps positioning and compass heading , when they be available . this collection of factor from various source be use for joint optimization of the graph . additionally , we marginalize old lidar scan for pose optimization , rather than match scan to a global map like loam . scan-matching at a local scale instead of a global scale signiﬁcantly im- prove the real-time performance of the system , a do the selective introduction of keyframes , and an efﬁcient slide window approach that register a new keyframe to a ﬁxed- size set of prior “ sub-keyframes. ” the main contribution of our work can be summarize a follow : a tightly-coupled lidar inertial odometry framework build atop a factor graph , that be suitable for multi-sensor fusion and global optimization . an efﬁcient , local slide window-based scan-matching approach that enable real-time performance by regis- tering selectively choose new keyframes to a ﬁxed-size set of prior sub-keyframes . the propose framework be extensively validate with test across various scale , vehicle , and environments.arxiv:2007.00258v2 [ cs.ro ] 7 jul 2020xxxxx xx imu measurementsimu preintegrationfactorlidar odometryfactorloop closurefactorscan match lidar sub-keyframeslidar frameslidar keyframexgps factorgps measurementrobot state nodefig . 1 : the system structure of lio-sam . the system receive input from a 3d lidar , an imu and optionally a gps . four type of factor be introduce to construct the factor graph . : ( a ) imu preintegration factor , ( b ) lidar odometry factor , ( c ) gps factor , and ( d ) loop closure factor . the generation of these factor be discuss in section iii . ii . r elated work lidar odometry be typically perform by ﬁnding the relative transformation between two consecutive frame us- ing scan-matching method such a icp [ 3 ] and gicp [ 4 ] . instead of match a full point cloud , feature-based match method have become a popular alternative due to their computational efﬁciency . for example , in [ 5 ] , a plane- base registration approach be propose for real-time lidar odometry . assuming operation in a structured environment , it extract plane from the point cloud and match them by solve a least-squares problem . a collar line-based method be propose in [ 6 ] for odometry estimation . in this method , line segment be randomly generate from the original point cloud and use later for registration . however , a scan ’ s point cloud be often skewed because of the rotation mechanism of modern 3d lidar , and sensor motion . solely use lidar for pose estimation be not ideal since registration use skew point cloud or feature will eventually cause large drift . therefore , lidar be typically use in conjunction with other sensor , such a imu and gps , for state estimation and mapping . such a design scheme , utilize sensor fusion , can typically be group into two category : loosely-coupled fusion and tightly-coupled fusion . in loam [ 1 ] , imu be introduce to de-skew the lidar scan and give a motion prior for scan-matching . however , the imu be not involve in the optimization process of the algorithm . thus loam can be classiﬁed a a loosely-coupled method . a lightweight and ground-optimized lidar odometry and mapping ( lego- loam ) method be propose in [ 7 ] for ground vehicle map- ping task [ 8 ] . its fusion of imu measurement be the same a loam . a more popular approach for loosely-coupled fusion be use extend kalman ﬁlters ( ekf ) . for example , [ 9 ] - [ 13 ] integrate measurement from lidar , imu , and optionally gps use an ekf in the optimization stage for robot state estimation . tightly-coupled system usually offer improved accuracy and be presently a major focus of ongoing research [ 14 ] . in [ 15 ] , preintegrated imu measurement be exploit for de-skewing point cloud . a robocentric lidar-inertial state estimator , r-lins , be present in [ 16 ] . r-lins use an error-state kalman ﬁlter to correct a robot ’ s state estimaterecursively in a tightly-coupled manner . due to the lack of other available sensor for state estimation , it suffer from drift during long-during navigation . a tightly-coupled lidar inertial odometry and mapping framework , liom , be introduce in [ 17 ] . liom , which be the abbreviation for lio-mapping , jointly optimize measurement from lidar and imu and achieve similar or good accuracy when compare with loam . since liom be design to process all the sensor measurement , real-time performance be not achieve - it run at about 0:6real-time in our test . iii . l idar inertial odometry via smoothing and mapping a . system overview we ﬁrst deﬁne frame and notation that we use throughout the paper . we denote the world frame a wand the robot body frame a b . we also assume the imu frame coincides with the robot body frame for convenience . the robot state xcan be write a : x= [ rt ; pt ; vt ; bt ] t ; ( 1 ) where r2so ( 3 ) be the rotation matrix , p2r3is the position vector , vi the speed , and bis the imu bias . the transformation t2se ( 3 ) frombtowis represent a t= [ rjp ] . an overview of the proposed system be show in figure 1 . the system receive sensor data from a 3d lidar , an imu and optionally a gps . we seek to estimate the state of the robot and it trajectory use the observation of these sensor . this state estimation problem can be formulate a a maximum a posteriori ( map ) problem . we use a factor graph to model this problem , a it be well suit to perform inference when compare with bayes net . with the assumption of a gaussian noise model , the map inference for our problem be equivalent to solve a nonlinear least-squares problem [ 18 ] . note that without loss of generality , the propose system can also incorporate measurement from other sensor , such a elevation from an altimeter or head from a compass . we introduce four type of factor along with one variable type for factor graph construction . this variable , represent the robot ’ s state at a speciﬁc time , be attributedto the node of the graph . the four type of factor be : ( a ) imu preintegration factor , ( b ) lidar odometry factor , ( c ) gps factor , and ( d ) loop closure factor . a new robot state node xi add to the graph when the change in robot pose exceed a user-deﬁned threshold . the factor graph be optimize upon the insertion of a new node use incremental smoothing and mapping with the bayes tree ( isam2 ) [ 19 ] . the process for generate these factor be describe in the following section . b. imu preintegration factor the measurement of angular velocity and acceleration from an imu be deﬁned use eqs . 2 and 3 : ^ ! t= ! t+b ! t+n ! t ( 2 ) ^at=rbw t ( at","['see discussion , st at , and author pr ofiles f or this public ation at : http : //www .researchgate.ne t/public ation/342623264 lio-sam : t ightly-cou plead lidar inertial odometry via smoothing and mapping preprint · july 2020 doi : 10.48550/ arxiv .2007.00258 citation 1reads 5,526 6 author s , include : tixiao shan 34 publica tions 4,228 citations see profile brendan englot stevens instit ute of t echnolog y 115 publica tions 6,038 citations see profile drew me yers massachuse tt instit ute of t echnolog y 7 publica tions 1,552 citations see profile wei w ang univ ersity of wisc onsin–madison 79 publica tions 2,762 citations see profile all c ontent f ollo wing this p age be uplo aded b y tixiao shan on 12 july 2020 . the user have r equest ed enhanc ement of the do wnlo aded file.lio-sam : tightly-coupled lidar inertial odometry via smoothing and mapping tixiao shan , brendan englot , drew meyers , wei wang , carlo ratti , and daniela rus abstract — we propose a framework for tightly-coupled lidar inertial odometry via smoothing and mapping , lio-sam , that achieve highly accurate , real-time mobile robot trajectory es- timation and map-building . lio-sam formulates lidar-inertial odometry atop a factor graph , allow a multitude of relative and absolute measurement , include loop closure , to be incorporate from different source a factor into the system . the estimated motion from inertial measurement unit ( imu ) pre-integration de-skews point cloud and produce an initial guess for lidar odometry optimization . the obtained lidar odometry solution be use to estimate the bias of the imu . to ensure high performance in real-time , we marginalize old lidar scan for pose optimization , rather than match lidar scan to a global map . scan-matching at a local scale instead of a global scale signiﬁcantly improve the real-time performance of the system , a do the selective introduction of keyframes , and an efﬁcient slide window approach that register a new keyframe to a ﬁxed-size set of prior “ sub-keyframes. ” the', 'performance of the system , a do the selective introduction of keyframes , and an efﬁcient slide window approach that register a new keyframe to a ﬁxed-size set of prior “ sub-keyframes. ” the propose method be extensively evaluate on datasets gather from three platform over various scale and environment . i. i ntroduction state estimation , localization and mapping be fundamen- tal prerequisite for a successful intelligent mobile robot , require for feedback control , obstacle avoidance , and plan- ning , among many other capability . using vision-based and lidar-based sensing , great effort have be devote to achieve high-performance real-time simultaneous lo- calization and mapping ( slam ) that can support a mobile robot ’ s six degree-of-freedom state estimation . vision-based method typically use a monocular or stereo camera and triangulate feature across successive image to determine the camera motion . although vision-based method be es- pecially suitable for place recognition , their sensitivity to initialization , illumination , and range make them unreliable when they alone be use to support an autonomous navi- gation system . on the other hand , lidar-based method be largely invariant to illumination change . especially with the recent availability of long-range , high-resolution 3d lidar , such a the velodyne vls-128 and ouster os1-128 , lidar become more suitable to directly capture the ﬁne detail of an environment in 3d space . therefore , this paper focus on lidar-based state estimation and mapping method . many lidar-based state estimation and mapping method have be propose in the last two decade . among them , the t. shan , d. meyers , w. wang , and c. ratti be with the department of urban studies and planning , massachusetts institute of technology , usa , fshant , drewm , wweiwang , ratti g @ mit.edu . b. englot be with the department of mechanical engineering , stevens institute of technology , usa , benglot @ stevens.edu . t. shan , w. wang , and d. rus be with the computer science &', '@ mit.edu . b. englot be with the department of mechanical engineering , stevens institute of technology , usa , benglot @ stevens.edu . t. shan , w. wang , and d. rus be with the computer science & artiﬁcial intelligence laboratory , massachusetts institute of technology , usa , fshant , wweiwang , rusg @ mit.edu .lidar odometry and mapping ( loam ) method propose in [ 1 ] for low-drift and real-time state estimation and mapping be among the most widely use . loam , which use a lidar and an inertial measurement unit ( imu ) , achieve state-of- the-art performance and have be rank a the top lidar- base method since it release on the kitti odometry benchmark site [ 2 ] . despite it success , loam present some limitation - by save it data in a global voxel map , it be often difﬁcult to perform loop closure detection and incorporate other absolute measurement , e.g. , gps , for pose correction . its online optimization process become less efﬁcient when this voxel map become dense in a feature-rich environment . loam also suffer from drift in large-scale test , a it be a scan-matching base method at it core . in this paper , we propose a framework for tightly-coupled lidar inertial odometry via smoothing and mapping , lio- sam , to address the aforementioned problem . we assume a nonlinear motion model for point cloud de-skew , estimate the sensor motion during a lidar scan use raw imu measurement . in addition to de-skewing point cloud , the estimate motion serve a an initial guess for lidar odometry optimization . the obtained lidar odometry solution be then use to estimate the bias of the imu in the factor graph . by introduce a global factor graph for robot trajectory estimation , we can efﬁciently perform sensor fusion use lidar and imu measurement , incorporate place recognition among robot pose , and introduce absolute measurement , such a gps positioning and compass heading , when they be available . this collection of factor from various source be use for joint optimization of the graph . additionally , we', 'measurement , such a gps positioning and compass heading , when they be available . this collection of factor from various source be use for joint optimization of the graph . additionally , we marginalize old lidar scan for pose optimization , rather than match scan to a global map like loam . scan-matching at a local scale instead of a global scale signiﬁcantly im- prove the real-time performance of the system , a do the selective introduction of keyframes , and an efﬁcient slide window approach that register a new keyframe to a ﬁxed- size set of prior “ sub-keyframes. ” the main contribution of our work can be summarize a follow : \\x0fa tightly-coupled lidar inertial odometry framework build atop a factor graph , that be suitable for multi-sensor fusion and global optimization . \\x0fan efﬁcient , local slide window-based scan-matching approach that enable real-time performance by regis- tering selectively choose new keyframes to a ﬁxed-size set of prior sub-keyframes . \\x0fthe propose framework be extensively validate with test across various scale , vehicle , and environments.arxiv:2007.00258v2 [ cs.ro ] 7 jul 2020xxxxx xx imu measurementsimu preintegrationfactorlidar odometryfactorloop closurefactorscan match lidar sub-keyframeslidar frameslidar keyframexgps factorgps measurementrobot state nodefig . 1 : the system structure of lio-sam . the system receive input from a 3d lidar , an imu and optionally a gps . four type of factor be introduce to construct the factor graph . : ( a ) imu preintegration factor , ( b ) lidar odometry factor , ( c ) gps factor , and ( d ) loop closure factor . the generation of these factor be discuss in section iii . ii . r elated work lidar odometry be typically perform by ﬁnding the relative transformation between two consecutive frame us- ing scan-matching method such a icp [ 3 ] and gicp [ 4 ] . instead of match a full point cloud , feature-based match method have become a popular alternative due to their computational efﬁciency . for example , in [ 5 ] , a plane- base registration', '[ 4 ] . instead of match a full point cloud , feature-based match method have become a popular alternative due to their computational efﬁciency . for example , in [ 5 ] , a plane- base registration approach be propose for real-time lidar odometry . assuming operation in a structured environment , it extract plane from the point cloud and match them by solve a least-squares problem . a collar line-based method be propose in [ 6 ] for odometry estimation . in this method , line segment be randomly generate from the original point cloud and use later for registration . however , a scan ’ s point cloud be often skewed because of the rotation mechanism of modern 3d lidar , and sensor motion . solely use lidar for pose estimation be not ideal since registration use skew point cloud or feature will eventually cause large drift . therefore , lidar be typically use in conjunction with other sensor , such a imu and gps , for state estimation and mapping . such a design scheme , utilize sensor fusion , can typically be group into two category : loosely-coupled fusion and tightly-coupled fusion . in loam [ 1 ] , imu be introduce to de-skew the lidar scan and give a motion prior for scan-matching . however , the imu be not involve in the optimization process of the algorithm . thus loam can be classiﬁed a a loosely-coupled method . a lightweight and ground-optimized lidar odometry and mapping ( lego- loam ) method be propose in [ 7 ] for ground vehicle map- ping task [ 8 ] . its fusion of imu measurement be the same a loam . a more popular approach for loosely-coupled fusion be use extend kalman ﬁlters ( ekf ) . for example , [ 9 ] - [ 13 ] integrate measurement from lidar , imu , and optionally gps use an ekf in the optimization stage for robot state estimation . tightly-coupled system usually offer improved accuracy and be presently a major focus of ongoing research [ 14 ] . in [ 15 ] , preintegrated imu measurement be exploit for de-skewing point cloud . a robocentric lidar-inertial state estimator , r-lins , be present', 'a major focus of ongoing research [ 14 ] . in [ 15 ] , preintegrated imu measurement be exploit for de-skewing point cloud . a robocentric lidar-inertial state estimator , r-lins , be present in [ 16 ] . r-lins use an error-state kalman ﬁlter to correct a robot ’ s state estimaterecursively in a tightly-coupled manner . due to the lack of other available sensor for state estimation , it suffer from drift during long-during navigation . a tightly-coupled lidar inertial odometry and mapping framework , liom , be introduce in [ 17 ] . liom , which be the abbreviation for lio-mapping , jointly optimize measurement from lidar and imu and achieve similar or good accuracy when compare with loam . since liom be design to process all the sensor measurement , real-time performance be not achieve - it run at about 0:6\\x02real-time in our test . iii . l idar inertial odometry via smoothing and mapping a . system overview we ﬁrst deﬁne frame and notation that we use throughout the paper . we denote the world frame a wand the robot body frame a b . we also assume the imu frame coincides with the robot body frame for convenience . the robot state xcan be write a : x= [ rt ; pt ; vt ; bt ] t ; ( 1 ) where r2so ( 3 ) be the rotation matrix , p2r3is the position vector , vi the speed , and bis the imu bias . the transformation t2se ( 3 ) frombtowis represent a t= [ rjp ] . an overview of the proposed system be show in figure 1 . the system receive sensor data from a 3d lidar , an imu and optionally a gps . we seek to estimate the state of the robot and it trajectory use the observation of these sensor . this state estimation problem can be formulate a a maximum a posteriori ( map ) problem . we use a factor graph to model this problem , a it be well suit to perform inference when compare with bayes net . with the assumption of a gaussian noise model , the map inference for our problem be equivalent to solve a nonlinear least-squares problem [ 18 ] . note that without loss of generality , the propose system can also incorporate', 'noise model , the map inference for our problem be equivalent to solve a nonlinear least-squares problem [ 18 ] . note that without loss of generality , the propose system can also incorporate measurement from other sensor , such a elevation from an altimeter or head from a compass . we introduce four type of factor along with one variable type for factor graph construction . this variable , represent the robot ’ s state at a speciﬁc time , be attributedto the node of the graph . the four type of factor be : ( a ) imu preintegration factor , ( b ) lidar odometry factor , ( c ) gps factor , and ( d ) loop closure factor . a new robot state node xi add to the graph when the change in robot pose exceed a user-deﬁned threshold . the factor graph be optimize upon the insertion of a new node use incremental smoothing and mapping with the bayes tree ( isam2 ) [ 19 ] . the process for generate these factor be describe in the following section . b. imu preintegration factor the measurement of angular velocity and acceleration from an imu be deﬁned use eqs . 2 and 3 : ^ ! t= ! t+b ! t+n ! t ( 2 ) ^at=rbw t ( at\\x00g ) +ba t+na t ; ( 3 ) where ^ ! tand^atare the raw imu measurement in bat timet.^ ! tand^atare affect by a slowly varying bias bt and white noise nt.rbw t be the rotation matrix from wto b.gis the constant gravity vector in w. we can now use the measurement from the imu to infer the motion of the robot . the velocity , position and rotation of the robot at time t+ \\x01tcan be compute a follow : vt+\\x01t=vt+g\\x01t+rt ( ^at\\x00ba t\\x00na t ) \\x01t ( 4 ) pt+\\x01t=pt+vt\\x01t+1 2g\\x01t2 +1 2rt ( ^at\\x00ba t\\x00na t ) \\x01t2 ( 5 ) rt+\\x01t=rtexp ( ( ^ ! t\\x00b ! t\\x00n ! t ) \\x01t ) ; ( 6 ) where rt=rwb t =rbw tt . here we assume that the angular velocity and the acceleration of bremain constant during the above integration . we then apply the imu preintegration method propose in [ 20 ] to obtain the relative body motion between two timesteps . the preintegrated measurement \\x01vij , \\x01pij , and \\x01rijbetween time iandjcan be compute use : \\x01vij=rt i ( vj\\x00vi\\x00g\\x01tij ) ( 7 )', 'propose in [ 20 ] to obtain the relative body motion between two timesteps . the preintegrated measurement \\x01vij , \\x01pij , and \\x01rijbetween time iandjcan be compute use : \\x01vij=rt i ( vj\\x00vi\\x00g\\x01tij ) ( 7 ) \\x01pij=rt i ( pj\\x00pi\\x00vi\\x01tij\\x001 2g\\x01t2 ij ) ( 8 ) \\x01rij=rt irj : ( 9 ) due to space limitation , we refer the reader to the de- scription from [ 20 ] for the detailed derivation of eqs . 7 - 9 . besides it efﬁciency , apply imu preintegration also naturally give u one type of constraint for the factor graph - imu preintegration factor . the imu bias be jointly optimize alongside the lidar odometry factor in the graph . c. lidar odometry factor when a new lidar scan arrives , we ﬁrst perform fea- ture extraction . edge and planar feature be extract by evaluate the roughness of point over a local region . points with a large roughness value be classiﬁed a edge feature . similarly , a planar feature be categorize by a small roughness value . we denote the extracted edge and planarfeatures from a lidar scan at time ias fe iand fp irespectively . all the feature extract at time icompose a lidar framefi , where fi=ffe i ; fp ig . note that a lidar frame fis represent inb . a more detailed description of the feature extraction process can be find in [ 1 ] , or [ 7 ] if a range image be use . using every lidar frame for compute and add factor to the graph be computationally intractable , so we adopt the concept of keyframe selection , which be widely use in the visual slam ﬁeld . using a simple but effective heuristic approach , we select a lidar frame fi+1as a keyframe when the change in robot pose exceed a user-deﬁned threshold when compare with the previous state xi . the newly save keyframe , fi+1 , be associate with a new robot state node , xi+1 , in the factor graph . the lidar frame between two keyframes be discard . adding keyframes in this way not only achieve a balance between map density and memory consumption but also help maintain a relatively sparse factor graph , which be suitable for real-time nonlinear', 'adding keyframes in this way not only achieve a balance between map density and memory consumption but also help maintain a relatively sparse factor graph , which be suitable for real-time nonlinear optimization . in our work , the position and rotation change threshold for add a new keyframe be choose to be 1mand10\\x0e . let u assume we wish to add a new state node xi+1to the factor graph . the lidar keyframe that be associate with this state be fi+1 . the generation of a lidar odometry factor be describe in the following step : 1 ) sub-keyframes for voxel map : we implement a sliding window approach to create a point cloud map contain a ﬁxed number of recent lidar scan . instead of optimize the transformation between two consecutive lidar scan , we extract the nmost recent keyframes , which we call the sub-keyframes , for estimation . the set of sub-keyframes ffi\\x00n ; : : : ; figis then transform into frame wusing the transformationsfti\\x00n ; : : : ; tigassociated with them . the transformed sub-keyframes be merge together into a voxel mapmi . since we extract two type of feature in the previous feature extraction step , miis compose of two sub- voxel map that be denote me i , the edge feature voxel map , andmp i , the planar feature voxel map . the lidar frame and voxel map be relate to each other a follow : mi=fme i ; mp ig where : me i=0fe i [ 0fe i\\x001 [ : : : [ 0fe i\\x00n mp i=0fp i [ 0fp i\\x001 [ : : : [ 0fp i\\x00n : 0fe iand0fp iare the transform edge and planar feature inw.me iandmp iare then downsampled to eliminate the duplicated feature that fall in the same voxel cell . in this paper , nis choose to be 25 . the downsample resolution for me iandmp iare0:2mand0:4m , respectively . 2 ) scan-matching : we match a newly obtain lidar frame fi+1 , which be alsoffe i+1 ; fp i+1g , tomivia scan- matching . various scan-matching method , such a [ 3 ] and [ 4 ] , can be utilize for this purpose . here we opt to use the method propose in [ 1 ] due to it computational efﬁciency and robustness in various challenge environment .', 'method , such a [ 3 ] and [ 4 ] , can be utilize for this purpose . here we opt to use the method propose in [ 1 ] due to it computational efﬁciency and robustness in various challenge environment . we ﬁrst transform ffe i+1 ; fp i+1gfrombtowand obtain f0fe i+1 ; 0fp i+1g . this initial transformation be obtain by use the predicted robot motion , ~ti+1 , from the imu . foreach feature in0fe i+1or0fp i+1 , we then ﬁnd it edge or planar correspondence in me iormp i . for the sake of brevity , the detailed procedure for ﬁnding these correspondence be omit here , but be describe thoroughly in [ 1 ] . 3 ) relative transformation : the distance between a fea- ture and it edge or planar patch correspondence can be compute use the following equation : dek= ( pe i+1 ; k\\x00pe i ; u ) \\x02 ( pe i+1 ; k\\x00pe i ; v ) pe i ; u\\x00pe i ; v ( 10 ) dpk= ( pp i+1 ; k\\x00pp i ; u ) ( pp i ; u\\x00pp i ; v ) \\x02 ( pp i ; u\\x00pp i ; w ) ( pp i ; u\\x00pp i ; v ) \\x02 ( pp i ; u\\x00pp i ; w ) ; ( 11 ) where k , u , v , and ware the feature index in their corresponding set . for an edge feature pe i+1 ; kin0fe i+1 , pe i ; u andpe i ; vare the point that form the corresponding edge line in me i . for a planar feature pp i+1 ; kin0fp i+1 , pp i ; u , pp i ; v , andpp i ; wform the corresponding planar patch in mp i . the gaussnewton method be then use to solve for the optimal transformation by minimize : min ti+1\\x1ax pi+1 ; k20fe i+1dek+x pi+1 ; k20fp i+1dpk\\x1b : at last , we can obtain the relative transformation \\x01ti ; i+1 between xiandxi+1 , which be the lidar odometry factor link these two pose : \\x01ti ; i+1=tt iti+1 ( 12 ) we note that an alternative approach to obtain \\x01ti ; i+1 be to transform sub-keyframes into the frame of xi . in other word , we match fi+1to the voxel map that be represent in the frame of xi . in this way , the real relative transformation \\x01ti ; i+1can be directly obtain . because the transformed features0fe iand0fp ican be reuse multiple time , we instead opt to use the approach describe in sec . iii-c.1 for it computational efﬁciency . d. gps', 'be directly obtain . because the transformed features0fe iand0fp ican be reuse multiple time , we instead opt to use the approach describe in sec . iii-c.1 for it computational efﬁciency . d. gps factor though we can obtain reliable state estimation and map- ping by utilize only imu preintegration and lidar odometry factor , the system still suffer from drift during long- duration navigation task . to solve this problem , we can introduce sensor that offer absolute measurement for elim- inating drift . such sensor include an altimeter , compass , and gps . for the purpose of illustration here , we discuss gps , a it be widely use in real-world navigation system . when we receive gps measurement , we ﬁrst transform them to the local cartesian coordinate frame use the method propose in [ 21 ] . upon the addition of a new node to the factor graph , we then associate a new gps factor with this node . if the gps signal be not hardware-synchronized with the lidar frame , we interpolate among gps measurement linearly base on the timestamp of the lidar frame . ( a ) handheld device ( b ) clearpath jackal ( c ) duffy 21 fig . 2 : datasets be collect on 3 platform : ( a ) a custom-built handheld device , ( b ) an unmanned ground vehicle - clearpath jackal , ( c ) an electric boat - duffy 21 . we note that add gps factor constantly when gps reception be available be not necessary because the drift of li- dar inertial odometry grow very slowly . in practice , we only add a gps factor when the estimate position covariance be large than the received gps position covariance . e. loop closure factor thanks to the utilization of a factor graph , loop closure can also be seamlessly incorporate into the propose sys- tem , a oppose to loam and liom . for the purpose of illustration , we describe and implement a naive but effective euclidean distance-based loop closure detection approach . we also note that our propose framework be compatible with other method for loop closure detection , for example , [ 22 ] and [ 23 ] , which', 'euclidean distance-based loop closure detection approach . we also note that our propose framework be compatible with other method for loop closure detection , for example , [ 22 ] and [ 23 ] , which generate a point cloud descriptor and use it for place recognition . when a new state xi+1is add to the factor graph , we ﬁrst search the graph and ﬁnd the prior state that be close to xi+1in euclidean space . as be show in fig . 1 , for example , x3is one of the returned candidate . we then try to match fi+1to the sub-keyframes ff3\\x00m ; : : : ; f3 ; : : : ; f3+mg use the scan-matching method discuss in section iii- c. note that fi+1and the past sub-keyframes be ﬁrst transform into wbefore scan-matching . we obtain the relative transformation \\x01t3 ; i+1and add it a a loop closure factor to the graph . throughout this paper , we choose the index mto be 12 , and the search distance for loop closure be set to be 15mfrom a new state xi+1 . in practice , we ﬁnd add loop closure factor be espe- cially useful for correct the drift in a robot ’ s altitude , when gps be the only absolute sensor available . this be because the elevation measurement from gps be very inaccurate - give rise to altitude error approach 100 min our test , in the absence of loop closure . iv . e xperiments we now describe a series of experiment to qualitatively and quantitatively analyze our propose framework . the sensor suite use in this paper include a velodyne vlp- 16 lidar , a microstrain 3dm-gx5-25 imu , and a reach m gps . for validation , we collect 5 different datasets across various scale , platform and environment . these datasets be refer to a rotation , walking , campus , park and amsterdam , respectively . the sensor mounting platform be show in fig . 2 . the ﬁrst three datasets be collect usinga custom-built handheld device on the mit campus . the park dataset be collect in a park cover by vegetation , use an unmanned ground vehicle ( ugv ) - the clearpath jackal . the last dataset , amsterdam , be collect by mount the sensor on a boat', '. the park dataset be collect in a park cover by vegetation , use an unmanned ground vehicle ( ugv ) - the clearpath jackal . the last dataset , amsterdam , be collect by mount the sensor on a boat and cruising in the canal of amsterdam . the detail of these datasets be show in table i . table i : dataset detail dataset scanselevation change ( m ) trajectory length ( m ) max rotation speed ( \\x0e/s ) rotation 582 0 0 213.9 walking 6502 0.3 801 133.7 campus 9865 1.0 1437 124.8 park 24691 19.0 2898 217.4 amsterdam 107656 0 19065 17.2 we compare the propose lio-sam framework with loam and liom . in all the experiment , loam and lio- sam be force to run in real-time . liom , on the other hand , be give inﬁnite time to process every sensor measurement . all the method be implement in c++ and execute on a laptop equip with an intel i7-10710u cpu use the robot operate system ( ros ) [ 24 ] in ubuntu linux . we note that only the cpu be use for computation , without parallel compute enable . our implementation of lio-sam be freely available on github1 . supplementary detail of the experiment perform , include complete visualization of all test , can be find at the link below2 . ( a ) test environment ( b ) loam ( c ) lio-sam fig . 3 : mapping result of loam and lio-sam in the rotation test . liom fail to produce meaningful result . a. rotation dataset in this test , we focus on evaluate the robustness of our framework when only imu preintegration and lidar odometry 1https : //github.com/tixiaoshan/lio-sam 2https : //youtu.be/a0h8coorzjufactors be add to the factor graph . the rotation dataset be collect by a user hold the sensor suite and perform a series of aggressive rotational maneuver while stand still . the maximum rotational speed encounter in this test be 133.7\\x0e=s . the test environment , which be populate with structure , be show in fig . 3 ( a ) . the map obtain from loam and lio-sam be show in figs . 3 ( b ) and ( c ) respec- tively . because liom use the same initialization pipeline from [ 25 ] , it inherit the', ', be show in fig . 3 ( a ) . the map obtain from loam and lio-sam be show in figs . 3 ( b ) and ( c ) respec- tively . because liom use the same initialization pipeline from [ 25 ] , it inherit the same initialization sensitivity of visual-inertial slam and be not able to initialize properly use this dataset . due to it failure to produce meaningful result , the map of liom be not show . as be show , the map of lio-sam preserve more ﬁne structural detail of the environment compare with the map of loam . this be because lio-sam be able to register each lidar frame precisely in so ( 3 ) , even when the robot undergoes rapid rotation . b . walking dataset this test be design to evaluate the performance of our method when the system undergoes aggressive translation and rotation in se ( 3 ) . the maximum translational and rotational speed encounter be this dataset be 1.8 m/s and 213.9\\x0e/s respectively . during the data gathering , the user hold the sensor suite show in fig . 2 ( a ) and walk quickly across the mit campus ( fig . 4 ( a ) ) . in this test , the map of loam , show in fig . 4 ( b ) , diverge at multiple location when aggressive rotation be encounter . liom outperform loam in this test . however , it map , show in fig . 4 ( c ) , still diverge slightly in various location and consists of numerous blurry structure . because liom be design to process all sensor measurement , it only run at 0:56\\x02real- time while other method be run in real-time . finally , lio-sam outperforms both method and produce a map that be consistent with the available google earth imagery . c. campus dataset table ii : end-to-end translation error ( meter ) dataset loam liom lio-odom lio-gps lio-sam campus 192.43 fail 9.44 6.87 0.12 park 121.74 34.60 36.36 2.93 0.04 amsterdam fail fail fail 1.21 0.17 this test be design to show the beneﬁts of introduce gps and loop closure factor . in order to do this , we purposely disable the insertion of gps and loop closure factor into the graph . when both gps and loop closure factor be disable ,', 'of introduce gps and loop closure factor . in order to do this , we purposely disable the insertion of gps and loop closure factor into the graph . when both gps and loop closure factor be disable , our method be refer to a lio-odom , which only utilize imu preintegration and lidar odometry factor . when gps factor be use , our method be refer to aslio-gps , which use imu preintegration , lidar odometry , and gps factor for graph construction . lio-sam us all factor when they be available . to gather this dataset , the user walk around the mit campus use the handheld device and return to the same position . because of the numerous building and tree in the ( a ) google earth ( b ) loam ( c ) liom ( d ) lio-sam fig . 4 : mapping result of loam , liom , and lio-sam use the walking dataset . the map of loam in ( b ) diverge multiple time when aggressive rotation be encounter . liom outperform loam . however , it map show numerous blurry structure due to inaccurate point cloud registration . lio-sam produce a map that be consistent with the google earth imagery , without use gps . map area , gps reception be rarely available and inaccu- rate most of the time . after ﬁltering out the inconsistent gps measurement , the region where gps be available be show in fig . 5 ( a ) a green segment . these region correspond to the few area that be not surround by building or tree . the estimated trajectory of loam , lio-odom , lio- gps , and lio-sam be show in fig . 5 ( a ) . the result of liom be not show due to it failure to initialize properly and produce meaningful result . as be show , the trajectory of loam drift signiﬁcantly when compare with all other method . without the correction of gps data , the trajectory of lio-odom begin to visibly drift at the low right corner of the map . with the help of gps data , lio-gps can correct the drift when it be available . however , gps data be not available in the late part of the dataset . as a result , lio- gps be unable to close the loop when the robot return to the start', 'correct the drift when it be available . however , gps data be not available in the late part of the dataset . as a result , lio- gps be unable to close the loop when the robot return to the start position due to drift . on the other hand , lio- sam can eliminate the drift by add loop closure factor to the graph . the map of lio-sam be well-aligned with google earth imagery and show in fig . 5 ( b ) . the relative translational error of all method when the robot return to the start be show in table ii . d. park dataset in this test , we mount the sensor on a ugv and drive the vehicle along a forested hiking trail . the robot return to it initial position after 40 minute of drive . the ugv be drive on three road surface : asphalt , ground cover by grass , and dirt-covered trail . due to it lack of suspension , the robot undergoes low amplitude but high frequency vibra- tions when drive on non-asphalt road . to mimic a challenging mapping scenario , we only use gps measurement when the robot be in widely open area , which be indicate by the green segment in fig . 6 ( a ) . such a mapping scenario be representative of a task in which a robot must map multiple gps-denied region and periodically return to region with gps availability to correct the drift . similar to the result in the previous test , loam , liom , and lio-odom suffer from signiﬁcant drift , since no absolute correction data be available . additionally , liom only run at0:67\\x02real-time , while the other method run in real-time . though the trajectory of lio-gps and lio-sam coincide in the horizontal plane , their relative translational error be different ( table ii ) . because no reliable absolute elevation measurement be available , lio-gps suffers from drift in altitude and be unable to close the loop when return to the robot ’ s initial position . lio-sam have no such problem , a it utilize loop closure factor to eliminate the drift . e. amsterdam dataset finally , we mount the sensor suite on a boat and cruise along the canal of amsterdam for 3', 'have no such problem , a it utilize loop closure factor to eliminate the drift . e. amsterdam dataset finally , we mount the sensor suite on a boat and cruise along the canal of amsterdam for 3 hour . although the movement of the sensor be relatively smooth in this test , map the canal be still challenge for several reason . many bridge over the canal pose degenerate scenario , a there be few useful feature when the boat be under them , similar to move through a long , featureless corridor . the number of planar feature be also signiﬁcantly less , a the ground be not present . we observe many false detection from the lidar when direct sunlight be in the sensor ﬁeld-of-view , which occur about 20 % of the time during data gather . we also only obtain intermittent gps reception due to the presence of bridge and city building overhead . due to these challenge , loam , liom , and lio-odom all fail to produce meaningful result in this test . similar to the problem encounter in the park dataset , lio-gps be unable to close the loop when return to the robot ’ s initial position because of the drift in altitude , which far motivate our usage of loop closure factor in lio-sam . f . benchmarking results table iii : rmse translation error w.r.t gps dataset loam liom lio-odom lio-gps lio-sam park 47.31 28.96 23.96 1.09 0.96 since full gps coverage be only available in the park dataset , we show the root mean square error ( rmse ) result w.r.t to the gps measurement history , which be treat as\\x00200\\x001000100200300400\\x00500\\x00400\\x00300\\x00200\\x001000 loamlio-odomlio-gpslio-samgps availability\\x0020\\x0015\\x0010\\x00505\\x0020\\x0015\\x0010\\x00505 ( a ) trajectory comparison ( b ) lio-sam map align with google earth fig . 5 : results of various method use the campus dataset that be gather on the mit campus . the red dot indicate the start and end location . the trajectory direction be clock-wise . liom be not show because it fail to produce meaningful result . ground truth . this rmse error do not take the error along thezaxis into account . as be show in table iii ,', 'direction be clock-wise . liom be not show because it fail to produce meaningful result . ground truth . this rmse error do not take the error along thezaxis into account . as be show in table iii , lio-gps and lio-sam achieve similar rmse error with respect to the gps ground truth . note that we could far reduce the error of these two method by at least an order of magni- tude by give them full access to all gps measurement . however , full gps access be not always available in many mapping setting . our intention be to design a robust system that can operate in a variety of challenge environment . the average runtime for the three compete method to register one lidar frame across all ﬁve datasets be show in table iv . throughout all test , loam and lio-sam be force to run in real-time . in other word , some lidar frame be drop if the runtime take more than 100 m when the lidar rotation rate be 10hz . liom be give inﬁnite time to process every lidar frame . as be show , lio-sam us signiﬁcantly less runtime than the other two method , which make it more suitable to be deploy on low-power −800−700−600−500−400−300−200−100 0 100−300−200−1000100200 loam liom lio-odom lio-gps lio-sam gps availability ( a ) trajectory comparison ( b ) lio-sam map align with google earth fig . 6 : results of various method use the park dataset that be gather in pleasant valley park , new jersey . the red dot indicate the start and end location . the trajectory direction be clock-wise . table iv : runtime of map for process one scan ( m ) dataset loam liom lio-sam stress test rotation 83.6 fail 41.9 13 \\x02 walking 253.6 339.8 58.4 13 \\x02 campus 244.9 fail 97.8 10 \\x02 park 266.4 245.2 100.5 9 \\x02 amsterdam fail fail 79.3 11 \\x02 embed system . we also perform stress test on lio-sam by feed it the data faster than real-time . the maximum data playback speed be record and show in the last column of table iv when lio-sam achieve similar performance without failure compare with the result when the data playback speed be 1\\x02real-time . as be show ,', 'speed be record and show in the last column of table iv when lio-sam achieve similar performance without failure compare with the result when the data playback speed be 1\\x02real-time . as be show , lio-sam be able to process data faster than real-time up to 13 \\x02 . we note that the runtime of lio-sam be more signif- icantly inﬂuenced by the density of the feature map , and less affect by the number of node and factor in the factor graph . for instance , the park dataset be collect in a feature-rich environment where the vegetation result in a large quantity of feature , whereas the amsterdam dataset yield a sparse feature map . while the factor graph of the park test consist of 4,573 node and 9,365 factor , the graph in the amsterdam test have 23,304 node and 49,617 factor . despite this , lio-sam us less time in the amsterdam testfig . 7 : map of lio-sam align with google earth . a oppose to the runtime in the park test . v. c onclusions and discussion we have propose lio-sam , a framework for tightly- couple lidar inertial odometry via smoothing and mapping , for perform real-time state estimation and mapping in complex environment . by formulate lidar-inertial odom- etry atop a factor graph , lio-sam be especially suitable for multi-sensor fusion . additional sensor measurement can easily be incorporate into the framework a new factor . sensors that provide absolute measurement , such a a gps , compass , or altimeter , can be use to eliminate the drift of lidar inertial odometry that accumulate over long duration , or in feature-poor environment . place recognition can also be easily incorporate into the system . to improve the real-time performance of the system , we propose a sliding window approach that marginalize old lidar frame for scan- matching . keyframes be selectively add to the factor graph , and new keyframes be register only to a ﬁxed- size set of sub-keyframes when both lidar odometry and loop closure factor be generate . this scan-matching at a local scale rather than a global scale facilitate', 'be register only to a ﬁxed- size set of sub-keyframes when both lidar odometry and loop closure factor be generate . this scan-matching at a local scale rather than a global scale facilitate the real- time performance of the lio-sam framework . the propose method be thoroughly evaluate on datasets gather on three platform across a variety of environment . the result show that lio-sam can achieve similar or good accuracy when compare with loam and liom . future work involve test the propose system on unmanned aerial vehicle . references [ 1 ] j. zhang and s. singh , “ low-drift and real-time lidar odometry and mapping , ” autonomous robots , vol . 41 ( 2 ) : 401-416 , 2017 . [ 2 ] a. geiger , p. lenz , and r. urtasun , “ are we ready for autonomous driving ? the kitti vision benchmark suite ” , ieee international conference on computer vision and pattern recognition , pp . 3354- 3361 , 2012 . [ 3 ] p.j . besl and n.d. mckay , “ a method for registration of 3d shapes , ” ieee transactions on pattern analysis and machine intelligence , vol . 14 ( 2 ) : 239-256 , 1992 . [ 4 ] a. segal , d. haehnel , and s. thrun , “ generalized-icp , ” proceedings of robotics : science and systems , 2009 . [ 5 ] w.s . grant , r.c . v oorhies , and l. itti , “ finding planes in lidar point clouds for real-time registration , ” ieee/rsj international conference on intelligent robots and systems , pp . 4347-4354 , 2013 . [ 6 ] m. velas , m. spanel , and a. herout , “ collar line segments for fast odometry estimation from velodyne point clouds , ” ieee interna- tional conference on robotics and automation , pp . 4486-4495 , 2016 . [ 7 ] t. shan and b. englot , “ lego-loam : lightweight and ground- optimize lidar odometry and mapping on variable terrain , ” ieee/rsj international conference on intelligent robots and systems , pp . 4758-4765 , 2018 . [ 8 ] t. shan , j. wang , k. doherty , and b. englot , “ bayesian generalized kernel inference for terrain traversability mapping , ” in conference on robot learning , pp . 829-838 , 2018 . [', '2018 . [ 8 ] t. shan , j. wang , k. doherty , and b. englot , “ bayesian generalized kernel inference for terrain traversability mapping , ” in conference on robot learning , pp . 829-838 , 2018 . [ 9 ] s. lynen , m.w . achtelik , s. weiss , m. chli , and r. siegwart , “ a robust and modular multi-sensor fusion approach applied to ma v navigation , ” ieee/rsj international conference on intelligent robots and systems , pp . 3923-3929 , 2013 . [ 10 ] s. yang , x. zhu , x. nian , l. feng , x. qu , and t. mal , “ a robust pose graph approach for city scale lidar mapping , ” ieee/rsj international conference on intelligent robots and systems , pp . 1175- 1182 , 2018 . [ 11 ] m. demir and k. fujimura , “ robust localization with low-mounted multiple lidars in urban environments , ” ieee intelligent trans- portation systems conference , pp . 3288-3293 , 2019 . [ 12 ] y . gao , s. liu , m. atia , and a. noureldin , “ ins/gps/lidar inte- grate navigation system for urban and indoor environments use hybrid scan matching algorithm , ” sensors , vol . 15 ( 9 ) : 23286-23302 , 2015 . [ 13 ] s. hening , c.a . ippolito , k.s . krishnakumar , v . stepanyan , and m. teodorescu , “ 3d lidar slam integration with gps/ins for ua vs in urban gps-degraded environment , ” aiaa infotech @ aerospace conference , pp . 448-457 , 2017 . [ 14 ] c. chen , h. zhu , m. li , and s. you , “ a review of visual-inertial simultaneous localization and mapping from filtering-based and optimization-based perspectives , ” robotics , vol . 7 ( 3 ) :45 , 2018 . [ 15 ] c. le gentil , , t. vidal-calleja , and s. huang , “ in2lama : inertial lidar localisation and mapping , ” ieee international conference on robotics and automation , pp . 6388-6394 , 2019 . [ 16 ] c. qin , h. ye , c.e . pranata , j. han , s. zhang , and ming liu , “ r- lins : a robocentric lidar-inertial state estimator for robust and efﬁcient navigation , ” arxiv:1907.02233 , 2019 . [ 17 ] h. ye , y . chen , and m. liu , “ tightly coupled 3d lidar inertial odometry and mapping , ” ieee', 'state estimator for robust and efﬁcient navigation , ” arxiv:1907.02233 , 2019 . [ 17 ] h. ye , y . chen , and m. liu , “ tightly coupled 3d lidar inertial odometry and mapping , ” ieee international conference on robotics and automation , pp . 3144-3150 , 2019 . [ 18 ] f. dellaert and m. kaess , “ factor graphs for robot perception , ” foundations and trends in robotics , vol . 6 ( 1-2 ) : 1-139 , 2017 . [ 19 ] m. kaess , h. johannsson , r. roberts , v . ila , j.j. leonard , and f. dellaert , “ isam2 : incremental smoothing and mapping using the bayes tree , ” the international journal of robotics research , vol . 31 ( 2 ) : 216-235 , 2012 . [ 20 ] c. forster , l. carlone , f. dellaert , and d. scaramuzza , “ on-manifold preintegration for real-time visual-inertial odometry , ” ieee trans- action on robotics , vol . 33 ( 1 ) : 1-21 , 2016 . [ 21 ] t. moore and d. stouch , “ a generalized extended kalman filter implementation for the robot operating system , ” intelligent au- tonomous systems , vol . 13 : 335-348 , 2016 . [ 22 ] g. kim and a. kim , “ scan context : egocentric spatial descriptor for place recognition within 3d point cloud map , ” ieee/rsj interna- tional conference on intelligent robots and systems , pp . 4802-4809 , 2018 . [ 23 ] j. guo , p. vk borges , c. park , and a. gawel , “ local descriptor for robust place recognition use lidar intensity , ” ieee robotics and automation letters , vol . 4 ( 2 ) : 1470-1477 , 2019 . [ 24 ] m. quigley , k. conley , b. gerkey , j. faust , t. foote , j. leibs , r. wheeler , and a.y . ng , “ ros : an open-source robot operating system , ” ieee icra workshop on open source software , 2009 . [ 25 ] t. qin , p. li , and s. shen , “ vins-mono : a robust and versatile monocular visual-inertial state estimator , ” ieee transactions on robotics , vol . 34 ( 4 ) : 1004-1020 , 2018 . view publication stats']",https://rpg.ifi.uzh.ch/docs/VO_Part_I_Scaramuzza.pdf
3.pdf,"citation : jia , g. ; li , x. ; zhang , d. ; xu , w. ; lv , h. ; shi , y. ; cai , m. visual-slam classical framework and key techniques : a review . sensors 2022 ,22 , 4582. http : // doi.org/10.3390/s22124582 academic editor : cosimo distante received : 8 may 2022 accepted : 7 june 2022 published : 17 june 2022 publisher ’ s note : mdpi stay neutral with regard to jurisdictional claim in publish map and institutional afﬁl- iations . copyright : © 2022 by the author . licensee mdpi , basel , switzerland . this article be an open access article distribute under the term and condition of the creative commons attribution ( cc by ) license ( http : // creativecommons.org/licenses/by/ 4.0/ ) . sensor review visual-slam classical framework and key techniques : a review guanwei jia1 , xiaoying li1 , dongming zhang1 , * , weiqing xu2,3 , * , haojie lv1 , yan shi2,3and maolin cai2,3 1school of physics and electronics , henan university , kaifeng 475004 , china ; jiaguanwei @ henu.edu.cn ( g.j . ) ; lixiaoying1 @ stu.scu.edu.cn ( x.l . ) ; haojielv @ henu.edu.cn ( h.l . ) 2school of automation science and electrical engineering , beihang university , beijing 100191 , china ; shiyan @ buaa.edu.cn ( y.s . ) ; caimaolin @ buaa.edu.cn ( m.c . ) 3pneumatic and thermodynamic energy storage and supply beijing key laboratory , beijing 100191 , china *correspondence : zdm @ henu.edu.cn ( d.z . ) ; weiqing.xu @ buaa.edu.cn ( w.x . ) ; tel./fax : +86-10-82339160 abstract : with the signiﬁcant increase in demand for artiﬁcial intelligence , environmental map recon- struction have become a research hotspot for obstacle avoidance navigation , unmanned operation , and virtual reality . the quality of the map play a vital role in position , path planning , and obstacle avoidance . this review start with the development of slam ( simultaneous localization and mapping ) and proceeds to a review of v-slam ( visual-slam ) from it proposal to the present , with a summary of it historical milestone . in this context , the ﬁve part of the classic v-slam framework—visual sensor , visual odometer , backend optimization , loop detection , and mapping—are explain separately . meanwhile , the detail of the late method be show ; vi-slam ( visual inertial slam ) be review and extend . the four critical technique of v-slam and it technical difﬁculties be summarize a feature detection and matching , selection of keyframes , uncertainty technology , and expression of map . finally , the development direction and need of the v-slam ﬁeld be propose . keywords : visual-slam ; classical framework ; key technique ; developmental need 1 . introduction advances in computer technology have expand the ﬁeld of “ unmanned operation , ” in which machine replace human , so that human can signiﬁcantly broaden the scope of their work . most machine currently complete task base on available scene map , but robot build for work in unknown environment have high requirement , because it be difﬁcult to achieve full automation . slam ( simultaneous localization and mapping ) be a technology that can achieve the autonomous positioning of mobile machine [ 1 ] for navigation , path planning , and target tracking . v-slam can use a visual sensor that work a a human eye to obtain information about the robot ’ s environment and , then , build a model , while accurately estimate it movement [ 2 ] , all without any prior information about the environment . it can also move into to unknown setting that human can not reach ; so , it have be study in-depth . v-slam be widely use on the ground [ 3,4 ] , under the water [ 5,6 ] , or in the air [ 7,8 ] , a show in figure 1 . it have broad application in resource detection , obstacle avoidance navigation , and uncrewed operation . this review begin with the development of slam , a review of v-slam from it proposal to the present day , and a summary of it historical milestone . then , it explain the ﬁve part of the classic v-slam framework : the frontend ( include visual sensor and visual odometer ) , backend optimization , loop detection , and mapping . the four critical technique of v-slam and their technical difﬁculties be summarize in context . finally , the development direction and need of v-slam research be propose . sensors 2022 ,22 , 4582. http : //doi.org/10.3390/s22124582 http : //www.mdpi.com/journal/sensorssensors 2022 ,22 , 4582 2 of 29 figure 1 . multiple application of slam . 2 . the development of v-slam in 1986 , smith [ 9 ] , who be study spatial uncertainty ’ s description and transfor- mation representation , publish a groundbreaking article on slam and propose the concept of a probabilistic slam . in 1988 , he [ 10 ] propose to use it to estimate a state vector . then durrant-whyte [ 11,12 ] propose a ﬁlter-based backend optimization algorithm , and his research result prompt smith [ 9 ] to write paper on landmark . the similarity in their error in estimate the robot ’ s position prove that the milestone must be correlate . however , the amount of calculation to achieve the state vector be huge . researchers decouple the landmark , so that mapping and positioning be treat a independent part , and slam development enter a bottleneck period . a breakthrough come when researcher realize that the error be a matter of estimation , and the slam problem be see a convergent , a theory ﬁrst propose by csorba [ 13 ] . in 2006 , v-slam [ 14 ] be propose a a branch of research , and it attract the attention of researcher , who publish numerous paper . a variety of slam algorithm and solution base on visual sensor be also propose , such a mono-slam [ 15 ] ( monocular-slam ) , base on monocular camera , and ptam [ 16 ] ( parallel tracking and mapping ) , which introduce the possibility of run various slam task in parallel . the solution use bundle adjustment ( ba ) [ 17 ] base on nonlinear optimize keyframes to solve position and map structure . orb-slam [ 18 ] ( oriented fast and rotated brief ) be propose base on ptam , which be currently the most effective feature-based method . in addition , v-slam use the direct method have also be suggested—-lsd-slam [ 19 ] ( large-scale direct monocular slam ) be one of these . based on this , dso-slam ( direct sparse odometry slam ) [ 20 ] have also be propose and be currently the best solution for estimate accuracy and operating efﬁciency . the v-slam system can run on various equipment terminal [ 21,22 ] . kimera have be design and be suitable for the broad slam ﬁeld with metric semantics modularity [ 23 ] . more and more university laboratory be engage in v-slam research , for example , the dyson robotics laboratory of imperial college , london , uk ; the automation system laboratory , eth zurich , switzerland ; the machine vision research group of the technical university of munich , germany ; and the laboratory for information & decision systems ( lids ) , massachusetts institute of technology , cambridge , ma , usa [ 24 ] . the overall development process of v-slam be show in figure 2.sensors 2022 ,22 , 4582 3 of 29 figure 2 . development of v-slam . 3 . v-slam classical framework v-slam be currently one of the critical technology in robotics , autonomous driving , and augment reality and be the basis for intelligent mobile platform to perceive the surrounding environment . since image or video can provide rich environmental informa- tion , most research on positioning and mapping focus on the v-slam algorithm . with the increase in the popularity and applicability of machine vision , the number of enterprise engage in this research be also increase , for example , in china . in 2020 , although affect by the covid-19 pandemic , 637 new company be start , a show in figure 3 . figure 3 . the number of newly add enterprise in china ’ s machine vision industry . the development of machine vision have accelerate the research of v-slam and it classical framework , a show in figure 4 . the procedure of v-slam be generally divide into two part : the frontend and the backend . at the frontend , the visual sensor be mainly responsible for collect data during movement and transmit the data to the visual odometer , which estimate the data of adjacent image or point to form a local map and assess the robot ’ s position . the backend be responsible for optimize the frontend information and , ﬁnally , produce a complete map . the purpose of loop detection be to judge whether the position the robot have walk be coincident by compare before and after information to avoid drift.sensors 2022 ,22 , 4582 4 of 29 figure 4 . framework of v-slam . 3.1 . frontend the frontend of v-slam be to process the input image and obtain the motion rela- tionship while the camera move to determine the position of the current frame . it mainly contain two part , namely a visual sensor and a visual odometer . the visual sensor be responsible for reading and preprocessing the camera image information . the visual odometer [ 25 ] estimate the camera movement base on the data from adjacent image to provide a good initial value for the backend . 3.1.1 . visual sensor with advance in computer technology , visual sensor have signiﬁcantly improve in resolution , pixel , and focus . according to different work method , these be divide into monocular , stereo , and rgb-d ( red green blue-depth ) cameras [ 26 ] . the monocular camera have a simple structure and fast calculation speed but lack the depth of information and have scale blur [ 27 ] . stereo camera can obtain depth information indoors or outdoors through the four step of calibration , correction , matching , and calculation , but the amount of computation need be signiﬁcant . rgb-d camera have become popular in the last ten year [ 28,29 ] because they can obtain image color and depth information at the same time [ 30 ] . v-slam , base on an rgb-d camera , have develop rapidly [ 31–33 ] because of two key technology : structure light and time of ﬂight ( tof ) . however , it be susceptible to light interference and a limited measurement range . the depth detection range of kinect , which use rgb-d camera a visual sensor , be only 1.2–3.5 m , and the visible spectrum be only 43in the vertical and 57in the horizontal direction [ 34 ] . 3.1.2 . visual odometry after the visual sensor collect the image information , the visual odometer [ 25 ] de- termines the position and direction of the robot by analyze the camera image . it pay attention only to the local consistency of the trajectory , and the operational model recon- structs the path incrementally . according to the type of feature extraction need , it be classiﬁed into the feature-point or direct method . the ﬁrst method [ 25 ] extract sparse fea- tures from an image , complete frame-matching through the descriptor , and then calculate the position accord to the constraint relationship among the element . the orb-slam ( oriented fast and rotated brief ) propose by mur-artal [ 18 ] be a well-known system that use the feature point method and be the core feature of the v-slam . the direct method introduce the idea of optical-ﬂow tracking . based on the assumption of constant luminosity , the optimization goal be to minimize the luminosity error to solve position variable . dso ( direct sparse odometry ) [ 20 ] be one of the few system that use the pure direct method to calculate visual odometry . svo ( semi-direct visual odometry ) [ 35 ] use the direct method in the sparse model-based image alignment part of the frontend.sensors 2022 ,22 , 4582 5 of 29 the frontend use the feature-point method be the mainstream one of the visual odometer . speciﬁc feature detection and matching be summarize in section 3.1 . com- pared with the complete v-slam , vo have a good real-time performance . if research be only require on the camera path instead of the environment map , vo be the good choice . it sacriﬁces global consistency to obtain real-time compute performance . visual inertial slam ( vi-slam ) be favor in research and application [ 36 ] to good solve the obstacle of pose accuracy and adaptability in complex dynamic scene [ 37 ] . the highlight be the complementary function between v-slam and imu ( inertial measurement units ) [ 38 ] . the slam be use to help the imu eliminate accumulate error and complete closed-loop detection ; the imu be use to support the slam in solve the position ac- curacy with less texture and fast movement [ 39 ] . jun et al . improve the initial convergence speed for a high positioning accuracy with a monocular camera and imu [ 40 ] . chai et al . employ enhance vanish point optimization to correct the cumulative drift error of the vi- slam system effectively [ 41 ] xu et al . achieve an improved positioning accuracy of 10 % in vi-slam with a fusion of the feature point and optical ﬂow method [ 42 ] . ecken- hoff et al . perform real-time high-precision positioning and efﬁcient result calibration use a multi-camera visual–inertial navigation system [ 43 ] . zhu et al . achieve well accuracy and robustness by combine a stereo camera and imu component with sparse mapping in the vi-slam algorithm [ 44 ] . multisensor fusion base on vi-slam have become a research hotspot for achieve high position accuracy and robustness . shan et al . enhance the extraction of visual depth information . they improve the accuracy of visual recognition by use the close coupling of the visual–inertial system and the inertial lidar system [ 45 ] . zhang et al . integrate sensor fusion with a laser rangeﬁnder and monocular camera slam to solve the limited camera depth range [ 46 ] . yang et al . achieve accurate and robust localization result for complex indoor scene by fuse super-bandwidth with vi-slam [ 47 ] . with the deepening of research and application , the fusion of vi-slam with a more complex structure and high-precision multisensors will continue to be a research direction . at the frontend , the work principle of monocular , stereo , and rgb-d camera be introduce in the visual sensor part . the feature-point method and the direct method be use to reconstruct the path by vo . it be also one of the decisive factor for the development of v-slam . 3.2 . backend optimization the backend receive the original data collect by the visual sensor provide by the frontend and perform the calculation and optimization . in v-slam , the frontend and computer-vision research ﬁelds be more related . at the same time , the backend optimization be essentially a state estimation problem that use either the ﬁlter-based or the nonlinear optimization method . earlier on , the ﬁlter-based method be the main one in the backend of v-slam . it be summarize in paper [ 11,12 ] . its core purpose be iterate and update the state quantity continuously , a uniformly describe by the bayesian ﬁlter model . the ﬁlter-based method represent by kf ( kalman filter ) [ 48 ] can optimize and process the frontend data . however , error occur , and the real-time performance of the algorithm can not be guarantee . to solve the above problem , the extended kalman filter ( ekf ) propose by moutarlier [ 49 ] be of great help in deal with uncertain information . for ekf , a mobile robot move in the environment and use visual sensor on the robot to observe know landmark ( a show in figure 5 ) . at time k , the state vector xk describe the position and direction of the vehicle , and the control vector be uk . the position vector be an miof the ith landmark . the part of the ith landmark be zik . gray and dark gray be the predicted position of the mobile robot and landmark , while white represent the actual position of the robot and landmarks.sensors 2022 ,22 , 4582 6 of 29 figure 5 . essential issue of slam . equation ( 1 ) indicate that the basis of the ekf slam be to describe the vehicle movement : p ( xkjxk","['citation : jia , g. ; li , x. ; zhang , d. ; xu , w. ; lv , h. ; shi , y. ; cai , m. visual-slam classical framework and key techniques : a review . sensors 2022 ,22 , 4582. http : // doi.org/10.3390/s22124582 academic editor : cosimo distante received : 8 may 2022 accepted : 7 june 2022 published : 17 june 2022 publisher ’ s note : mdpi stay neutral with regard to jurisdictional claim in publish map and institutional afﬁl- iations . copyright : © 2022 by the author . licensee mdpi , basel , switzerland . this article be an open access article distribute under the term and condition of the creative commons attribution ( cc by ) license ( http : // creativecommons.org/licenses/by/ 4.0/ ) . sensor review visual-slam classical framework and key techniques : a review guanwei jia1 , xiaoying li1 , dongming zhang1 , * , weiqing xu2,3 , * , haojie lv1 , yan shi2,3and maolin cai2,3 1school of physics and electronics , henan university , kaifeng 475004 , china ; jiaguanwei @ henu.edu.cn ( g.j . ) ; lixiaoying1 @ stu.scu.edu.cn ( x.l . ) ; haojielv @ henu.edu.cn ( h.l . ) 2school of automation science and electrical engineering , beihang university , beijing 100191 , china ; shiyan @ buaa.edu.cn ( y.s . ) ; caimaolin @ buaa.edu.cn ( m.c . ) 3pneumatic and thermodynamic energy storage and supply beijing key laboratory , beijing 100191 , china *correspondence : zdm @ henu.edu.cn ( d.z . ) ; weiqing.xu @ buaa.edu.cn ( w.x . ) ; tel./fax : +86-10-82339160 abstract : with the signiﬁcant increase in demand for artiﬁcial intelligence , environmental map recon- struction have become a research hotspot for obstacle avoidance navigation , unmanned operation , and virtual reality . the quality of the map play a vital role in position , path planning , and obstacle avoidance . this review start with the development of slam ( simultaneous localization and mapping ) and proceeds to a review of v-slam ( visual-slam ) from it proposal to the present , with a summary of it historical milestone . in this context , the ﬁve part of the', 'localization and mapping ) and proceeds to a review of v-slam ( visual-slam ) from it proposal to the present , with a summary of it historical milestone . in this context , the ﬁve part of the classic v-slam framework—visual sensor , visual odometer , backend optimization , loop detection , and mapping—are explain separately . meanwhile , the detail of the late method be show ; vi-slam ( visual inertial slam ) be review and extend . the four critical technique of v-slam and it technical difﬁculties be summarize a feature detection and matching , selection of keyframes , uncertainty technology , and expression of map . finally , the development direction and need of the v-slam ﬁeld be propose . keywords : visual-slam ; classical framework ; key technique ; developmental need 1 . introduction advances in computer technology have expand the ﬁeld of “ unmanned operation , ” in which machine replace human , so that human can signiﬁcantly broaden the scope of their work . most machine currently complete task base on available scene map , but robot build for work in unknown environment have high requirement , because it be difﬁcult to achieve full automation . slam ( simultaneous localization and mapping ) be a technology that can achieve the autonomous positioning of mobile machine [ 1 ] for navigation , path planning , and target tracking . v-slam can use a visual sensor that work a a human eye to obtain information about the robot ’ s environment and , then , build a model , while accurately estimate it movement [ 2 ] , all without any prior information about the environment . it can also move into to unknown setting that human can not reach ; so , it have be study in-depth . v-slam be widely use on the ground [ 3,4 ] , under the water [ 5,6 ] , or in the air [ 7,8 ] , a show in figure 1 . it have broad application in resource detection , obstacle avoidance navigation , and uncrewed operation . this review begin with the development of slam , a review of v-slam from it proposal to the present day , and a summary of', 'detection , obstacle avoidance navigation , and uncrewed operation . this review begin with the development of slam , a review of v-slam from it proposal to the present day , and a summary of it historical milestone . then , it explain the ﬁve part of the classic v-slam framework : the frontend ( include visual sensor and visual odometer ) , backend optimization , loop detection , and mapping . the four critical technique of v-slam and their technical difﬁculties be summarize in context . finally , the development direction and need of v-slam research be propose . sensors 2022 ,22 , 4582. http : //doi.org/10.3390/s22124582 http : //www.mdpi.com/journal/sensorssensors 2022 ,22 , 4582 2 of 29 figure 1 . multiple application of slam . 2 . the development of v-slam in 1986 , smith [ 9 ] , who be study spatial uncertainty ’ s description and transfor- mation representation , publish a groundbreaking article on slam and propose the concept of a probabilistic slam . in 1988 , he [ 10 ] propose to use it to estimate a state vector . then durrant-whyte [ 11,12 ] propose a ﬁlter-based backend optimization algorithm , and his research result prompt smith [ 9 ] to write paper on landmark . the similarity in their error in estimate the robot ’ s position prove that the milestone must be correlate . however , the amount of calculation to achieve the state vector be huge . researchers decouple the landmark , so that mapping and positioning be treat a independent part , and slam development enter a bottleneck period . a breakthrough come when researcher realize that the error be a matter of estimation , and the slam problem be see a convergent , a theory ﬁrst propose by csorba [ 13 ] . in 2006 , v-slam [ 14 ] be propose a a branch of research , and it attract the attention of researcher , who publish numerous paper . a variety of slam algorithm and solution base on visual sensor be also propose , such a mono-slam [ 15 ] ( monocular-slam ) , base on monocular camera , and ptam [ 16 ] ( parallel tracking and mapping ) , which', 'slam algorithm and solution base on visual sensor be also propose , such a mono-slam [ 15 ] ( monocular-slam ) , base on monocular camera , and ptam [ 16 ] ( parallel tracking and mapping ) , which introduce the possibility of run various slam task in parallel . the solution use bundle adjustment ( ba ) [ 17 ] base on nonlinear optimize keyframes to solve position and map structure . orb-slam [ 18 ] ( oriented fast and rotated brief ) be propose base on ptam , which be currently the most effective feature-based method . in addition , v-slam use the direct method have also be suggested—-lsd-slam [ 19 ] ( large-scale direct monocular slam ) be one of these . based on this , dso-slam ( direct sparse odometry slam ) [ 20 ] have also be propose and be currently the best solution for estimate accuracy and operating efﬁciency . the v-slam system can run on various equipment terminal [ 21,22 ] . kimera have be design and be suitable for the broad slam ﬁeld with metric semantics modularity [ 23 ] . more and more university laboratory be engage in v-slam research , for example , the dyson robotics laboratory of imperial college , london , uk ; the automation system laboratory , eth zurich , switzerland ; the machine vision research group of the technical university of munich , germany ; and the laboratory for information & decision systems ( lids ) , massachusetts institute of technology , cambridge , ma , usa [ 24 ] . the overall development process of v-slam be show in figure 2.sensors 2022 ,22 , 4582 3 of 29 figure 2 . development of v-slam . 3 . v-slam classical framework v-slam be currently one of the critical technology in robotics , autonomous driving , and augment reality and be the basis for intelligent mobile platform to perceive the surrounding environment . since image or video can provide rich environmental informa- tion , most research on positioning and mapping focus on the v-slam algorithm . with the increase in the popularity and applicability of machine vision , the number of enterprise engage in this', 'tion , most research on positioning and mapping focus on the v-slam algorithm . with the increase in the popularity and applicability of machine vision , the number of enterprise engage in this research be also increase , for example , in china . in 2020 , although affect by the covid-19 pandemic , 637 new company be start , a show in figure 3 . figure 3 . the number of newly add enterprise in china ’ s machine vision industry . the development of machine vision have accelerate the research of v-slam and it classical framework , a show in figure 4 . the procedure of v-slam be generally divide into two part : the frontend and the backend . at the frontend , the visual sensor be mainly responsible for collect data during movement and transmit the data to the visual odometer , which estimate the data of adjacent image or point to form a local map and assess the robot ’ s position . the backend be responsible for optimize the frontend information and , ﬁnally , produce a complete map . the purpose of loop detection be to judge whether the position the robot have walk be coincident by compare before and after information to avoid drift.sensors 2022 ,22 , 4582 4 of 29 figure 4 . framework of v-slam . 3.1 . frontend the frontend of v-slam be to process the input image and obtain the motion rela- tionship while the camera move to determine the position of the current frame . it mainly contain two part , namely a visual sensor and a visual odometer . the visual sensor be responsible for reading and preprocessing the camera image information . the visual odometer [ 25 ] estimate the camera movement base on the data from adjacent image to provide a good initial value for the backend . 3.1.1 . visual sensor with advance in computer technology , visual sensor have signiﬁcantly improve in resolution , pixel , and focus . according to different work method , these be divide into monocular , stereo , and rgb-d ( red green blue-depth ) cameras [ 26 ] . the monocular camera have a simple structure and fast calculation speed but', 'to different work method , these be divide into monocular , stereo , and rgb-d ( red green blue-depth ) cameras [ 26 ] . the monocular camera have a simple structure and fast calculation speed but lack the depth of information and have scale blur [ 27 ] . stereo camera can obtain depth information indoors or outdoors through the four step of calibration , correction , matching , and calculation , but the amount of computation need be signiﬁcant . rgb-d camera have become popular in the last ten year [ 28,29 ] because they can obtain image color and depth information at the same time [ 30 ] . v-slam , base on an rgb-d camera , have develop rapidly [ 31–33 ] because of two key technology : structure light and time of ﬂight ( tof ) . however , it be susceptible to light interference and a limited measurement range . the depth detection range of kinect , which use rgb-d camera a visual sensor , be only 1.2–3.5 m , and the visible spectrum be only 43\\x0ein the vertical and 57\\x0ein the horizontal direction [ 34 ] . 3.1.2 . visual odometry after the visual sensor collect the image information , the visual odometer [ 25 ] de- termines the position and direction of the robot by analyze the camera image . it pay attention only to the local consistency of the trajectory , and the operational model recon- structs the path incrementally . according to the type of feature extraction need , it be classiﬁed into the feature-point or direct method . the ﬁrst method [ 25 ] extract sparse fea- tures from an image , complete frame-matching through the descriptor , and then calculate the position accord to the constraint relationship among the element . the orb-slam ( oriented fast and rotated brief ) propose by mur-artal [ 18 ] be a well-known system that use the feature point method and be the core feature of the v-slam . the direct method introduce the idea of optical-ﬂow tracking . based on the assumption of constant luminosity , the optimization goal be to minimize the luminosity error to solve position variable . dso ( direct', 'introduce the idea of optical-ﬂow tracking . based on the assumption of constant luminosity , the optimization goal be to minimize the luminosity error to solve position variable . dso ( direct sparse odometry ) [ 20 ] be one of the few system that use the pure direct method to calculate visual odometry . svo ( semi-direct visual odometry ) [ 35 ] use the direct method in the sparse model-based image alignment part of the frontend.sensors 2022 ,22 , 4582 5 of 29 the frontend use the feature-point method be the mainstream one of the visual odometer . speciﬁc feature detection and matching be summarize in section 3.1 . com- pared with the complete v-slam , vo have a good real-time performance . if research be only require on the camera path instead of the environment map , vo be the good choice . it sacriﬁces global consistency to obtain real-time compute performance . visual inertial slam ( vi-slam ) be favor in research and application [ 36 ] to good solve the obstacle of pose accuracy and adaptability in complex dynamic scene [ 37 ] . the highlight be the complementary function between v-slam and imu ( inertial measurement units ) [ 38 ] . the slam be use to help the imu eliminate accumulate error and complete closed-loop detection ; the imu be use to support the slam in solve the position ac- curacy with less texture and fast movement [ 39 ] . jun et al . improve the initial convergence speed for a high positioning accuracy with a monocular camera and imu [ 40 ] . chai et al . employ enhance vanish point optimization to correct the cumulative drift error of the vi- slam system effectively [ 41 ] xu et al . achieve an improved positioning accuracy of 10 % in vi-slam with a fusion of the feature point and optical ﬂow method [ 42 ] . ecken- hoff et al . perform real-time high-precision positioning and efﬁcient result calibration use a multi-camera visual–inertial navigation system [ 43 ] . zhu et al . achieve well accuracy and robustness by combine a stereo camera and imu component with sparse mapping in the', 'calibration use a multi-camera visual–inertial navigation system [ 43 ] . zhu et al . achieve well accuracy and robustness by combine a stereo camera and imu component with sparse mapping in the vi-slam algorithm [ 44 ] . multisensor fusion base on vi-slam have become a research hotspot for achieve high position accuracy and robustness . shan et al . enhance the extraction of visual depth information . they improve the accuracy of visual recognition by use the close coupling of the visual–inertial system and the inertial lidar system [ 45 ] . zhang et al . integrate sensor fusion with a laser rangeﬁnder and monocular camera slam to solve the limited camera depth range [ 46 ] . yang et al . achieve accurate and robust localization result for complex indoor scene by fuse super-bandwidth with vi-slam [ 47 ] . with the deepening of research and application , the fusion of vi-slam with a more complex structure and high-precision multisensors will continue to be a research direction . at the frontend , the work principle of monocular , stereo , and rgb-d camera be introduce in the visual sensor part . the feature-point method and the direct method be use to reconstruct the path by vo . it be also one of the decisive factor for the development of v-slam . 3.2 . backend optimization the backend receive the original data collect by the visual sensor provide by the frontend and perform the calculation and optimization . in v-slam , the frontend and computer-vision research ﬁelds be more related . at the same time , the backend optimization be essentially a state estimation problem that use either the ﬁlter-based or the nonlinear optimization method . earlier on , the ﬁlter-based method be the main one in the backend of v-slam . it be summarize in paper [ 11,12 ] . its core purpose be iterate and update the state quantity continuously , a uniformly describe by the bayesian ﬁlter model . the ﬁlter-based method represent by kf ( kalman filter ) [ 48 ] can optimize and process the frontend data . however , error occur , and', ', a uniformly describe by the bayesian ﬁlter model . the ﬁlter-based method represent by kf ( kalman filter ) [ 48 ] can optimize and process the frontend data . however , error occur , and the real-time performance of the algorithm can not be guarantee . to solve the above problem , the extended kalman filter ( ekf ) propose by moutarlier [ 49 ] be of great help in deal with uncertain information . for ekf , a mobile robot move in the environment and use visual sensor on the robot to observe know landmark ( a show in figure 5 ) . at time k , the state vector xk describe the position and direction of the vehicle , and the control vector be uk . the position vector be an miof the ith landmark . the part of the ith landmark be zik . gray and dark gray be the predicted position of the mobile robot and landmark , while white represent the actual position of the robot and landmarks.sensors 2022 ,22 , 4582 6 of 29 figure 5 . essential issue of slam . equation ( 1 ) indicate that the basis of the ekf slam be to describe the vehicle movement : p ( xkjxk\\x001 , uk ) , xk=f ( xk\\x001 , uk ) +wk . ( 1 ) thef ( \\x01 ) function be the kinematics model of the vehicle , wkstands for the additivity , and the observation model be as describe in equation ( 2 ) : p ( zkjxk , m ) , zk=h ( xk , m ) +vk ( 2 ) the h ( \\x01 ) function be the observed geometric shape , and vkrepresents the additivity . figure 6 show that a series of the standard deviation of landmark location varies over time . figure 6 . the standard deviation of the landmark location change with time . for v-slam , the number of map point and position will increase while the system be run . the covariance scale and the mean that the ekf need to maintain and update will also become more extensive . meanwhile , the linear approximation between the motion and observation work in a small range , and severe nonlinear error will be cause at long distance . in paper [ 50 ] , the experimental data of backend optimization use ekf show that when the timesteps be 3000 , the estimate error', 'range , and severe nonlinear error will be cause at long distance . in paper [ 50 ] , the experimental data of backend optimization use ekf show that when the timesteps be 3000 , the estimate error of the vehicle position xv and orientation qvwas within the standard deviation limit during the ﬁrst 600 s of vehicle operation ( figure 7 ) . however , when the timesteps be 16,000 , the estimated error of the vehicle position xvand orientation qvwere not within the standard deviation limit in the ﬁrst 3200 s ( figure 8 ) .sensors 2022 ,22 , 4582 7 of 29 figure 7 . results for the ﬁrst 600 s ( 3000 timesteps ) : ( a ) error in xv , ( b ) error in qv . figure 8 . results for the ﬁrst 3200 s ( 16,000 timesteps ) : ( a ) error in xv , ( b ) error in qv . due to the limitation of the ekf algorithm for nonlinear , non-gaussian system , re- searcher also propose the pf ( particle filter ) [ 51 ] method . particles be use to describe the position or map point , and the probability density distribution of the approximate state be solve by randomly sample the particle . the advantage of the pf method be that state estimation be not sensitive to data association but have good performance in linear approximation . considering the different characteristic and advantage of the ekf and pf algorithm , montemerlo [ 52,53 ] apply an rbpf [ 54 ] ( rao-blackwell ’ s particle filter ) to a robot slam and name it “ the fast-slam algorithm ” . the algorithm break the slam problem into a robot localization and an environmental feature position estimation problem . the pf algorithm be use to estimate the position of the entire path , combine the advantage of ekf and probabilistic method . it not only reduce the computational complexity but also have good robustness . with the development of digital image processing , ﬁlter-based technology be gradually be replace by graph-based optimization [ 55 ] . in 1998 , golfarelli [ 56 ] propose a truss model with each route a an adjustable bar and each landmark a node . a spring connect two adjacent', 'be replace by graph-based optimization [ 55 ] . in 1998 , golfarelli [ 56 ] propose a truss model with each route a an adjustable bar and each landmark a node . a spring connect two adjacent node that show the constrain relationship between them . the spring stiffness coefﬁcient represent the uncertainty of the constraint . the spring and node represent route and landmark , respectively , a show in figure 9.sensors 2022 ,22 , 4582 8 of 29 figure 9 . spring–node model : ( a ) graph-based representation of an environment and ( b ) equivalent truss . from a mechanical point of view , the model be construct by combine an axial linear spring with a rotary spring ( figure 10 ) . kaandkrare the spring constant . the model include a linear spring ( black ) and a rotate spring ( gray ) . figure 10 . the truss have a linear elastic spring ( black ) and a rotational elastic spring ( gray ) . then , there be equation ( 3 ) : kaµ1 dx , krµs2 dy , ( 3 ) where dx=z+¥ \\x00¥z+¥ \\x00¥d ( x , y , c ) jxjdxdy ( 4 ) dy=z+¥ \\x00¥z+¥ \\x00¥d ( x , y , c ) jyjdxdy . ( 5 ) cis the covariance matrix of the route r. suppose the robot start to explore the unknown area from the know landmark v0and ﬁnally meet landmark vm . ifvmhad be reach before , the orderly connect line segment between v0and vmwould form an open polygon ( figure 11a ) . if not , a closed polygon be create ( figure 11b ) . figure 11 . error correction of the open polygon ( a ) and close polygon ( b ) . when reﬂecting the correction effect , there be two critical parameter : the average percentage error son this path and the average percentage error rin the direction of the path . the change in the parameter before and after modiﬁcation be show in table 1.sensors 2022 ,22 , 4582 9 of 29 table 1 . comparison of parameter change before and after correction . s r before correction 0.049 0.033 after correction 0.046 0.032 figure 12 show the correction comparison perform 1 , 5 , and 50 time . the actual map be gray , and the correct one be black . figure 12 . comparison of a 1-time', '0.033 after correction 0.046 0.032 figure 12 show the correction comparison perform 1 , 5 , and 50 time . the actual map be gray , and the correct one be black . figure 12 . comparison of a 1-time correction ( a ) , 5-times correction ( b ) and 50-times correction ( c ) . as show in figure 13 , in the ﬁrst ten tour , the average error of the landmark position measure before the correction be reduce to 20–30 % , and in the subsequent tour , it drop below 10 % . figure 13 . comparison of elastic correction and weighted average . the backend optimization mainly introduce the ﬁlter-based backend ( such a ekf ) and the nonlinear optimization backend ( such a graph optimization ) . v-slam tends to adopt a nonlinear optimization method with good effect and stability . 3.3 . loop detection loop detection be use to correct error . it can add constraint to other frame except for adjacent frame and closely relate to position and mapping . the backend estimate the maximum error in optimize the data provide by the frontend , and loop detection can eliminate the inﬂuence cause by error accumulation . paper [ 57 ] compare three method of loop detection for the monocular camera to complete v-slam and introduce three match method : image-to-image [ 58,59 ] , map-to-map [ 60 ] , and image-to-map [ 61 ] . the map-to-map matching method ﬁnds the correspondence between the identity in two submaps . the match result indicate that although common identity between the two image can be find , the number of common feature be not enough . a similar feature point need to be find between the late frame and the map in image-to-map matching . among them , the research methodsensors 2022 ,22 , 4582 10 of 29 of cummins [ 62 ] be use to judge the position of the camera and the position of other object through a three-point position calculation . image-to-image matching determines a consistent point between the late and previous image use the method of cummins [ 62 ] , which use the graphic feature obtain from the standard string vocabulary to', 'matching determines a consistent point between the late and previous image use the method of cummins [ 62 ] , which use the graphic feature obtain from the standard string vocabulary to detect the same position between two string . the recall curve in figure 14 show the inﬂuence of the probability threshold on system reliability . the image-to-image method have the high accuracy . figure 14 . performance comparison of three closed-loop detection method . figure 15 show an aerial view of the keble courtyard . by compare the mapping before and after loop detection , the correction effect of the loop detection on the mapping be noticeable . figure 15 . aerial view of the courtyard and comparison before and after loop detection . in loop detection , “ bag of word ” ( bow ) have widespread application [ 63–66 ] . bag of word [ 67 ] refers to a technology that can use a visual vocabulary tree to turn the content of a picture into a digital vector for transmission . the step of construct a visual vocabulary tree [ 68 ] be show in figure 16 . for the newly enter image frame , each feature be traverse down from the root node of the word tree , and the node with the small hamming distance be then traverse down to the leaf node . the number on each leaf node be calculate , and the image expression vector vi form . the data unit structure of vector vi ( index , value ) , namely ( word index , weight ) . the weight be deﬁned by equation ( 6 ) . wi t=t f ( i , it ) \\x02id f ( i ) , wi t=wi t/åwi t ( 6 ) sensors 2022 ,22 , 4582 11 of 29 where tf ( i , it ) be the weight component generate when the bow vector be generate ; idf ( i ) be the weight component generate when the dictionary be develop . figure 16 . construction step of a visual bag of word . each word node store a reverse index , make ﬁnding the most relevant image of the word more accessible , a show in figure 17 . it be divide into layer lw , and layer 0 represent the node where the word be locate , which may contain multiple feature . the inverse text frequency of', 'word more accessible , a show in figure 17 . it be divide into layer lw , and layer 0 represent the node where the word be locate , which may contain multiple feature . the inverse text frequency of node 1 be 0.79 in image 68 and 0.73 in image 82 in figure 17 . figure 17 . vocabulary tree of image database . after the bow vector of the two picture be obtain , the similarity between them be compare : l1\\x00scores ( v1 , v2 ) =1\\x001 2 v1 jv1j\\x00v2 jv2j ( 7 ) using the database to simplify the retrieval process , the normalized score function be obtain a follow : h ( vt , vtj ) =s ( vt , vtj ) s ( vt , vt\\x00dt ) ( 8 ) the candidate image that satisfy the requirement be reserve and enter into group match for veriﬁcation . the function of group matching be to prevent competition between continuous shot in the database query . after a time of consistency veriﬁcation , only one group be reserve for structural consistency veriﬁcation . data set of new college [ 69 ] , bicocca 2009-02-25b [ 70 ] , and ford campus [ 71 ] be use for training ; malaga6l [ 72 ] and citycentre [ 62 ] be use for test . compared with fab-map 2.0 [ 73 ] ( the input be the disjoint image sequence , a show in table 2b , thesensors 2022 ,22 , 4582 12 of 29 test result be show in table 2a ) . by default , the algorithm achieve high accuracy , and there be no false positive in with test dataset . in the malaga6l dataset , there be a high recall rate despite the inﬂuence of light and view depth . in citycentre , the input be continuous image information . still , the variation between the loop close image be more signiﬁcant than in the other datasets , so the recall rate be relatively low , but the accuracy rate be still 100 % . see table 2 for speciﬁc data . table 2 . comparison of accuracy and recall rate : ( a ) accuracy and recall rate of test system ; ( b ) accuracy and recall rate of fab-map2.0 . ( a ) ( b ) precision and recall of our system precision and recall of fab-map 2.0 dataset # images precision ( % ) recall ( % ) dataset # images min . p', 'b ) accuracy and recall rate of fab-map2.0 . ( a ) ( b ) precision and recall of our system precision and recall of fab-map 2.0 dataset # images precision ( % ) recall ( % ) dataset # images min . p precision ( % ) recall ( % ) newcollege 5266 100 55.92 malaga6l 462 98 % 100 68.52 bicocca25b 4924 100 81.2 citycentre 2474 98 % 100 38.77 ford2 1182 100 79.45 malaga6l 869 100 74.75 citycentre 2474 100 30.61 since the cumulative error always exist , loop detection be signiﬁcant to the v-slam system . the most widely use and most effective method be the bag-of-words method , which can effectively improve and optimize accuracy and be very helpful for construct a globally consistent map . 3.4 . mapping mapping be for more precise positioning , navigation , obstacle avoidance , reconstruc- tion , and interaction . whether it be frontend or backend optimization , optimization make adequate preparation for map . as early a 2007 , davison [ 15 ] propose the ﬁrst real-time monocular visual system with ekf a the backend , name mono-slam . the most signiﬁcant advantage be real-time image and no drift . the camera and coordinate frame be show in figure 18 . figure 18 . relative position of the camera and coordinate system . considering that the monocular camera can not obtain depth information , a small amount of prior scene information be use to help the system start . a constant velocity and angular velocity model be use , a show in figure 19 . the robot walk along a circular trajectory ( yellow ) with a radius of 0.75 m , a show in figure 20 . the uncertainty increase before the loop be close , and a drift correction be make . the disadvantage of this method be that the scene be narrow , the number of landmark be limited , and the sparse feature point be easy to lose.sensors 2022 ,22 , 4582 13 of 29 figure 19 . visualization of “ smooth ” motion model . figure 20 . a humanoid robot walk in a circular trajectory of radius 0.75 m : ( a ) early exploration and ﬁrst turn , ( b ) mapping back signiﬁcantly more uncertainty ; ( c', '“ smooth ” motion model . figure 20 . a humanoid robot walk in a circular trajectory of radius 0.75 m : ( a ) early exploration and ﬁrst turn , ( b ) mapping back signiﬁcantly more uncertainty ; ( c ) just before loop close , maximum uncertainty ; and ( d ) end of a circle with closed-loop and drift correct . subsequently , klein [ 16 ] propose ptam ( parallel tracking mapping ) in 2007 , the ﬁrst v-slam system to process tracking and mapping in two parallel thread . it be also the ﬁrst to use a nonlinear optimization backend solution , lay the foundation for the backend processing of v-slam to be dominate by nonlinear optimization ( the process be show in figure 21 ) . ptam also propose the key-frames mechanism ; instead of carefully process each image , several key image be strung together to optimize the trajectory and map . ptam can place virtual object on a virtual plane , and it contribute to combine ar ( augmented reality ) with slam . figure 22 show the effect of ptam on the desktop . figure 21 . ptam process diagram . following the parallel-thread structure [ 74 ] of ptam , mur-artal [ 18 ] propose the orb-slam three-thread structure ( figure 23 ) . it can realize the construction of sparse map but can only satisfy the positioning demand ; it can not provide navigation , obstacle avoidance , or other functions.sensors 2022 ,22 , 4582 14 of 29 figure 22 . ptam effect diagram . figure 23 . overview of the orb-slam system . the visual sensor use in orb-slam [ 18 ] in 2015 be a monocular camera , which have the problem of scale drift . based on the shortcoming of orb-slam , orb-slam2 [ 75 ] be propose in 2016 a the ﬁrst slam system for monocular , stereo , and rgb-d camera . a thread be set up not to affect loop detection to execute a global ba [ 17 ] ( bundle adjustment ) . it contain a lightweight positioning model that use vo to track the unmapped area and match map point to achieve zero-drift positioning . figure 24 show orb-slam2 keyframes and the visibility graph . figure 24 . keyframes and visibility', 'model that use vo to track the unmapped area and match map point to achieve zero-drift positioning . figure 24 show orb-slam2 keyframes and the visibility graph . figure 24 . keyframes and visibility graph of orb-slam2 . given the long calculation time of the orb-slam system , engle [ 19 ] propose lsd- slam ( large-scale direct monocular slam ) in 2014 . lsd-slam propose an image match algorithm to estimate the similarity transformation between keyframes directly.sensors 2022 ,22 , 4582 15 of 29 it do not need to extract the feature descriptor of the image , and the transformation between two frame can be obtain by optimize the optical measurement error . the ﬁnal result be a semi-dense map , which work well in place with weak texture . the proposal of lsd-slam mark the transition from sparse map to semi-dense map , and the process be show in figure 25 . figure 25 . overview of the lsd-slam system . the svo-slam ( semi-direct visual odometry ) [ 35 ] algorithm use semi-direct visual odometry without calculate a large number of descriptor and be extremely fast . it can reach 300 frame per second on consumer laptop and 55 frame per second on a uav ( unmanned aerial vehicle ) . svo-slam ﬁrst propose the concept of a depth ﬁlter ( a show in figure 26 ) to estimate the position of critical point and use the inverse depth a a parameterized form . the schematic diagram of it effect be show in figure 27 . the disadvantage of this method be that it discard the backend optimization and loop detection , have a cumulative error in the position estimation , and be difﬁcult to relocate after loss . figure 26 . schematic diagram of depth ﬁlter . figure 27 . effect diagram of svo-slam.sensors 2022 ,22 , 4582 16 of 29 in 2016 , dso-slam [ 20 ] ( direct sparse odometry s ) be propose and prove to be good than lsd-slam in term of accuracy , stability , and speed . the algorithm do not consider prior information and could directly optimize photometric error . the optimization range be not all frame but a sliding window form by the', ', stability , and speed . the algorithm do not consider prior information and could directly optimize photometric error . the optimization range be not all frame but a sliding window form by the most recent frame and the previous few . in addition to perfect the error model of direct method position estimation , dso-slam also add an afﬁne brightness transformation , photometric correction , and depth optimization . still , this method do not have loop detection . its effect diagram be show in figure 28 . the red line inset illustrate a cycle the start and end position , visualize the drift accumulate during with the tracked trajectory . figure 28 . effect diagram of dso-slam . the late method of v-slam have not be compare and summarize in a review article . therefore , we extend this review to the late method . the result have distin- guished the content of publish review [ 36,37 ] and enrich the current state of the v-slam ﬁeld . the late method be show in table 3 , with detail of the latest algorithm , hardware requirement , performance , and characteristics . table 3 . demonstration late v-slam method . latest algorithm hardware requirements scenario performance characteristics dynaslam [ 76 ] monocular , stereo and rgb-ddynamic scenario ; static maptracked trajectory : > 87.37 % ; average of the rpe : 0.45 % 1 . dynamic object detection ; 2 . background inpainting ; 3 . multi-view geometry , deep learning . hoofr slam [ 77 ] multi and stereo camera ; urban ; karlsruhe ; campuscpu time : 62.235 m ; gpu time : 36.154 ms1 . large-scale unknown environment ; 2 . hardware-software mapping ; 3 . low-power . pl-slam [ 78 ] stereo camera ; room , industrial scenarioruntime : 57.05 m ( kitti ) ; runtime : 37.48 m ( euroc ) 1 . points and line segment ; 2 . more diverse in 3d element ; 3 . lower computational time.sensors 2022 ,22 , 4582 17 of 29 table 3 . cont . latest algorithm hardware requirements scenario performance characteristics cubeslam [ 79 ] monocular camera indoor ; outdoormean tan error : 4.42 m ; mean', '2022 ,22 , 4582 17 of 29 table 3 . cont . latest algorithm hardware requirements scenario performance characteristics cubeslam [ 79 ] monocular camera indoor ; outdoormean tan error : 4.42 m ; mean depth error : 4.9 % ; runtime : 365.2 ms1 . 3d cuboid object detection ; 2 . long-range geometric and scale ; 3 . static and dynamic scene . doorslam [ 80 ] two quadcopters feature stereo camerasfootball ﬁeld ; threshold ( 1 % ) : ate ( 2.1930 m ) ; threshold ( 75 % ) : ate ( 18.255 m ) 1 . peer-to-peer communication ; 2 . distributed pose graph optimization ; 3 . outlier rejection mechanism . dymslam [ 81 ] stereo camera ; laser scanner ; indoor ; corridorsrsem of move object : position [ cm ] : 10.81 ; rotation [ \\x0e ] : 2.04721 . 4d ( 3d + time ) dynamic scene ; 2 . improving accuracy of boundary ; 3 . dynamic environment . tima slam [ 82 ] multi-camera system hall ; laboratory ; corridorseuroc/asl : 0.023 ; kitti odometry : 0.581 . independent tracking ; 2 . without precalibration ; 3 . better compatibility . fsd-slam [ 83 ] monocular camera indoorate : 0.018793 m rpe : 0.028753 m1 . accurate camera pose estimation ; 2 . enhanced dynamic covariance scaling ; 3 . point cloud integration . dsp-slam [ 84 ] monocular , stereo , stereo + lidarcars ; chairsfaster iteration time : 4 s ; fewer iteration : 101 . pose and shape with less drift ; 2 . maintaining a consistent global map ; 3 . almost real-time performance . more robust performance of the novel method have be achieve by dynaslam , hoofr slam , pl-slam , dsp-slam , etc . the advantage of cubeslam be well robustness with 3d cuboid detection and edge ﬁltering . robust perception and the backend of doorslam have be well simulate use datasets and test ﬁeld experiment . robust 4d ( 3d + time ) dynamic scene reconstruction have be create with dymslam . a robust multi-camera slam system ( tima slam ) have be develop for map accuracy . the robustness of fsd-slam have be well carry out with a fast movement scarce visual environment . meanwhile , the diversiﬁcation scenario', 'slam system ( tima slam ) have be develop for map accuracy . the robustness of fsd-slam have be well carry out with a fast movement scarce visual environment . meanwhile , the diversiﬁcation scenario have illustrate to verify it state-of-the-art novel method , such a urban area , campus , corridor , football stadium , laboratory . etc . the mapping development have go through various v-slam algorithm such a mono-slam , ptam , orb-slam , lsd-slam , svo-slam , and dso-slam ; so , accuracy , stability , and speed in map have be improve . because each have it shortcoming , choose an appropriate mapping technology accord to the application environment be necessary . meanwhile , the late method have be demonstrate for more robust performance and diversiﬁcation scenario , which have enrich the current state of the v-slam ﬁelds . 4 . v-slam key techniques in the development of v-slam , every technical step be crucial . however , every critical technical link have technical difﬁculties and obstacle . some directly affect the ﬁnalsensors 2022 ,22 , 4582 18 of 29 positioning and mapping result of v-slam ; so , solve the essential technology of v- slam can signiﬁcantly optimize the ﬁnal result and provide an excellent convenience for it application . 4.1 . feature detection and matching the primary purpose of the frontend of v-slam be to provide good data for the backend , and , in this , the visual odometer [ 25 ] play a key role . it determine the position and direction of the robot by analyze the camera image . then , depend on whether feature need to be extract , it can be divide into a feature point method or a direct method . feature point be most commonly use because of it high stability [ 63,85,86 ] . photos and other kind of original information be continuous analog signal ; so , they must be transform into a digital form for a computer to process the data . digital image be often store in the form of a gray matrix , but the gray value will change with the light factor and object material . to far study the', 'form for a computer to process the data . digital image be often store in the form of a gray matrix , but the gray value will change with the light factor and object material . to far study the identiﬁcation and positioning of critical point , it be necessary to select those point in the image that will not change with a change of perspective , namely , feature point . in some scene , the feature point of the image do not meet the requirement ; so , more stable human-designed feature point be generate . the most common feature be sift [ 87 ] , surf [ 88 ] , and orb [ 89 ] . 4.1.1 . sift sift ( scale invariant feature transform ) [ 87 ] be a classic feature point with image- rotation and size invariability . it also have robustness to image noise and a visible illumina- tion change [ 85,90–92 ] . however , the large dimension of feature point make it challenge to complete a real-time and accurate calculation , much less rapid positioning and mapping . the extraction step of sift feature be show in figure 29 . figure 29 . extraction step of sift feature . the 16\\x0216 rectangular-block pixel around the key point be select and divide into 4\\x024 subareas . a feature point generates sift feature vector of 4 \\x024\\x028 dimension . when use the sift algorithm , the distance between each feature point of the ﬁrst image and all feature point of the second image need to be calculate . feature vector and descriptor be show in figures 30 and 31 . figure 30 . 128-dimensional sift feature vector.sensors 2022 ,22 , 4582 19 of 29 figure 31 . sift keypoint descriptor . 4.1.2 . surf surf ( speeded up robust features ) [ 88 ] be propose base on the sift algorithm . it be mainly improve to deal with the disadvantage of the slow operation speed and extensive computation of the sift algorithm . the surf feature be widely use a a feature extraction and analysis method for v-slam [ 86,93,94 ] , and the algorithm ﬂow be show in figure 32 . figure 32 . steps of surf feature algorithm . firstly , a gaussian pyramid scale-space need to be construct .', 'analysis method for v-slam [ 86,93,94 ] , and the algorithm ﬂow be show in figure 32 . figure 32 . steps of surf feature algorithm . firstly , a gaussian pyramid scale-space need to be construct . surf adopt deter- minant approximation image of a hessian matrix , which be deﬁned a : h ( f ( x , y ) ) =2 4¶2f ¶x2¶2f ¶x¶y ¶2f ¶x¶y¶2f ¶y23 5 ( 9 ) the pyramid image have many layer , and each have image with different scale . figure 33a be the traditional way to build a pyramid structure . it change the image size and use a gaussian function to smooth the sublayers repeatedly . the surf algorithm only change the ﬁlter size , omit the downsampling process , and improve the processing speed ( figure 33b ) . figure 33 . different variation in scale space : ( a ) sift : ﬁlter remain unchanged , image size be change , ( b ) surf : image size be unchanged , and the ﬁlter size be change . the pixel point process by the hessian matrix be compare with the 26 point in the three-dimensional ﬁeld , and those with the most vital feature be select a feature point . a square box be take around the feature point and divide into 16 subareas . each subarea calculate the horizontal and vertical haar wavelet feature of 25 pixel . thesensors 2022 ,22 , 4582 20 of 29 process be show in figure 34 . each feature point be a vector of 16 \\x024 = 64 dimension , which be half the size of sift . figure 34 . construction of surf feature point descriptor . 4.1.3 . orb feature orb ( oriented fast and rotated brief ) [ 89 ] be the combination of a fast [ 95 ] ( features from accelerated segment test ) feature detection operator and a brief [ 96 ] ( binary robust independent elementary features ) descriptor . the fast be a feature point with a breakneck calculation speed . it be mainly apply to detect a noticeable change in gray level in local image , but the repeatability be not strong , and the distribution be not uniform . a brief be a binary string that improve the accuracy of real-time feature detection and data extraction . many v-slam algorithm', 'the repeatability be not strong , and the distribution be not uniform . a brief be a binary string that improve the accuracy of real-time feature detection and data extraction . many v-slam algorithm adopt orb feature [ 97,98 ] , and the ﬂow be show in figure 35 . figure 35 . steps of the orb feature algorithm . to determine whether pixel point p be a crucial point of fast , we only need to judge whether the difference between the gray value of n consecutive point and p among the 16 surround pixel exceed the threshold [ 95 ] . the position of the 16 point be show in figure 36 . after ﬁnding the key point , the grayscale centroid technique be use to calculate the direction of the feature . figure 36 . a 12-point segment test corner detection in an image patch . the result of the brief algorithm be a binary string . to increase the noise resistance of feature descriptor , gaussian smoothing be need for the image . with the feature point pas the center , a neighborhood window of s \\x02s be take , and a pair of point piand qiare randomly select to compare the two point ’ pixel size . if i ( pi ) > i ( qi ) , 1 in the binary string will be generate ; otherwise , 0 . n pair of point be randomly select in the window , and the above step be repeat to form a binary code . the code describe the feature point , namely , the feature descriptor.sensors 2022 ,22 , 4582 21 of 29 the orb feature point be the most commonly use because of it speed . however , appropriate feature point should be select accord to different visual information and scene requirement . correct feature match relief much of the burden of backend optimization . 4.2 . selection of keyframes if there be error in position estimation , the frame-to-frame alignment method will cause cumulative ﬂoating . therefore , a v-slam base on critical frame [ 99 ] be propose . keyframes play a role in ﬁltering to prevent useless or wrong information from enter the optimization and damage the positioning construction ’ s accuracy . the screening of keyframes follow the', '. keyframes play a role in ﬁltering to prevent useless or wrong information from enter the optimization and damage the positioning construction ’ s accuracy . the screening of keyframes follow the principle of “ three pass , four detection ” ( figure 37 ) . based on satisfy the requirement of the internal point , one of the other three detection condition should be satisﬁed . figure 37 . principle of “ three pass , four detection ” . after ﬁltering , if the current frame be a keyframe , it should be input into the local diagram building and loopback detection module . the critical frame insertion process be show in figure 38 . figure 38 . flow diagram for insert keyframes . christian [ 99 ] propose a keyframe selection method base on entropy similarity , which can reduce position ﬂoating . the entropy ratio be submit to treat mismatch in the course of a trajectory and the different scene . it can be calculate by the ratio of parameter entropy h ( xk , k+j ) ( the motion estimation from the previous keyframe kto the current frame j ) to the parameter entropy h ( xk , k+1 ) ( the motion estimation from the keyframe kto the next frame k+ 1 ) , a follow : a=h\\x10 xk : k+j\\x11 h ( xk : k+1 ) ( 10 ) figure 39 show the entropy ratio from frame 50 to all other frame in the fr1/desk dataset . if afalls below the predeﬁned threshold for the current frame , the previous frame be select a the new keyframe and insert into the map.sensors 2022 ,22 , 4582 22 of 29 figure 39 . entropy ratio from frame 50 to other frame . keyframe selection strategy be an essential factor for algorithm performance , signiﬁ- cantly reduce the calculation pressure of the backend optimization . the effective selection of keyframes contributes to better positioning and image building . 4.3 . uncertainty technology the uncertainty of perception information may come from an imperfection , such a block a reference object . it may also come from uncertainty bring about by randomness—for example , the inﬂuence of unknown external force such a mobile robot', 'may come from an imperfection , such a block a reference object . it may also come from uncertainty bring about by randomness—for example , the inﬂuence of unknown external force such a mobile robot roller slippage , sensor parameter ( such a resolution ) , or observation noise . since the mobile robot must rely on sensor to obtain information when the environment be unknown , the uncertainty of perceptual detail will lead to inaccuracy in the constructed environment model , a show in figure 40 . figure 40 . system process uncertain information . the measurement error of the sensor be relate to the measurement accuracy and the condition and time of measurement . as for the uncertainty of sensor observation data , high resolution or multisensor data fusion can be adopt to solve the problem and minimize error accumulation . for example , in paper [ 8 ] , visual and inertial sensor be combine to comprehensively process image and measurement information to reduce error cause by objective uncertainty . visual inertial odometry ( vio ) integrate a visual inertial measurement unit ( imu ) and vo . paper [ 8 ] describe a vision-based localization and environment mapping and mpc ( model predictive control ) trajectory track with obstacle avoidance for autonomous mav navigation in gps-denied clutter environment . the ﬂight-tested micro aerial vehicle perform automatic takeoff and landing and autonomous obstacle avoidance trajectory track . the track trajectory on the ﬂight ﬁeld be show in figure 41 ( blue : reference trajectory , red : mav ( micro air vehicle trajectory ) . the positioning and track accuracy during the ﬂight be show in table 4 , and the data show that the positioning and track accuracy be very high , with an error as low a 6 cm . table 4 . localization and track accuracy in a ﬂying arena . rms of localization error ( m ) rms of tracking error ( m ) circular trajectory 0.067 0.131 3d trajectory 0.077 0.219sensors 2022 ,22 , 4582 23 of 29 figure 41 . trajectory track in the ﬂying arena during trajectory', 'error ( m ) rms of tracking error ( m ) circular trajectory 0.067 0.131 3d trajectory 0.077 0.219sensors 2022 ,22 , 4582 23 of 29 figure 41 . trajectory track in the ﬂying arena during trajectory tracking : ( a ) circular trajectory ; ( b ) 3d trajectory . uncertainty come from various source , which affect the ﬁnal mapping . different sensor have different characteristic and advantage , so adopt multisensor fusion can reduce the negative impact . meanwhile , some new algorithm be able to minimize the inﬂuence of uncertainty on v-slam . 4.4 . expression of maps mobile robot sense the surrounding environment through sensor and eventually build environmental map . since map building mainly serve for position , it be neces- sary to create map accord to different environmental requirement . the construction method of an environment map mainly include a grid map , a topological map , and an octree . a two-dimensional grid map [ 100 ] be widely use in the navigation ﬁeld of mobile robot , such a path planning and real-time obstacle avoidance . an rgb-d camera can obtain a 3d point cloud of the scene in real-time and establish a local grid map use depth information , a show in figure 42 . figure 42 . establishment process of a 2d grid map . a topology map be base on a raster map , abstract the environment into a graph model [ 101 ] . a grid-based map be a set of small area separate by narrow passage , such a doorway , know a critical boundary . the segmented submap be map to the same-type map , where node correspond to region and arcs connect adjacent area . the establishment step be show in figure 43 . figure 43 . establishment process of a topology map . figure 44a be a metric map where cell with occupancy value below the threshold in the occupancy grid be consider free space ( denote by c ) . for each point ( x , y ) in c , there be two near point in c , name the base point of ( x , y ) . the distance between ( x , y ) and it base point be the gap of ( x , y ) . figure 44b below depicts a voronoi map [ 102 ] . the', 'c , there be two near point in c , name the base point of ( x , y ) . the distance between ( x , y ) and it base point be the gap of ( x , y ) . figure 44b below depicts a voronoi map [ 102 ] . the key to divide free space be to ﬁnd critical point , and figure 44c represent the vital point . the required boundary be obtain by connect each crucial point with it reference point ( figure 44d ) . figure 44f be an example of a topology map.sensors 2022 ,22 , 4582 24 of 29 figure 44 . extraction of a topology map : ( a ) metric map ; ( b ) voronoi diagram ; ( c ) critical point ; ( d ) critical line ; ( e ) topological region , and ( f ) topological graph . an octo map [ 103 ] be a ﬂexible , compress , and constantly update map . each octree node represent a space in a cubic volume , call a voxel . the block be recursively subdivide into eight subblocks until a give minimum voxel size be reach , a show in figure 45 . the minimum voxel size determine the resolution of the octree . the mapping construction process be show in figure 46 . figure 45 . diagram of octree . figure 46 . establishment process of octree map . figure 47 show an example of query an octree map of active voxels at several different resolution , where multiple solution of the same map can be obtain at any one time by limit the depth of the query.sensors 2022 ,22 , 4582 25 of 29 figure 47 . display effect of 0.08 , 0.64 , and 1.28 m resolution . environmental map type mainly include grid , topology , and octree . v-slam should also selectively determine the kind of final map base on different scene type and sensor type . 5 . developmental needs for v-slam v-slam have be develop towards strong robustness and real-time performance in recent year . more and more new technology have emerge make v-slam more widely use . however , it still face many signiﬁcant challenge in a diversiﬁed application environment . when visual sensor acquire indoor and outdoor information , similar scene be in- evitable . environmental factor lead to low accuracy in feature', 'challenge in a diversiﬁed application environment . when visual sensor acquire indoor and outdoor information , similar scene be in- evitable . environmental factor lead to low accuracy in feature matching ; so , it be necessary to improve the quality of the original image acquisition and reduce the inﬂuence of the external environment . the key to the accuracy and efﬁciency of global map construction be the selection of high-precision keyframes , which determine the amount of backend optimization task and the graphic construction effect of v-slam . therefore , accuracy and efﬁciency in handle valid keyframes be require . due to the uncertainty cause by a single sensor , multisensor fusion be need to solve the problem . at the same time , improve data coupling and use be require . a new map model base on the exist map form need to be develop for a multitasking and complex scene . with the development of new v-slam system , new multisensor fusion , and new algorithm , it be believe that v-slam will play an essential role in “ unmanned operation ” and continue to explore new developmental direction . 6 . conclusions v-slam have become a hot spot for solve localization and mapping in autonomous navigation without human intervention . it be necessary to classify and summarize the research status of v-slam for more in-depth exploration . the v-slam originate from slam with visual sensor . therefore , the historical milestone of slam and v-slam be outline . five part of the classic v-slam framework be explain separately , include frontend ( visual sensor and visual odometry ) , backend optimization , loop detection , and mapping . each section detail the current state of the v-slam ﬁelds . meanwhile , the detail of the late method be show ; the novel vi-slam method be review and extend . the four critical technique of v-slam and their technical difﬁculties be summa- rized in context , include feature detection and matching , the selection of keyframes , uncertainty technology , and expression of map . the key result', 'and their technical difﬁculties be summa- rized in context , include feature detection and matching , the selection of keyframes , uncertainty technology , and expression of map . the key result be extract from the literature . finally , the development direction and need of v-slam research be propose for high accuracy positioning , a low runtime , and strong robustness.sensors 2022 ,22 , 4582 26 of 29 author contributions : conceptualization and methodology , w.x . and d.z . ; validation , w.x . and g.j . ; formal analysis , x.l . and h.l . ; investigation , g.j . ; resource , m.c . ; data curation , x.l . ; writing— original draft preparation , g.j. , d.z . and x.l . ; writing—review and editing , w.x. , h.l . and m.c . ; supervision , w.x. , y.s . and m.c . ; project administration , y.s . and m.c . ; fund acquisition , w.x . and m.c . all author have read and agree to the publish version of the manuscript . funding : the research work present in this study be ﬁnancially support by the china postdoc- toral science foundation ( 2021m701096 ) ; the scientiﬁc and technological project in henan province ( 212102210500 , 222102220090 , 222102210195 ) ; and the national natural science foundation of china ( 51875012 ) , the fundamental research funds for the central universities ( ywf-21-bj-j-613 ) , and the beijing outstanding young scientists program ( bjjwzyjh01201910006021 ) . institutional review board statement : not applicable . informed consent statement : not applicable . data availability statement : not applicable . conﬂicts of interest : the author declare no conﬂict of interest . references 1 . leonard , j.j. ; durrant-whyte , h.f . simultaneous map building , and localization for an autonomous mobile robot . in proceedings of the iros ‘ 91 : ieee/rsj international workshop on intelligent robots and systems ‘ 91 , osaka , japan , 3–5 november 1991 . 2 . wang , h. ; huang , s. ; khosoussi , k. ; frese , u. ; dissanayake , g. ; liu , b. dimensionality reduction for point feature slam problems with', 'systems ‘ 91 , osaka , japan , 3–5 november 1991 . 2 . wang , h. ; huang , s. ; khosoussi , k. ; frese , u. ; dissanayake , g. ; liu , b. dimensionality reduction for point feature slam problems with spherical covariance matrices . automatica 2015 ,51 , 149–157 . [ crossref ] 3 . szendy , b. ; bal ázs , e. ; szab ó-resch , m.z . ; vamossy , z . simultaneous localization and mapping with turtlebotii . in proceedings of the cinti , the 16th ieee international symposium on ieee , budapest , hungary , 19–21 november 2015 . 4 . lategahn , h. ; geiger , a. ; kitt , b . visual slam for autonomous ground vehicle . in proceedings of the ieee international conference on robotics & automation , shanghai , china , 9–13 may 2011 . 5 . beall , c. ; lawrence , b.j . ; ila , v . ; dellaert , f. 3d reconstruction of underwater structures . in proceedings of the ieee/rsj international conference on intelligent robots & systems , taipei , taiwan , 18–22 october 2010 . 6 . kim , a. ; eustice , r.m . real-time visual slam for autonomous underwater hull inspection using visual saliency . ieee trans . robot . 2013 ,29 , 719–733 . [ crossref ] 7 . li , r. ; liu , j. ; zhang , l. ; hang , y. lidar/mems imu integrated navigation ( slam ) method for a small uav in indoor environments . in proceedings of the 2014 dgon inertial sensors and systems ( iss ) , karlsruhe , germany , 16–17 september 2014 . 8 . marzat , j. ; bertrand , s. ; eudes , a. ; sanfourche , m. ; moras , j. reactive mpc for autonomous mav navigation in indoor cluttered environments : flight experiments . ifac-papersonline 2017 ,50 , 15996–16002 . [ crossref ] 9 . smith , r. ; cheeseman , p . on the representation of spatial uncertainty . int . j . robot res . 1986 ,5 , 56–68 . [ crossref ] 10 . smith , r. ; self , m. ; cheeseman , p . estimating uncertain spatial relationships in robotics . mach . intell . pattern recognit . 1988 ,5 , 435–461 . 11 . durrant-whyte , h. ; bailey , t. simultaneous localization and mapping : part , i. ieee robot . autom . mag . 2006 ,13 ,', 'in robotics . mach . intell . pattern recognit . 1988 ,5 , 435–461 . 11 . durrant-whyte , h. ; bailey , t. simultaneous localization and mapping : part , i. ieee robot . autom . mag . 2006 ,13 , 99–110 . [ crossref ] 12 . durrant-whyte , h. ; bailey , t. simultaneous localization and mapping ( slam ) : part ii . ieee robot . autom . mag . 2006 ,13 , 108–117 . [ crossref ] 13 . csorba , m. simultaneous localisation and map building ; university of oxford : oxford , uk , 1997 . 14 . kootstra , g. guest editorial : special issue on visual slam . ieee trans . robot . 2008 ,24 , 929–931 . 15 . davison , a.j . ; reid , i.d . ; molton , n.d. ; stasse , o. monoslam : real-time single camera slam . ieee trans . pattern anal . mach . intell . 2007 ,29 , 1052–1067 . [ crossref ] 16 . klein , g. ; murray , d. parallel tracking and mapping for small ar workspaces . in proceedings of the 2007 6th ieee and acm international symposium on mixed and augmented reality , nara , japan , 13–16 november 2007 ; pp . 225–234 . 17 . triggs , b. ; mclauchlan , p .f . ; hartley , r.i. ; fitzgibbon , a.w . bundle adjustment-a modern synthesis . in vision algorithms : theory and practice . iwv a 1999 . lecture notes in computer science ; triggs , b. , zisserman , a. , szeliski , r. , eds . ; springer : berlin/heidelberg , germany , 1999 . 18 . mur-artal , r. ; montiel , j.m.m . ; tardos , j.d . orb-slam : a versatile and accurate monocular slam system . ieee trans . robot . 2015 ,31 , 1147–1163 . [ crossref ] 19 . engel , j. ; schps , t. ; cremers , d. lsd-slam : large-scale direct monocular slam ; springer : cham , switzerland , 2014 ; pp . 834–849 . 20 . engel , j. ; koltun , v . ; cremers , d. direct sparse odometry . ieee trans . pattern anal . mach . intell . 2018 ,40 , 611–625 . [ crossref ] [ pubmed ] 21 . tong , q. ; li , p . ; shen , s. vins-mono : a robust and versatile monocular visual-inertial state estimator . ieee trans . robot . 2017 , 34 , 1004–1020.sensors 2022 ,22 , 4582 27 of 29 22 . lynen , s. ; sattler , t. ; bosse , m.', 'vins-mono : a robust and versatile monocular visual-inertial state estimator . ieee trans . robot . 2017 , 34 , 1004–1020.sensors 2022 ,22 , 4582 27 of 29 22 . lynen , s. ; sattler , t. ; bosse , m. ; hesch , j. ; pollefeys , m. ; siegwart , r. get out of my lab : large-scale , real-time visual-inertial localization . robotics : science & systems ; mit press : rome , italy , 2015 . 23 . rosinol , a. ; abate , m. ; chang , y. ; carlone , l. kimera : an open-source library for real-time metric-semantic localization and mapping . in proceedings of the 2020 ieee international conference on robotics and automation ( icra ) , paris , france , 31 may–31 august 2020 ; pp . 1689–1696 . 24 . rosinol , a. ; violette , a. ; abate , m. ; hughes , n. ; chang , y. ; shi , j. kimera : from slam to spatial perception with 3d dynamic scene graphs . ind . robot 2021 ,40 , 1510–1546 . [ crossref ] 25 . nister , d. ; naroditsky , o. ; bergen , j.r . visual odometry . in proceedings of the ieee computer society conference on computer vision & pattern recognition , washington , dc , usa , 27 june–2 july 2004 . 26 . huang , b. ; zhao , j. ; liu , j . a survey of simultaneous localization and mapping with an envision in 6g wireless networks . arxiv 2019 , arxiv:1909.05214 . 27 . liu , h. ; zhang , g. ; bao , h. a survey of monocular simultaneous localization and mapping . j. comput . aided des . comput . graph . 2016 ,28 , 855–868 . 28 . khoshelham , k. ; elberink , s.o . accuracy and resolution of kinect depth data for indoor mapping applications . sensors 2012 , 12 , 1437 . [ crossref ] 29 . henry , p . ; krainin , m. ; herbst , e. ; ren , x. ; fox , d. rgb-d mapping : using depth cameras for dense 3d modeling of indoor environments . exp . robot . 2014 ,79 , 477–491 . 30 . grisetti , g. ; kümmerle , r. ; strasdat , h. ; konolige , k. g2o : a general framework for ( hyper ) graph optimization . in proceedings of the 2011 ieee international conference on robotics and automation , shanghai , china , 9–13 may 2011 ; pp . 3607–3613 . 31 .', ': a general framework for ( hyper ) graph optimization . in proceedings of the 2011 ieee international conference on robotics and automation , shanghai , china , 9–13 may 2011 ; pp . 3607–3613 . 31 . endres , f. ; hess , j. ; sturm , j. ; cremers , d. ; burgard , w. 3-d mapping with an rgb-d camera . ieee trans . robot . 2014 ,30 , 177–187 . [ crossref ] 32 . xin , j. ; gou , j. ; ma , x. ; huang , k. ; zhang , y . a large viewing angle 3-dimensional v-slam algorithm with a kinect-based mobile robot system . jiqiren/robot 2014 ,36 , 560–568 . 33 . dryanovski , i. ; valenti , r.g . ; xiao , j . fast visual odometry and mapping from rgb-d data . in proceedings of the 2013 ieee international conference on robotics and automation , karlsruhe , germany , 6–10 may 2013 ; pp . 2305–2310 . 34 . stowers , j. ; hayes , m. ; bainbridge-smith , a. altitude control of a quadrotor helicopter using depth map from microsoft kinect sensor . in proceedings of the 2011 ieee international conference on mechatronics , istanbul , turkey , 13–15 april 2011 ; pp . 358–362 . 35 . forster , c. ; pizzoli , m. ; scaramuzza , d. svo : fast semi-direct monocular visual odometry . in proceedings of the 2014 ieee international conference on robotics and automation ( icra ) , hong kong , china , 31 may–7 june 2014 ; pp . 15–22 . 36 . myriam , s. ; val érie , r. ; alexis , d. ; nicolas , a . visual and visual-inertial slam : state of the art , classiﬁcation , and experimental benchmarking . j. sens . 2021 ,2021 , 2054828 . 37 . debeunne , c. ; vivet , d. a review of visual-lidar fusion based simultaneous localization and mapping . sensors 2020 ,20 , 2068 . [ crossref ] [ pubmed ] 38 . jung , j.h . ; cha , j. ; chung , j.y . monocular visual-inertial-wheel odometry using low-grade imu in urban areas . ieee trans . intell . transp . syst . 2020 ,23 , 925–938 . [ crossref ] 39 . fu , d. ; xia , h. ; qiao , y. monocular visual-inertial navigation for dynamic environment . remote sens . 2021 ,13 , 1610 . [ crossref ] 40 . cheng , j. ; zhang , l. ;', ', 925–938 . [ crossref ] 39 . fu , d. ; xia , h. ; qiao , y. monocular visual-inertial navigation for dynamic environment . remote sens . 2021 ,13 , 1610 . [ crossref ] 40 . cheng , j. ; zhang , l. ; chen , q . an improved initialization method for monocular visual-inertial slam . electronics 2021 ,10 , 3063 . [ crossref ] 41 . chai , w. ; li , c. ; zhang , m. an enhanced pedestrian visual-inertial slam system aided with vanishing point in indoor environments . sensors 2021 ,21 , 7428 . [ crossref ] 42 . xu , c. ; liu , z. ; li , z . robust visual-inertial navigation system for low precision sensors under indoor and outdoor environ- ments . remote sens . 2021 ,13 , 772 . [ crossref ] 43 . eckenhoff , k. ; geneva , p . ; bloecker , j. multi-camera visual-inertial navigation with online intrinsic and extrinsic calibration . in proceedings of the 2019 international conference on robotics and automation ( icra ) , montreal , canada , 20–24 may 2019 ; pp . 3158–3164 . 44 . chen , c. ; wang , l. ; zhu , h. keyframe-based stereo visual-inertial slam use nonlinear optimization . in proceedings of the global intelligence industry conference ( giic 2018 ) , beijing , china , 21–23 may 2018 ; volume 10835 , pp . 171–179 . 45 . shan , t. ; englot , b. ; ratti , c. lvi-sam : tightly-coupled lidar-visual-inertial odometry via smoothing and mapping . in proceedings of the 2021 ieee international conference on robotics and automation ( icra ) , xi ’ an , china , 30 may–5 june 2021 ; pp . 5692–5698 . 46 . zhang , z. ; zhao , r. ; liu , e. scale estimation and correction of the monocular simultaneous localization and mapping ( slam ) based on fusion of 1d laser range finder and vision data . sensors 2018 ,18 , 1948 . [ crossref ] 47 . yang , b. ; li , j. ; zhang , h. resilient indoor localization system based on uwb and visual–inertial sensors for complex environments . ieee trans . instrum . meas . 2021 ,70 , 1–14 . [ crossref ] 48 . cox , i.j . ; wilfong , g.t . autonomous robot vehicles ; springer : new york , ny , usa , 1990', 'sensors for complex environments . ieee trans . instrum . meas . 2021 ,70 , 1–14 . [ crossref ] 48 . cox , i.j . ; wilfong , g.t . autonomous robot vehicles ; springer : new york , ny , usa , 1990 . 49 . moutarlier , p . ; chatila , r. an experimental system for incremental environment modelling by an autonomous mobile robot . lect . notes control . inf . sci . 1990 ,139 , 327–346.sensors 2022 ,22 , 4582 28 of 29 50 . julier , s.j . ; uhlmann , j.k. a counter example to the theory of simultaneous localization and map building . in proceedings of the 2001 icra . ieee international conference on robotics and automation , seoul , korea , 21–26 may 2001 ; volume 4 , pp . 4238–4243 . 51 . gordon , n.j. ; salmond , d.j . ; smith , a . novel approach to nonlinear/non-gaussian bayesian state estimation . ieee proc . part f radar signal process . 1993 ,140 , 107–113 . [ crossref ] 52 . montemarlo , m. ; thrun , s. ; koller , d. ; wegbreit , b . fast slam : a factored solution to the simultaneous localization and mapping problem . in proceedings of the eighteenth national conference on artiﬁcial intelligence ( aaai-02 ) , edmonton , ab , canada , 28 july–1 august 2002 ; pp . 593–598 . 53 . montemerlo , m. ; thrun , s. ; koller , d. ; wegbreit , b . fast slam 2.0 : an improved particle ﬁltering algorithm for simultaneous localization and mapping that provably converge . in proceedings of the 18th international joint conference on artiﬁcial intelligence , san francisco , ca , usa , 9–15 august 2003 ; pp . 1151–1156 . 54 . murphy , k. ; russell , s. rao-blackwellised particle filtering for dynamic bayesian networks . in statistics for engineering and information science ; springer : new york , ny , usa , 2001 ; pp . 499–515 . 55 . thrun , s.b . ; montemerlo , m.s . the graph slam algorithm with applications to large-scale mapping of urban structures . int . j . robot . res . 2006 ,25 , 403–429 . [ crossref ] 56 . golfarelli , m. ; maio , d. ; rizzi , s. elastic correction of dead-reckoning errors in map building . in', 'mapping of urban structures . int . j . robot . res . 2006 ,25 , 403–429 . [ crossref ] 56 . golfarelli , m. ; maio , d. ; rizzi , s. elastic correction of dead-reckoning errors in map building . in proceedings of the 1998 ieee/rsj international conference on intelligent robots and systems . innovations in theory , practice and applications , victoria , bc , canada , 17 october 1998 ; volume 2 , pp . 905–911 . 57 . williams , b. ; cummins , m. ; neira , j. ; newman , p . ; reida , i. ; tard ósb , j . a comparison of loop closing technique in monocular slam . robot auton . syst . 2009 ,57 , 1188–1197 . [ crossref ] 58 . mur-artal , r. ; tard ós , j.d . orb-slam : tracking and mapping recognizable . in proceedings of the workshop on multi view geometry in robotics ( mvigro ) -rss 2014 , berkeley , ca , usa , 13 july 2014 . 59 . cummins , m. ; newman , p . accelerated appearance-only slam . in proceedings of the ieee international conference on robotics and automation , pasadena , ca , usa , 19–23 may 2008 ; pp . 1828–1833 . 60 . clemente , l.a. ; davison , a.j . ; reid , i.d . mapping large loops with a single hand-held camera . in robotics : science and systems iii ; mit press : cambridge , ma , usa , 2008 ; pp . 297–304 . 61 . williams , b.p . ; cummins , m. ; neira , j. ; newman , p .m . ; tard ós , j.d . an image-to-map loop closing method for monocular slam . in proceedings of the 2008 ieee/rsj international conference on intelligent robots and systems , nice , france , 22–26 september 2008 ; pp . 2053–2059 . 62 . cummins , m. ; newman , p . fab-map : probabilistic localization and mapping in the space of appearance . int . j . robot res . 2008 , 27 , 647–665 . [ crossref ] 63 . mur-artal , r. ; tard ós , j.d . fast relocalisation and loop closing in keyframe-based slam . in proceedings of the 2014 ieee international conference on robotics and automation ( icra ) , hong kong , china , 31 may–7 june 2014 ; pp . 846–853 . 64 . angeli , a. ; filliat , d. ; doncieux , s. fast and incremental method for', 'international conference on robotics and automation ( icra ) , hong kong , china , 31 may–7 june 2014 ; pp . 846–853 . 64 . angeli , a. ; filliat , d. ; doncieux , s. fast and incremental method for loop-closure detection using bags of visual words . ieee trans . robot . 2008 ,24 , 1027–1037 . [ crossref ] 65 . cummins , m. ; newman , p . highly scalable appearance-only slam –fab-map 2.0 . proc . robot . sci . syst . 2009 ,5 , 1–8 . 66 . galvez-lpez , d. ; tardos , j.d . bags of binary words for fast place recognition in image sequences . ieee trans . robot . 2012 ,28 , 1188–1197 . [ crossref ] 67 . gálvez-l ópez , d. ; tard ós , j.d . real-time loop detection with bags of binary words . in proceedings of the 2011 ieee/rsj international conference on intelligent robots and systems , san francisco , ca , usa , 25–30 september 2011 ; pp . 51–58 . 68 . nistér , d. ; stew énius , h. scalable recognition with a vocabulary tree . in proceedings of the 2006 ieee computer society conference on computer vision and pattern recognition ( cvpr ’ 06 ) , new york , ny , usa , 17–22 june 2006 ; pp . 2161–2168 . 69 . smith , m. ; baldwin , i. ; churchill , w. ; paul , r. ; newman , p . the new college vision and laser data set . int . j . robot . res . 2009 ,28 , 595–599 . [ crossref ] 70 . bonarini , a. ; burgard , w. ; fontana , g. ; matteucci , m. ; sorrenti , d.g . ; tardos , j.d . rawseeds : robotics advancement through web-publishing of sensorial and elaborated extensive data sets . in proceedings of the iros ’ 06 workshop on benchmarks in robotics research , beijing , china , 9–15 october 2006 ; pp . 1–5 . 71 . pandey , g. ; mcbride , j.r. ; eustice , r.m . ford campus vision and lidar data set . int . j . robot . res . 2011 ,30 , 1543–1552 . [ crossref ] 72 . blanco , j.l . ; moreno , f.a . ; gonzalez , j . a collection of outdoor robotic datasets with centimeter-accuracy ground truth . kluwer acad . publ . 2009 ,27 , 327–351 . [ crossref ] 73 . cummins , m. ; newman , p . appearance-only slam at large scale with', 'of outdoor robotic datasets with centimeter-accuracy ground truth . kluwer acad . publ . 2009 ,27 , 327–351 . [ crossref ] 73 . cummins , m. ; newman , p . appearance-only slam at large scale with fab-map 2.0 . int . robot . res . 2011 ,30 , 1100–1123 . [ crossref ] 74 . klein , g. ; murray , d. parallel tracking and mapping on a camera phone . in proceedings of the 2009 8th ieee international symposium on mixed and augmented reality , orlando , fl , usa , 19–22 october 2009 ; pp . 83–86 . 75 . mur-artal , r. ; tardos , j.d . orb-slam2 : an open-source slam system for monocular , stereo and rgb-d cameras . ieee trans . robot . 2017 ,33 , 1255–1262 . [ crossref ] 76 . bescos , b. ; f ácil , j.m . ; civera , j. dynaslam : tracking , mapping , and inpainting in dynamic scenes . ieee robot . autom . let . 2018 , 3 , 4076–4083 . [ crossref ] sensors 2022 ,22 , 4582 29 of 29 77 . nguyen , d.d . ; elouardi , a. ; florez , s.a.r . hoofr slam system : an embedded vision slam algorithm and it hard-ware- software mapping-based intelligent vehicles applications . ieee trans . intell . transp . 2018 ,20 , 4103–4118 . [ crossref ] 78 . gomez-ojeda , r. ; moreno , f.a . ; zuniga-noël , d. pl-slam : a stereo slam system through the combination of points and line segments . ieee trans . robot . 2019 ,35 , 734–746 . [ crossref ] 79 . yang , s. ; scherer , s. cubeslam : monocular 3-d object slam . ieee trans . robot . 2019 ,35 , 925–938 . [ crossref ] 80 . lajoie , p .y . ; ramtoula , b. ; chang , y. door-slam : distributed , online , and outlier resilient slam for robotic teams . ieee robot . autom . let . 2020 ,5 , 1656–1663 . [ crossref ] 81 . wang , c. ; luo , b. ; zhang , y. dymslam : 4d dynamic scene reconstruction based on geometrical motion segmentation . ieee robot . autom . let . 2020 ,6 , 550–557 . [ crossref ] 82 . ince , o.f . ; kim , j.s . tima slam : tracking independently and mapping altogether for an uncalibrated multi-camera system . sensors 2021 ,21 , 409 . [ crossref ] 83 . dong , x. ; cheng , l. ; peng , h.', ', o.f . ; kim , j.s . tima slam : tracking independently and mapping altogether for an uncalibrated multi-camera system . sensors 2021 ,21 , 409 . [ crossref ] 83 . dong , x. ; cheng , l. ; peng , h. fsd-slam : a fast semi-direct slam algorithm . complex intell . syst . 2022 ,8 , 1823–1834 . [ crossref ] 84 . wang , j. ; rünz , m. ; agapito , l. dsp-slam : object oriented slam with deep shape priors . in proceedings of the 2021 international conference on 3d vision ( 3dv ) , london , uk , 1–3 december 2021 ; pp . 1362–1371 . 85 . ali , a.m. ; nordin , m.j. sift based monocular slam with multi-clouds features for indoor navigation . in proceedings of the tencon 2010—2010 ieee region 10 conference , fukuoka , japan , 21–24 november 2010 ; pp . 2326–2331 . 86 . zhang , z. ; huang , y. ; li , c. ; kang , y. monocular vision simultaneous localization and mapping using surf . in proceedings of the 2008 7th world congress on intelligent control and automation , chongqing , china , 25–27 june 2008 ; pp . 1651–1656 . 87 . lowe , d.g . distinctive image features from scale-invariant keypoints . int . j. comput . vis . 2004 ,60 , 91–110 . [ crossref ] 88 . bay , h. ; ess , a. ; tuytelaars , t. ; gool , l. speeded-up robust features ( surf ) . comput . vis . image underst . 2008 ,110 , 346–359 . [ crossref ] 89 . rublee , e. ; rabaud , v . ; konolige , k. ; bradski , g. orb : an efﬁcient alternative to sift or surf . in proceedings of the 2011 international conference on computer vision , barcelona , spain , 6–13 november 2011 ; pp . 2564–2571 . 90 . wu , e. ; zhao , l. ; guo , y. ; zhou , w. ; wang , q. monocular vision slam base on key feature points selection . in proceedings of the 2010 ieee international conference on information and automation , harbin , china , 20–23 june 2010 ; pp . 1741–1745 . 91 . chen , c. ; chan , y. sift-based monocluar slam with inverse depth parameterization for robot localization . in proceedings of the 2007 ieee workshop on advanced robotics and its social impacts , hsinchu , taiwan , 9–11', 'sift-based monocluar slam with inverse depth parameterization for robot localization . in proceedings of the 2007 ieee workshop on advanced robotics and its social impacts , hsinchu , taiwan , 9–11 december 2007 ; pp . 1–6 . 92 . zhu , d. binocular vision-slam using improved sift algorithm . in proceedings of the 2010 2nd international workshop on intelligent systems and applications , wuhan , china , 22–23 may 2010 ; pp . 1–4 . 93 . you , y . the research of slam monocular vision based on the improved surf feather . in proceedings of the 2014 international conference on computational intelligence and communication networks , bhopal , india , 14–16 november 2014 ; pp . 344–348 . 94 . wang , y. ; feng , y . data association and map management for robot slam using local invariant features . in proceedings of the 2013 ieee international conference on mechatronics and automation , takamatsu , japan , 4–7 august 2013 ; pp . 1102–1107 . 95 . rosten , e. ; drummond , t. machine learn for very high-speed corner detection . in proceedings of the eccv 2006 : computer vision—eccv 2006 , graz , austria , 7–13 may 2006 ; pp . 430–443 . 96 . calonder , m. ; lepetit , v . ; fua , p . brief : binary robust independent elementary features . lect . notes comput . sci . 2010 ,63147 , 778–792 . 97 . xin , g. ; zhang , x. ; xi , w. ; song , j . a rgbd slam algorithm combining orb with prosac for indoor mobile robot . in proceedings of the 2015 4th international conference on computer science and network technology ( iccsnt ) , harbin , china , 19–20 december 2015 ; pp . 71–74 . 98 . jun , l. ; pan , t. ; tseng , k. ; pan , j . design of a monocular simultaneous localisation and mapping system with orb feature . in proceedings of the 2013 ieee international conference on multimedia and expo ( icme ) , san jose , ca , usa , 15–19 july 2013 ; pp . 1–4 . 99 . kerl , c. ; sturm , j. ; cremers , d. dense visual slam for rgb-d cameras . in proceedings of the 2013 ieee/rsj international conference on intelligent robots and systems , tokyo ,', '; pp . 1–4 . 99 . kerl , c. ; sturm , j. ; cremers , d. dense visual slam for rgb-d cameras . in proceedings of the 2013 ieee/rsj international conference on intelligent robots and systems , tokyo , japan , 3–7 november 2013 ; pp . 2100–2106 . 100 . thrun , s. learning metric-topological map for indoor mobile robot navigation . artif . intell . 1998 ,99 , 21–71 . [ crossref ] 101 . choset , h. ; nagatani , k. topological simultaneous localization and mapping ( slam ) : toward exact localization without explicit localization . ieee trans . robot . autom . 2002 ,17 , 125–137 . [ crossref ] 102 . zwynsvoorde , d.v . ; simeon , t. ; alami , r. incremental topological modeling using local voronoï-like graphs . in pro- ceedings of the 2000 ieee/rsj international conference on intelligent robots and systems ( iros 2000 ) , takamatsu , japan , 31 october–5 november 2000 ; volume 2 , pp . 897–902 . 103 . hornung , a. ; wurm , k.m . ; bennewitz , m. ; stachniss , c. ; burgard , w. octomap : an efﬁcient probabilistic 3d mapping framework base on octrees . auton . robot . 2013 ,34 , 189–206 . [ crossref ]']",https://doi.org/10.48550/arXiv.1804.11142
2.pdf,"/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045 /gid00001 /gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046citation : chen , w. ; shang , g. ; ji , a. ; zhou , c. ; wang , x. ; xu , c. ; li , z. ; hu , k. an overview on visual slam : from tradition to semantic . remote sens . 2022 ,14 , 3010. http : //doi.org/ 10.3390/rs14133010 academic editors : fabio remondino , radosław zimroz , denis guilhot and vittorio cannas received : 29 may 2022 accepted : 17 june 2022 published : 23 june 2022 publisher ’ s note : mdpi stay neutral with regard to jurisdictional claim in publish map and institutional afﬁl- iations . copyright : © 2022 by the author . licensee mdpi , basel , switzerland . this article be an open access article distribute under the term and condition of the creative commons attribution ( cc by ) license ( http : // creativecommons.org/licenses/by/ 4.0/ ) . remote sense review an overview on visual slam : from tradition to semantic weifeng chen1,2 , guangtao shang2 , aihong ji3 , chengjun zhou2 , xiyang wang2 , chonghui xu2 , zhenxiong li2 and kai hu2 , * 1school of mechanical and electronic engineering , quanzhou university of information engineering , quanzhou 362000 , china ; 002021 @ nuist.edu.cn 2school of automation , nanjing university of information science & technology , nanjing 210044 , china ; 20201222014 @ nuist.edu.cn ( g.s . ) ; 20211257010 @ nuist.edu.cn ( c.z . ) ; 20211267006 @ nuist.edu.cn ( x.w . ) ; 20211249101 @ nuist.edu.cn ( c.x . ) ; 20211257005 @ nuist.edu.cn ( z.l . ) 3lab of locomotion bioinspiration and intelligent robots , college of mechanical and electrical engineering , nanjing university of aeronautics & astronautics , nanjing 210016 , china ; meeahji @ nuaa.edu.cn *correspondence : 001600 @ nuist.edu.cn abstract : visual slam ( vslam ) have be develop rapidly due to it advantage of low-cost sensor , the easy fusion of other sensor , and rich environmental information . traditional vision- base slam research have make many achievement , but it may fail to achieve wished result in challenging environment . deep learning have promote the development of computer vision , and the combination of deep learning and slam have attract more and more attention . semantic information , a high-level environmental information , can enable robot to well understand the surrounding environment . this paper introduce the development of vslam technology from two aspect : traditional vslam and semantic vslam combine with deep learning . for traditional vslam , we summarize the advantage and disadvantage of indirect and direct method in detail and give some classical vslam open-source algorithm . in addition , we focus on the development of semantic vslam base on deep learning . starting with typical neural network cnn and rnn , we summarize the improvement of neural network for the vslam system in detail . later , we focus on the help of target detection and semantic segmentation for vslam semantic information introduction . we believe that the development of the future intelligent era can not be without the help of semantic technology . introducing deep learning into the vslam system to provide semantic information can help robots well perceive the surrounding environment and provide people with higher-level help . keywords : slam ; deep learning ; neural network ; computer vision ; semantic ; intelligent era 1 . introduction people need the mobile robot to perform some task by themselves , which need the robot to be able to adapt to an unfamiliar environment . therefore , slam [ 1 ] ( simultaneous localization and mapping ) , which enable localization and mapping in unfamiliar environ- ments , have become a necessary capacity for autonomous mobile robot . since it be ﬁrst propose in 1986 , slam have attract extensive attention from many researcher and de- veloped rapidly in robotics , virtual reality , and other ﬁelds . slam refers to self-positioning base on location and map , and building incremental map base on self-positioning . it be mainly use to solve the problem of robot localization and map construction when move in an unknown environment [ 2 ] . slam , a a basic technology , have be apply to mobile robot localization and navigation in the early stage . with the development of computer technology ( hardware ) and artiﬁcial intelligence ( software ) , robot research have receive more and more attention and investment . numerous researcher be commit to make robot more intelligent . slam be consider to be the key to promote the real autonomy of mobile robot [ 3 ] . some scholar divide slam into laser slam and visual slam ( vslam ) accord to the different sensor adopt [ 4 ] . compared with vslam , because of an early start , laser remote sens . 2022 ,14 , 3010. http : //doi.org/10.3390/rs14133010 http : //www.mdpi.com/journal/remotesensingremote sens . 2022 ,14 , 3010 2 of 47 slam study abroad be relatively mature and have be consider the preferred solution for mobile robot for a long time in the past . similar to human eye , vslam mainly use image a the information source of environmental perception , which be more consistent with human understanding and have more information than laser slam . in recent year , camera-based vslam research have attract extensive attention from researcher . due to the advantage of cheap , easy installation , abundant environmental information , and easy fusion with other sensor , many vision-based slam algorithm have emerge [ 5 ] . vslam have the advantage of rich environmental information and be consider to be able to give mobile robot strong perceptual ability and be apply in some speciﬁc scenario . therefore , this paper focus on vslam and comb out the algorithm derive from it . slam base on all kind of laser radar be not within the scope of discussion in this paper . interested reader can refer to [ 6–8 ] and other source in the literature . as one of the solution for autonomous robot navigation , traditional vslam be essentially a simple environmental understand base on image geometric feature [ 9 ] . because traditional vslam only use the geometric feature of the environment , such a point and line , to face this low-level geometry information , it can reach a high level in real-time . facing change in lighting , texture , and dynamic object be widespread , which show the obvious shortage , in term of position precision and robustness be ﬂawed [ 10 ] . although the map construct by traditional visual slam include important information in the environment and meet the position need of the robot to a certain extent . it be inadequate in support the autonomous navigation and obstacle avoidance task of the robot . furthermore , it can not meet the interaction need of the intelligent robot with the environment and human [ 11 ] . people ’ s demand for intelligent mobile robot be increase day by day , which put forward a high need for autonomous ability and the human–computer interaction ability of robot [ 12 ] . the traditional vslam algorithm can meet the basic positioning and navigation requirement of the robot , but can not complete higher-level task such a “ help me close the bedroom door ” , “ go to the kitchen and get me an apple ” , etc . to achieve such goal , robot need to recognize information about object in the scene , ﬁnd out their location and build semantic map . with the help of semantic information , the data association be upgrade from the traditional pixel level to the object level . furthermore , the perceptual geometric environment information be assign with semantic label to obtain a high-level semantic map . it can help the robot to understand the autonomous environment and human–computer interaction [ 13 ] . we believe that the rapid development of deep learning provide a bridge for the introduction of semantic information into vslam . especially in semantic map construction , combine it with vlam can enable robot to gain high-level perception and understanding of the scene . it signiﬁcantly improve the interaction ability between robot and the environment [ 14 ] . in 2016 , cadena et al . [ 15 ] ﬁrst propose to divide the development of slam into three stage . in their description , we be in a stage of robust perception , a show in figure 1 . they describe the emphasis and contribution of slam in different time from three aspect : classical , algorithmic , and robust . ref . [ 16 ] summarize the development of vision-based slam algorithm from 2010 to 2016 and provide a toolkit to help beginner . yousif et al . [ 17 ] discuss the elementary framework of vslam and summarize several mathematical problem to help reader make the best choice . bavle et al . [ 18 ] summa- rized the robot slam technology and point out the development trend of robot scene understanding . starting from the fusion of vision and visual inertia , servieres et al . [ 19 ] re- viewed and compare important method and summarized excellent algorithm emerge in slam . azzam et al . [ 20 ] conduct a comprehensive study on feature-based method . they classiﬁed the reviewed method accord to the visual feature observe in the environment . furthermore , they also propose possible problem and solution for the development of slam in the future . ref . [ 21 ] introduces in detail the slam method base on monocular , binocular , rgb-d , and visual-inertial fusion , and give the exist problem and future direction . ref . [ 22 ] describe the opportunity and challenge of vslam fromremote sens . 2022 ,14 , 3010 3 of 47 geometry to deep learning and forecast the development prospect of vslam in the future semantic era . the classical age ( 1986 ā2004 ) the algorithmic-analysis age ( 2004 ā2015 ) the roboust-perception age ( 2015 āpresent ) theoretical framework and mathematical model of slam . observability θconsistency θconvergence and algorithm efficiency of slam . robustness of slam and ability to understand high-level environmental information . figure 1 . overview of slam development era . the development of slam have go through three main stage : theoretical framework , algorithm analysis , and advance robust perception . the time point be not strictly limit , but rather represent the development of slam at a certain stage and the hot issue that people be interested in . as you can see , there be some survey and summary of vision-based slam tech- nologies . however , most of them only focus on one aspect of vslam , without a more comprehensive summary of the development of vslam . furthermore , the above review focus more on traditional visual slam algorithm , while semantic slam combine with deep learning be not introduce in detail . so , a comprehensive review of vision-based slam algorithm be necessary to help researcher and student launch their effort at visual slam technology to obtain an overview of this large ﬁeld . to give reader a deep and more comprehensive understanding of the ﬁeld of slam , we review the history of general slam algorithm from inception to the present . in addition , we summarize the key solution drive the technological evolution of slam solution . the work of slam be describe from the formation of point problem to the most commonly used state method . rather than focus on just one aspect , we present the key main approach to show the connection between the research that have bring the slam approach to it current state . in addition , we review the evolution of slam from traditional to semantic , a perspective that cover major , interesting , and lead design approach throughout history . on this basis , we make a comprehensive summary of deep learn slam algorithm . semantic vslam be also explain in detail to help reader well understand the characteristic of semantic vslam . we think our work can help reader well understand robot environment perception . our work on semantic vslam can provide reader with a good idea and provide a useful reference for future slam research and even robot autonomous sensing . therefore , this paper comprehensively supplement and update the development of vision-based slam technology . furthermore , this paper divide the development of vision-based slam into two stage : traditional vslam and semantic vslam integrate deep learning . so reader can better understand the research hot spot of vslam and grasp the development direction of vslam . we believe the traditional phase slam problem mainly solve the framework problem of the algorithm . in the semantic era , slam focus on advanced situational awareness and system robustness in combination with deep learning . our review make the following contribution to the state of the art :  we have review the development of vision-based slam more comprehensively , we review the recent research progress in the ﬁeld of simultaneous localization and map construction base on environmental semantic information .  starting with a convolutional neural network ( cnn ) and a recurrent neural net- work ( rnn ) , we describe the application of deep learning in vslam in detail . to our knowledge , this be the ﬁrst review to introduce vslam from a neural network perspective.remote sens . 2022 ,14 , 3010 4 of 47  we describe the combination of semantic information and vslam in detail and point out the development direction of vslam in the semantic era . we mainly introduce and summarize the outstanding research achievement in the combination of semantic information and traditional visual slam in system localization and map construction , and make an in-depth comparison between traditional visual slam and semantic slam . finally , the future research direction of semantic slam be propose . speciﬁcally , in section 1 , this paper introduce the characteristic of traditional vslam in detail , include the direct method and the indirect method base on the front-end vision odometer , and make a comparison between the depth camera-based vslam and the classical vslam integrate with imu . in section 2 , this paper be divide into two part . we ﬁrstly introduce the combination of deep learning and vslam from two neural network , cnn and rnn . we believe that introduce deep learning into semantic vslam be the precondition for the development of semantic vslam . furthermore , this stage can also be regard a the beginning of semantic vslam . then , this paper describe the process of deep learn leading semantic vslam to the advanced stage from the aspect of target detection and semantic segmentation . so this paper summarize the development direction of semantic vslam from three aspect of localization , mapping , and elimination of dynamic object . in section 3 , this paper introduce some mainstream slam data set , and some outstanding laboratory in this area . in the end , we summarize the current research and point out the direction of vslam research in the future . the section table of content for this article be show in figure 2 . vslam tranditional vslam development status of slam semantic vslam conclusion and prospect monocular / stereo vslam section2 section5 section3 sensors commonly used in vslam section2.1 assessment tools and dataset section2.2 slam development analysis based on literature data section2.3 section4 section3.1 section3.1.1 vslam based on the feature-based method vslam based on direct method section3.2 visual-inertial slam rgb-d slam loosely-coupled visual-inertial section3.1.2 section3.3.1 section3.3.2 tightly-coupled visual-inertial traditional vslam deep learning with vslam modern semantic vslam semantic vslam outstanding scholars and teams section2.4 section3.3 neural networks with vslam section4.1 cnn with vslam rnn with vslam section4.1.1 section4.1.2 modern semantic vslam section4.2 image information extraction section4.2.1 semantic with location section4.2.2 semantic with mapping elimination of dynamic objects section4.2.3 section4.2.4 figure 2 . structure diagram for the rest of this paper . this paper focus on the second chapter of semantic vslam . we consider the introduction of neural network a the beginning of semantic vslam . we start with a deep neural network , describe it combination with vslam , and then explain modern semantic vslam in detail from the aspect of object detection and semantic segmentation base on deep learning , and make a summary and prospect . 2 . development status of slam 2.1 . sensors commonly used in vslam the sensor use in the vslam typically include the monocular camera , stereo camera , and rgb-d camera . the monocular camera and the stereo camera have similar principle and can be use in a wide range of indoor and outdoor environment . as a special form of camera , the rgb-d camera can directly obtain image depth mainly by actively emittingremote sens . 2022 ,14 , 3010 5 of 47 infrared structure light or calculate time-of-ﬂight ( tof ) . it be convenient to use , but sensitive to light , and can only be use indoors in most case [ 23 ] . events camera a appear in recent year , a new camera sensor , a picture of a different from the traditional camera . events camera be “ event ” , can be as simple a “ pixel brightness change ” . the change of event camera output be pixel brightness , slam algorithm base on the event camera be still only in the preliminary study stage [ 24 ] . in addition , a a classical slam system base on vision , visual-inertial fusion have achieve excellent result in many aspect . in figure 3 , we compare the main feature of different camera . camera advantage disadvantage simple structure , low cost , can be use indoor and outdoor . images alone can not determine this true scale . the farther the distance that can be measure ; it can be use indoors and outdoors . parallax calculation be very resource-intensive . can provide rich information , and also do not need to be a time-consuming or binocular depth calculation . narrow measure range , large noise , small field of vision , susceptible to sunlight interference . event camera have the advantage of low delay , high dynamic range ( hdr ) , no motion blur , very low power consumption , and low data bandwidth . single event have little effective information and sparse and incomplete data . monocular stereo event depth sensor ( ir projector +ir camera ) color camera microphone array rgb-d figure 3 . comparison between different camera . an event camera be not a speciﬁc type of camera , but a camera that can obtain “ event information ” . “ traditional camera ” work at a constant frequency and have natural drawback , such a lag , blurring , and overexposure when shoot high-speed object . however , the event camera , a neuro-based method of process information similar to the human eye , have none of these problem . 2.2 . assessment tools and dataset slam problem have be around for decade . in the past few decade , many excellent algorithm have emerge , each of which have contribute to the rapid development of slam technology to vary degree , despite it different focus . each algorithm have to be compare fairly . generally speak , we can evaluate a slam algorithm from multiple perspective such a time consumption , complexity , and accuracy . however , the most important one be that we pay the most attention to it accuracy . ate ( absolute trajectory error ) and rpe ( relative pose error ) be the two most important indicator use to evaluate the accuracy of slam . the relative pose error be use to calculate the difference of pose change in the same two-time stamp , which be suitable for estimate system drift . the absolute trajectory error directly calculate the difference between the real value of the camera pose and the estimate value of the slam system . the basic principle of ate and rpe be a follows.remote sens . 2022 ,14 , 3010 6 of 47 assumptions : the give pose estimate be d. the subscript represent time t ( or frame ) , where it be assume that the time of each frame of the estimate pose and the real pose be align , and the total number of frame be the same . ate : the absolute trajectory error be the direct difference between the estimate pose and the real pose , which can directly reﬂect the accuracy of the algorithm and the global trajectory consistency . it should be note that the estimate pose and ground truth be usually not in the same coordinate system , so we need to pair them ﬁrst : for stereo slam and rgb-d slam , the scale be uniform , so we need to calculate a transformation matrix from the estimate pose to the real pose by the least square method s2se ( 3 ) . for monocular camera with scale uncertainty , we need to calculate a similar transformation matrix s2sim ( 3 ) from the estimate pose to the real pose . so the ate of frame i be deﬁned a follow : fi : =qi","['/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045 /gid00001 /gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046citation : chen , w. ; shang , g. ; ji , a. ; zhou , c. ; wang , x. ; xu , c. ; li , z. ; hu , k. an overview on visual slam : from tradition to semantic . remote sens . 2022 ,14 , 3010. http : //doi.org/ 10.3390/rs14133010 academic editors : fabio remondino , radosław zimroz , denis guilhot and vittorio cannas received : 29 may 2022 accepted : 17 june 2022 published : 23 june 2022 publisher ’ s note : mdpi stay neutral with regard to jurisdictional claim in publish map and institutional afﬁl- iations . copyright : © 2022 by the author . licensee mdpi , basel , switzerland . this article be an open access article distribute under the term and condition of the creative commons attribution ( cc by ) license ( http : // creativecommons.org/licenses/by/ 4.0/ ) . remote sense review an overview on visual slam : from tradition to semantic weifeng chen1,2 , guangtao shang2 , aihong ji3 , chengjun zhou2 , xiyang wang2 , chonghui xu2 , zhenxiong li2 and kai hu2 , * 1school of mechanical and electronic engineering , quanzhou university of information engineering , quanzhou 362000 , china ; 002021 @ nuist.edu.cn 2school of automation , nanjing university of information science & technology , nanjing 210044 , china ; 20201222014 @ nuist.edu.cn ( g.s . ) ; 20211257010 @ nuist.edu.cn ( c.z . ) ; 20211267006 @ nuist.edu.cn ( x.w . ) ; 20211249101 @ nuist.edu.cn ( c.x . ) ; 20211257005 @ nuist.edu.cn ( z.l . ) 3lab of locomotion bioinspiration and intelligent robots , college of mechanical and electrical engineering , nanjing university of aeronautics & astronautics , nanjing 210016 , china ; meeahji @ nuaa.edu.cn *correspondence : 001600 @ nuist.edu.cn abstract : visual slam ( vslam ) have be develop rapidly due to it advantage of low-cost sensor , the easy fusion of other sensor , and rich environmental information . traditional vision- base slam research have make many', '( vslam ) have be develop rapidly due to it advantage of low-cost sensor , the easy fusion of other sensor , and rich environmental information . traditional vision- base slam research have make many achievement , but it may fail to achieve wished result in challenging environment . deep learning have promote the development of computer vision , and the combination of deep learning and slam have attract more and more attention . semantic information , a high-level environmental information , can enable robot to well understand the surrounding environment . this paper introduce the development of vslam technology from two aspect : traditional vslam and semantic vslam combine with deep learning . for traditional vslam , we summarize the advantage and disadvantage of indirect and direct method in detail and give some classical vslam open-source algorithm . in addition , we focus on the development of semantic vslam base on deep learning . starting with typical neural network cnn and rnn , we summarize the improvement of neural network for the vslam system in detail . later , we focus on the help of target detection and semantic segmentation for vslam semantic information introduction . we believe that the development of the future intelligent era can not be without the help of semantic technology . introducing deep learning into the vslam system to provide semantic information can help robots well perceive the surrounding environment and provide people with higher-level help . keywords : slam ; deep learning ; neural network ; computer vision ; semantic ; intelligent era 1 . introduction people need the mobile robot to perform some task by themselves , which need the robot to be able to adapt to an unfamiliar environment . therefore , slam [ 1 ] ( simultaneous localization and mapping ) , which enable localization and mapping in unfamiliar environ- ments , have become a necessary capacity for autonomous mobile robot . since it be ﬁrst propose in 1986 , slam have attract extensive attention from many researcher and', 'mapping in unfamiliar environ- ments , have become a necessary capacity for autonomous mobile robot . since it be ﬁrst propose in 1986 , slam have attract extensive attention from many researcher and de- veloped rapidly in robotics , virtual reality , and other ﬁelds . slam refers to self-positioning base on location and map , and building incremental map base on self-positioning . it be mainly use to solve the problem of robot localization and map construction when move in an unknown environment [ 2 ] . slam , a a basic technology , have be apply to mobile robot localization and navigation in the early stage . with the development of computer technology ( hardware ) and artiﬁcial intelligence ( software ) , robot research have receive more and more attention and investment . numerous researcher be commit to make robot more intelligent . slam be consider to be the key to promote the real autonomy of mobile robot [ 3 ] . some scholar divide slam into laser slam and visual slam ( vslam ) accord to the different sensor adopt [ 4 ] . compared with vslam , because of an early start , laser remote sens . 2022 ,14 , 3010. http : //doi.org/10.3390/rs14133010 http : //www.mdpi.com/journal/remotesensingremote sens . 2022 ,14 , 3010 2 of 47 slam study abroad be relatively mature and have be consider the preferred solution for mobile robot for a long time in the past . similar to human eye , vslam mainly use image a the information source of environmental perception , which be more consistent with human understanding and have more information than laser slam . in recent year , camera-based vslam research have attract extensive attention from researcher . due to the advantage of cheap , easy installation , abundant environmental information , and easy fusion with other sensor , many vision-based slam algorithm have emerge [ 5 ] . vslam have the advantage of rich environmental information and be consider to be able to give mobile robot strong perceptual ability and be apply in some speciﬁc scenario . therefore , this paper', '] . vslam have the advantage of rich environmental information and be consider to be able to give mobile robot strong perceptual ability and be apply in some speciﬁc scenario . therefore , this paper focus on vslam and comb out the algorithm derive from it . slam base on all kind of laser radar be not within the scope of discussion in this paper . interested reader can refer to [ 6–8 ] and other source in the literature . as one of the solution for autonomous robot navigation , traditional vslam be essentially a simple environmental understand base on image geometric feature [ 9 ] . because traditional vslam only use the geometric feature of the environment , such a point and line , to face this low-level geometry information , it can reach a high level in real-time . facing change in lighting , texture , and dynamic object be widespread , which show the obvious shortage , in term of position precision and robustness be ﬂawed [ 10 ] . although the map construct by traditional visual slam include important information in the environment and meet the position need of the robot to a certain extent . it be inadequate in support the autonomous navigation and obstacle avoidance task of the robot . furthermore , it can not meet the interaction need of the intelligent robot with the environment and human [ 11 ] . people ’ s demand for intelligent mobile robot be increase day by day , which put forward a high need for autonomous ability and the human–computer interaction ability of robot [ 12 ] . the traditional vslam algorithm can meet the basic positioning and navigation requirement of the robot , but can not complete higher-level task such a “ help me close the bedroom door ” , “ go to the kitchen and get me an apple ” , etc . to achieve such goal , robot need to recognize information about object in the scene , ﬁnd out their location and build semantic map . with the help of semantic information , the data association be upgrade from the traditional pixel level to the object level . furthermore , the perceptual', 'out their location and build semantic map . with the help of semantic information , the data association be upgrade from the traditional pixel level to the object level . furthermore , the perceptual geometric environment information be assign with semantic label to obtain a high-level semantic map . it can help the robot to understand the autonomous environment and human–computer interaction [ 13 ] . we believe that the rapid development of deep learning provide a bridge for the introduction of semantic information into vslam . especially in semantic map construction , combine it with vlam can enable robot to gain high-level perception and understanding of the scene . it signiﬁcantly improve the interaction ability between robot and the environment [ 14 ] . in 2016 , cadena et al . [ 15 ] ﬁrst propose to divide the development of slam into three stage . in their description , we be in a stage of robust perception , a show in figure 1 . they describe the emphasis and contribution of slam in different time from three aspect : classical , algorithmic , and robust . ref . [ 16 ] summarize the development of vision-based slam algorithm from 2010 to 2016 and provide a toolkit to help beginner . yousif et al . [ 17 ] discuss the elementary framework of vslam and summarize several mathematical problem to help reader make the best choice . bavle et al . [ 18 ] summa- rized the robot slam technology and point out the development trend of robot scene understanding . starting from the fusion of vision and visual inertia , servieres et al . [ 19 ] re- viewed and compare important method and summarized excellent algorithm emerge in slam . azzam et al . [ 20 ] conduct a comprehensive study on feature-based method . they classiﬁed the reviewed method accord to the visual feature observe in the environment . furthermore , they also propose possible problem and solution for the development of slam in the future . ref . [ 21 ] introduces in detail the slam method base on monocular , binocular , rgb-d , and visual-inertial fusion', 'propose possible problem and solution for the development of slam in the future . ref . [ 21 ] introduces in detail the slam method base on monocular , binocular , rgb-d , and visual-inertial fusion , and give the exist problem and future direction . ref . [ 22 ] describe the opportunity and challenge of vslam fromremote sens . 2022 ,14 , 3010 3 of 47 geometry to deep learning and forecast the development prospect of vslam in the future semantic era . the classical age ( 1986 ā2004 ) the algorithmic-analysis age ( 2004 ā2015 ) the roboust-perception age ( 2015 āpresent ) theoretical framework and mathematical model of slam . observability θconsistency θconvergence and algorithm efficiency of slam . robustness of slam and ability to understand high-level environmental information . figure 1 . overview of slam development era . the development of slam have go through three main stage : theoretical framework , algorithm analysis , and advance robust perception . the time point be not strictly limit , but rather represent the development of slam at a certain stage and the hot issue that people be interested in . as you can see , there be some survey and summary of vision-based slam tech- nologies . however , most of them only focus on one aspect of vslam , without a more comprehensive summary of the development of vslam . furthermore , the above review focus more on traditional visual slam algorithm , while semantic slam combine with deep learning be not introduce in detail . so , a comprehensive review of vision-based slam algorithm be necessary to help researcher and student launch their effort at visual slam technology to obtain an overview of this large ﬁeld . to give reader a deep and more comprehensive understanding of the ﬁeld of slam , we review the history of general slam algorithm from inception to the present . in addition , we summarize the key solution drive the technological evolution of slam solution . the work of slam be describe from the formation of point problem to the most commonly used state', '. in addition , we summarize the key solution drive the technological evolution of slam solution . the work of slam be describe from the formation of point problem to the most commonly used state method . rather than focus on just one aspect , we present the key main approach to show the connection between the research that have bring the slam approach to it current state . in addition , we review the evolution of slam from traditional to semantic , a perspective that cover major , interesting , and lead design approach throughout history . on this basis , we make a comprehensive summary of deep learn slam algorithm . semantic vslam be also explain in detail to help reader well understand the characteristic of semantic vslam . we think our work can help reader well understand robot environment perception . our work on semantic vslam can provide reader with a good idea and provide a useful reference for future slam research and even robot autonomous sensing . therefore , this paper comprehensively supplement and update the development of vision-based slam technology . furthermore , this paper divide the development of vision-based slam into two stage : traditional vslam and semantic vslam integrate deep learning . so reader can better understand the research hot spot of vslam and grasp the development direction of vslam . we believe the traditional phase slam problem mainly solve the framework problem of the algorithm . in the semantic era , slam focus on advanced situational awareness and system robustness in combination with deep learning . our review make the following contribution to the state of the art : \\x0f we have review the development of vision-based slam more comprehensively , we review the recent research progress in the ﬁeld of simultaneous localization and map construction base on environmental semantic information . \\x0f starting with a convolutional neural network ( cnn ) and a recurrent neural net- work ( rnn ) , we describe the application of deep learning in vslam in detail . to our knowledge , this', '. \\x0f starting with a convolutional neural network ( cnn ) and a recurrent neural net- work ( rnn ) , we describe the application of deep learning in vslam in detail . to our knowledge , this be the ﬁrst review to introduce vslam from a neural network perspective.remote sens . 2022 ,14 , 3010 4 of 47 \\x0f we describe the combination of semantic information and vslam in detail and point out the development direction of vslam in the semantic era . we mainly introduce and summarize the outstanding research achievement in the combination of semantic information and traditional visual slam in system localization and map construction , and make an in-depth comparison between traditional visual slam and semantic slam . finally , the future research direction of semantic slam be propose . speciﬁcally , in section 1 , this paper introduce the characteristic of traditional vslam in detail , include the direct method and the indirect method base on the front-end vision odometer , and make a comparison between the depth camera-based vslam and the classical vslam integrate with imu . in section 2 , this paper be divide into two part . we ﬁrstly introduce the combination of deep learning and vslam from two neural network , cnn and rnn . we believe that introduce deep learning into semantic vslam be the precondition for the development of semantic vslam . furthermore , this stage can also be regard a the beginning of semantic vslam . then , this paper describe the process of deep learn leading semantic vslam to the advanced stage from the aspect of target detection and semantic segmentation . so this paper summarize the development direction of semantic vslam from three aspect of localization , mapping , and elimination of dynamic object . in section 3 , this paper introduce some mainstream slam data set , and some outstanding laboratory in this area . in the end , we summarize the current research and point out the direction of vslam research in the future . the section table of content for this article be show in figure 2 . vslam', 'in this area . in the end , we summarize the current research and point out the direction of vslam research in the future . the section table of content for this article be show in figure 2 . vslam tranditional vslam development status of slam semantic vslam conclusion and prospect monocular / stereo vslam section2 section5 section3 sensors commonly used in vslam section2.1 assessment tools and dataset section2.2 slam development analysis based on literature data section2.3 section4 section3.1 section3.1.1 vslam based on the feature-based method vslam based on direct method section3.2 visual-inertial slam rgb-d slam loosely-coupled visual-inertial section3.1.2 section3.3.1 section3.3.2 tightly-coupled visual-inertial traditional vslam deep learning with vslam modern semantic vslam semantic vslam outstanding scholars and teams section2.4 section3.3 neural networks with vslam section4.1 cnn with vslam rnn with vslam section4.1.1 section4.1.2 modern semantic vslam section4.2 image information extraction section4.2.1 semantic with location section4.2.2 semantic with mapping elimination of dynamic objects section4.2.3 section4.2.4 figure 2 . structure diagram for the rest of this paper . this paper focus on the second chapter of semantic vslam . we consider the introduction of neural network a the beginning of semantic vslam . we start with a deep neural network , describe it combination with vslam , and then explain modern semantic vslam in detail from the aspect of object detection and semantic segmentation base on deep learning , and make a summary and prospect . 2 . development status of slam 2.1 . sensors commonly used in vslam the sensor use in the vslam typically include the monocular camera , stereo camera , and rgb-d camera . the monocular camera and the stereo camera have similar principle and can be use in a wide range of indoor and outdoor environment . as a special form of camera , the rgb-d camera can directly obtain image depth mainly by actively emittingremote sens . 2022 ,14 , 3010 5 of 47 infrared', 'wide range of indoor and outdoor environment . as a special form of camera , the rgb-d camera can directly obtain image depth mainly by actively emittingremote sens . 2022 ,14 , 3010 5 of 47 infrared structure light or calculate time-of-ﬂight ( tof ) . it be convenient to use , but sensitive to light , and can only be use indoors in most case [ 23 ] . events camera a appear in recent year , a new camera sensor , a picture of a different from the traditional camera . events camera be “ event ” , can be as simple a “ pixel brightness change ” . the change of event camera output be pixel brightness , slam algorithm base on the event camera be still only in the preliminary study stage [ 24 ] . in addition , a a classical slam system base on vision , visual-inertial fusion have achieve excellent result in many aspect . in figure 3 , we compare the main feature of different camera . camera advantage disadvantage simple structure , low cost , can be use indoor and outdoor . images alone can not determine this true scale . the farther the distance that can be measure ; it can be use indoors and outdoors . parallax calculation be very resource-intensive . can provide rich information , and also do not need to be a time-consuming or binocular depth calculation . narrow measure range , large noise , small field of vision , susceptible to sunlight interference . event camera have the advantage of low delay , high dynamic range ( hdr ) , no motion blur , very low power consumption , and low data bandwidth . single event have little effective information and sparse and incomplete data . monocular stereo event depth sensor ( ir projector +ir camera ) color camera microphone array rgb-d figure 3 . comparison between different camera . an event camera be not a speciﬁc type of camera , but a camera that can obtain “ event information ” . “ traditional camera ” work at a constant frequency and have natural drawback , such a lag , blurring , and overexposure when shoot high-speed object . however , the event camera , a neuro-based', '” . “ traditional camera ” work at a constant frequency and have natural drawback , such a lag , blurring , and overexposure when shoot high-speed object . however , the event camera , a neuro-based method of process information similar to the human eye , have none of these problem . 2.2 . assessment tools and dataset slam problem have be around for decade . in the past few decade , many excellent algorithm have emerge , each of which have contribute to the rapid development of slam technology to vary degree , despite it different focus . each algorithm have to be compare fairly . generally speak , we can evaluate a slam algorithm from multiple perspective such a time consumption , complexity , and accuracy . however , the most important one be that we pay the most attention to it accuracy . ate ( absolute trajectory error ) and rpe ( relative pose error ) be the two most important indicator use to evaluate the accuracy of slam . the relative pose error be use to calculate the difference of pose change in the same two-time stamp , which be suitable for estimate system drift . the absolute trajectory error directly calculate the difference between the real value of the camera pose and the estimate value of the slam system . the basic principle of ate and rpe be a follows.remote sens . 2022 ,14 , 3010 6 of 47 assumptions : the give pose estimate be d. the subscript represent time t ( or frame ) , where it be assume that the time of each frame of the estimate pose and the real pose be align , and the total number of frame be the same . ate : the absolute trajectory error be the direct difference between the estimate pose and the real pose , which can directly reﬂect the accuracy of the algorithm and the global trajectory consistency . it should be note that the estimate pose and ground truth be usually not in the same coordinate system , so we need to pair them ﬁrst : for stereo slam and rgb-d slam , the scale be uniform , so we need to calculate a transformation matrix from the estimate pose to the real pose by', 'coordinate system , so we need to pair them ﬁrst : for stereo slam and rgb-d slam , the scale be uniform , so we need to calculate a transformation matrix from the estimate pose to the real pose by the least square method s2se ( 3 ) . for monocular camera with scale uncertainty , we need to calculate a similar transformation matrix s2sim ( 3 ) from the estimate pose to the real pose . so the ate of frame i be deﬁned a follow : fi : =qi\\x001spi ( 1 ) similar to rpe , rmse be recommend for ate statistic . rmse ( f1 : n , d ) : = ( 1 mm å i=1ktrans ( fi ) k2 ) 1 2 ( 2 ) rpe : relative pose error mainly describe the accuracy ( compare with real pose ) of two frame separate by a ﬁxed time difference d , which be equivalent to the error of the odometer directly measure . so the rpe of the frame i be deﬁned a follow : ei : = ( qi\\x001qi+d ) \\x001 ( pi\\x001pi+d ) ( 3 ) given the total number n and the interval d , we can obtain ( m=n\\x00d ) rpe.then we can use the root mean square error rmse to calculate this error and obtain a popula- tion value : rmse ( e1 : n , d=1 mm å i=1ktrans ( ei ) k2 ) 1 2 ( 4 ) trans ( ei ) represent the translation part of the relative pose error . we can evaluate the performance of the algorithm from the size of the rmse value . however , in practice , we ﬁnd that there be many choice for the selection of d. to comprehensively measure the performance of the algorithm , we can calculate the average rmse traverse all d : rmse = ( e1 : n=1 nn å d=1rmse ( e1 : n , d ) ( 5 ) evo [ 25 ] be an evaluation tool for the python version of the slam system that can be use with a variety of data set . in addition to ate and rpe , data can be obtain , it can also draw a comparison diagram of the test algorithm and real trajectory . is a very convenient assessment kit . slambench2 [ 26 ] be a publicly available software framework that evaluate current and future slam system through an extensible list of data set . it include open and close source code while use a comparable and speciﬁed list of performance metric . it', 'that evaluate current and future slam system through an extensible list of data set . it include open and close source code while use a comparable and speciﬁed list of performance metric . it support a variety of exist slam algorithm and datasets , such a elasticfusion [ 27 ] , orb-slam2 [ 28 ] , and okvis [ 29 ] , and integrate new slam algorithm and datasets be straightforward . in addition , we also need to use datasets to test speciﬁc visualization of the algorithm . common data set use to test various aspect of slam performance be illustrate in table 1 . tum data set mainly include multi-view data set , 3d object recognition and segmentation , scene recognition , 3d model matching , vsalm , and other data in various direction . according to the direction apply , it can be divide into tum rgb-d [ 30 ] , tum monovo [ 31 ] , and tum vi [ 32 ] . among them , the tum rgb-d data set mainly contain indoor image with real ground track . furthermore , it provide two measure to evaluate local accuracy and global consistency of orbit , namely relative attitude error and absolute trajectory error . tum monovo be use to assess monocular system , whichremote sens . 2022 ,14 , 3010 7 of 47 contain both indoor and outdoor image . due to the variety of scenario , ground au- thenticity be not available , but rather large sequence with the same starting position be perform , allow evaluation of cyclic drift . tum vi be employ to the evaluation of the visual-inertial odometer . the kitti [ 33 ] dataset be a famed outdoor environment data set jointly found by the karlsruhe institute of technology and toyota american institute of technology . it be the large computer vision algorithm evaluation data set in the world under autonomous drive scenario , include monocular vision , binocular vision , velodyne lidar , pos trajectory , etc . it be the most widely use outdoor data set . the euroc [ 34 ] dataset a visual inertia data set develop by eth zurich . cityscapes [ 35 ] be a dataset relate to autonomous driving , focus on', ', etc . it be the most widely use outdoor data set . the euroc [ 34 ] dataset a visual inertia data set develop by eth zurich . cityscapes [ 35 ] be a dataset relate to autonomous driving , focus on pixel-level scene segmentation and instance annotation . in addition , many datasets be use in various scenario , such a icl-nuim [ 36 ] , nyu rgb-d [ 37 ] , ms coco [ 38 ] , etc . table 1 . common open-source datasets for slam . dataset sensor environmentground- truthavailability development cityscapes stereo outdoor gps [ 35 ] daimler ag r & d , max planck institute for informatics , tu darmstadt visaul inference group kittistereo/3d laser scanneroutdoor gps/ins [ 33 ] karlsruhe institute of technology and toyota american institute of technology tum rgb-d rgb-d indoormotion capture [ 30 ] tum monovomonocular indoor/outdoor loop drift [ 31 ] technical university of munich tum vi stereo/imu indoor/outdoormotion capture [ 32 ] euroc stereo/imu indoorstation/motion capture [ 34 ] eidgenössische technische hochschule zürich icl-nuim rgb-d indoor3d surface model slam estimation [ 36 ] imperial college lodon 2.3 . slam development analysis based on literature data since the advent of slam , it have be widely use in the ﬁeld of robotics . as show in figure 4 , this paper select about 1000 hot article relate to mobile robot in the last two decade and make this keyword heat map . the large the circle be , the high the frequency of the keyword appear . the circle layer show the time from the past to the present from the inside out , and the redder the color , the more attractive it be . connecting line indicate that there be a connection between different keywords ( data from the web of science core collection ) . as show in figure 5 , the number of citation of visual slam and semantic slam-related paper be increase rapidly . especially around 2017 , visual slam and semantic slam saw their citation skyrocket . many advance have be make in traditional vslam research . to enable robot to perceive the surround environment from a', 'around 2017 , visual slam and semantic slam saw their citation skyrocket . many advance have be make in traditional vslam research . to enable robot to perceive the surround environment from a high level , the research of semantic vslam have receive extensive attention . semantic slam have attract more and more attention in recent year . furthermore , a show in figure 6 , this paper have select about 5000 article from the web of science core collection . judging from the title of journal about slam publish , slam be a topic of interest in robotics . as can be see from the above data , slam research have always be a hot topic . with the rapid development of deep learning , the ﬁeld of computer vision have make unprecedented progress . therefore , vslam also usher in a period of rapid development . combining semantic information with vslam be go to be a hot topic for a long time . the development of semantic vslam can make robot truly autonomous.remote sens . 2022 ,14 , 3010 8 of 47 figure 4 . hot word in mobile robot ﬁeld . \\x01 ясщнъахп\\x01ψρζς вхябнш\\x01 ψρζς ӷӱӱӱ ӷӱӱӱ ӵӱӱӱ ӵӱӱӱ ӳӱӱӱ ӳӱӱӱ ӱӱӱӱ ӳӱӱӷ\\x01\\x01\\x01ӳӱӱӷ\\x01\\x01\\x01ӳӱӱӹ\\x01\\x01ӳӱӱӹ\\x01\\x01ӳӱӱӱ\\x01\\x01\\x01ӳӱӱӱ\\x01\\x01ӳӱӱӳ\\x01\\x01ӳӱӱӳ\\x01\\x01 ӳӱӱӵ\\x01\\x01\\x01ӳӱӱӵ\\x01\\x012ӱӱӷ\\x01\\x01ӳӱӱӷ\\x01ӳӱӱӹ\\x01ӳӱӱӹ\\x01ӳӱӳӱ\\x01ӳӱӳӱ θханахыъя ӱ\\x01 figure 5 . citations for web of science article on visual slam and semantic slam in recent year ( data be a of december 2021 ) . 020 40 60 80 100 120 140 160 180 200 220 240 260 280 figure 6 . publication title about slam on web of science.remote sens . 2022 ,14 , 3010 9 of 47 2.4 . outstanding scholars and teams in addition , many scholar and team have make indelible contribution to the research of slam . as show in figure 7 , we analyze approximately 4000 article from 2000 to 2022 ( data from the web of science website ) . a large font indicate that the author have receive the most attention , and vice versa . the country to which they belong be present in figure 8 . the computer vision group at the technical university of munich in germany be a leader in this ﬁeld . the team publish a variety of classic visual slam', 'to which they belong be present in figure 8 . the computer vision group at the technical university of munich in germany be a leader in this ﬁeld . the team publish a variety of classic visual slam solution such a dso [ 39 ] and lsd-slam [ 40 ] , which improve the performance of all aspect of visual slam . the robotics and perception group at the university of zurich , switzerland , also contribute to the rapid development of slam technology by develop svo and vo/vio trajectory assessment tool . in addition , the computer vision and ensemble laboratory of eth zurich have also make a lot of effort in this ﬁeld . furthermore , they have make a lot of breakthrough progress in the ﬁeld of visual semantic localization in large-scale outdoor mapping . the laboratory of robotics , sensing , and real-time group slam at the university of zaragoza in spain be one of the big contributor to the development of slam . the orb-slam series launch by the laboratory be a landmark scheme in visual slam , which have a far-reaching inﬂuence on the research of slam . in addition , the effort of many scholar and team have promote the rapid development of visual semantic slam and lay a foundation for solve various problem in the future . table 2 show the work of some excellent team and their team website for your reference , you can check the website of the team by the number quote after it name . some scholar have make outstanding contribution to semantic vslam research . niko sünderhauf [ 41 ] and their team , for example , have make many advance in robot scene understanding and semantic vslam . the team be dedicate to make a robot understand what it see be one of the most fascinating goal . to this end , they develop novel method for semantic mapping and semantic slam by combine object detection with simulta- neous localization and mapping ( slam ) technique . the team [ 42 ] of researcher be part of the australian centre for robotic vision and be base at the queensland university of technology in brisbane , australia . they work on', ""( slam ) technique . the team [ 42 ] of researcher be part of the australian centre for robotic vision and be base at the queensland university of technology in brisbane , australia . they work on novel approach to slam ( simultaneous localization and mapping ) that create semantically meaningful map by combine geomet- ric and semantic information . we believe such semantically enrich map will help robots understand our complex world and will ultimately increase the range and sophistication of interaction that robot can have in domestic and industrial deployment scenario . table 2 . some great team and their contribution . team works the dyson robotics lab at imperial college [ 43 ] code-slam [ 44 ] , elasticfusion [ 27 ] , fusion++ [ 45 ] , semanticfusion [ 46 ] computer vision group tum department of informatics technical university of munich [ 47 ] d3vo [ 48 ] , dm-vio [ 49 ] , lsd-slam [ 40 ] , ldso [ 50 ] , dso [ 39 ] autonomous intelligent systems university of freiburg [ 51 ] gmapping [ 52 ] , rgb-d slamv2 [ 53 ] hkust aerial robotics group [ 54 ] vins-mono [ 55 ] , vins-fusion [ 56 ] , event-based stereo visual odometry [ 57 ] uw robotics and state estimation lab [ 58 ] dart [ 59 ] , da-rnn [ 60 ] , rgb-d mapping [ 61 ] robotics , perception and real time group universidad de zaragoza [ 62 ] orb-slam2 [ 28 ] , real-time monocular object slam [ 63 ] remote sens . 2022 ,14 , 3010 10 of 47 r\\x07 # \\x04 ! \\x07ćzć ¡= & ? \\x15aćëćµx\\x13\\x13 ćvć\\x18\\x17 } ; +te\\x17ć-ć\\x91 ( \\x16n\\x08ć\\x11ć eu\\x02\\x18 ; + > ć \\x9c\\x02-m+\\x1b > ć ( á÷jhć a & ¡\\x94\\x15ać\\x97ć \\x88 $ óo. , bć $ ć\\x02\\x08\\x11\\x16 ( nć\\x02ć \\x97\\x15þ ? ć \\x1bqe ) \\x17\\x7f\\x8ać x ? \\x13xwć ć \\x13c\\x1ak\\x13\\x89kcćmćo\\x16\\x08\\x19\\x7fćăći\\x8e//\\x15a \\x15£\\x15\\x92 & ć\\x94ć \\x11d \\x8e\\x9f\\x02\\x08wćqćñ\\x9eûï¾\\x99\\x9ać % \\x0e % p\\x04ć\\x04ć\\x1b ; \\x02-\\x02üx\\x80\\x80\\x02ćmć \\x1bx\\x18m\\x17-\\x8c\\x02xäć\\x18ćr\\x07 z\\x07\\x0e\\x07\\x12\\x12\\x07 ć0ć r\\x07 ø % ć # ! så < ć # ć ~\\x04\\x0e\\x12'ć @ , b { @ \\x1a× , ć @ ć ã , \\x1a . { \\x1aćgćp \\x0e \\x0e\\x12ć\\x04ć0 r\\x04ć'ć ßë¿íâćòć } qu ) q\\x02t\\x1bć°ć ~ ! *s ! \\x04 # 0ćfć & \\x15i & ć & ć ; \\x02\\x1b\\x9c\\x17u ) \\x02\\x08+\\x1bć > ć æìwcć º\\x8fô \\x1ak óćùïùć # \\x0e\\x12 ! \\x04ć ćêíþçć s , b , \\x93on ćsć kànkúôć\\x06\\x0e \\x01\\x03 \\x03\\x05\\x0f \\x0f g $ \\x88 b $ gće % ć = \\x13\\x13¢ćf\\x07\\x8ds\\x07eć'ć nè\\x90.k\\x9d\\x9d\\x90ćnć ; \\x8c\\x17\\x18ć ) ć\\x80é\\x02\\x18èüć \\x8b\\x17ćõć \\x04\\x08 \\x07\\x0f \\x0f ç\\x9ec½c\\x99éýcć\\x9ać \\x12 ÿ \\x0er\\x040ćodj\\x08\\x11ć/ć =\\x15=a ? ćić |\\x13ì\\x8f¹"", ""ć æìwcć º\\x8fô \\x1ak óćùïùć # \\x0e\\x12 ! \\x04ć ćêíþçć s , b , \\x93on ćsć kànkúôć\\x06\\x0e \\x01\\x03 \\x03\\x05\\x0f \\x0f g $ \\x88 b $ gće % ć = \\x13\\x13¢ćf\\x07\\x8ds\\x07eć'ć nè\\x90.k\\x9d\\x9d\\x90ćnć ; \\x8c\\x17\\x18ć ) ć\\x80é\\x02\\x18èüć \\x8b\\x17ćõć \\x04\\x08 \\x07\\x0f \\x0f ç\\x9ec½c\\x99éýcć\\x9ać \\x12 ÿ \\x0er\\x040ćodj\\x08\\x11ć/ć =\\x15=a ? ćić |\\x13ì\\x8f¹ ćyćâ\\x1a \\x13 ć|ć ¶ $ î\\x93 úć { ć od\\x02\\x08\\x11ć\\x11ć \\x89ð\\x1a\\x1akwc¬¢mûc ćmć *ð ćeć s $ ..ć @ ć » ñ\\x95\\x95ob.ć @ ćãj/\\x91n\\x08ćuć\\x92|vv y ? ćyć\\x04 \\x04\\x0e < '\\x07r õ ! % ć ć % \\x04 * \\x0elćsć êàýć āoj\\x08\\x11ćqć0\\x8d\\x04\\x0ećzć\\x12\\x04 øć ) qć > ć dd\\x19\\x19puć äxt\\x02-+ ) \\x02\\x02ćyp ćpć \\x12\\x04*öly\\xad* ålyć < ć } \\x02\\x08\\x8a ć r ! # 'ć -\\x17mt+\\x18ć > ćj\\x08h\\x19j/ćæć\\x12 # \\x07y® '\\x07 < \\x04ć ć\\x1bò\\x18eć * ~ e0ć ·j£=ć < f\\x07 ć p\\x08h\\x16 ( u\\x02\\x08ć\\x9fć/wd\\x19ućqć\\x11\\x16p\\x11\\x16\\x19ć\\x02ć\\x06\\x0e \\x01\\x03 \\x03\\x05\\x0f \\x0f figure 7 . a distinguished scholar in the ﬁeld of visual slam . contribution lower low middlehigh higher figure 8 . contribution of different country in the slam ﬁeld ( colors from light to dark indicate contribution from low to high ) . 3 . traditional vslam cadena et al . [ 15 ] propose a classical vslam framework , which mainly consist of two part : front-end and back-end , a show in figure 9 . the front end provide real-time camera pose estimation , while the back end provide map update and optimiza- tions . speciﬁcally , mature visual slam system include sensor data collection , front-end visual odometer , back-end optimization , loop closure detection , and map construction module [ 64 ] .remote sens . 2022 ,14 , 3010 11 of 47 sensor data front-end back-end loop closure mapping visual odometry optimization feature extraction data association map estimization figure 9 . the typical visual slam system framework . 3.1 . monocular/stereo vslam in this section , we will elaborate on the vslam algorithm base on monocular or stereo camera . for the vslam system , the visual odometer , a the front-end of slam , be an indispensable part [ 65 ] . ref . [ 20 ] point out that vslam can be divide into the direct method and indirect method accord to the different image information collect by the front-end visual odometer . the indirect method need to select a certain number of representative point from the collect image , call key point , and detect and match them in the following image to"", 'front-end visual odometer . the indirect method need to select a certain number of representative point from the collect image , call key point , and detect and match them in the following image to gain the camera pose . it not only save the key information of the image but also reduce the amount of calculation , so it be widely use . the direct method use all the information of the image without preprocessing and directly operate on pixel intensity , which have high robustness in an environment with sparse texture [ 66 ] . both the indirect method and direct method have be widely concern and develop to different degree . 3.1.1 . vslam based on the feature-based method the core of indirect vslam be to detect , extract and match geometric feature ( point , line , or plane ) , estimate camera pose , and build an environment map while retain important information , it can effectively reduce calculation , so it have be widely use [ 67 ] . the vslam method base on point feature have long be take into account a the mainstream method of indirect vslam due to it simplicity and practicality [ 68 ] . feature extraction mostly adopt corner extraction method in the early , such a harris [ 69 ] , fast [ 70 ] , gftt [ 71 ] , etc . however , in many scenario , simple corner can- not provide reliable feature , which prompt researcher to seek more stable local image feature . nowadays , typical vslam method base on point feature ﬁrstly use feature detection algorithm , such a sift [ 72 ] , surf [ 73 ] , and orb [ 74 ] , to extract key point in the image for match . then gain pose after minimize reprojection error . feature point and correspond descriptor in the image be employ for data association . furthermore , data association in initialization be complete through the matching of feature descrip- tor [ 75 ] . in table 3 , we list common traditional feature extraction algorithm and compare their main performance to help reader have a more comprehensive understanding . table 3 . comparison table of commonly use feature', 'we list common traditional feature extraction algorithm and compare their main performance to help reader have a more comprehensive understanding . table 3 . comparison table of commonly use feature extraction algorithm . method year type speedrotation invariancescale invarianceillumination invarianceanti- invariance orb [ 74 ] 2011 point high yes yes yes stronger surf [ 73 ] 2008 point middle yes yes no week fast [ 70 ] 2006 point high no yes no week sift [ 72 ] 2004 point low yes yes yes strong shi-tomasi [ 71 ] 1994 coner middle yes no yes week harris [ 69 ] 1988 coner low yes no yes week lsd [ 76 ] 2010 line middle yes yes yes strongerremote sens . 2022 ,14 , 3010 12 of 47 davidson et al . [ 77 ] propose monoslam in 2007 . this algorithm be consider to be the ﬁrst real-time monocular vslam algorithm , which can achieve real-time drift free- motion structure recovery . the front end track the sparse feature point shi-tomasi corner point for feature point matching , and the back end use extended kalman filter ( ekf ) [ 78 ] for optimization , which can build the sparse environment map online in real-time . this algorithm have a milestone signiﬁcance in slam research , but the ekf method lead to a square growth between storage and state quantity , so it be not suitable for large-scale scenario . in the same year , the advent of ptam [ 79 ] improve monoslam ’ s inability to work steadily for long period in a wide range of environment . ptam , a the ﬁrst slam algorithm use nonlinear optimization at the back end , solve the problem of fast data growth in the ﬁlter-based method . furthermore , it separate track and map into two different thread for the ﬁrst time . the front end use fast corner detection to extract and estimate camera motion use image feature , and the back end be responsible for nonlinear optimization and mapping . it not only ensure the real-time performance of slam in the calculation of camera pose but also ensure the accuracy of the whole slam system . however , because there be no loopback', 'and mapping . it not only ensure the real-time performance of slam in the calculation of camera pose but also ensure the accuracy of the whole slam system . however , because there be no loopback detection module , it will accumulate error during long-running in 2015 , mur-artal et al . propose the orb-slam [ 80 ] . this algorithm be regard a the excellent successor of ptam , and base on ptam , add a loop closure detection module , which effectively reduce the cumulative error . as a real-time monocular visual slam system that use orb feature matching , the whole process be carry out around orb feature . as show in figure 10 , the three thread of track , local mapping , and loop closure detection be use innovatively . in addition , the loop closure detection thread use the word bag model dbow [ 81 ] for loop closure . the loop closure method base on the bow model can detect the loop closure quickly by detect the image similarity . furthermore , achieve good result in the processing speed and the accuracy of map construction . in late year , the team launch orb-slam2 [ 28 ] and orb-slam3 [ 82 ] . the orb-slam family be one of the most widely use visual slam solution due to it real-time cpu performance and robustness . however , the orb-slam series relies heavily on environmental feature , so it may be difﬁcult to obtain enough feature point in an environment without texture feature . the point feature-based slam system rely too much on the quality and quantity of point feature . it be difﬁcult to detect enough feature point in weak texture scene , such a corridor , window , white wall , etc . thus , affect the robustness and accuracy of the system and even lead to track failure . in addition , due to the rapid movement of the camera , illumination change , and other reason , the matching quantity and quality of point feature will decline seriously . to improve the feature-based slam algorithm , the application of line feature in slam system have attract more and more attention [ 83 ] . the commonly used line', 'feature will decline seriously . to improve the feature-based slam algorithm , the application of line feature in slam system have attract more and more attention [ 83 ] . the commonly used line feature extraction algorithm be lsd [ 76 ] . in recent year , with the improvement of computer compute capacity , vslam-based online feature have also be develop rapidly . smith et al . [ 84 ] propose a monocular vslam algorithm-based online feature extraction in 2006 . lines be represent by two endpoint , and line feature be use in the slam system to detect and track the two endpoint of line in small scene . the system can use line feature alone or in combination with point-line feature , which be of groundbreaking signiﬁcance in vslam research . in 2014 , perdices et al . propose lineslam , a line-based monocular slam algorithm [ 85 ] . for line extraction , this scheme adopt the line extraction scheme in [ 86 ] . it detect the line every time the keyframes be acquire . then use the unscented kalman filter ( ukf ) to predict the current camera state and vector probability distribution of the ground line . then , match the line prediction result with the detected line . because the scheme have no loop closure and the line segment be of inﬁnite length instead of ﬁnite length , it be difﬁcult to be use in practice.remote sens . 2022 ,14 , 3010 13 of 47 local mapping local mapping tracking tracking tracking loop closing loop closing image extract orb keyframe insertion candidates detection compute sim3 initialization and pose estimation new keyframe decision map point remove and add remove duplicate keyframes adjust and optimize local map loop detection loop fusion optimize essential graph loop correction mappoints keyframes covisibility graph spanning tree map initialization map place recognition visual vocabulary recognition database figure 10 . flow chart of orb-slam . as show in figure 11 , compare with point feature or line feature alone , the combi- nation of point feature and line feature increase the number of', 'database figure 10 . flow chart of orb-slam . as show in figure 11 , compare with point feature or line feature alone , the combi- nation of point feature and line feature increase the number of feature observation and data association . furthermore , line feature be less sensitive to light change than the point feature , which improve the positioning accuracy and robustness of the original system [ 76 ] . in 2016 , klein et al . [ 87 ] adopt the method of point-line fusion to improve the tracking failure of the slam system due to image blur cause by fast camera movement . in 2017 , pumarola et al . [ 88 ] publish monocular pl-slam , and gomez-ojeda et al . [ 89 ] publish stereo pl-slam in the same year . based on orb-slam , the two algorithm use the lsd detection algorithm to detect line feature and then combine the point-line feature in each link of slam . it can work even when most of the point feature disappear . furthermore , it improve the accuracy , robustness , and stability of the slam system , but the real-time performance be not good . figure 11 . comparison of point and line feature extraction in a weak texture environment . from leave to right be orb point feature extraction , lsd line feature extraction , and point-line combination feature extraction . in addition , in some environment , there be some obvious surface feature , which have arouse great interest of some researcher . ref . [ 90 ] propose a map construction method combine plane and line . by introduce surface feature into the real-time vslam system , the error be reduce and the system robustness be improve by com- bin low-level feature . in 2017 , li et al . [ 91 ] propose a vslam algorithm base on point , line , and plane fusion for an artiﬁcial environment . point feature be use for the initial estimation of the robot ’ s current pose . lines and plane be use to describe the environment . however , most plane only exist in the artiﬁcial environment , and few suitable plane can be find in the natural environment . these limit it', '. lines and plane be use to describe the environment . however , most plane only exist in the artiﬁcial environment , and few suitable plane can be find in the natural environment . these limit it application range.remote sens . 2022 ,14 , 3010 14 of 47 compared with the method that rely only on point feature , slam system that rely only on line or plane can only work stably in artiﬁcial environment in most case . the vslam method combine point , line , and surface feature improve the localization accuracy and robustness in weak texture scene , illumination change , and fast camera movement . however , the introduction of line and surface feature increase the time consumption of feature extraction and matching , which reduce the efﬁciency of the slam system . therefore , the vslam algorithm base on the point feature still occupy the mainstream position [ 92 ] . table 4 show a comparison of geometric feature . table 4 . comparison table of geometric feature . feature beneﬁts disbeneﬁts point is the most popular and commonly used feature , easy to store and match , and the speed be generally faster.it be difﬁcult to extract sufﬁcient feature in an environment of intense light and rapid camera rotation . line it have natural lighting and view angle invariance , while more ad- vanced feature also improve track robustness and accuracy . especially in certain artiﬁcial scene ( indoor , corri- dor ) , the interference of untextured or unreliable texture can be overcome.the detection and matching time of the line segment be long than that of the feature point . there be also no standard , universal slam op- timization and loopback module on the back end . line feature matching be also difﬁcult , for example , line segment be easy to fracture , do not have strong geometric constraint ( such a polar line geometric constraint ) , and do not have strong identiﬁcation of texture miss place . plane it have a more stable effect in artiﬁcial environments.the range be small and can only be operate in certain artiﬁcial', ') , and do not have strong identiﬁcation of texture miss place . plane it have a more stable effect in artiﬁcial environments.the range be small and can only be operate in certain artiﬁcial environment . 3.1.2 . vslam based on direct method different from feature-based method , the direct method operate directly on pixel intensity and can retain all information about the image . furthermore , the direct method cancel the process of feature extraction and matching , so the computational efﬁciency be well than the indirect method . furthermore , it have good adaptability to the environment with complex texture . it can still keep a good effect in the environment with miss feature [ 93 ] . the direct method be similar to the optical ﬂow , and they both have a strong assumption : gray-level invariance , the principle of which be show in figure 12 . 3 \\x01u *\\x12*\\x131 fyq 1\\x12 1\\x13p1 and p2 be the pixel coordinate of point p on the two image . using the first camera a a reference frame , the rotation and translation of the second camera be r , t δthe correspond lie group be t ε . the brightness error of two pixel of space point p be : here e be a scalar . the optimization objective be t he binary norm of the error , and the unweighted form be temporarily take . if there be multiple point pi in the space , the w hole camera pose estimation problem become : 1 2 1 2 -e i p i p =˄ ˅ ˄ ˅ 2min ( ) tj t e = 1 1 1 , 2 2 , min ( ) , ( ) ( ) .n t i iti i i ijt ee e i p i p == = - å figure 12 . schematic diagram of the direct method.remote sens . 2022 ,14 , 3010 15 of 47 in 2011 , newcombe et al . [ 94 ] propose the dtam algorithm , which be consider the ﬁrst practical direct method sof vslam . dtam allow track by compare the input image with those create by reconstructed map . the algorithm perform a precise and detailed reconstruction of the environment . however , it affect the computational cost of store and process the data , so it can only run in real-time on gpu . lsd- slam [ 40 ] neglect texture-free area to improve operational', 'the environment . however , it affect the computational cost of store and process the data , so it can only run in real-time on gpu . lsd- slam [ 40 ] neglect texture-free area to improve operational efﬁciency and can run in real- time on cup . lsd-slam , another major approach indirect method , combine featureless extraction with semi-dense reconstruction , and it core be a visual odometer use semi- dense reconstruction . the algorithm consists of three step : tracking , depth estimation , and map optimization . firstly , the photometric error be minimize to estimate the sensor pose . secondly , select a keyframe for in-depth estimation . finally , in the map optimization step , the new keyframe be merge into the map and optimize by use the posture optimization algorithm . in 2014 , forster et al . [ 95 ] propose the semi-direct visual slam algorithm svo . since the algorithm do not need to extract feature for each frame , it can run at high frame rate , which enable it to run in low-cost embedded system [ 80 ] . svo combine the advantage of the feature point method and direct method . the algorithm be divide into two main thread : motion estimation and mapping . motion estimation be carry out by feature point matching , but mapping be carry out by the direct method . svo have good result , but a a purely visual method , it only perform short-term data association , which limit it accuracy [ 82 ] . in 2018 , engel et al . [ 39 ] propose dso . dso can effectively use any image pixel , which make it robust even in featureless region and can gain more accurate result than svo . dso can calculate accurate camera attitude in poor feature point detector performance , improve the robustness of low-texture area or blur image . in addition , the dso use both geometric and photometric camera calibration result for high accuracy estimation . however , dso only consider local geometric consistency , so it inevitably produce cumulative error . furthermore , it be not a complete slam because it do not include loop closure ,', 'estimation . however , dso only consider local geometric consistency , so it inevitably produce cumulative error . furthermore , it be not a complete slam because it do not include loop closure , map reuse , etc . up to now , vslam have make many achievement in direct and indirect method . table 5 compare the advantage and disadvantage of the direct method and the indirect method to help reader good understand . table 5 . comparison between direct method and indirect method . method beneﬁts shortcomings indirect ( 1 ) the feature point itself be not sensi- tive to light , motion , and rotation , so it be relatively stable . ( 2 ) the camera move faster ( relatively direct method ) and can track successfully , with good robustness . ( 3 ) the research time be long and the scheme be mature . ( 1 ) it take a long time to extract , describe and match key point . ( 2 ) the feature point loss scenario can not be use . ( 3 ) only sparse map can be construct . direct ( 1 ) fast speed , can save the calculation of feature point , and descriptor time . ( 2 ) it can be use in situation where fea- tures be miss ( such a white wall ) , and the feature point method will de- teriorate rapidly in this case . ( 3 ) semi- dense and even dense map can be con- structed . ( 1 ) since the gray level be assume to be unchanged , it be susceptible to the change in illumination . ( 2 ) slow camera movement or high sampling frequency be require ( can be improve by image pyramid ) . ( 3 ) the differentiation of single-pixel or pixel block be not strong , and the strategy of quantity instead of quality be adopt . 3.2 . rgb-d slam an rgb-d camera be a visual sensor launch in recent year . it can simultaneously collect environmental color image and depth image , and directly gain depth map mainly by actively emit infrared structured light or calculate time-of-ﬂight ( tof ) [ 96 ] . the rgb-d camera , a a special camera , can gain three-dimensional information in spaceremote sens . 2022 ,14 , 3010 16 of 47 more conveniently . so it have be', 'time-of-ﬂight ( tof ) [ 96 ] . the rgb-d camera , a a special camera , can gain three-dimensional information in spaceremote sens . 2022 ,14 , 3010 16 of 47 more conveniently . so it have be widely concern and develop in three-dimensional reconstruction [ 97 ] . kinectfusion [ 98 ] be the ﬁrst real-time 3d reconstruction system base on an rgb-d camera . it use a point cloud create by the depth to estimate the camera pose through icp ( iterative closest point ) . then splice multi-frame point cloud collection base on the camera pose , and express reconstruction result by the tsdf ( truncated sign distance function ) model . the 3d model can be construct in real-time with gpu acceleration . however , the system have not be optimize by loop closure . furthermore , there will be obvious error in long-term operation , and the rgb information of the rgb-d camera have not be fully utilized . in contrast , elasticfusion [ 27 ] make full use of the color and depth information of the rgb-d camera . it estimate the camera pose by the color consistency of rgb and estimate the camera pose by icp . then improve the estimation accuracy of the camera pose by constantly optimize and reconstruct the map . finally , the surfel model be use for map representation , but it could only be reconstruct in a small indoor scene . kinitinuous [ 99 ] add loop closure base on kinectfusion and make non-rigid body transformation for 3d rigid body reconstruction by use a deformation graph for the ﬁrst time . so it make the result of two-loop closure reconstruction overlap , achieve good result in an indoor environment . compared with the above algorithm , rgb-d slamv2 [ 53 ] be a very excellent and comprehensive system . it include image feature detection , optimization , loop closure , and other module , which be suitable for beginner to carry out secondary development . although the rgb-d camera be more convenient to use , the rgb-d camera be extremely sensitive to light . furthermore , there be many problem with narrow , noisy , and small', 'secondary development . although the rgb-d camera be more convenient to use , the rgb-d camera be extremely sensitive to light . furthermore , there be many problem with narrow , noisy , and small horizon , so most of the situation be only use in the room . in addition , the exist algorithm must be implement use gpu . so the mainstream traditional vslam system still do not use the rgb-d camera a the main sensor . however , in three-dimensional reconstruction in the interior , the rgb-d camera be widely use . in addition , because of the ability to build a dense environment map , the semantic vslam direction , rgb-d camera be widely use . table 6 show the classic slam algorithm base on rgb-d camera . table 6 . some slam algorithm for sensor with an rgb-d camera . method year camera tracking loop closure code resource kinectfusion [ 98 ] 2011 direct no [ 100 ] kinitinuous [ 99 ] 2012 direct yes [ 101 ] rgb-d slamv2 [ 53 ] 2013 indirect yes [ 102 ] elasticfusion [ 27 ] 2016 direct yes [ 103 ] dvo-slam [ 104 ] 2017 direct yes [ 105 ] bundlefusion [ 106 ] 2017 hybrid yes [ 107 ] rgbdtam [ 108 ] 2017 direct yes [ 109 ] 3.3 . visual-inertial slam the pure visual slam algorithm have achieve many achievement . however , it be still difﬁcult to solve the effect of image blur cause by fast camera movement and poor illumination by use only the camera a a single sensor [ 110 ] . imu be consider to be one of the most complementary sensor to the camera . it can obtain accurate estimation at high frequency in a short time , and reduce the impact of dynamic object on the camera . in addition , the camera data can effectively correct the cumulative drift of imu [ 111 ] . at the same time , due to the miniaturization and cost reduction of camera and imu , visual-inertial fusion have also achieve rapid development . furthermore , it become the preferred method of sensor fusion , which be favor by many researcher [ 112 ] . nowadays , visual-inertial fusion can be divide into loosely couple and tightly couple accord to whether image', 'the preferred method of sensor fusion , which be favor by many researcher [ 112 ] . nowadays , visual-inertial fusion can be divide into loosely couple and tightly couple accord to whether image feature information be add to the state vector [ 113 ] . loosely coupled mean the imu and the camera estimate their motion , respectively , and then fuse their pose estimation . tightly couple refers to the combination of the state of imu and the state ofremote sens . 2022 ,14 , 3010 17 of 47 the camera to jointly construct the equation of motion and observation , and then perform state estimation [ 114 ] . 3.3.1 . loosely coupled visual-inertial the loosely couple core be to fuse the position and pose calculate by the vision sensor and imu , respectively . the fusion have no impact on the result obtain by the two . generally , the fusion be perform through ekf . stephen weiss [ 115 ] provide groundbreaking insight in their doctoral thesis . ref . [ 116 ] propose an efﬁcient loose couple method , and good experimental result be obtain by use an rgb-d camera and imu . the loose-coupling implementation be relatively simple , but the fusion result be prone to error and there have be little research in this area . 3.3.2 . tightly coupled visual-inertial the core of the tightly couple be to combine the state of the vision sensor and imu through an optimized ﬁlter . it need the image feature to be add to the feature vector to jointly construct the motion equation and observation equation . then perform state estimation to obtain the pose information . tightly couple need full use of visual and inertial measurement information , which be complicate in method implementation but can achieve high pose estimation accuracy . therefore , it be also the mainstream method , and many breakthrough have be make in this area . in 2007 , mourikis et al . [ 117 ] propose msckf . the core of msckf be to fuse imu and visual information under the ekf in a tightly couple way . compared with the vo algorithm alone , msckf can adapt to more', 'mourikis et al . [ 117 ] propose msckf . the core of msckf be to fuse imu and visual information under the ekf in a tightly couple way . compared with the vo algorithm alone , msckf can adapt to more intense motion and texture loss , with high robustness . speed and accuracy can also reach a high level . msckf have be widely use in robot , uav , and ar/vr ﬁelds . however , because the backend use the kalman ﬁlter method , global information can not be use for optimization , and no loopback detection will cause error accumulation . ref . [ 29 ] propose okvis base on a fusion of binocular vision and imu . however , it only output six degree of freedom pose without loopback detection and map , so it be not a complete slam in a strict sense . although it have good accuracy , it pose will be loose when it run for a long time . although these two algorithm have achieve good result , they have not be widely promote . the lack of loop closure modules inevitably lead to cumulative error when run for long period of time . the emergence of vins-mono [ 55 ] break this situation . in 2018 , a team from the hong kong university of science and technology ( hkust ) introduce a monocular inertially tightly couple vins-mono algorithm . it have since release it expanded version , vins- fusion , which support multi-sensor integration , include monocular + imu , stereo + imu , and even stereo only , and also provide a version with gps . vins-mono be a classic fusion of vision and imu . its position accuracy be comparable to okvis , and it have a more complete and robust initialization and loop closure detection process than okvis . at the same time , vins-mono have set a standard for the research and application of visual slam , which be more monocular +imu . in the navigation of robot , especially the autonomous navigation of uavs , monocular camera be not limit by rgb-d camera ( susceptible to illumination and limited depth information ) and stereo camera ( occupy a large space ) . it can adapt to indoor , outdoor and different', 'monocular camera be not limit by rgb-d camera ( susceptible to illumination and limited depth information ) and stereo camera ( occupy a large space ) . it can adapt to indoor , outdoor and different illumination environment with good adaptability . as a supplement to cameras , inertial sensor can effectively solve the problem that a single camera can not cope with . visual inertial fusion be bind to become a long-term hot direction of slam research . however , the introduction of multiple sensor will lead to an increase in data , which have a high requirement on compute capacity [ 118 ] . therefore , we believe that the next hot issue of visual-inertial fusion will be reﬂected in the efﬁcient processing of sensor fusion data . how to make good use of data from different sensor will be a long-term attractive hot issue . due to the rich information acquisition , convenient use and low price of visual sensor , the environment map construct be closer to the real environment recognize by human being . after decade of development , vision-based slam technology have achieve many excellent achievement . table 7 summarize some ofremote sens . 2022 ,14 , 3010 18 of 47 the best visual-based slam algorithm , compare their performance in key area , and provide open-source address to help reader make good choice . table 7 . best visual-based slam algorithm . method sensor front-end back-end loop closure mapping code resource monoslam [ 77 ] m p f no sparse [ 119 ] ptam [ 79 ] m p o no sparse [ 120 ] orb-slam2 [ 28 ] m/s/r p o yes sparse [ 121 ] pl-svo [ 122 ] m pl o no sparse [ 123 ] visual pl-slam [ 88 ] m/s pl o yes sparse [ 124 ] dtam [ 94 ] m d o no dense [ 125 ] svo [ 95 ] m h o no sparse [ 126 ] lsd-slam [ 40 ] m/s d o yes semi-dense [ 127 ] dso [ 39 ] m d o no sparse [ 128 ] method sensor coupling back-end loop closure mapping code resource visual-inertialmsckf [ 117 ] m + i t f no sparse [ 129 ] okvis [ 29 ] s + i t o no sparse [ 130 ] rovio [ 131 ] m + i t f no sparse [ 132 ] vins-mono [ 55 ] m + i t o yes sparse [', 'code resource visual-inertialmsckf [ 117 ] m + i t f no sparse [ 129 ] okvis [ 29 ] s + i t o no sparse [ 130 ] rovio [ 131 ] m + i t f no sparse [ 132 ] vins-mono [ 55 ] m + i t o yes sparse [ 133 ] sensor : m represent monocular camera ; s represent stereo camera ; r represent rgb-d camera and i represent imu . front-end : p represent point ; pl represent point-line ; d represent direct ; h represent hybrid . back-end : f represent filtering ; o represent optimization . coupling : t represent tightly . in this chapter , we summarize the traditional vision-based slam algorithm , and summarize some excellent algorithm for your reference , hop to give reader a more comprehensive understanding . next , we will cover vslam with semantic information fusion , aim to explore the ﬁeld of slam more deeply . 4 . semantic vslam semantic slam refers to a slam system that can not only obtain geometric informa- tion of the unknown environment and robot movement information but also detect and identify target in the scene . it can obtain semantic information such a their functional attribute and relationship with surround object , and even understand the content of the whole environment [ 134 ] . traditional vslam represent the environment in the form of point cloud and so on , which to u be a bunch of meaningless point . to perceive the world from both geometric and content level and provide good service to human , robot need to further abstract the feature of these point and understand them [ 135 ] . with deep learning development , researcher have gradually realize it possible help to slam problem [ 136 ] . semantic information can help slam to understand the map at a high level . furthermore , it lessen the dependence of the slam system on feature point and improve the robustness of the system [ 137 ] . modern semantic vslam system can not do without the help of deep learning , and feature attribute and association relation obtain through learning can be use in different task [ 138 ] . as an important branch of machine', 'system can not do without the help of deep learning , and feature attribute and association relation obtain through learning can be use in different task [ 138 ] . as an important branch of machine learning , deep learning have achieve remarkable result in image recognition [ 139 ] , semantic understanding [ 140 ] , image match [ 141 ] , 3d reconstruction [ 142 ] , and other task . the application of deep learning in computer vision can greatly ease the problem encounter by traditional method [ 143 ] . traditional vslam system have achieve commendable result in many aspect , but there be still many challenge problem to be solve [ 144 ] . ref . [ 145 ] have summarize deep learning-based vslam in detail and point out the problem exist in traditional vslam . these work [ 146–149 ] suggest that deep learning should be use to replace some module of traditional slam , such a loop closure and pose estimation , to improve the traditional method.remote sens . 2022 ,14 , 3010 19 of 47 machine learning be a subset of artiﬁcial intelligence that use statistical technique to provide the ability to ” learn “ data from a computer without complex programming . unlike task-speciﬁc algorithm , deep learning be a subset of machine learn base on learn data . it be inspire by the function and structure of what be know a artiﬁcial neural network . deep learning gain great ﬂexibility and power by learn to display the world a simpler concept and hierarchy , and to calculate more abstract representation base on less abstract concept . the most important difference between traditional machine learning and deep learning be the performance of data scale . deep learn algorithm do not work well when the data be very small , because they need big data to perfectly identify and understand it . the performance of machine learn algorithms depends on the accuracy of feature identiﬁed and extract . deep learning algorithm , on the other hand , identify these high-level feature from the data , thus reduce the effort to develop an entirely new', 'on the accuracy of feature identiﬁed and extract . deep learning algorithm , on the other hand , identify these high-level feature from the data , thus reduce the effort to develop an entirely new feature extractor for each problem . deep learning be a subset of machine learning , which have prove to be a more powerful and promising branch of the industry compare to traditional machine learn algorithm . it realize many function that traditional machine learning can not achieve with it layered characteristic . slam system need to collect a large amount of information in the environment , so there be a huge amount of data to calculate , and the deep learning model be just suitable for solve this problem . this paper believe that semantic vslam be an evolve process . in the early stage , some researcher try to improve the performance of vslam by extract semantic information in the environment use neural network such a cnn . in the modern stage , target detection , semantic segmentation , and other deep learning method be powerful tool to promote the development of semantic vslam . therefore , in this chapter , we will ﬁrst describe the application of typical neural network in vslam . we believe that this be the premise of the development of modern semantic vslam . the application of neural network in vslam provide a model for modern semantic vslam . this paper believe that a neural network be a bridge to introduce semantic information into the modern semantic vslam system and obtain rapid development . 4.1 . neural networks with vslam figure 13 show the typical framework of cnn and rnn . cnn can capture spatial feature from the image , which help u accurately identify the object and it relationship with other object in the image [ 150 ] . the characteristic of rnn be that it can process an image or numerical data . because of the memory capacity of the network itself , it can learn data type with contextual correlation [ 151 ] . in addition , other type of neural network such a dnn ( deep neural networks ) also', 'because of the memory capacity of the network itself , it can learn data type with contextual correlation [ 151 ] . in addition , other type of neural network such a dnn ( deep neural networks ) also have some tentative work , but it be in the initial stage . this paper note that cnn have the advantage of extract feature of thing with a certain model , and then classify , identify , predict , or decide base on the feature . it can be helpful to different module of vslam . in addition , this paper believe that rnn have great advantage in help to establish consistency between nearby frame . furthermore , the high-level feature have good differentiation , which can help robots to good complete data association.remote sens . 2022 ,14 , 3010 20 of 47 neural network ffnn ǃ dnn ă cnn rnn hidden state unfold feed-forward neural network use filter and pool the size of the input and the resulting output be fix spatial data ( such a image ) recurring network that feed the result back into the network the size of the input and the result output may vary temporal/sequential data ( such a text or video ) architecture input/output ideal usage scenario input layer convolutional layer pooling layer fully connected layer output layer rnn output input rnn output input rnn output input rnn output input figure 13 . structure block diagram of cnn and rnn . cnn be suitable for extract unmarked feature from hierarchical or spatial data . rnn be suitable for temporal data and other type of sequential data . 4.1.1 . cnn with vslam traditional inter-frame estimation method adopt feature-based method or direct method to identify camera pose through multi-view geometry [ 152 ] . features-based method need complex feature extraction and matching . direct method rely on pixel intensity value , which make it difﬁcult for traditional method to obtain wished result in environment such a intense illumination or sparse texture [ 153 ] . in contrast , method base on deep learning be more intuitive and concise . that be because they do not need to', 'wished result in environment such a intense illumination or sparse texture [ 153 ] . in contrast , method base on deep learning be more intuitive and concise . that be because they do not need to extract environmental feature , feature matching , and complex geometric operation [ 154 ] . as the feature detection layer of cnn learn through training data , it avoid feature extraction in display and learn implicitly from train data during use . refs . [ 155,156 ] and other work have make a detailed summary . cnn ’ s advantage in image processing have be fully veriﬁed . for example , visual depth estimation improve the problem that monocular camera can not obtain reliable depth information [ 157 ] . in 2017 , tateno et al . [ 158 ] propose a real-time slam system “ cnn-slam “ base on cnn in the framework of lsd-slam . as show in figure 14 , the algorithm obtain a reliable depth map by train the depth estimation network model . cnn be use for depth prediction , which be input into subsequent module such a traditional pose estimation to improve positioning and mapping accuracy . in addition , cnn semantic segmentation module be add to the framework , which provide help for advanced information perception of the vslam system . similar work use the network to estimate depth information include code-slam [ 42 ] and dvso [ 159 ] based on a stereo camera . in the same year , godard et al . [ 160 ] propose an unsupervised image depth estimation scheme . unsupervised learning be improve by use stereo data set , and then a single frame be use for pose estimation , which have a great improvement compare with other scheme . cnn not only solve the problem that traditional method can not obtain reliable depth data by use a monocular camera but also improve the defect of traditional method in camera pose estimation . in 2020 , yang et al . [ 48 ] propose d3vo . in this method , deep learning be use from three aspect , include depth estimation , pose estimation , and un- certainty estimation . the prediction depth , pose and', 'et al . [ 48 ] propose d3vo . in this method , deep learning be use from three aspect , include depth estimation , pose estimation , and un- certainty estimation . the prediction depth , pose and uncertainty be closely combine into a direct visual odometer to simultaneously improve the performance of front-end tracking and back-end nonlinear optimization . however , self-supervised method be difﬁcult to adapt to all environment . in addition , qin et al . [ 161 ] propose a semantic feature-basedremote sens . 2022 ,14 , 3010 21 of 47 localization method in 2020 , which effectively solve the problem that traditional visual slam method be prone to track loss . its principle be to use cnn to detect semantic feature in the narrow and crowd environment of an underground parking lot , lack of gps signal , dim light , and sparse texture . then use u-net [ 162 ] to perform semantic seg- mentation to separate parking line , speed bump , and other indicator on the ground , and then use odometer information . the semantic feature be map to the global coordinate system to build the parking lot map . then the semantic feature be match with the previously construct map to locate the vehicle . finally , ekf be use to integrate visual positioning result and odometer information to ensure the system can obtain continuous and stable positioning result in the underground parking environment . zhu et al . [ 163 ] learn rotation and translation by use cnn to focus on different quadrant of optical ﬂow input . however , the end-to-end method to replace the visual odometer be simple and crude but without theoretical support and generalization ability . input rgb image cnn depth prediction cnn semantic segmentation camera pose estimation key-frame initialization pose graph optimization frame-wise depth refinement every key-frame every frame global map and semantic label fusion figure 14 . the structure of cnn-slam . loop closure detection can eliminate cumulative trajectory error and map error , and determine the accuracy of the whole', 'global map and semantic label fusion figure 14 . the structure of cnn-slam . loop closure detection can eliminate cumulative trajectory error and map error , and determine the accuracy of the whole system , which be essentially a scene identiﬁcation problem [ 164 ] . traditional method be match by artiﬁcially design sparse feature or pixel-level dense feature . deep learning can learn high-level feature in image through neural network . furthermore , it recognition rate can reach a high level by use the powerful recognition ability of deep learning to extract higher-level robust feature of image . in this way , the system can have strong adaptability to image change such a perspective and illumination and improve the loop closure image recognition ability [ 165 ] . therefore , scene identiﬁcation base on deep learning can improve the accuracy of loop clo- sure detection , and cnn have also obtain many reliable effect for loop closure detection . memon et al . [ 166 ] propose a dictionary-based deep learning method , which be different from the traditional bow dictionary and use higher-level and more abstract deep learning feature . this method do not need to create vocabulary , have high memory efﬁciency , and have a faster run speed than similar method . however , this paper be only base on the likeness score detection cycle , so it be not widely representative . li et al . [ 167 ] propose a learn feature-based visual slam system name dxslam , which solve the limitation of the above method . local and global feature be extract from each frame use cnn , and these feature be then feed into modern slam pipeline for posture tracking , local mapping , and reposition . compared with traditional bow-based method , it achieve high efﬁciency and low computational cost . in addition , qin et al . [ 168 ] use cnn to extract environmental semantic information and model the visual scene a a semantic subgraph . it can effectively improve the efﬁciency of loopback detection by use semantic information . refs . [ 169,170 ] and', 'semantic information and model the visual scene a a semantic subgraph . it can effectively improve the efﬁciency of loopback detection by use semantic information . refs . [ 169,170 ] and others describe in detail the achievement of deep learning in many aspect . however , with the introduction of more complex and good model , how to ensure the real-time performance of model calculation ? how to good set in the loop closure detection model in resource-constrained platform , and the lightweight of the model be also a major problem [ 171 ] .remote sens . 2022 ,14 , 3010 22 of 47 cnn have achieve good result in replace some module of the traditional vslam algorithm , such a depth estimation and loop closure detection . its stability be still not as good a the traditional vslam algorithm [ 172 ] . in contrast , the semantic information extraction of the cnn system have bring well effect . the process of traditional vslam be optimize by use cnn to extract the semantic information of the environment with higher-level feature , make the traditional vslam achieve good result . using a neural network to extract semantic information and combine it with vslam will be an area of great interest . with the help of semantic information , the data association be upgrade from the traditional pixel level to the object level . the perceptual geometric environment information be assign with semantic label to obtain a high-level semantic map . it can help the robot to understand the autonomous environment and human–computer interaction . table 8 show some main application link of the cnn network in vslam . some be involve in many aspect , only the main contribution be list here . table 8 . cnn use for vslam . part method contribution image depth estimationcnn-slam [ 158 ] the depth estimation be perform only on the keyframe , which improve the computing efﬁciency . undeepvo [ 173 ] real-scale monocular vision odometer be realize in an unsupervised way . code-slam [ 44 ] a real-time monocular slam system be implement that allow', 'improve the computing efﬁciency . undeepvo [ 173 ] real-scale monocular vision odometer be realize in an unsupervised way . code-slam [ 44 ] a real-time monocular slam system be implement that allow simultaneous optimiza- tion of camera motion and map . dvso [ 159 ] design a novel deep network that reﬁnes predict depth from a single image in a two-stage process . pose estimation detone et al . [ 174 ] it use only the location of point , not the descriptor of local point . vinet [ 175 ] the ability to combine the information in a speciﬁc area naturally and cleverly can signiﬁcantly reduce drift . d3vo [ 48 ] the propose monocular visual odometer framework utilizes deep learning network at three level . zhu et al . [ 163 ] present a novel four-branch network to learn the rotation and translation by leverage convolutional neural networks ( cnns ) to focus on different quadrant of optical ﬂow input . loop closure memon et al . [ 166 ] two deep neural network be use together to speed up the loop closure detection and to ignore the effect of mobile object on loop closure detection . li et al . [ 167 ] train a visual vocabulary of local feature with a bag of words ( bow ) method . based on the local feature , global feature , and vocabulary , a highly reliable loop closure detection method be build . qin et al . [ 168 ] models the visual scene a a semantic sub-graph by only preserve the semantic and geometric information from object detection . semantic informationcnn-slam [ 158 ] by integrate geometry and semantic information , a map with semantic information be generate . naseer et al . [ 176 ] to achieve real-time semantic segmentation and maintain a good efficiency of differentiation . semanticfusion [ 46 ] the semantic prediction of cnn ’ s multiple view can be probabilistically integrate into the map . qin et al . [ 161 ] a novel semantic feature use in the visual slam framework be propose . bowman et al . [ 177 ] an optimization problem for sensor state and semantic landmark location be propose . 4.1.2 . rnn', '[ 161 ] a novel semantic feature use in the visual slam framework be propose . bowman et al . [ 177 ] an optimization problem for sensor state and semantic landmark location be propose . 4.1.2 . rnn with vslam the research of rnn ( recurrent neural network ) begin in the 1980s and 1990s and develop into one of the classical deep learn algorithm in the early 21st century . long short-term memory networks ( lstm ) be one of the most common recurrent neural network [ 178 ] . lstm be a variant of rnn , which remember a controllable amount of previous train data or forgets it more properly [ 179 ] . as show in figure 15 , the structure of lstm and the equation of state of it different module be give . lstm with special implicit unit can preserve input for a long time . lstm inherit most characteristic of the rnn model and solve the vanishing gradient problem cause by the gradual reduction ofremote sens . 2022 ,14 , 3010 23 of 47 the gradient back transmission process . as another variant of rnn , gru ( gated recurrent unit ) be easy to train and can improve training efﬁciency [ 180 ] . rnn have some advantage in learn nonlinear feature of sequence because of it memorization and parameter sharing . rnn construct by introduce a convolutional neural network cnn can deal with computer vision problem involve sequence input [ 181 ] . xt ct-1 ht-1stanh tf ti totkäå ä ä1.t t t t tc f c i k -= ä + å cell state forget gate 1 ( * [ , ] ) .t f t t ff w h x b s- = + input gate 1 1 ( * [ , ] ) , tanh ( * [ , ] ) .t i t t i t k t t k f w h x b k w h x b s- -= + = + output gate 1 0 ( [ , ] ) , tanh ( ) .t o t t t t to w h x b h o c s- = * + = * 9u $ u\\x0e\\x12 iu\\x0e\\x12 $ u iuiu 㓶㜎⣦ᘱ 䚇ᘈ䰘 䗃 ޕ 䰘䗃 ࠪ 䰘 䚇ᘈ䰘⣦ᘱᯩ〻ѫ 䗃ޕ䰘⣦ᘱᯩ〻ѫ 䗃ࠪ䰘⣦ᘱᯩ〻ѫ ᴤᯠਾⲵ㓶㜎⣦ᘱѫ 䚇ᘈ䰘⣦ᘱᯩ〻ѫ˖ ( * [ , ] ) .ht ct ht 䗃ޕ䰘⣦ᘱᯩ〻ѫ˖ 䗃ࠪ䰘⣦ᘱᯩ〻ѫ˖ᴤᯠਾⲵ㓶㜎⣦ᘱѫ˖ ( * [ , ] ) , ( [ , ] ) , cell state forget gate input gate output gate tanh s 9u $ u\\x0e\\x12 iu\\x0e\\x12 $ u iuiu tanh 㓶㜎⣦ᘱ s䚇 ᘈ 䰘䗃 ޕ 䰘䗃 ࠪ 䰘 sf i ok ( * [ , ] ) . 䚇ᘈ䰘⣦ᘱᯩ〻ѫ 䗃ޕ䰘⣦ᘱᯩ〻ѫ ( * [ , ] ) , 䗃ࠪ䰘⣦ᘱᯩ〻ѫ ᴤᯠਾⲵ㓶㜎⣦ᘱѫ . c f c i k äå ä ästanh s stanh figure 15 . the', 'gate output gate tanh s 9u $ u\\x0e\\x12 iu\\x0e\\x12 $ u iuiu tanh 㓶㜎⣦ᘱ s䚇 ᘈ 䰘䗃 ޕ 䰘䗃 ࠪ 䰘 sf i ok ( * [ , ] ) . 䚇ᘈ䰘⣦ᘱᯩ〻ѫ 䗃ޕ䰘⣦ᘱᯩ〻ѫ ( * [ , ] ) , 䗃ࠪ䰘⣦ᘱᯩ〻ѫ ᴤᯠਾⲵ㓶㜎⣦ᘱѫ . c f c i k äå ä ästanh s stanh figure 15 . the basic framework of lstam . in pose estimation , the end-to-end deep learning method be introduce to solve pose parameter between frame of visual image without feature matching and complex ge- ometric operation . it can quickly obtain the relative pose parameter between frame by directly inputting nearby frame [ 182 ] . xue et al . [ 183 ] use deep learning to learn the process of feature selection and realize pose estimation base on rnn . in pose estimation , rotation and displacement be train separately , which have good adaptability compare with traditional method . in 2021 , teed et al . [ 184 ] introduce droid-slam , whose core be a learnable update operator . as show in figure 16 , the update operator be a 3 \\x023 convolutional gru with a hidden state of h. the iterative application of the update op- erator create a series of attitude and depth that converge to a ﬁxed point that reﬂects a real reconstruction . the algorithm be an end-to-end neural network architecture for visual slam , which have great advantage over previous work in challenge environment . most exist method adopt to combine cnn with rnn to improve the overall performance of vslam . cnn and rnn can be combine use a separate layer , with the output of cnn a the input of rnn . on the one hand , it can automatically learn the effective feature representation of the vo problem through cnn . on the other hand , it can implicitly model the timing model ( motion model ) and data association model ( image sequence ) through rnn [ 185 ] . in 2017 , yu et al . [ 60 ] combine rnn with kinectfusion to carry out semantic annotation on rgb-d collect image to reconstruct a 3d semantic map . they introduce a new loop closure unit into rnn to solve the problem of gpu compute resource consumption . this method make full use of the advantage of rnn to realize the', 'a 3d semantic map . they introduce a new loop closure unit into rnn to solve the problem of gpu compute resource consumption . this method make full use of the advantage of rnn to realize the annotation of semantic information . high-level feature have good discrimination and help the robot to good complete the data association . due to the use of rgb-d camera , they can only be operate in indoor environment . deepseqslam [ 186 ] solve this problem well . in this scheme , a trainable cnn+rnn architecture be use to jointly learn visual and location representation from a single monocular image sequence . an rnn be use to integrate temporal information on short image sequence . at the same time , use the dynamic information processing function of these network , end-to-end position and sequence position learn be realize for the ﬁrst time . furthermore , the ability to learn meaningful temporal relationship from single image sequence of large drive datasets . in run time , accuracy , and calculation need , sequence-based method be signiﬁcantly superior to traditional method and can operate stably in outdoor environments.remote sens . 2022 ,14 , 3010 24 of 47 droid-slam pose core ijc ijh convgru convgru 1 ( ( , ) ) c ij c i ig p d -õ õ 1 j c i i d1 ( , ( , 1 j c i i ( , , , õ å å dba dba ijrijw e´ ää 澳å 澳åg drl ijppose * ijp cijpijrijwrl * ijp1 ( ( , ) ) c ij c i ig p d -õ õ 1 j c i i d1 ( , ( , 1 j c i i ( , ( , ( , õ figure 16 . the core framework of droid-slam . cnn can be combine with many link of vlsam , such a feature extraction and matching , depth estimation , and pose estimation , and have achieve good result in these aspect . rnn , by contrast , have a small scope of application , but it have a great advantage in help to establish consistency between nearby frame . rnn be a common method for data-driven timing modeling in deep learning . inertial data such a high frame rate angular velocity and acceleration output by imu have strict dependence on timing , which be especially suitable for rnn model . based', 'modeling in deep learning . inertial data such a high frame rate angular velocity and acceleration output by imu have strict dependence on timing , which be especially suitable for rnn model . based on this , clark et al . [ 175 ] propose to use a conventional small lstm network to process the original data of imu and obtain the motion characteristic under imu data . finally , they combine visual motion feature with imu motion feature , and send it into a core lstm network for feature fusion and pose estimation . its principle of it be show in figure 17 . compared with pose estimation , we believe that rnn be more attractive for it contri- bution to visual-inertial data fusion . this method can effectively fuse visual-inertial data and be more convenient than traditional method . similar work , such a [ 187,188 ] , prove the effectiveness of the fusion strategy , which provide good performance compare with direct fusion . this paper give the contribution of rnn to partial vslam in table 9 . this paper introduce the combination of deep learning and traditional vslam from the classical neural network cnn and rnn in this section . table 10 show some excellent algorithm combine neural network with vslam.remote sens . 2022 ,14 , 3010 25 of 47 rgbt rgbt+1 corr lstm lstm lstm lstm imu imu imu 6 6 6visual-inertial fusion by concatenaing feature vector 5x5 1x1 64 128 256 3x3 473 256 512 512 512 512 1024 se3 se3 composition layer poset rgbt+1 rgbt+2 corr lstm lstm lstm lstm imu imu imu 66 65x5 1x1 64 128 256 3x3 473 256 512 512 512 512 1024 se3 poset+1 vinitialised by flownet weight se3 composition layer figure 17 . clark et al . propose a framework for visual inertia fusion use lstm . table 9 . rnn use for vslam . part method contribution xue et al . [ 183 ] proposing a dual-branch recurrent network to learn the rotation and translation separately by leverage current cnn for feature representation and rnn for image sequence reason . vo teed et al . [ 184 ] it consist of recurrent iterative update of camera pose and', 'and translation separately by leverage current cnn for feature representation and rnn for image sequence reason . vo teed et al . [ 184 ] it consist of recurrent iterative update of camera pose and pixel-wise depth through a dense bundle adjustment layer . da-rnn [ 60 ] a novel framework for joint 3d scene mapping and semantic labeling . deepseqslam [ 186 ] a trainable cnn+rnn architecture for jointly learn visual and positional representation from a single monocular image sequence of a route . clark et al . [ 175 ] it be the ﬁrst end-to-end trainable method for visual-inertial odometry which perform a fusion of the data at an intermediate feature-representation level . deepvio [ 187 ] it reduce the impact of inaccurate camera-imu calibration and unsynchronized and miss data . vio chen et al . [ 188 ] it propose a novel end-to-end selective sensor fusion framework for monocular vio . yasin et al . [ 189 ] using adversarial training and self-adaptive visual-inertial sensor fusion . wong et al . [ 190 ] the fusion method of visual inertia + depth data set be propose for the ﬁrst time to further enhance the complementary advantage of visual and inertial sensor . table 10 . an excellent algorithm combine neural network with vslam . method year sensor neural network supervision cnn-slam [ 158 ] 2017 monocular cnn supervised deepvo [ 191 ] 2017 monocular r-cnn supervised code-slam [ 44 ] 2018 monocular u-net supervised dvso [ 159 ] 2018 stereo dispnet semi-supervised undeepvo [ 173 ] 2018 monocular vgg encoder-decoder unsupervised cnn-svo [ 192 ] 2019 monocular cnn hybrid vo ganvo [ 193 ] 2019 monocular gan unsupervised li et al . [ 194 ] 2019 monocular cnn supervised d3vo [ 48 ] 2020 monocular cnn hybrid deepseqslam [ 186 ] 2020 monocular cnn+rnn supervised deepslam [ 145 ] 2021 monocular r-cnn unsupervised lift-slam [ 195 ] 2021 monocular dnn supervised zhang et al . [ 196 ] 2021 stereo u-net encoder-decoder unsupervised vinet [ 175 ] 2017 monocular + imu cnn + lstm supervised violearner [ 197 ] 2020 monocular + imu', '] 2021 monocular dnn supervised zhang et al . [ 196 ] 2021 stereo u-net encoder-decoder unsupervised vinet [ 175 ] 2017 monocular + imu cnn + lstm supervised violearner [ 197 ] 2020 monocular + imu cnn unsupervised vio deepvio [ 187 ] 2019 stereo + imu cnn + lstm supervised chen et al . [ 188 ] 2019 monocular + imu flownet + lstm supervised kim et al . [ 198 ] 2021 monocular + imu cnn + lstm unsupervised gurturk et al . [ 199 ] 2021 monocular + imu cnn + lstm supervisedremote sens . 2022 ,14 , 3010 26 of 47 4.2 . modern semantic vslam deep learning have make many achievement in pose estimation , depth estimation , and loop closure detection . however , in vslam , deep learning be currently unable to shake the dominance of traditional method . however , apply deep learning to semantic vslam research can obtain more valuable discovery , which can quickly promote to development of semantic vslam . refs . [ 60,158,168 ] use cnn or rnn to extract semantic information in the environment to improve the performance of different module in traditional vslam . the semantic information be use for pose estimation and loopback detection . it signif- icantly improve the performance of traditional method and prove the effectiveness of semantic information for the vslam system . this paper believe that this provide technical support for the development of modern semantic vslam and be the beginning of modern semantic vslam . using deep learn method such a target detection and semantic segmentation to create a semantic map , which be an important representative period of semantic slam development . refs . [ 135,200 ] point out that semantic slam can be divide into two type accord to different target detection method . one be to detect target use traditional method . real-time monocular object slam be the most common one , use a large number of binary word and a database of object model to provide real-time detection . however , it ’ s very limited because there be many type of 3d object entity for semantic class such a ” cars. “', 'number of binary word and a database of object model to provide real-time detection . however , it ’ s very limited because there be many type of 3d object entity for semantic class such a ” cars. “ another approach to slam be object recognition use deep learning method , such a those propose in [ 46 ] . semantics and slam may seem to be separate module , but they be not . in many application , the two go hand in hand . on the one hand , semantic information can help slam to improve the accuracy of mapping and localization , especially for complex dynamic scene [ 201 ] . the mapping and localization of traditional slam be mostly base on pixel-level geometric matching . with semantic information , we can upgrade the data association from the traditional pixel level to the object level , improve the accuracy of complex scene [ 202 ] . on the other hand , by use slam technology to calculate the position constraint between object , the consistency constraint can be apply to the recognition result of the same object at different angle and at different time , thus improve the accuracy of semantic understanding . the integration of semantic and slam not only contribute greatly to the improvement of the accuracy of both but also promote the application of slam in robotics , such a robot path planning and navigation , carry object accord to human instruction , do housework , and accompany human movement , etc . for example , we want a robot to walk from the bedroom to the kitchen to get an apple . how do that work ? relying on traditional slam , the robot calculate it location ( auto- matically ) and apple ’ s location ( manually ) and then do path planning and navigation . if the apple be in the refrigerator , you also need to manually set the relationship between the refrigerator and the apple . however , now with our semantic slam technology , it ’ s much more natural for a human to send a robot , “ please go to the kitchen and get me an apple ” , and the robot will do the rest automatically . if there be a', 'our semantic slam technology , it ’ s much more natural for a human to send a robot , “ please go to the kitchen and get me an apple ” , and the robot will do the rest automatically . if there be a contaminated ground in front of the robot during an operation , traditional path planning algorithm need to manually mark the contaminated area so the robot can bypass it [ 203 ] . semantic information can help robots good understand their surroundings . inte- grate semantic information into vslam be a grow ﬁeld that have receive more and more attention in recent year . this paper will elaborate on our understanding of semantic vslam from two aspect of localization , mapping , and dynamic object removal in this section . we believe the big contribution of deep learning for vslam be the introduction of semantic information . it can improve the performance of different module of traditional method to vary degree . especially in the construction of the semantic map , which promote the innovation of the whole intelligent robot ﬁeld.remote sens . 2022 ,14 , 3010 27 of 47 4.2.1 . image information extraction the core difference between modern semantic vslam and traditional vslam lie in the integration of the object detection module . it can obtain the attribute and semantic information of object in the environment [ 204 ] . the ﬁrst step of semantic vslam be to extract semantic information from the image gain by the camera . furthermore , semantic information base on image information can be achieve through classify image information [ 205 ] . traditional target detection relies on interpretable machine learn classiﬁers , such a decision tree and svm , to classify and realize target feature . however the detection process be slow , the accuracy be low and the generalization ability be weak [ 206 ] . image classiﬁcation base on deep learning can be divide into object detection , semantic segmentation , and instance segmentation , a show in figure 18 . figure 18 . from leave to right be the test renderers of yolov5 ,', 'base on deep learning can be divide into object detection , semantic segmentation , and instance segmentation , a show in figure 18 . figure 18 . from leave to right be the test renderers of yolov5 , deeplabv3 , and mask r-cnn . how to better extract semantic information from image be a hot research issue in computer vision , whose essence be to extract object character information from scene [ 207 ] . we believe that although neural network such a cnn also contribute to semantic infor- mation extraction , modern semantic vslam rely more on semantic extraction module such a target detection . object detection and image semantic segmentation be both meth- od of extract semantic information from image . semantic segmentation of image be to understand image at the pixel level to obtain deep-level information in the image , include space , category , and edge . semantic segmentation technology base on a deep neural network break through the bottleneck of traditional semantic segmentation [ 208 ] . compared with semantic segmentation , target detection only obtain the object information and spatial information of the image . furthermore , it identiﬁes the category of each object by draw the candidate box of the object , so target detection be faster than semantic segmentation [ 209 ] . compared with object detection , semantic segmentation technology have high accuracy , but it speed be much low [ 210 ] . target detection be divide into one-stage and two-stage structure [ 211 ] . early target detection algorithm use two-stage architecture . after create a series of candidate box a sample , sample classiﬁcation be carry out through a convolutional neural network . common algorithms include r-cnn [ 212 ] , fast r-cnn [ 213 ] , faster r-cnn [ 214 ] , and so on . later , yolo [ 215 ] creatively propose the one-stage structure . it directly carry out the two step of the two-stage in one step , complete the classiﬁcation and positioning of object in one step , and directly output the candidate box and it category obtain by', '. it directly carry out the two step of the two-stage in one step , complete the classiﬁcation and positioning of object in one step , and directly output the candidate box and it category obtain by regression . one-stage reduce the step of the target detection algorithm and directly convert the problem of target frame position into regression problem theory without the need to create candidate box , which be superior in speed . common algorithms include yolo and ssd [ 216 ] . in 2014 , the appearance of r-cnn subvert the traditional object detection scheme , improve the detection accuracy , and promote the rapid development of object detection technology . its core be to extract candidate region , then obtain feature vector through alexnet , and ﬁnally use svm classiﬁcation and frame correction . however , the speed of feature extraction be limit due to the serial feature extraction method use by r- cnn . ross propose fast r-cnn in 2015 to solve this problem well . region of interest pooling ( roi pooling ) operation be use in fast r-cnn to improve the efﬁciency of featureremote sens . 2022 ,14 , 3010 28 of 47 extraction , and region generation network ( rpn ) be use for coordinate correction . many candidate frame ( anchor ) be set in rpn . then the dependency relation of the anchor to the background be judge , to work out the coverage area of the anchor and determine whether the target be cover . in addition , yolo improve the accuracy of prediction , speed up the processing speed and increase the type of identiﬁed object , and propose a joint training method for target classiﬁcation and detection . yolo be one of the most widely use target detection algorithm , offer real-time detection and a series of improved version since then . different from object detection , semantic segmentation not only predict the position and category of object in the image but also accurately describe the boundary between different kind of object . however , in semantic segmentation technology , an ordinary convolutional neural', 'and category of object in the image but also accurately describe the boundary between different kind of object . however , in semantic segmentation technology , an ordinary convolutional neural network can not obtain enough information . to solve this problem , long et al . propose a fully convolutional neural network fcn [ 217 ] . compared with cnn , fcn do not have a fully connect layer . the new fcn obtain the spatial position of the feature map and fuse the output of different depth layer with the hierarchical structure . this method combine local information with global information and improve the accuracy of semantic segmentation . in segnet network propose by badriarayansn et al . [ 218 ] , the encoder-decoder structure be propose , which combine two independent network to improve the accuracy of segmentation . however , the combination of two independent network severely reduce the detection speed . zhao et al . propose pspnet [ 219 ] and a pyramid module , which fuse the feature of each level , such a a pyramid , and ﬁnally fuse the output to far improve the segmentation effect . in recent year , the continuous improvement of computer performance promote the rapid development of instance segmentation in vision . instance segmentation not only have the classiﬁcation on the pixel level ( semantic segmentation ) but also have the location information of different object ( target detection ) , even the same object can be detect . in 2017 , he et al . propose the mask r-cnn [ 220 ] . this algorithm be the pioneering work of instance segmentation . as show in figure 19 , it main idea be to add a branch for semantic segmentation base on faster r-cnn . convolutiona l backbone rpn roialign layer classification mask branch roi pool conv box regession layer \\u0e00 å figure 19 . the framework of mask-rcnn . although the target detection and segmentation technology base on a neural network have be perfect , it need to rely on powerful compute capacity to achieve real-time processing . vslam have a high requirement for', 'detection and segmentation technology base on a neural network have be perfect , it need to rely on powerful compute capacity to achieve real-time processing . vslam have a high requirement for real-time operation , so how efﬁciently separate the need object and it semantic information from the environment will be a long-term and hard task . as the basis of semantic vslam , after process semantic segmentation , we will pay attention to the inﬂuence of semantic information on different aspect of vslam . we will elaborate on three aspect of localization , mapping , and dy- namic object removal . object detection and semantic segmentation be both a mean of extract semantic information from image . table 11 show the contribution of some algorithm . object detection be faster than semantic segmentation . however , semantic segmentation be well in precision . instance segmentation integrates object detection and semantic segmentation , and have outstanding performance in precision , but can not guar- antee the run speed . for some scheme that can not provide the original paper , we provide the open-source code , such a yolov5.remote sens . 2022 ,14 , 3010 29 of 47 table 11 . part of the classical image detection algorithm . field model year contribution object detection r-cnn [ 212 ] 2014 the ﬁrst algorithm that successfully apply deep learning to target detection . fast r-cnn [ 213 ] 2015 image feature extraction be perform only once . faster r-cnn [ 214 ] 2017integrated into a network , the comprehensive performance have be greatly improve . ssd [ 216 ] 2016 ssd be an early incarnation of the single-phase model . yolo [ 215 ] 2016think of detection a a regression problem , use a network to output posi- tions and category . yolov5 [ 221 ] 2020 the environment be easy to conﬁgure and model training be very fast . semantic segmentationfcn [ 217 ] 2015it open the ﬁrst application of a convolutional neural network in semantic segmentation . segnet [ 218 ] 2017 a completely symmetrical structure be adopt . deeplabv1 [ 222', 'segmentationfcn [ 217 ] 2015it open the ﬁrst application of a convolutional neural network in semantic segmentation . segnet [ 218 ] 2017 a completely symmetrical structure be adopt . deeplabv1 [ 222 ] 2014 atrous convolution . deeplabv3+ [ 223 ] 2018 greatly reduce the number of parameter . pspnet [ 219 ] 2017a pyramid pooling module can aggregate contextual information from different region . instance segmentationmask r-cnn [ 220 ] 2017it can not only detect the target in the image but also give a high-quality segmentation result for each target . yolact [ 224 ] 2019based on the one-stage target detection algorithm , the overall architecture design be very lightweight and achieves good result in speed and effect . 4.2.2 . semantic with location location accuracy be one of the most basic assessment standard in the slam system and be a precondition for mobile robot to perform many task [ 225 ] . introducing environ- mental semantic information can effectively improve the scale uncertainty and cumulative drift in visual slam localization , thus improve the localization accuracy to vary degree [ 226 ] . bowman et al . [ 177 ] propose a sensor state estimation and semantic landmark location optimization problem , which integrate metric information , semantic information , and data association . after obtain semantic information from target detection , they introduce the expectation-maximization ( em ) and calculate the probability of data association accord to the result of semantic classiﬁcation . they successfully convert semantic slam into a probability problem and improve the localization accuracy of the slam system . however , there be many strong assumption in this paper . such a the projection of the three-dimensional center of the object should be close to the center of the detection network , which be not easy to meet in practice . in 2020 , zhao et al . [ 227 ] of xi ’ an jiaotong university propose a landmark visual semantic slam system for a large-scale outdoor environment . its core be to combine a 3d', 'to meet in practice . in 2020 , zhao et al . [ 227 ] of xi ’ an jiaotong university propose a landmark visual semantic slam system for a large-scale outdoor environment . its core be to combine a 3d point cloud in orb-slam with semantic segmentation information in the convolutional neural network model pspnet-101 . it can build a 3d semantic map of a large-scale environment . they propose a method to associate real landmark with a point cloud map . it associate architectural landmark with the semantic point cloud and associate landmark obtain from google maps with a semantic 3d map for urban area navigation . with the help of a semantic point cloud , the system realize landmark-based relocation in a wide range of outdoor environment without gps information . its process be show in figure 20 . in 2018 , eth zurich propose vso [ 228 ] base on semantic information for autonomous drive scenario . this scheme solve the problem of visual slam localization in the environment of outdoor lighting change . it establish constraint between semantic information with image and take advantage of the advantage that semantic information be not affect by angle of view , scale , and illumination . similarly , stenborg et al . [ 229 ] also propose solution to such problem . in the aspect of trajectory estimation , geometric feature can only provide short- term constraint for camera pose , which will produce large deviation in a wide rangeremote sens . 2022 ,14 , 3010 30 of 47 of environment . in contrast , object , a higher-level feature , can keep their semantic information unchanged when light intensity , observation distance , and angle change . for example , a table be still a table under any light and angle , and it more stable performance can provide long-term constraint for the camera posture . in addition , semantic slam can effectively solve the problem that traditional visual slam be sensitive to illumination change and interfere with the robustness of system position . we believe that vslam localization be essentially', 'effectively solve the problem that traditional visual slam be sensitive to illumination change and interfere with the robustness of system position . we believe that vslam localization be essentially camera pose estimation . semantic information can improve the position accuracy of traditional vslam system under strong illumination and high camera rotation . however , in practice , the introduction of semantic information will inevitably slow down the operation of the whole system , which be an urgent problem to be solve in vslam . we believe that in most case , traditional vslam still perform well in localization accuracy . however , semantic help for vslam system to improve localization accuracy be also worthy of research . table 12 compare the difference between traditional method and semantic method for vslam localization . semantic segmentation orb slam gps fusion landmark association topological mapping current frame current frame semantic point cloud transformation matrix semantic point clouds point clouds with landmarks figure 20 . zhao et al . propose a large-scale outdoor position process use semantic information . table 12 . comparison between traditional method and semantic method for vslam localization . method characteristic traditional epipolar geometry , perspective- n-point , iterative closest point , optical flow ···geometric feature can only provide short-term constraint for camera pose and may fail in environment with strong light and fast motion . semantic semantic label , data association the semantic information can remain constant when the light intensity , obser- vation distance , and angle change . 4.2.3 . semantic with mapping another key juncture of vslam and deep learning be the semantic map construction of slam , and most semantic vslam system be base on this idea [ 230 ] . for a robot to understand the environment as well a a human and perform different task from one place to another require a different skill than a geometric map can provide [ 231 ] . robots should have the ability', 'understand the environment as well a a human and perform different task from one place to another require a different skill than a geometric map can provide [ 231 ] . robots should have the ability to have a human-centered understanding of their environment . it need to distinguish between a room and a hallway , or the different function of a kitchen and a living room in the future [ 232 ] . therefore , semantic attribute involve human concept ( such a room type , object , and their spatial layout ) , which be consider a necessary attribute of future robot [ 233 ] . in recent year , with the rapid development of deep learning , a semantic map contain semantic information have gradually come into people ’ s view [ 234 ] . the semantic map in the semantic slam system enable robot to obtain geometric information such a feature point of the environment . furthermore , it also identiﬁes object in the environment and obtain semantic information such a location , attribute , and category . compared with the map construct by traditional vslam , the robot can be equip with perceptual ability . it be signiﬁcant for the robot to deal withremote sens . 2022 ,14 , 3010 31 of 47 a complex environment and complete human–computer interaction [ 235 ] . semantic map construction be one of the hot topic in slam research [ 236 ] . in 2005 , galindo et al . [ 237 ] propose the concept of a semantic map . as show in figure 21 , it be represent by two parallel layer : spatial representation and semantic representation . it provide robot with an inference ability similar to human to the environment ( for example , a bedroom be a room contain a bed ) . later , vasudevan et al . [ 238 ] far strengthened people ’ s understanding of semantic map . local gridmap 1local gridmap 2image 1image 2 place 1place 2n2 n1 n3 a area 1room 1bed 1sofa 1room 2apart ment living room bedr oom bed sofa room object thing anchoring spatial hierarchy conceptual hierarchy anchoring anchoring figure 21 . the semantic map concept mention in galindo ’ s article .', ""2apart ment living room bedr oom bed sofa room object thing anchoring spatial hierarchy conceptual hierarchy anchoring anchoring figure 21 . the semantic map concept mention in galindo ’ s article . in recent year , deep learning technology have develop rapidly . more and more researcher combine deep learning with slam technology . they use target detection , semantic segmentation , and other algorithm to obtain semantic information about the environment . furthermore , integrate it into the environment map to construct the en- vironment semantic map [ 239 ] . as show in figure 22 , the research on semantic map construction be mainly divide into two direction : scene-oriented semantic map construc- tion and object-oriented semantic map construction . for sence for object the scene-oriented semantic map focus on the robo t 's perception of the environment , while the object-oriented semantic map focus on the robot 's interaction with entity . figure 22 . different type of semantic map . most scenario-oriented semantic map be base on deep learning method , which map 2d semantic information to 3d point cloud . scenario-oriented semantic map can help robots good understand their environment [ 240 ] . in 2020 , mit propose kimera [ 241 ] . this be a mature scenario-oriented semantic slam algorithm . ref . [ 242 ] propose an algorithm of semantic map construction orient to the scene . based on rtabmap [ 243 ] , yolo be use for target detection . after roughly estimate the position of the object , they use the canny operator to detect the edge of the target object in the depth image . then they achieve accurate segmentation of the object by process edge base on the region growth algorithm . through the non-deep learning semantic segmentation algorithm , they solve the problem of large compute resource in traditional semantic map construction , ad construct the scene-oriented semantic map in real-time . the scene-oriented semantic map will help the robot well understand the environment , and build a more"", 'semantic map construction , ad construct the scene-oriented semantic map in real-time . the scene-oriented semantic map will help the robot well understand the environment , and build a more expressiveremote sens . 2022 ,14 , 3010 32 of 47 environment map . however , this method can not provide more help for a robot to know the environment , prevent the robot and the environment of the individual to interact , to a certain extent restrict the intellectualized degree of the robot [ 244 ] . in addition , such algorithm need to carry out pixel-level semantic segmentation of object in the scene , which lead to much system calculation and low real-time performance . therefore , some scholar turn to object-oriented semantic map construction algorithm [ 245 ] . an object-oriented semantic map refers to a map that contain only partial instance semantic information , and the semantic information exist independently in the method of cluster [ 246 ] . this type of map allow robot to operate and maintain the semantic information of each entity on the map . so it be more conducive for robot to understand the environment and interact with entity in the environment , improve the practicality of the map [ 247 ] . reference [ 45 ] propose a voxel-based semantic visual slam system base on mask-rcnn and kinectfusion algorithm . after object detection by the mask- rcnn algorithm , object detection result be fuse with the tsdf model base on voxel foreground theory to construct an object-oriented semantic map . although the accuracy of detection be guarantee , it still can not solve the problem of the poor real-time performance of the algorithm . ref . [ 248 ] propose a lightweight object-oriented slam system , which effectively solve the problem of data association and attitude estimation , and solve the problem of the poor real-time performance of the above method . the core framework be develop base on orb-slam2 and use yolov3 a an object detector to fuse semantic thread . in the tracer thread , boundary box , semantic label , and', 'of the above method . the core framework be develop base on orb-slam2 and use yolov3 a an object detector to fuse semantic thread . in the tracer thread , boundary box , semantic label , and point cloud information be fuse , and the object-oriented semi-dense semantic map be construct . experimental result show that compare with orb-slam2 , the scheme can deal with multiple class of object with different scale and direction in a complex environment , and can better express the environment . however , for some large object , accurate pose estimation be not possible . similarly , university college london propose dsp-slam [ 249 ] . at present , most semantic map construction method need to deal with both instance segmentation and semantic segmentation at the same time , which lead to poor real-time performance of the system [ 250 ] . table 13 list some semantic map construction work . in addition , when deal with dynamic object , most algorithms realize system robustness by eliminate dynamic object , which will make the system lose much useful information . therefore , slam orient to dynamic scene be an urgent problem to be solve [ 251 ] . table 13 . part of the excellent semantic mapping algorithm . reference year sensor semantic label map contribution vineet et al . [ 252 ] 2015 s random forest voxelthe ﬁrst system can perform dense , large-scale , out- door semantic reconstruction of a scene in real-time . zhao et al . [ 253 ] 2016 d svm voxeluse temporal information and higher-order clique to enforce the labeling consistency for each image label result . li et al . [ 254 ] 2016 d deeplabv2 voxelthere be no need to obtain a semantic segmentation for each frame in a sequence . semanticfusion [ 46 ] 2016 d cnn with crf surfelallows the cnn ’ s semantic prediction from multi- ple viewpoint to be probabilistically fuse into a dense semantically annotate map . yang et al . [ 255 ] 2017 s cnn with crf gridfurther , optimize 3d grid label through a novel crf model . panopticfusion [ 256 ] 2020 dpspnet with crf mask', 'into a dense semantically annotate map . yang et al . [ 255 ] 2017 s cnn with crf gridfurther , optimize 3d grid label through a novel crf model . panopticfusion [ 256 ] 2020 dpspnet with crf mask r-cnn with crfvoxela novel online volumetric semantic mapping sys- tem at the level of stuff and thing . kimera [ 241 ] 2020 s + i pixel-wise meshit be modular and allow replace each module or execute them in isolation . avp-slam [ 161 ] 2020 m + i + e u-net voxel autonomous park . roadmap [ 257 ] 2021 r + m + i + e cnn voxela framework of on-vehicle mapping , on-cloud maintenance , and user-end localization . sensor : s represent stereo camera ; m represent monocular camera ; i represent imu ; e represent encoder ; r represent rtk-gps and d represent rgb-d camera.remote sens . 2022 ,14 , 3010 33 of 47 4.2.4 . elimination of dynamic objects traditional vslam algorithm assume that object in the environment be static or low-motion , which affect the applicability of the vslam system in actual scene [ 258 ] . when dynamic object exist in the environment ( such a people , vehicle and pet ) , they will bring wrong observation data to the system and reduce the accuracy and robustness of the system [ 259 ] . traditional method solve the inﬂuence of some outlier on the system through the ransac algorithm . however , if dynamic object occupy most of the image area or moving object be fast , reliable observation data still can not be obtain [ 260 ] . as show in figure 23 , the camera can not accurately capture data due to dynamic object . so how to solve the impact of dynamic object on the slam system have become the goal of many researcher . now , the solution to the problem of disturbance bring by dynamic object to the slam system be consistent . that be , before the visual odometer , use target detection and image segmentation algorithm to ﬁlter out the dynamic area in the image . then use static environment point to calculate the nearby position of the camera and construct a map contain semantic information [ 261 ] . figure', ""to ﬁlter out the dynamic area in the image . then use static environment point to calculate the nearby position of the camera and construct a map contain semantic information [ 261 ] . figure 24 show a typical structure . although the inﬂuence of dynamic object can not be completely solve , the robustness of the system be greatly improve . x ( a ) x1 x1 x2 e1 e2 l1 l2 x2 x 1x1'x2 x2' ( b ) figure 23 . traditional method use geometric constraint to judge whether an object be move or not . for example , in ( a ) , x be a static point in space , so the spatial transformation relation can be obtain smoothly . in ( b ) , the motion of space point x1 will bring systematic error after it move to x2 . image feature extraction dynamic points remove pose estimation track key frame decision tracking object detection or semantic segmentation semantic labels key frame key frame insertion local ba key frame culling local mapping loop correction loop dection loop closing point cloud splicing dynamic points remove semantic mapping figure 24 . typical dynamic object removal frame.remote sens . 2022 ,14 , 3010 34 of 47 in 2018 , bescos et al . [ 262 ] propose the dynaslam algorithm for visual slam for dynamic scenario base on orb-slam2 . the system provide interface for monocular , stereo , and rgb-d camera . for monocular and stereo camera , mask-rcnn be use to segment dynamic object in each frame to avoid feature extraction of dynamic object in the slam system . if an rgb-d camera be use , the method of multi-view geometry be use for more accurate motion segmentation . dynamic segment be remove from the current frame and map . however , this method choose to remove all potentially moving object , such a parked car . this may lead to too few remain stationary feature point and affect camera pose estimation . in the same year , the tsinghua university team propose a complete slam system ds-slam [ 263 ] base on orb-slam2 . its core be the orb-slam2 add a semantic network segmentation , and a a separate thread run in real-time . it"", 'university team propose a complete slam system ds-slam [ 263 ] base on orb-slam2 . its core be the orb-slam2 add a semantic network segmentation , and a a separate thread run in real-time . it can remove object in the scene dynamic segmentation and create a separate thread to build a dense semantic octree map to help the robot to achieve a high level of the task . some method use semantic information to hide object that be consider to be dynamic . although such method improve the inﬂuence of dynamic object on the system to a certain extent , the one-size-ﬁts-all approach may cause the system to lose many useful feature point . for example , a car park on the roadside may be regard a a dynamic object and all feature point carry by it be ﬁltered out [ 264 ] . however , a car stationary on the side of the road can be use a a reliable feature point in the system . however , it can even be a major source of high-quality feature point . reference [ 265 ] propose the integration of semantic information into traditional vslam method . this method do not need to motion detection . the introduction of conﬁdence , give each object a different possible movement probability , to judge whether an object be in motion . furthermore , the semantic label distribution be combine with map point observation consistency , to estimate the reliability of each 3d point measurement . then use it in the map of pose estimation and optimization step . this method can handle object that be consider dynamic but be stationary , such a car park on the side of the road . reference [ 266 ] be base on the optical ﬂow method to remove dynamic object . its core idea be base on orb-slam2 . in it front end , four cnn neural network be use to simultaneously predict the depth , posture , optical ﬂow , and semantic mask of each frame . by calculate the rigid optical ﬂow synthesize by depth and posture and compare the estimate optical ﬂow , the initial motion region be obtain . the algorithm can distinguish the move object from the current scene and', 'rigid optical ﬂow synthesize by depth and posture and compare the estimate optical ﬂow , the initial motion region be obtain . the algorithm can distinguish the move object from the current scene and retain the feature point of the static object . avoiding the removal of the move object base on the category attribute only , which lead to the tracking failure of the slam system . the article [ 267 ] have present a visual slam system that be build on orb-slam2 and performs robustly and accurately in dynamic environment through discard the move feature point with the help of semantic information obtain by mask-rcnn and depth information provide by rgb-d camera . this method try to exploit more reliable feature point for camera pose estimation by ﬁnding out the static feature point extract from movable object , which would beneﬁt a lot when static object could not provide enough feature point in the scene . semantic information can better help the system to solve the interference bring by dynamic object , due to the high consumption of compute resource . however , the exist scheme be generally not real-time enough to be widely promote to practical robot , and the application scenario be greatly limited [ 268 ] . in addition , semantic infor- mation may not be available at the camera frame rate , or may not always provide accurate data [ 269 ] . assigning an image region to the wrong semantic class may unnecessarily exclude it from posture estimation , which can be critical in a sparsely textured environ- ment [ 270 ] . current solution to this problem focus on use method such a optical ﬂow to detect object that be move in the scene [ 271 ] . although the exist algorithm have achieve good result in data set , they have not achieve very reliable result in practical engineering . table 14 show the vslam algorithm use a deep neural network to improve the dynamic environment in recent years.remote sens . 2022 ,14 , 3010 35 of 47 table 14 . some excellent vslam algorithm for dynamic scenario in recent year . model year', 'neural network to improve the dynamic environment in recent years.remote sens . 2022 ,14 , 3010 35 of 47 table 14 . some excellent vslam algorithm for dynamic scenario in recent year . model year sensor scene dynamic detection dataset code resource reddy et al . [ 272 ] 2016 stereo outdoor [ 273 ] kitti 0 dynaslam [ 262 ] 2018 monocular/stereo/ rgb-doutdoor/indoor mask r-cnn kitti/tum rgb-d [ 274 ] ds-slam [ 263 ] 2018 rgb-d indoor segnet tum rgb-d [ 275 ] detect-slam [ 276 ] 2018 rgb-d indoor ssd tum rgb-d [ 277 ] wang et al . [ 278 ] 2019 rgb-d indoor yolov3 nyu depth dataset v2 0 slamantic [ 265 ] 2019 monocular/stereo outdoor mask r-cnn tum rgb-d/ vkitti [ 279 ] dynslam [ 280 ] 2018 stereo outdoor cascades [ 281 ] kitti [ 282 ] stdyn-slam [ 283 ] 2022 stereo outdoor segnet kitti [ 284 ] posefusion [ 285 ] 2018 rgb-d indoor openpose freiburg rgb-d slam [ 286 ] rds-slam [ 287 ] 2021 rgb-d indoor segnet/mask r-cnn tum rgb-d [ 288 ] yo-slam [ 289 ] 2021 rgb-d indoor yolact tum rgb-d 0 zhang et al . [ 290 ] 2021 panoramic data yolact [ 291 ] 0 doe-slam [ 292 ] 2021 monocular indoor self-initiated * tum rgb-d 0 drso-slam [ 293 ] 2021 rgb-d indoor mask r-cnn tum rgb-d 0 ddl-slam [ 294 ] 2020 rgb-d indoor dunet tum rgb-d 0 rdmo-slam [ 295 ] 2021 rgb-d indoor mask r-cnn tum rgb-d 0 code resourece : 0 represent no code resource . dynamic detection : [ 242 ] represent please refer to this paper ; self-initiated * represent refer to the method propose in this paper . 5 . conclusions and prospect simultaneous localization and mapping be a major research problem in the robotics community , where a great deal of effort have be devote to develop new method to maximize their robustness and reliability . vision-based slam technology have experience many year of development , and many excellent algorithm have emerge , which have be successfully apply in various ﬁelds such a robotics and uav . the rapid devel- opment of deep learning have promote the innovation of the computer ﬁeld , and the combination of the two have become', 'be successfully apply in various ﬁelds such a robotics and uav . the rapid devel- opment of deep learning have promote the innovation of the computer ﬁeld , and the combination of the two have become an active research ﬁeld . therefore , the research on vs- lam have receive more and more attention . in addition , with the advent of the intelligent era , high requirement be put forward for the autonomy of mobile robot . in order to realize advanced environment perception of robot , semantic vslam have be propose and develop rapidly . traditional vslam only restore the geometric feature of the en- vironment when construct the environment map , which can not meet the requirement of robot navigation , human–computer interaction , autonomous exploration , and other application . however , the early semantic map construction method generally adopt the model library matching method , which require the construction of an object model library in advance , which have great limitation and be not conducive to popularization and applica- tion . with the improvement of computer performance and the rapid development of deep learning technology , vslam technology be combine with deep learn technology to ﬁll the deﬁciency of the traditional vslam system . in recent year , a the most promising and advantageous computer vision processing method , deep learning technology have be widely concern by slam researcher . in the semantic slam system , environmental semantic information can be directly learn from pre-trained image set and real-time perceived image set by deep learning technique . it can also be use to make good use of large data set , give the system great generalization capability . when construct a semantic map , the semantic slam system can use the deep learning method to detect and classify object in the environment and construct a map with rich information , which have good practicality . in this article , we investigate most of the most advanced visual slam solution that use feature to locate robot and map their', 'construct a map with rich information , which have good practicality . in this article , we investigate most of the most advanced visual slam solution that use feature to locate robot and map their surroundings . we classify them accord to the feature type rely on by feature-based visual slam method ; traditional vslam and vslam combine with deep learning . the strength and weakness of each category be thoroughly investigate and , where applicable , the challenge that each solution overcomesremote sens . 2022 ,14 , 3010 36 of 47 be highlight . this work demonstrate the importance of use vision a the only external perceptual sensor to solve slam problem . this be mainly because the camera be an ideal sensor because it be light , passive , low-power , and capable of capture rich and unique information about a scene . however , the use of vision require reliable algorithm with good performance and consistency under variable lighting condition , due to move people or object , phantom of featureless area , transition between day and night , or any other unforeseen circumstance . therefore , slam system use vision a the only sensor remain a challenging and promising research area . image matching and data association be still open research ﬁelds in computer vision and robot vision , respectively . the choice of detector and descriptor directly affect the performance of the system to track salient feature , identify previously see area , build a consistent environmental model , and work in real-time . data correlation in particular require long-term navigation , despite a grow database and a constantly change and complex environment . accepting bad association will cause serious error in the entire slam system , mean that location calculation and map construction will be inconsistent . in addition , we highlight the development of vslam that fuse semantic information . the vslam system combine with semantic information achieves well result in term of robustness , precision , and high-level perception . more attention', 'of vslam that fuse semantic information . the vslam system combine with semantic information achieves well result in term of robustness , precision , and high-level perception . more attention will be pay to the research of semantic vlsam . semantic vslam will fundamentally improve the autonomous interaction ability of robot . combined with other study , we make the following prospect for the future devel- opment of vslam : ( 1 ) engineering application . after decade of development , vslam have be widely use in many ﬁelds such a robotics . however , slam be sensitive to environmental illumination , high-speed motion , motion interference and other problem , so how to improve the robustness of the system and build large-scale map for a long time be all worthy of challenge . the two main scenario use in slam be base on embedded platform such a smart phone or drone , and 3d reconstruction , scene understanding and deep learning . how to balance real-time and accuracy be an important open question . solutions for dynamic , unstructured , complex , uncertain and large-scale environment remain to be explore . ( 2 ) theoretical support . the information feature learn through deep learning still lack intuitive meaning and clear theoretical guidance . at present , deep learning be mainly apply to local sub-modules of slam , such a depth estimation and closed-loop detection . however , how to apply deep learning to the entire slam system remain a big challenge . traditional vslam still have advantage in position and navigation . although some module of traditional method be improve by deep learning , the scope of deep learning be generally not wide , and it may achieve good result in some data set , but it may be unstable in another scene . the positioning and mapping process involve a lot of mathematical formula , and deep learning have drawbacks in deal with mathematical problem while use deep learning have few data to carry out relevant training , and this method be more traditional . the slam framework do not present', 'have drawbacks in deal with mathematical problem while use deep learning have few data to carry out relevant training , and this method be more traditional . the slam framework do not present signiﬁcant advantage and be not yet available . the main algorithm of slam technology . in the future , slam will gradually absorb deep learning method and improve training number data set be use to improve the accuracy and robustness of positioning and mapping . ( 3 ) high-level environmental information perception , and human–computer inter- action . with the further development of deep learning , the research and application of semantic vslam will have a huge space for development . in the future intelligent era , people ’ s demand for intelligent autonomous mobile robot will increase rapidly . how to use semantic vslam technology to good improve the autonomous ability of robot will be a long-term and difﬁcult task . although there have be some excellent achievement in recent year , compare with the classical vslam algorithm , semantic vslam be still in the development stage . currently , there be not many open source solution for semantic slam , and the application of semantic slam be still in the initial stage , mainly because the construction of an accurate semantic map require a lot of compute resource . thisremote sens . 2022 ,14 , 3010 37 of 47 severely interferes with the real-time performance of slam . with the continuous improve- ment of hardware level in the future , the problem of the poor real-time performance of slam system may be greatly improve . ( 4 ) establish a sound evaluation system . semantic vslam technology have develop rapidly in recent year . however , compare with traditional vslam , there be no perfect evaluation criterion for the time be . in slam system research , ate or rpe be generally use to evaluate the system performance . however , both of these evaluation criterion be base on the pose estimation result of the slam system , and there be no universally recognize reliable evaluation', 'to evaluate the system performance . however , both of these evaluation criterion be base on the pose estimation result of the slam system , and there be no universally recognize reliable evaluation criterion for the effect of map construction . for a semantic slam system , how to evaluate the accuracy of semantic information acquisition and how to evaluate the effect of semantic map construction be the issue that should be consider in the evaluation criterion of the semantic slam system . furthermore , it be not a long-term solution to evaluate only by subjective indicator . in the future , it will be a hot topic how to establish systematic evaluation indicator for semantic vslam . author contributions : conceptualization , w.c. , k.h . and g.s . ; methodology , w.c. , k.h . and g.s . ; software , c.z . and x.w . ; formal analysis , w.c. , k.h . and g.s . ; investigation , w.c. , k.h . and a.j . ; writing—original draft preparation , g.s . ; writing—review w.c. , k.h . and g.s . ; edit , w.c. , k.h . and g.s . ; visualization , a.j. , x.w. , c.x. , and z.l . ; supervision , w.c. , k.h . and a.j . ; project administration , w.c. , k.h . and a.j . ; fund acquisition , a.j . all author have read and agree to the publish version of the manuscript . funding : this work be support by the national key r & d programme of china ( grant nos . 2019yfb1309600 ) , national natural science foundation of china ( grant nos . 51875281 and 51861135306 ) . these fund come from a.j . data availability statement : not applicable . acknowledgments : research in this article be support by the national key r & d programme of china ( grant no . 2019yfb1309600 ) , national natural science foundation of china ( grant nos . 51875281 and 51861135306 ) be deeply appreciate . the author would like to express heartfelt thanks to the reviewer and editor who submit valuable revision to this article . conﬂicts of interest : the author declare no conﬂict of interest . abbreviations the following abbreviation be use in this manuscript : slam', 'and editor who submit valuable revision to this article . conﬂicts of interest : the author declare no conﬂict of interest . abbreviations the following abbreviation be use in this manuscript : slam stimulation location and mapping vslam visual stimulation location and mapping cnn convolutional neural network rnn recurrent neural network imu inertial measurement unit evo python package for the evaluation of odometry and slam ate absolute trajectory error rpe relative pose error tum technical university of munich tof time-of-flight cpu central processing unit gpu graphics processing unit bow bags of binary words ukf unscented kalman filter icp iterative closest point tsdf truncated signed distance function vo visual odometry vio visual-inertial odometry dnn deep neural networks lstm long short-term memory networks gru gated recurrent unitremote sens . 2022 ,14 , 3010 38 of 47 3d three-demensional em expectation-maximization mit massachusetts institute of technology uav unmanned aerial vehicle references 1 . smith , r.c . ; cheeseman , p . on the representation and estimation of spatial uncertainty . int . j . robot . res . 1986 ,5 , 56–68 . [ crossref ] 2 . deng , g. ; li , j. ; li , w. ; wang , h. slam : depth image information for mapping and inertial navigation system for localization . in proceedings of the 2016 asia-paciﬁc conference on intelligent robot systems ( acirs ) , tokyo , japan , 20–22 july 2016 ; pp . 187–191 . 3 . cui , l. ; ma , c. sof-slam : a semantic visual slam for dynamic environments . ieee access 2019 ,7 , 166528–166539 [ crossref ] 4 . bresson , g. ; alsayed , z. ; yu , l. ; glaser , s. simultaneous localization and mapping : a survey of current trends in autonomous driving . ieee trans . intell . veh . 2017 ,2 , 194–220 . [ crossref ] 5 . karlsson , n. ; bernardo , e.d . ; ostrowski , j. ; goncalves , l. ; pirjanian , p . ; munich , m.e . the vslam algorithm for robust localization and mapping . in proceedings of the proceedings of the 2005 ieee international conference on robotics and', 'goncalves , l. ; pirjanian , p . ; munich , m.e . the vslam algorithm for robust localization and mapping . in proceedings of the proceedings of the 2005 ieee international conference on robotics and automation , barcelona , spain , 18–22 april 2005 ; pp . 24–29 . 6 . hess , w. ; kohler , d. ; rapp , h. ; andor , d. real-time loop closure in 2d lidar slam . in proceedings of the 2016 ieee international conference on robotics and automation ( icra ) , stockholm , sweden , 16–21 may 2016 ; pp . 1271–1278 . 7 . grisetti , g. ; stachniss , c. ; burgard , w. improving grid-based slam with rao-blackwellized particle filters by adaptive proposals and selective resampling . in proceedings of the proceedings of the 2005 ieee international conference on robotics and automation , barcelona , spain , 18–22 april 2005 ; pp . 2432–2437 . 8 . khan , m.u . ; zaidi , s.a.a . ; ishtiaq , a. ; bukhari , s.u.r . ; samer , s. ; farman , a . a comparative survey of lidar-slam and lidar base sensor technologies . in proceedings of the 2021 mohammad ali jinnah university international conference on computing ( majicc ) , karachi , pakistan , 15–17 july 2021 ; pp . 1–8 . 9 . gupta , a. ; fernando , x . simultaneous localization and mapping ( slam ) and data fusion in unmanned aerial vehicles : recent advances and challenges . drones 2022 ,6 , 85 . [ crossref ] 10 . arun , a. ; nirmaladevi , p . a survey on current semantic level algorithms for improve performance in cbir . in proceedings of the materials science and engineering conference series , chennai , india , 1 february 2021 ; p. 012118 . 11 . burguera , a. ; bonin-font , f. ; font , e.g . ; torres , a.m . combining deep learning and robust estimation for outlier-resilient underwater visual graph slam . j. mar . sci . eng . 2022 ,10 , 511 . [ crossref ] 12 . alatise , m.b . ; hancke , g.p . a review on challenges of autonomous mobile robot and sensor fusion methods . ieee access 2020 , 8 , 39830–39846 . [ crossref ] 13 . wang , p . ; cheng , j. ; feng , w. efﬁcient construction of', 'g.p . a review on challenges of autonomous mobile robot and sensor fusion methods . ieee access 2020 , 8 , 39830–39846 . [ crossref ] 13 . wang , p . ; cheng , j. ; feng , w. efﬁcient construction of topological semantic map with 3d information . j. intell . fuzzy syst . 2018 ,35 , 3011–3020 . [ crossref ] 14 . wang , s. ; clark , r. ; wen , h. ; trigoni , n. end-to-end , sequence-to-sequence probabilistic visual odometry through deep neural network . int . j . robot . res . 2017 ,37 , 513–542 . [ crossref ] 15 . cadena , c. ; carlone , l. ; carrillo , h. ; latif , y. ; scaramuzza , d. ; neira , j. ; reid , i. ; leonard , j.j. past , present , and future of simultaneous localization and mapping : toward the robust-perception age . ieee trans . robot . 2016 ,32 , 1309–1332 . [ crossref ] 16 . taketomi , t. ; uchiyama , h. ; ikeda , s. visual slam algorithm : a survey from 2010 to 2016 . ipsj trans . comput . vis . appl . 2017 ,9 , 16 . [ crossref ] 17 . yousif , k. ; bab-hadiashar , a. ; hoseinnezhad , r. an overview to visual odometry and visual slam : applications to mobile robotics . intell . ind . syst . 2015 ,1 , 289–311 . [ crossref ] 18 . bavle , h. ; sánchez-lópez , j.l . ; schmidt , e.f. ; voos , h. from slam to situational awareness : challenges and survey . arxiv 2021 , arxiv:2110.00273 . 19 . servières , m. ; renaudin , v . ; dupuis , a. ; antigny , n. visual and visual-inertial slam : state of the art , classiﬁcation , and experimental benchmarking . j. sens . 2021 ,2021 , 2054828 . [ crossref ] 20 . azzam , r. ; taha , t. ; huang , s. ; zweiri , y. feature-based visual simultaneous localization and mapping : a survey . sn appl . sci . 2020 ,2 , 224 . [ crossref ] 21 . macario barros , a. ; michel , m. ; moline , y. ; corre , g. ; carrel , f. a comprehensive survey of visual slam algorithms . robotics 2022 ,11 , 24 . [ crossref ] 22 . li , r. ; wang , s. ; gu , d. ongoing evolution of visual slam from geometry to deep learning : challenges and opportunities . cogn . comput . 2018 ,10 , 875–889 . [', '2022 ,11 , 24 . [ crossref ] 22 . li , r. ; wang , s. ; gu , d. ongoing evolution of visual slam from geometry to deep learning : challenges and opportunities . cogn . comput . 2018 ,10 , 875–889 . [ crossref ] 23 . medeiros esper , i. ; smolkin , o. ; manko , m. ; popov , a. ; from , p .j . ; mason , a . evaluation of rgb-d multi-camera pose estimation for 3d reconstruction . appl . sci . 2022 ,12 , 4134 . [ crossref ] 24 . zuo , y. ; yang , j. ; chen , j. ; wang , x. ; wang , y. ; kneip , l. devo : depth-event camera visual odometry in challenging conditions . arxiv 2022 , arxiv:2202.02556 . 25 . evo . python . available online : http : //github.com/michaelgrupp/evo ( access on 25 april 2022 ) .remote sens . 2022 ,14 , 3010 39 of 47 26 . bodin , b. ; wagstaff , h. ; saecdi , s. ; nardi , l. ; vespa , e. ; mawer , j. ; nisbet , a. ; lujan , m. ; furber , s. ; davison , a.j . ; et al . slambench2 : multi-objective head-to-head benchmarking for visual slam . in proceedings of the 2018 ieee international conference on robotics and automation ( icra ) , brisbane , australia , 21–25 may 2018 ; pp . 3637–3644 . 27 . whelan , t. ; salas-moreno , r.f . ; glocker , b. ; davison , a.j . ; leutenegger , s. elasticfusion : real-time dense slam and light source estimation . int . j . robot . res . 2016 ,35 , 1697–1716 . [ crossref ] 28 . mur-artal , r. ; tardós , j.d . orb-slam2 : an open-source slam system for monocular , stereo , and rgb-d cameras . ieee trans . robot . 2017 ,33 , 1255–1262 . [ crossref ] 29 . leutenegger , s. ; lynen , s. ; bosse , m. ; siegwart , r. ; furgale , p . keyframe-based visual–inertial odometry use nonlinear optimization . int . j . robot . res . 2014 ,34 , 314–334 . [ crossref ] 30 . tum rgb-d . available online : http : //vision.in.tum.de/data/datasets/rgbd-dataset ( access on 25 april 2022 ) . 31 . tum monovo . available online : http : //vision.in.tum.de/mono-dataset ( access on 25 april 2022 ) . 32 . tum vi . available online : http :', '( access on 25 april 2022 ) . 31 . tum monovo . available online : http : //vision.in.tum.de/mono-dataset ( access on 25 april 2022 ) . 32 . tum vi . available online : http : //vision.in.tum.de/data/datasets/visual-inertial-dataset ( access on 25 april 2022 ) . 33 . kitti . available online : http : //www.cvlibs.net/datasets/kitti/ ( access on 22 may 2022 ) . 34 . euroc . available online : http : //projects.asl.ethz.ch/datasets/doku.php ? id=kmavvisualinertialdatasets ( access on 25 april 2022 ) . 35 . cityscapes . available online : http : //www.cityscapes-dataset.com/ ( access on 25 april 2022 ) . 36 . icl-nuim . available online : http : //www.doc.ic.ac.uk/ahanda/vafric/iclnuim.html ( access on 25 april 2022 ) . 37 . nyu rgb-d . available online : http : //cs.nyu.edu/silberman/datasets/ ( access on 25 april 2022 ) . 38 . ms coco . available online : http : //paperswithcode.com/dataset/coco ( access on 25 april 2022 ) . 39 . engel , j. ; koltun , v . ; cremers , d. direct sparse odometry . ieee trans . pattern anal . mach . intell . 2018 ,40 , 611–625 . [ crossref ] [ pubmed ] 40 . engel , j. ; schöps , t. ; cremers , d. lsd-slam : large-scale direct monocular slam . in proceedings of the computer vision—eccv 2014 , zurich , switzerland , 6–12 september 2014 ; pp . 834–849 . 41 . niko sünderhauf . available online : http : //nikosuenderhauf.github.io/projects/sceneunderstanding/ ( access on 12 june 2022 ) . 42 . semanticslam.ai . available online : http : //www.semanticslam.ai/ ( access on 12 june 2022 ) . 43 . the dyson robotics lab at imperial college . available online : http : //www.imperial.ac.uk/dyson-robotics-lab ( access on 25 april 2022 ) . 44 . bloesch , m. ; czarnowski , j. ; clark , r. ; leutenegger , s. ; davison , a.j . codeslam - learning a compact , optimisable representa- tion for dense visual slam . in proceedings of the 2018 ieee/cvf conference on computer vision and pattern recognition , salt lake city , ut , usa , 18–23 june 2018 ; pp . 2560–2568 . 45 . mccormac , j. ; clark , r. ;', 'slam . in proceedings of the 2018 ieee/cvf conference on computer vision and pattern recognition , salt lake city , ut , usa , 18–23 june 2018 ; pp . 2560–2568 . 45 . mccormac , j. ; clark , r. ; bloesch , m. ; davison , a. ; leutenegger , s. fusion++ : volumetric object-level slam . in proceedings of the 2018 international conference on 3d vision ( 3dv ) , verona , italy , 5–8 september 2018 ; pp . 32–41 . 46 . mccormac , j. ; handa , a. ; davison , a. ; leutenegger , s. semanticfusion : dense 3d semantic mapping with convolutional neural network . in proceedings of the 2017 ieee international conference on robotics and automation ( icra ) , singapore , 29 may–3 june 2017 ; pp . 4628–4635 . 47 . computer vision group tum department of informatics technical university of munich . available online : http : //vision.in . tum.de/research ( access on 25 april 2022 ) . 48 . yang , n. ; stumberg , l.v . ; wang , r. ; cremers , d. d3vo : deep depth , deep pose and deep uncertainty for monocular visual odometry . in proceedings of the 2020 ieee/cvf conference on computer vision and pattern recognition ( cvpr ) , seattle , wa , usa , 13–19 june 2020 ; pp . 1278–1289 . 49 . stumberg , l.v . ; cremers , d. dm-vio : delayed marginalization visual-inertial odometry . ieee robot . autom . lett . 2022 ,7 , 1408–1415 . [ crossref ] 50 . gao , x. ; wang , r. ; demmel , n. ; cremers , d. ldso : direct sparse odometry with loop closure . in proceedings of the 2018 ieee/rsj international conference on intelligent robots and systems ( iros ) , madrid , spain , 1–5 october 2018 ; pp . 2198–2204 . 51 . autonomous intelligent systems university of freiburg . available online : http : //ais.informatik.uni-freiburg.de/index_en.php ( access on 23 may 2022 ) . 52 . grisetti , g. ; stachniss , c. ; burgard , w. improved techniques for grid mapping with rao-blackwellized particle filters . ieee trans . robot . 2007 ,23 , 34–46 . [ crossref ] 53 . endres , f. ; hess , j. ; sturm , j. ; cremers , d. ; burgard , w. 3-d mapping with an rgb-d', 'with rao-blackwellized particle filters . ieee trans . robot . 2007 ,23 , 34–46 . [ crossref ] 53 . endres , f. ; hess , j. ; sturm , j. ; cremers , d. ; burgard , w. 3-d mapping with an rgb-d camera . ieee trans . robot . 2013 ,30 , 177–187 . [ crossref ] 54 . hkust aerial robotics group . available online : http : //uav.hkust.edu.hk/ ( access on 25 april 2022 ) . 55 . qin , t. ; li , p . ; shen , s. vins-mono : a robust and versatile monocular visual-inertial state estimator . ieee trans . robot . 2018 ,34 , 1004–1020 . [ crossref ] 56 . qin , t. ; pan , j. ; cao , s. ; shen , s. a general optimization-based framework for local odometry estimation with multiple sensors . arxiv 2019 , arxiv:1901.03638 . 57 . zhou , y. ; gallego , g. ; shen , s. event-based stereo visual odometry . ieee trans . robot . 2021 ,37 , 1433–1450 . [ crossref ] 58 . uw robotics and state estimation lab . available online : http : //rse-lab.cs.washington.edu/projects/ ( access on 25 april 2022 ) . 59 . schmidt , t. ; newcombe , r. ; fox , d. dart : dense articulate real-time tracking with consumer depth camera . auton . robot . 2015 ,39 , 239–258 . [ crossref ] 60 . xiang , y. ; fox , d. da-rnn : semantic mapping with data associated recurrent neural networks . arxiv 2017 , arxiv:1703.03098.remote sens . 2022 ,14 , 3010 40 of 47 61 . henry , p . ; krainin , m. ; herbst , e. ; ren , x. ; fox , d. rgb-d mapping : using depth cameras for dense 3d modeling of indoor environments . in experimental robotics : the 12th international symposium on experimental robotics , khatib , o. , kumar , v . , sukhatme , g. , eds . ; springer : berlin/heidelberg , germany , 2014 ; pp . 477–491 . 62 . robotics , perception and real time group universidad de zaragoza . available online : http : //robots.unizar.es/slamlab/ ( access on 25 april 2022 ) . 63 . gálvez-lópez , d. ; salas , m. ; tardós , j.d . ; montiel , j.m.m . real-time monocular object slam . robot . auton . syst . 2016 ,75 , 435–449 . [ crossref ] 64 . taheri , h. ; xia , z.c . slam ; deﬁnition', ', d. ; salas , m. ; tardós , j.d . ; montiel , j.m.m . real-time monocular object slam . robot . auton . syst . 2016 ,75 , 435–449 . [ crossref ] 64 . taheri , h. ; xia , z.c . slam ; deﬁnition and evolution . eng . appl . artif . intell . 2021 ,97 , 104032 . [ crossref ] 65 . lin , l. ; wang , w. ; luo , w. ; song , l. ; zhou , w. unsupervised monocular visual odometry with decoupled camera pose estimation . digit . signal process . 2021 ,114 , 103052 . [ crossref ] 66 . zhu , k. ; jiang , x. ; fang , z. ; gao , y. ; fujita , h. ; hwang , j.-n. photometric transfer for direct visual odometry . knowl.-based syst . 2021 ,213 , 106671 . [ crossref ] 67 . guclu , o. ; can , a.b . k-slam : a fast rgb-d slam approach for large indoor environment . comput . vis . image underst . 2019 , 184 , 31–44 . [ crossref ] 68 . cai , l. ; ye , y. ; gao , x. ; li , z. ; zhang , c. an improved visual slam base on afﬁne transformation for orb feature extraction . optik 2021 ,227 , 165421 . [ crossref ] 69 . harris , c. ; stephens , m. a combined corner and edge detector . in proceedings of the alvey vision conference , manchester , uk , 31 august–2 september 1988 . 70 . rosten , e. ; drummond , t. machine learning for high-speed corner detection . in proceedings of the computer vision—eccv 2006 , graz , austria , 7–13 may 2006 ; pp . 430–443 . 71 . jianbo , s. ; tomasi . good feature to track . in proceedings of the 1994 proceedings of ieee conference on computer vision and pattern recognition , seattle , wa , usa , 21–23 june 1994 ; pp . 593–600 . 72 . lowe , d.g . distinctive image features from scale-invariant keypoints . int . j. comput . vis . 2004 ,60 , 91–110 . [ crossref ] 73 . bay , h. ; ess , a. ; tuytelaars , t. ; van gool , l. speeded-up robust features ( surf ) . comput . vis . image underst . 2008 ,110 , 346–359 . [ crossref ] 74 . rublee , e. ; rabaud , v . ; konolige , k. ; bradski , g. orb : an efﬁcient alternative to sift or surf . in proceedings of the 2011 international conference on computer vision , barcelona', '] 74 . rublee , e. ; rabaud , v . ; konolige , k. ; bradski , g. orb : an efﬁcient alternative to sift or surf . in proceedings of the 2011 international conference on computer vision , barcelona , spain , 6–13 november 2011 ; pp . 2564–2571 . 75 . ali , a.m. ; nordin , m.j. sift base monocular slam with multi-clouds feature for indoor navigation . in proceedings of the tencon 2010—2010 ieee region 10 conference , fukuoka , japan , 21–24 november 2010 ; pp . 2326–2331 . 76 . gioi , r.g.v . ; jakubowicz , j. ; morel , j.m . ; randall , g. lsd : a fast line segment detector with a false detection control . ieee trans . pattern anal . mach . intell . 2010 ,32 , 722–732 . [ crossref ] 77 . davison , a.j . ; reid , i.d . ; molton , n.d. ; stasse , o. monoslam : real-time single camera slam . ieee trans . pattern anal . mach . intell . 2007 ,29 , 1052–1067 . [ crossref ] 78 . hu , k. ; wu , j. ; weng , l. ; zhang , y. ; zheng , f. ; pang , z. ; xia , m. a novel federate learn approach base on the conﬁdence of federated kalman ﬁlters . int . j. mach . learn . cybern . 2021 ,12 , 3607-3627 . [ crossref ] 79 . klein , g. ; murray , d. parallel tracking and mapping for small ar workspaces . in proceedings of the 2007 6th ieee and acm international symposium on mixed and augmented reality , nara , japan , 13–16 november 2007 ; pp . 225–234 . 80 . mur-artal , r. ; montiel , j.m.m . ; tardós , j.d . orb-slam : a versatile and accurate monocular slam system . ieee trans . robot . 2015 ,31 , 1147–1163 . [ crossref ] 81 . galvez-lópez , d. ; tardos , j.d . bags of binary words for fast place recognition in image sequences . ieee trans . robot . 2012 ,28 , 1188–1197 . [ crossref ] 82 . campos , c. ; elvira , r. ; rodríguez , j.j.g . ; montiel , j.m.m . ; tardós , j.d . orb-slam3 : an accurate open-source library for visual , visual–inertial , and multimap slam . ieee trans . robot . 2021 ,37 , 1874–1890 . [ crossref ] 83 . vakhitov , a. ; funke , j. ; moreno-noguer , f. accurate and linear time pose estimation from points and', ', and multimap slam . ieee trans . robot . 2021 ,37 , 1874–1890 . [ crossref ] 83 . vakhitov , a. ; funke , j. ; moreno-noguer , f. accurate and linear time pose estimation from points and lines . in computer vision—eccv 2016 ; springer : cham , switzerland , 2016 ; pp . 583–599 . 84 . smith , p . ; reid , i.d . ; davison , a.j . real-time monocular slam with straight lines . bmvc 2006 ,6 , 17–26 . 85 . perdices , e. ; lópez , l.m . ; cañas , j.m . lineslam : visual real time localization using lines and ukf . in robot2013 : first iberian robotics conference : advances in robotics ; armada , m.a. , sanfeliu , a. , ferre , m. , eds . ; springer international publishing : cham , switzerland , 2014 ; volume 1 , pp . 663–678 . 86 . montero , a.s. ; nayak , a. ; stojmenovic , m. ; zaguia , n. robust line extraction base on repeat segment direction on image contour . in proceedings of the 2009 ieee symposium on computational intelligence for security and defense applications , ottawa , on , canada , 8–10 july 2009 ; pp . 1–7 . 87 . klein , g. ; murray , d. improving the agility of keyframe-based slam . in proceedings of the computer vision—eccv 2008 , marseille , france , 12–18 october 2008 ; pp . 802–815 . 88 . pumarola , a. ; vakhitov , a. ; agudo , a. ; sanfeliu , a. ; moreno-noguer , f. pl-slam : real-time monocular visual slam with point and line . in proceedings of the 2017 ieee international conference on robotics and automation ( icra ) , singapore , 29 may–3 june 2017 ; pp . 4503–4508 . 89 . gomez-ojeda , r. ; moreno , f.a . ; zuñiga-noël , d. ; scaramuzza , d. ; gonzalez-jimenez , j. pl-slam : a stereo slam system through the combination of points and line segments . ieee trans . robot . 2019 ,35 , 734-746 . [ crossref ] remote sens . 2022 ,14 , 3010 41 of 47 90 . gee , a.p . ; chekhlov , d. ; calway , a. ; mayol-cuevas , w. discovering higher level structure in visual slam . ieee trans . robot . 2008 ,24 , 980–990 . [ crossref ] 91 . li , h. ; hu , z. ; chen , x. plp-slam : a visual slam method based on', ', w. discovering higher level structure in visual slam . ieee trans . robot . 2008 ,24 , 980–990 . [ crossref ] 91 . li , h. ; hu , z. ; chen , x. plp-slam : a visual slam method based on point-line-plane feature fusion . robot 2017 ,39 , 214–220 . 92 . zhang , n. ; zhao , y . fast and robust monocular visua-inertial odometry using points and lines . sensors 2019 ,19 , 4545 . [ crossref ] 93 . he , x. ; gao , w. ; sheng , c. ; zhang , z. ; pan , s. ; duan , l. ; zhang , h. ; lu , x. lidar-visual-inertial odometry based on optimized visual point-line features . remote sens . 2022 ,14 , 622 . [ crossref ] 94 . newcombe , r.a. ; lovegrove , s.j . ; davison , a.j . dtam : dense track and map in real-time . in proceedings of the 2011 international conference on computer vision , barcelona , spain , 6–13 november 2011 ; pp . 2320–2327 . 95 . forster , c. ; pizzoli , m. ; scaramuzza , d. svo : fast semi-direct monocular visual odometry . in proceedings of the 2014 ieee international conference on robotics and automation ( icra ) , hong kong , china , 31 may–7 june 2014 ; pp . 15–22 . 96 . zhang , j. ; ganesh , p . ; volle , k. ; willis , a. ; brink , k. low-bandwidth and compute-bound rgb-d planar semantic slam . sensors 2021 ,21 , 5400 . [ crossref ] [ pubmed ] 97 . filatov , a. ; zaslavskiy , m. ; krinkin , k. multi-drone 3d building reconstruction method . mathematics 2021 ,9 , 3033 . [ crossref ] 98 . newcombe , r.a. ; izadi , s. ; hilliges , o. ; molyneaux , d. ; kim , d. ; davison , a.j . ; kohi , p . ; shotton , j. ; hodges , s. ; fitzgibbon , a. kinectfusion : real-time dense surface mapping and tracking . in proceedings of the 2011 10th ieee international symposium on mixed and augmented reality , basel , switzerland , 26–29 october 2011 ; pp . 127–136 . 99 . kaess , m. ; fallon , m. ; johannsson , h. ; leonard , j. kintinuous : spatially extend kinectfusion . in proceedings of the rss workshop on rgb-d : advanced reasoning with depth cameras , sydney , australia , 9–10 july 2012 ; p. 9 . 100 . kinectfusion .', 'j. kintinuous : spatially extend kinectfusion . in proceedings of the rss workshop on rgb-d : advanced reasoning with depth cameras , sydney , australia , 9–10 july 2012 ; p. 9 . 100 . kinectfusion . available online : http : //github.com/parikagoel/kinectfusion ( access on 21 april 2022 ) . 101 . kinitinuous . available online : http : //github.com/mp3guy/kintinuous ( access on 21 april 2022 ) . 102 . rgb-dslamv2 . available online : http : //github.com/felixendres/rgbdslam_v2 ( access on 21 april 2022 ) . 103 . elasticfusion . available online : http : //github.com/mp3guy/elasticfusion ( access on 21 april 2022 ) . 104 . yan , z. ; ye , m. ; ren , l. dense visual slam with probabilistic surfel map . ieee trans . vis . comput . graph . 2017 ,23 , 2389–2398 . [ crossref ] 105 . dvo-slam . available online : http : //github.com/tum-vision/dvo_slam ( access on 21 april 2022 ) . 106 . dai , a. ; nießner , m. ; zollhöfer , m. ; izadi , s. ; theobalt , c. bundlefusion : real-time globally consistent 3d reconstruction using on-the-fly surface reintegration . acm trans . graph . 2017 ,36 , 1 . [ crossref ] 107 . bundlefusion . available online : http : //github.com/niessner/bundlefusion ( access on 21 april 2022 ) . 108 . concha , a. ; civera , j. rgbdtam : a cost-effective and accurate rgb-d tracking and mapping system . in proceedings of the 2017 ieee/rsj international conference on intelligent robots and systems ( iros ) , british , co , canada , 24–28 september 2017 ; pp . 6756–6763 . 109 . rgbdtam . available online : http : //github.com/alejocb/rgbdtam ( access on 21 april 2022 ) . 110 . liu , y. ; zhao , c. ; ren , m. an enhanced hybrid visual-inertial odometry system for indoor mobile robot . sensors 2022 ,22 , 2930 . [ crossref ] 111 . xie , h. ; chen , w. ; wang , j. hierarchical forest base fast online loop closure for low-latency consistent visual-inertial slam . robot . auton . syst . 2022 ,151 , 104035 . [ crossref ] 112 . lee , w. ; eckenhoff , k. ; yang , y. ; geneva , p . ; huang , g.', 'online loop closure for low-latency consistent visual-inertial slam . robot . auton . syst . 2022 ,151 , 104035 . [ crossref ] 112 . lee , w. ; eckenhoff , k. ; yang , y. ; geneva , p . ; huang , g. visual-inertial-wheel odometry with online calibration . in proceedings of the 2020 ieee/rsj international conference on intelligent robots and systems ( iros ) , las vegas , nv , usa 24 october–24 janaury 2020 ; pp . 4559–4566 . 113 . cheng , j. ; zhang , l. ; chen , q . an improved initialization method for monocular visual-inertial slam . electronics 2021 ,10 , 3063 . [ crossref ] 114 . jung , j.h . ; cha , j. ; chung , j.y . ; kim , t.i . ; seo , m.h . ; park , s.y . ; yeo , j.y . ; park , c.g . monocular visual-inertial-wheel odometry using low-grade imu in urban areas . ieee trans . intell . transp . syst . 2022 ,23 , 925-938 . [ crossref ] 115 . weiss , s. vision based navigation for micro helicopters . ph.d. thesis , eth zürich , zürich , switzerland , 2012 . 116 . falquez , j.m . ; kasper , m. ; sibley , g. inertial aid dense & semi-dense method for robust direct visual odometry . in proceedings of the 2016 ieee/rsj international conference on intelligent robots and systems ( iros ) , daejeon , korea , 9–14 october 2016 ; pp . 3601–3607 . 117 . mourikis , a.i . ; roumeliotis , s.i . a multi-state constraint kalman filter for vision-aided inertial navigation . in proceedings of the proceedings 2007 ieee international conference on robotics and automation , roma , italy , 10–14 april 2007 ; pp . 3565-3572 . 118 . wisth , d. ; camurri , m. ; das , s. ; fallon , m. uniﬁed multi-modal landmark tracking for tightly coupled lidar-visual-inertial odometry . ieee robot . autom . lett . 2021 ,6 , 1004–1011 . [ crossref ] 119 . monoslam . available online : http : //github.com/rrg-polito/mono-slam ( access on 22 april 2022 ) . 120 . ptam . available online : http : //github.com/oxford-ptam/ptam-gpl ( access on 22 april 2022 ) . 121 . orb-slam2 . available online : http : //github.com/raulmur/orb_slam2 ( access on 22', ') . 120 . ptam . available online : http : //github.com/oxford-ptam/ptam-gpl ( access on 22 april 2022 ) . 121 . orb-slam2 . available online : http : //github.com/raulmur/orb_slam2 ( access on 22 april 2022 ) . 122 . gomez-ojeda , r. ; briales , j. ; gonzalez-jimenez , j. pl-svo : semi-direct monocular visual odometry by combine point and line segment . in proceedings of the 2016 ieee/rsj international conference on intelligent robots and systems ( iros ) , daejeon , korea , 9–14 october 2016 ; pp . 4211–4216 . 123 . pl-svo . available online : http : //github.com/rubengooj/pl-svo ( access on 22 april 2022 ) . 124 . pl-slam . available online : http : //github.com/rubengooj/pl-slam ( access on 22 april 2022 ) . 125 . dtam . available online : http : //github.com/anuranbaka/opendtam ( access on 22 april 2022 ) .remote sens . 2022 ,14 , 3010 42 of 47 126 . svo . available online : http : //github.com/uzh-rpg/rpg_svo ( access on 22 april 2022 ) . 127 . lsd-slam . available online : http : //github.com/tum-vision/lsds_lam ( access on 21 april 2022 ) . 128 . dso . available online : http : //github.com/jakobengel/dso ( access on 22 april 2022 ) . 129 . msckf-mono . available online : http : //github.com/daniilidis-group/msckf_mono ( access on 22 april 2022 ) . 130 . okvis . available online : http : //github.com/ethz-asl/okvis ( access on 21 april 2022 ) . 131 . bloesch , m. ; omari , s. ; hutter , m. ; siegwart , r. robust visual inertial odometry use a direct ekf-based approach . in proceedings of the 2015 ieee/rsj international conference on intelligent robots and systems ( iros ) , hamburg , germany , 28 september–2 october 2015 ; pp . 298–304 . 132 . rovio . available online : http : //github.com/ethz-asl/rovio ( access on 22 april 2022 ) . 133 . vins-mono . available online : http : //github.com/hkust-aerial-robotics/vins-mono ( access on 22 april 2022 ) . 134 . sualeh , m. ; kim , g.-w. semantics aware dynamic slam based on 3d modt . sensors 2021 ,21 , 6355 . [ crossref ] 135 . wang , s. ; gou , g. ; sui , h.', '( access on 22 april 2022 ) . 134 . sualeh , m. ; kim , g.-w. semantics aware dynamic slam based on 3d modt . sensors 2021 ,21 , 6355 . [ crossref ] 135 . wang , s. ; gou , g. ; sui , h. ; zhou , y. ; zhang , h. ; li , j. cdsfusion : dense semantic slam for indoor environment using cpu computing . remote sens . 2022 ,14 , 979 . [ crossref ] 136 . vishnyakov , b. ; sgibnev , i. ; sheverdin , v . ; sorokin , a. ; masalov , p . ; kazakhmedov , k. ; arseev , s. real-time semantic slam with dcnn-based feature point detection , match and dense point cloud aggregation . int . arch . photogramm . remote sens . spatial inf . sci.2021 , xliii-b2-2021 , 399–404 . [ crossref ] 137 . li , p . ; zhang , g. ; zhou , j. ; yao , r. ; zhang , x. ; zhou , j . study on slam algorithm based on object detection in dynamic scene . in proceedings of the 2019 international conference on advanced mechatronic systems ( icamechs ) , shiga , japan , 26–28 august 2019 ; pp . 363–367 . 138 . xu , d. ; vedaldi , a. ; henriques , j.f . moving slam : fully unsupervised deep learning in non-rigid scenes . in proceedings of the 2021 ieee/rsj international conference on intelligent robots and systems ( iros ) , prague , czech republic , 27 september–1 october 2021 ; pp . 4611–4617 . 139 . hu , k. ; wu , j. ; li , y. ; lu , m. ; weng , l. ; xia , m. fedgcn : federated learning-based graph convolutional networks for non-euclidean spatial data . mathematics .2022 ,10 , 1000 . [ crossref ] 140 . hu , k. ; weng , c. ; zhang , y. ; jin , j. ; xia , q . an overview of underwater vision enhancement : from traditional methods to recent deep learning . j. mar . sci . eng . 2022 ,10 , 241 . [ crossref ] 141 . hu , k. ; li , m. ; xia , m. ; lin , h. multi-scale feature aggregation network for water area segmentation . sensors 2022 ,14 , 206 . [ crossref ] 142 . lechelek , l. ; horna , s. ; zrour , r. ; naudin , m. ; guillevin , c. a hybrid method for 3d reconstruction of mr images . j . imaging 2022 ,8 , 103 . [ crossref ] 143 . hu , k. ; ding , y. ; jin , j.', ', l. ; horna , s. ; zrour , r. ; naudin , m. ; guillevin , c. a hybrid method for 3d reconstruction of mr images . j . imaging 2022 ,8 , 103 . [ crossref ] 143 . hu , k. ; ding , y. ; jin , j. ; weng , l. ; xia , m. skeleton motion recognition based on multi-scale deep spatio-temporal features . appl . sci . 2022 ,12 , 1028 . [ crossref ] 144 . michael , e. ; summers , t.h . ; wood , t.a . ; manzie , c. ; shames , i. probabilistic data association for semantic slam at scale . arxiv 2022 , arxiv:2202.12802 . 145 . li , r. ; wang , s. ; gu , d. deepslam : a robust monocular slam system with unsupervised deep learning . ieee trans . ind . electron . 2021 ,68 , 3577–3587 . [ crossref ] 146 . garg , r. ; bg , v .k . ; carneiro , g. ; reid , i. unsupervised cnn for single view depth estimation : geometry to the rescue . in proceedings of the computer vision—eccv 2016 , amsterdam , the netherlands , 11–14 october 2016 ; pp . 740–756 . 147 . mukherjee , a. ; chakraborty , s. ; saha , s.k . learning deep representation for place recognition in slam . in proceedings of the pattern recognition and machine intelligence , kolkata , india , 5–8 december 2017 ; pp . 557–564 . 148 . gao , x. ; zhang , t. unsupervised learn to detect loop use deep neural network for visual slam system . auton . robot . 2017 ,41 , 1–18 . [ crossref ] 149 . oh , j. ; eoh , g. variational bayesian approach to condition-invariant feature extraction for visual place recognition . appl . sci . 2021 ,11 , 8976 . [ crossref ] 150 . mumuni , a. ; mumuni , f. cnn architectures for geometric transformation-invariant feature representation in computer vision : a review . sn comput . sci . 2021 ,2 , 340 . [ crossref ] 151 . ma , r. ; wang , r. ; zhang , y. ; pizer , s. ; mcgill , s.k . ; rosenman , j. ; frahm , j.-m. rnnslam : reconstructing the 3d colon to visualize miss region during a colonoscopy . med . image anal . 2021 ,72 , 102100 . [ crossref ] [ pubmed ] 152 . wang , k. ; ma , s. ; chen , j. ; ren , f. ; lu , j . approaches , challenges , and', 'visualize miss region during a colonoscopy . med . image anal . 2021 ,72 , 102100 . [ crossref ] [ pubmed ] 152 . wang , k. ; ma , s. ; chen , j. ; ren , f. ; lu , j . approaches , challenges , and applications for deep visual odometry : toward complicated and emerging areas . ieee trans . cogn . dev . syst . 2022 ,14 , 35–49 . [ crossref ] 153 . hong , s. ; bangunharcana , a. ; park , j.-m. ; choi , m. ; shin , h.-s . visual slam-based robotic mapping method for planetary construction . sensors 2021 ,21 , 7715 . [ crossref ] 154 . duan , c. ; junginger , s. ; huang , j. ; jin , k. ; thurow , k. deep learning for visual slam in transportation robotics : a review . transp . saf . environ . 2020 ,1 , 177–184 . [ crossref ] 155 . loo , s.y . ; shakeri , m. ; tang , s.h . ; mashohor , s. ; zhang , h. online mutual adaptation of deep depth prediction and visual slam . arxiv 2021 , arxiv:2111.04096 . 156 . kim , j.j.y . ; urschler , m. ; riddle , p .j . ; wicker , j.s . symbiolcd : ensemble-based loop closure detection use cnn-extracted objects and visual bag-of-words . in proceedings of the 2021 ieee/rsj international conference on intelligent robots and systems ( iros ) , prague , czech republic , 27 september–1 october 2021 ; p. 5425.remote sens . 2022 ,14 , 3010 43 of 47 157 . steenbeek , a. ; nex , f. cnn-based dense monocular visual slam for real-time uav exploration in emergency conditions . drones 2022 ,6 , 79 . [ crossref ] 158 . tateno , k. ; tombari , f. ; laina , i. ; navab , n. cnn-slam : real-time dense monocular slam with learned depth prediction . in proceedings of the 2017 ieee conference on computer vision and pattern recognition ( cvpr ) , honolulu , hi , usa , 21–26 july 2017 ; pp . 6565–6574 . 159 . yang , n. ; wang , r. ; stückler , j. ; cremers , d. deep virtual stereo odometry : leveraging deep depth prediction for monocular direct sparse odometry . in proceedings of the computer vision—eccv 2018 , munich , germany , 8–14 september 2018 ; pp . 835-852 . 160 . godard , c. ; aodha , o.m . ;', 'depth prediction for monocular direct sparse odometry . in proceedings of the computer vision—eccv 2018 , munich , germany , 8–14 september 2018 ; pp . 835-852 . 160 . godard , c. ; aodha , o.m . ; brostow , g.j . unsupervised monocular depth estimation with left-right consistency . in proceedings of the 2017 ieee conference on computer vision and pattern recognition ( cvpr ) , honolulu , hi , usa , 21–26 july 2017 ; pp . 6602–6611 . 161 . qin , t. ; chen , t. ; chen , y. ; su , q. avp-slam : semantic visual mapping and localization for autonomous vehicles in the parking lot . in proceedings of the ieee/rsj international conference on intelligent robots systems , las vegas , nv , usa , 24 october 2020–24 january 2020 ; pp . 5939–5945 . 162 . ronneberger , o. ; fischer , p . ; brox , t. u-net : convolutional networks for biomedical image segmentation . in proceedings of the medical image computing and computer-assisted intervention—miccai 2015 , munich , germany , 5–9 october 2015 ; pp . 234–241 . 163 . zhu , r. ; yang , m. ; liu , w. ; song , r. ; yan , b. ; xiao , z. deepavo : efﬁcient pose reﬁning with feature distil for deep visual odometry . neurocomputing 2022 ,467 , 22–35 . [ crossref ] 164 . luo , y. ; xiao , y. ; zhang , y. ; zeng , n. detection of loop closure in visual slam : a stacked assort auto-encoder base approach . optoelectron . lett . 2021 ,17 , 354–360 . [ crossref ] 165 . wen , s. ; zhao , y. ; yuan , x. ; wang , z. ; zhang , d. ; manfredi , l. path planning for active slam base on deep reinforcement learn under unknown environment . intell . serv . robot . 2020 ,13 , 263–272 . [ crossref ] 166 . memon , a.r . ; wang , h. ; hussain , a. loop closure detection use supervise and unsupervised deep neural network for monocular slam system . robot . auton . syst . 2020 ,126 , 103470 . [ crossref ] 167 . li , d. ; shi , x. ; long , q. ; liu , s. ; yang , w. ; wang , f. ; wei , q. ; qiao , f. dxslam : a robust and efﬁcient visual slam system with deep feature . in proceedings of the 2020 ieee/rsj', '. li , d. ; shi , x. ; long , q. ; liu , s. ; yang , w. ; wang , f. ; wei , q. ; qiao , f. dxslam : a robust and efﬁcient visual slam system with deep feature . in proceedings of the 2020 ieee/rsj international conference on intelligent robots and systems ( iros ) , las vegas , nv , usa , 24 october 2020–24 january 2020 ; pp . 4958–4965 . 168 . qin , c. ; zhang , y. ; liu , y. ; lv , g. semantic loop closure detection base on graph match in multi-objects scene . j. vis . commun . image represent . 2021 ,76 , 103072 . [ crossref ] 169 . chen , c. ; wang , b. ; lu , c.x . ; trigoni , a. ; markham , a . a survey on deep learning for localization and mapping : towards the age of spatial machine intelligence . arxiv 2020 , arxiv:2006.12567 . 170 . ye , x. ; ji , x. ; sun , b. ; chen , s. ; wang , z. ; li , h. drm-slam : towards dense reconstruction of monocular slam with scene depth fusion . neurocomputing 2020 ,396 , 76–91 . [ crossref ] 171 . cao , j. ; zeng , b. ; liu , j. ; zhao , z. ; su , y . a novel relocation method for simultaneous localization and mapping base on deep learning algorithm . comput . electr . eng . 2017 ,63 , 79–90 . [ crossref ] 172 . arshad , s. ; kim , g.-w. role of deep learning in loop closure detection for visual and lidar slam : a survey . sensors 2021 , 21 , 1243 . [ crossref ] 173 . li , r. ; wang , s. ; long , z. ; gu , d. undeepvo : monocular visual odometry through unsupervised deep learning . in pro- ceedings of the 2018 ieee international conference on robotics and automation ( icra ) , brisbane , australia , 21–25 may 2018 ; pp . 7286–7291 . 174 . detone , d. ; malisiewicz , t. ; rabinovich , a.j.a . toward geometric deep slam . arxiv 2017 , arxiv:1707.07410 . 175 . clark , r. ; wang , s. ; wen , h. ; markham , a. ; trigoni , a. vinet : visual-inertial odometry a a sequence-to-sequence learning problem . in proceedings of the aaai , québec city , qc , canada , 23–26 october 2017 . 176 . naseer , t. ; oliveira , g.l . ; brox , t. ; burgard , w. semantics-aware visual localization', 'learning problem . in proceedings of the aaai , québec city , qc , canada , 23–26 october 2017 . 176 . naseer , t. ; oliveira , g.l . ; brox , t. ; burgard , w. semantics-aware visual localization under challenge perceptual condition . in proceedings of the 2017 ieee international conference on robotics and automation ( icra ) , singapore , 29 may–3 june 2017 ; pp . 2614–2620 . 177 . bowman , s.l . ; atanasov , n. ; daniilidis , k. ; pappas , g.j . probabilistic data association for semantic slam . in proceedings of the 2017 ieee international conference on robotics and automation ( icra ) , singapore , 29 may–3 june 2017 ; pp . 1722–1729 . 178 . hu , k. ; zheng , f. ; weng , l. ; ding , y. ; jin , j . action recognition algorithm of spatio–temporal differential lstm based on feature enhancement . appl . sci . 2021 ,11 , 7876 . [ crossref ] 179 . chen , w. ; zheng , f. ; gao , s. ; hu , k. an lstm with differential structure and its application in action recognition . math . probl . eng . 2022 ,2022 , 7316396 . [ crossref ] 180 . cao , b. ; li , c. ; song , y. ; qin , y. ; chen , c. network intrusion detection model based on cnn and gru . appl . sci . 2022 ,12 , 4184 . [ crossref ] 181 . chen , e.z . ; wang , p . ; chen , x. ; chen , t. ; sun , s. pyramid convolutional rnn for mri image reconstruction . ieee trans . med . imaging 2022 . [ crossref ] [ pubmed ] 182 . sang , h. ; jiang , r. ; wang , z. ; zhou , y. ; he , b . a novel neural multi-store memory network for autonomous visual navigation in unknown environment . ieee robot . autom . lett . 2022 ,7 , 2039–2046 . [ crossref ] remote sens . 2022 ,14 , 3010 44 of 47 183 . xue , f. ; wang , q. ; wang , x. ; dong , w. ; wang , j. ; zha , h. guided feature selection for deep visual odometry . in proceedings of the computer vision—accv 2018 , perth , australia , 2–6 december 2018 ; pp . 293–308 . 184 . teed , z. ; deng , j. droid-slam : deep visual slam for monocular , stereo , and rgb-d cameras . arxiv 2021 , arxiv:2108.10869 . 185 . turan , m. ; almalioglu ,', 'december 2018 ; pp . 293–308 . 184 . teed , z. ; deng , j. droid-slam : deep visual slam for monocular , stereo , and rgb-d cameras . arxiv 2021 , arxiv:2108.10869 . 185 . turan , m. ; almalioglu , y. ; araujo , h. ; konukoglu , e. ; sitti , m. deep endovo : a recurrent convolutional neural network ( rcnn ) base visual odometry approach for endoscopic capsule robot . neurocomputing 2018 ,275 , 1861–1870 . [ crossref ] 186 . chancán , m. ; milford , m. deepseqslam : a trainable cnn+rnn for joint global description and sequence-based place recognition . arxiv 2020 , arxiv:2011.08518 . 187 . han , l. ; lin , y. ; du , g. ; lian , s. deepvio : self-supervised deep learning of monocular visual inertial odometry use 3d geometric constraints . in proceedings of the 2019 ieee/rsj international conference on intelligent robots and systems ( iros ) , venetian macao , macau , china , 3–8 november 2019 ; pp . 6906–6913 . 188 . chen , c. ; rosa , s. ; miao , y. ; lu , c.x . ; wu , w. ; markham , a. ; trigoni , n. selective sensor fusion for neural visual-inertial odometry . in proceedings of the 2019 ieee/cvf conference on computer vision and pattern recognition ( cvpr ) , long beach , ca , usa , 15–20 june 2019 ; pp . 10534–10543 . 189 . almalioglu , y. ; turan , m. ; saputra , m.r.u . ; de gusmão , p .p .b . ; markham , a. ; trigoni , n. selfvio : self-supervised deep monocular visual–inertial odometry and depth estimation . neural netw . 2022 ,150 , 119–136 . [ crossref ] 190 . wong , a. ; fei , x. ; tsuei , s. ; soatto , s. unsupervised depth completion from visual inertial odometry . ieee robot . autom . lett . 2020 ,5 , 1899–1906 . [ crossref ] 191 . wang , s. ; clark , r. ; wen , h. ; trigoni , n. deepvo : towards end-to-end visual odometry with deep recurrent convolutional neural networks . in proceedings of the 2017 ieee international conference on robotics and automation ( icra ) , singapore , 29 may–3 june 2 2017 ; pp . 2043–2050 . 192 . loo , s.y . ; amiri , a.j . ; mashohor , s. ; tang , s.h . ; zhang , h.', 'ieee international conference on robotics and automation ( icra ) , singapore , 29 may–3 june 2 2017 ; pp . 2043–2050 . 192 . loo , s.y . ; amiri , a.j . ; mashohor , s. ; tang , s.h . ; zhang , h. cnn-svo : improving the mapping in semi-direct visual odometry using single-image depth prediction . in proceedings of the 2019 international conference on robotics and automation ( icra ) , montreal , qc , canada , 20–24 may 2019 ; pp . 5218–5223 . 193 . almalioglu , y. ; saputra , m.r.u . ; gusmão , p .p .b.d . ; markham , a. ; trigoni , n. ganvo : unsupervised deep monocular visual odometry and depth estimation with generative adversarial networks . in proceedings of the 2019 international conference on robotics and automation ( icra ) , montreal , qc , canada , 20–24 may 2019 ; pp . 5474–5480 . 194 . li , y. ; ushiku , y. ; harada , t. pose graph optimization for unsupervised monocular visual odometry . in proceedings of the 2019 international conference on robotics and automation ( icra ) , montreal , qc , canada , 20–24 may 2019 ; pp . 5439-5445 . 195 . bruno , h.m.s . ; colombini , e.l. lift-slam : a deep-learning feature-based monocular visual slam method . neurocomputing 2021 ,455 , 97–110 . [ crossref ] 196 . zhang , s. ; lu , s. ; he , r. ; bao , z . stereo visual odometry pose correction through unsupervised deep learning . sensors 2021 , 21 , 4735 . [ crossref ] 197 . shamwell , e.j . ; lindgren , k. ; leung , s. ; nothwang , w.d . unsupervised deep visual-inertial odometry with online error correction for rgb-d imagery . ieee trans . pattern anal . mach . intell . 2020 ,42 , 2478–2493 . [ crossref ] [ pubmed ] 198 . kim , y. ; yoon , s. ; kim , s. ; kim , a. unsupervised balanced covariance learning for visual-inertial sensor fusion . ieee robot . autom . lett . 2021 ,6 , 819–826 . [ crossref ] 199 . gurturk , m. ; yuseﬁ , a. ; aslan , m.f . ; soycan , m. ; durdu , a. ; masiero , a . the ytu dataset and recurrent neural network base visual-inertial odometry . measurement 2021 ,184 , 109878 . [ crossref ]', ', m. ; yuseﬁ , a. ; aslan , m.f . ; soycan , m. ; durdu , a. ; masiero , a . the ytu dataset and recurrent neural network base visual-inertial odometry . measurement 2021 ,184 , 109878 . [ crossref ] 200 . guan , p . ; cao , z. ; chen , e. ; liang , s. ; tan , m. ; yu , j . a real-time semantic visual slam approach with point and object . int . j. adv . robot . syst . 2020 ,17 , 1729881420905443 . [ crossref ] 201 . hempel , t. ; al-hamadi , a . an online semantic mapping system for extend and enhance visual slam . eng . appl . artif . intell . 2022 ,111 , 104830 . [ crossref ] 202 . qian , z. ; patath , k. ; fu , j. ; xiao , j. semantic slam with autonomous object-level data association . in proceedings of the 2021 ieee international conference on robotics and automation ( icra ) , xi ’ an , china , 30 may–5 june 2021 ; pp . 11203–11209 . 203 . dr pablo f alcantarilla . available online : http : //blog.slamcore.com/age-of-perception ( access on 12 june 2022 ) . 204 . zhang , l. ; wei , l. ; shen , p . ; wei , w. ; zhu , g. ; song , j. semantic slam based on object detection and improved octomap . ieee access 2018 ,6 , 75545–75559 . [ crossref ] 205 . hu , k. ; zhang , d. ; xia , m. cdunet : cloud detection unet for remote sensing imagery . remote sens . 2021 ,13 , 4533 . [ crossref ] 206 . hu , k. ; zhang , y. ; weng , c. ; wang , p . ; deng , z. ; liu , y . an underwater image enhancement algorithm based on generative adversarial network and natural image quality evaluation index . j. mar . sci . eng . 2021 ,9 , 691 . [ crossref ] 207 . pazhani , a.a.j . ; vasanthanayaki , c. object detection in satellite image by fast r-cnn incorporate with enhanced roi pooling ( frrnet-eroi ) framework . earth sci . inform . 2022 ,15 , 553–561 . [ crossref ] 208 . hoang , t.m . ; zhou , j. ; fan , y . image compression with encoder-decoder matched semantic segmentation . in proceedings of the 2020 ieee/cvf conference on computer vision and pattern recognition workshops ( cvprw ) , seattle , wa , usa , 14–19 june 2020 ; pp .', 'matched semantic segmentation . in proceedings of the 2020 ieee/cvf conference on computer vision and pattern recognition workshops ( cvprw ) , seattle , wa , usa , 14–19 june 2020 ; pp . 619–623 . 209 . hu , k. ; lu , f. ; lu , m. ; deng , z. ; liu , y . a marine object detection algorithm based on ssd and feature enhancement . complexity 2020 ,2020 , 5476142 . [ crossref ] 210 . shao , f. ; chen , l. ; shao , j. ; ji , w. ; xiao , s. ; ye , l. ; zhuang , y. ; xiao , j . deep learning for weakly supervised object detection and localization : a survey . neurocomputing 2022 ,496 , 192–207 . [ crossref ] remote sens . 2022 ,14 , 3010 45 of 47 211 . liang , w. ; xu , p . ; guo , l. ; bai , h. ; zhou , y. ; chen , f. a survey of 3d object detection . multimed . tools appl . 2021 ,80 , 29617–29641 . [ crossref ] 212 . girshick , r. ; donahue , j. ; darrell , t. ; malik , j . rich feature hierarchies for accurate object detection and semantic segmentation . in proceedings of the 2014 ieee conference on computer vision and pattern recognition , columbus , oh , usa , 23–28 june 2014 ; pp . 580–587 . 213 . girshick , r. fast r-cnn . in proceedings of the 2015 ieee international conference on computer vision ( iccv ) , santiago , chile , 7–13 december 2015 ; pp . 1440–1448 . 214 . ren , s. ; he , k. ; girshick , r. ; sun , j . faster r-cnn : towards real-time object detection with region proposal networks . ieee trans . pattern anal . mach . intell . 2017 ,39 , 1137–1149 . [ crossref ] [ pubmed ] 215 . redmon , j. ; divvala , s. ; girshick , r. ; farhadi , a . you only look once : uniﬁed , real-time object detection . in proceedings of the 2016 ieee conference on computer vision and pattern recognition ( cvpr ) , las vegas , nv , usa , 27–30 june 2016 ; pp . 779–788 . 216 . liu , w. ; anguelov , d. ; erhan , d. ; szegedy , c. ; reed , s. ; fu , c.-y . ; berg , a.c. ssd : single shot multibox detector . in proceedings of the computer vision—eccv 2016 , amsterdam , the netherlands , 11–14 october 2016 ; pp . 21–37 . 217 .', '; reed , s. ; fu , c.-y . ; berg , a.c. ssd : single shot multibox detector . in proceedings of the computer vision—eccv 2016 , amsterdam , the netherlands , 11–14 october 2016 ; pp . 21–37 . 217 . long , j. ; shelhamer , e. ; darrell , t. fully convolutional network for semantic segmentation . in proceedings of the 2015 ieee conference on computer vision and pattern recognition ( cvpr ) , boston , ma , usa , 7–12 june 2015 ; pp . 3431-3440 . 218 . badrinarayanan , v . ; kendall , a. ; cipolla , r. segnet : a deep convolutional encoder-decoder architecture for image segmentation . ieee trans . pattern anal . mach . intell . 2017 ,39 , 2481–2495 . [ crossref ] [ pubmed ] 219 . zhao , h. ; shi , j. ; qi , x. ; wang , x. ; jia , j. pyramid scene parsing network . in proceedings of the 2017 ieee conference on computer vision and pattern recognition ( cvpr ) , honolulu , hi , usa , 21–26 july 2017 ; pp . 6230–6239 . 220 . he , k. ; gkioxari , g. ; dollár , p . ; girshick , r.b . mask r-cnn . in proceedings of the ieee international conference on computer vision , venice , italy , 22–29 october 2017 ; pp . 2980–2988 . 221 . yolov5 . available online : http : //github.com/ultralytics/yolov5 ( access on 23 april 2022 ) . 222 . chen , l.-c. ; papandreou , g. ; kokkinos , i. ; murphy , k. ; yuille , a.l . semantic image segmentation with deep convolutional net and fully connect crfs . arxiv 2014 , arxiv:1412.7062 . 223 . deeplabv3+ . available online : http : //github.com/tramac/awesome-semantic-segmentation-pytorch ( access on 23 april 2022 ) . 224 . bolya , d. ; zhou , c. ; xiao , f. ; lee , y.j . yolact : real-time instance segmentation . in proceedings of the 2019 ieee/cvf international conference on computer vision ( iccv ) , seoul , korea , 27 october–2 november 2019 ; pp . 9156-9165 . 225 . dang , x. ; rong , z. ; liang , x. sensor fusion-based approach to eliminating moving objects for slam in dynamic environments . sensors 2021 ,21 , 230 . [ crossref ] 226 . tschopp , f. ; nieto , j.i . ; siegwart , r.y . ;', 'liang , x. sensor fusion-based approach to eliminating moving objects for slam in dynamic environments . sensors 2021 ,21 , 230 . [ crossref ] 226 . tschopp , f. ; nieto , j.i . ; siegwart , r.y . ; cadena , c. superquadric object representation for optimization-based semantic slam . arxiv 2021 , arxiv:2109.09627 . 227 . zhao , z. ; mao , y. ; ding , y. ; ren , p . ; zheng , n. visual-based semantic slam with landmarks for large-scale outdoor environment . in proceedings of the 2019 2nd china symposium on cognitive computing and hybrid intelligence ( cchi ) , xi ’ an , china , 21–22 september 2019 ; pp . 149–154 . 228 . lianos , k.-n. ; schönberger , j.l . ; pollefeys , m. ; sattler , t. vso : visual semantic odometry . in proceedings of the computer vision—eccv 2018 , munich , germany , 8–14 september 2018 ; pp . 246–263 . 229 . stenborg , e. ; toft , c. ; hammarstrand , l. long-term visual localization using semantically segmented images . in proceedings of the 2018 ieee international conference on robotics and automation ( icra ) , brisbane , australia , 21–25 may 2018 ; pp . 6484-6490 . 230 . dai , w. ; zhang , y. ; li , p . ; fang , z. ; scherer , s. rgb-d slam in dynamic environments using point correlations . ieee trans . pattern anal . mach . intell . 2022 ,44 , 373–389 . [ crossref ] 231 . han , x. ; li , s. ; wang , x. ; zhou , w. semantic mapping for mobile robots in indoor scenes : a survey . information 2021 ,12 , 92 . [ crossref ] 232 . liao , z. ; hu , y. ; zhang , j. ; qi , x. ; zhang , x. ; wang , w. so-slam : semantic object slam with scale proportional and symmetrical texture constraints . ieee robot . autom . lett . 2022 ,7 , 4008–4015 . [ crossref ] 233 . ran , t. ; yuan , l. ; zhang , j. ; he , l. ; huang , r. ; mei , j . not only look however , infer : multiple hypothesis clustering of data association inference for semantic slam . ieee trans . instrum . meas . 2021 ,70 , 1–9 . [ crossref ] 234 . yin , r. ; cheng , y. ; wu , h. ; song , y. ; yu , b. ; niu , r. fusionlane : multi-sensor', 'association inference for semantic slam . ieee trans . instrum . meas . 2021 ,70 , 1–9 . [ crossref ] 234 . yin , r. ; cheng , y. ; wu , h. ; song , y. ; yu , b. ; niu , r. fusionlane : multi-sensor fusion for lane marking semantic segmentation using deep neural networks . ieee trans . intell . transp . syst . 2022 ,23 , 1543-1553 . [ crossref ] 235 . han , b. ; xu , l. a monocular slam system with mask loop closing . in proceedings of the 2020 chinese control and decision conference ( ccdc ) , hefei , china , 22–24 august 2020 ; pp . 4762–4768 . 236 . yang , s. ; fan , g. ; bai , l. ; zhao , c. ; li , d. sgc-vslam : a semantic and geometric constraints vslam for dynamic indoor environments . sensors 2020 ,20 , 2432 . [ crossref ] [ pubmed ] 237 . galindo , c. ; safﬁotti , a. ; coradeschi , s. ; buschka , p . ; fernandez-madrigal , j.a . ; gonzalez , j. multi-hierarchical semantic map for mobile robotics . in proceedings of the 2005 ieee/rsj international conference on intelligent robots and systems , edmonton , ab , canada , 2–6 august 2005 ; pp . 2278–2283 . 238 . vasudevan , s. ; gächter , s. ; nguyen , v . ; siegwart , r. cognitive map for mobile robots—an object base approach . robot . auton . syst . 2007 ,55 , 359–371 . [ crossref ] 239 . yue , y. ; wen , m. ; zhao , c. ; wang , y. ; wang , d. cosem : collaborative semantic map matching framework for autonomous robots . ieee trans . ind . electron . 2022 ,69 , 3843–3853 . [ crossref ] remote sens . 2022 ,14 , 3010 46 of 47 240 . ashour , r. ; abdelkader , m. ; dias , j. ; almoosa , n.i . ; taha , t. semantic hazard labelling and risk assessment mapping during robot exploration . ieee access 2022 ,10 , 16337–16349 . [ crossref ] 241 . rosinol , a. ; abate , m. ; chang , y. ; carlone , l. kimera : an open-source library for real-time metric-semantic localization and mapping . in proceedings of the 2020 ieee international conference on robotics and automation ( icra ) , virtually , 31 may–31 august 2020 ; pp . 1689–1696 . 242 . mingyuan , m. ; hewei , z. ;', 'and mapping . in proceedings of the 2020 ieee international conference on robotics and automation ( icra ) , virtually , 31 may–31 august 2020 ; pp . 1689–1696 . 242 . mingyuan , m. ; hewei , z. ; simeng , l. ; baochang , z. semantic-rtab-map ( srm ) : a semantic slam system with cnns on depth image . math . found . comput . 2019 ,2 , 29–41 . 243 . labbé , m. ; michaud , f. appearance-based loop closure detection for online large-scale and long-term operation . ieee trans . robot . 2013 ,29 , 734–745 . [ crossref ] 244 . menini , d. ; kumar , s. ; oswald , m.r . ; sandström , e. ; sminchisescu , c. ; gool , l.v . a real-time online learning framework for joint 3d reconstruction and semantic segmentation of indoor scenes . ieee robot . autom . lett . 2022 ,7 , 1332–1339 . [ crossref ] 245 . zhuang , c. ; wang , z. ; zhao , h. ; ding , h. semantic part segmentation method base 3d object pose estimation with rgb-d image for bin-picking . robot . comput.-integr . manuf . 2021 ,68 , 102086 . [ crossref ] 246 . sousa , y.c . ; bassani , h.d.f . topological semantic mapping by consolidation of deep visual features . ieee robot . autom . lett . 2022 ,7 , 4110–4117 . [ crossref ] 247 . wang , f. ; zhang , c. ; zhang , w. ; fang , c. ; xia , y. ; liu , y. ; dong , h. object-based reliable visual navigation for mobile robot . sensors 2022 ,22 , 2387 . [ crossref ] [ pubmed ] 248 . wu , y. ; zhang , y. ; zhu , d. ; feng , y. ; coleman , s. ; kerr , d. eao-slam : monocular semi-dense object slam based on ensemble data association . in proceedings of the 2020 ieee/rsj international conference on intelligent robots and systems ( iros ) , las vegas , nv , usa , 24 october–24 janaury 2020 ; pp . 4966–4973 . 249 . wang , j. ; rünz , m. ; agapito , l. dsp-slam : object oriented slam with deep shape priors . in proceedings of the 2021 international conference on 3d vision ( 3dv ) , london , uk , 1–3 december 2021 ; pp . 1362–1371 . 250 . fu , j. ; huang , q. ; doherty , k. ; wang , y. ; leonard , j.j. a multi-hypothesis approach to', 'conference on 3d vision ( 3dv ) , london , uk , 1–3 december 2021 ; pp . 1362–1371 . 250 . fu , j. ; huang , q. ; doherty , k. ; wang , y. ; leonard , j.j. a multi-hypothesis approach to pose ambiguity in object-based slam . in proceedings of the 2021 ieee/rsj international conference on intelligent robots and systems ( iros ) , prague , czech republic , 27 september–1 october 2021 ; pp . 7639–7646 . 251 . zhai , r. ; yuan , y . a method of vision aided gnss positioning using semantic information in complex urban environment . remote sens . 2022 ,14 , 869 . [ crossref ] 252 . vineet , v . ; miksik , o. ; lidegaard , m. ; nießner , m. ; golodetz , s. ; prisacariu , v .a . ; kähler , o. ; murray , d.w. ; izadi , s. ; pérez , p . ; et al . incremental dense semantic stereo fusion for large-scale semantic scene reconstruction . in proceedings of the 2015 ieee international conference on robotics and automation ( icra ) , seattle , wa , usa , 26–30 may 2015 ; pp . 75–82 . 253 . zhao , z. ; chen , x . building 3d semantic map for mobile robot use rgb-d camera . intell . serv . robot . 2016 ,9 , 297–309 . [ crossref ] 254 . li , x. ; belaroussi , r. semi-dense 3d semantic mapping from monocular slam . arxiv 2016 , arxiv:1611.04144 . 255 . yang , s. ; huang , y. ; scherer , s. semantic 3d occupancy map through efﬁcient high order crfs . in proceedings of the 2017 ieee/rsj international conference on intelligent robots and systems ( iros ) , vancouver , bc , canada , 24–28 september 2017 ; pp . 590–597 . 256 . narita , g. ; seno , t. ; ishikawa , t. ; kaji , y. panopticfusion : online volumetric semantic mapping at the level of stuff and things . in proceedings of the 2019 ieee/rsj international conference on intelligent robots and systems ( iros ) , venetian macao , macau , china , 3–8 november 2019 ; pp . 4205–4212 . 257 . qin , t. ; zheng , y. ; chen , t. ; chen , y. ; su , q . a light-weight semantic map for visual localization towards autonomous driving . in proceedings of the 2021 ieee international conference on', '. qin , t. ; zheng , y. ; chen , t. ; chen , y. ; su , q . a light-weight semantic map for visual localization towards autonomous driving . in proceedings of the 2021 ieee international conference on robotics and automation ( icra ) , xi ’ an , china , 30 may–5 june 2021 ; pp . 11248–11254 . 258 . yan , l. ; hu , x. ; zhao , l. ; chen , y. ; wei , p . ; xie , h. dgs-slam : a fast and robust rgbd slam in dynamic environments combined by geometric and semantic information . remote sens . 2022 ,14 , 795 . [ crossref ] 259 . hu , z. ; zhao , j. ; luo , y. ; ou , j. semantic slam based on improved deeplabv3+ in dynamic scenarios . ieee access 2022 ,10 , 21160–21168 . [ crossref ] 260 . liu , x. ; nardari , g.v . ; ojeda , f.c . ; tao , y. ; zhou , a. ; donnelly , t. ; qu , c. ; chen , s.w . ; romero , r.a.f . ; taylor , c.j . ; et al . large-scale autonomous flight with real-time semantic slam under dense forest canopy . ieee robot . autom . lett . 2022 ,7 , 5512–5519 . [ crossref ] 261 . chen , b. ; peng , g. ; he , d. ; zhou , c. ; hu , b . visual slam based on dynamic object detection . in proceedings of the 2021 33rd chinese control and decision conference ( ccdc ) , kunming , china , 22–24 may 2021 ; pp . 5966–5971 . 262 . bescos , b. ; fácil , j.m . ; civera , j. ; neira , j. dynaslam : tracking , mapping , and inpainting in dynamic scenes . ieee robot . autom . lett . 2018 ,3 , 4076–4083 . [ crossref ] 263 . yu , c. ; liu , z. ; liu , x.j . ; xie , f. ; yang , y. ; wei , q. ; fei , q. ds-slam : a semantic visual slam towards dynamic environments . in proceedings of the 2018 ieee/rsj international conference on intelligent robots and systems ( iros ) , madrid , spain , 1–5 october 2018 ; pp . 1168–1174 . 264 . kaneko , m. ; iwami , k. ; ogawa , t. ; yamasaki , t. ; aizawa , k. mask-slam : robust feature-based monocular slam by masking using semantic segmentation . in proceedings of the 2018 ieee/cvf conference on computer vision and pattern recognition workshops ( cvprw ) , salt lake city , ut , usa , 18–22', 'slam by masking using semantic segmentation . in proceedings of the 2018 ieee/cvf conference on computer vision and pattern recognition workshops ( cvprw ) , salt lake city , ut , usa , 18–22 june 2018 ; pp . 371–3718.remote sens . 2022 ,14 , 3010 47 of 47 265 . schörghuber , m. ; steininger , d. ; cabon , y. ; humenberger , m. ; gelautz , m. slamantic—leveraging semantics to improve vslam in dynamic environments . in proceedings of the 2019 ieee/cvf international conference on computer vision workshop ( iccvw ) , seoul , korea , 27–28 october 2019 ; pp . 3759–3768 . 266 . lv , x. ; wang , b. ; ye , d. ; wang , s.j.a . semantic flow-guided motion removal method for robust mapping . arxiv 2020 , arxiv:2010.06876 . 267 . yuan , x. ; chen , s. sad-slam : a visual slam based on semantic and depth information . in proceedings of the 2020 ieee/rsj in- ternational conference on intelligent robots and systems ( iros ) , las vegas , nv , usa , 24 october–24 january 2020 ; pp . 4930–4935 . 268 . wen , s. ; li , p . ; zhao , y. ; zhang , h. ; sun , f. ; wang , z. semantic visual slam in dynamic environment . auton . robot . 2021 , 45 , 493–504 . [ crossref ] 269 . wu , j. ; xiong , j. ; guo , h. improving robustness of line feature for vio in dynamic scene . meas . sci . technol . 2022 ,33 , 065204 . [ crossref ] 270 . wang , m. ; wang , h. ; wang , z. ; li , y . a uav visual relocalization method using semantic object features based on internet of things . in proceedings of the wireless communications mobile computing , dubrovnik , croatia , 30 may–3 june 2022 . 271 . lu , z. ; hu , z. ; uchimura , k. slam estimation in dynamic outdoor environments : a review . in proceedings of the intelligent robotics and applications , singapore , 16–18 december 2009 ; pp . 255–267 . 272 . reddy , n.d. ; abbasnejad , i. ; reddy , s. ; mondal , a.k . ; devalla , v . incremental real-time multibody vslam with trajectory optimization use stereo camera . in proceedings of the 2016 ieee/rsj international conference on intelligent robots and', ', a.k . ; devalla , v . incremental real-time multibody vslam with trajectory optimization use stereo camera . in proceedings of the 2016 ieee/rsj international conference on intelligent robots and systems ( iros ) , daejeon , korea , 9–14 october 2016 ; pp . 4505–4510 . 273 . lenz , p . ; ziegler , j. ; geiger , a. ; roser , m. sparse scene ﬂow segmentation for move object detection in urban environment . in proceedings of the 2011 ieee intelligent vehicles symposium ( iv ) , baden-baden , germany , 5–9 june 2011 ; pp . 926–932 . 274 . dynaslam . available online : http : //github.com/bertabescos/dynaslam ( access on 24 april 2022 ) . 275 . ds-slam . available online : http : //github.com/ivipsourcecode/ds-slam ( access on 24 april 2022 ) . 276 . zhong , f. ; wang , s. ; zhang , z. ; chen , c. ; wang , y. detect-slam : making object detection and slam mutually beneﬁcial . in proceedings of the 2018 ieee winter conference on applications of computer vision ( wacv ) , lake tahoe , nv , usa , 12–15 march 2018 ; pp . 1001–1010 . 277 . detect-slam . available online : http : //github.com/liadbiz/detect-slam ( access on 24 april 2022 ) . 278 . wang , z. ; zhang , q. ; li , j. ; zhang , s. ; liu , j . a computationally efﬁcient semantic slam solution for dynamic scenes . remote sens . 2019 ,11 , 1363 . [ crossref ] 279 . slamantic . available online : http : //github.com/mthz/slamantic ( access on 25 april 2022 ) . 280 . barsan , i.a . ; liu , p . ; pollefeys , m. ; geiger , a . robust dense mapping for large-scale dynamic environments . in proceedings of the 2018 ieee international conference on robotics and automation ( icra ) , brisbane , australia , 21–25 may 2018 ; pp . 7510–7517 . 281 . dai , j. ; he , k. ; sun , j. instance-aware semantic segmentation via multi-task network cascades . in proceedings of the 2016 ieee conference on computer vision and pattern recognition ( cvpr ) , las vegas , nv , usa , 27–30 june 2016 ; pp . 3150–3158 . 282 . dynslam . available online : http : //github.com/andreibarsan/dynslam', 'conference on computer vision and pattern recognition ( cvpr ) , las vegas , nv , usa , 27–30 june 2016 ; pp . 3150–3158 . 282 . dynslam . available online : http : //github.com/andreibarsan/dynslam ( access on 25 april 2022 ) . 283 . esparza , d. ; flores , g. the stdyn-slam : a stereo vision and semantic segmentation approach for vslam in dynamic outdoor environments . ieee access 2022 ,10 , 18201–18209 . [ crossref ] 284 . stdyn-slam . available online : http : //github.com/danielaesparza/stdyn-slam ( access on 25 april 2022 ) . 285 . zhang , t. ; nakamura , y. posefusion : dense rgb-d slam in dynamic human environment . in proceedings of the international symposium on experimental robotics , buenos aires , argentina , 5–8 november 2018 ; pp . 772–780 . 286 . posefusion . available online : http : //github.com/conix-center/posefusion ( access on 25 april 2022 ) . 287 . liu , y. ; miura , j. rds-slam : real-time dynamic slam using semantic segmentation methods . ieee access 2021 ,9 , 23772-23785 . [ crossref ] 288 . rds-slam . available online : http : //github.com/yubaoliu/rds-slam ( access on 25 april 2022 ) . 289 . lai , d. ; li , c. ; he , b. yo-slam : a robust visual slam towards dynamic environments . in proceedings of the 2021 international conference on communications , information system and computer engineering ( cisce ) , beijing , china , 14–16 may 2021 ; pp . 720-725 . 290 . zhang , y. ; xu , x. ; zhang , n. ; lv , y . a semantic slam system for catadioptric panoramic cameras in dynamic environments . sensors 2021 ,21 , 5889 . [ crossref ] [ pubmed ] 291 . schönbein , m. ; geiger , a. omnidirectional 3d reconstruction in augmented manhattan world . in proceedings of the 2014 ieee/rsj international conference on intelligent robots and systems , chicago , il , usa , 14–18 september 2014 ; pp . 716–723 . 292 . hu , x. ; lang , j. doe-slam : dynamic object enhanced visual slam . sensors 2021 ,21 , 3091 . [ crossref ] 293 . yu , n. ; gan , m. ; yu , h. ; yang , k. drso-slam : a dynamic rgb-d slam', '. 292 . hu , x. ; lang , j. doe-slam : dynamic object enhanced visual slam . sensors 2021 ,21 , 3091 . [ crossref ] 293 . yu , n. ; gan , m. ; yu , h. ; yang , k. drso-slam : a dynamic rgb-d slam algorithm for indoor dynamic scenes . in proceedings of the 2021 33rd chinese control and decision conference ( ccdc ) , kunming , china , 22–24 may 2021 ; pp . 1052–1058 . 294 . ai , y. ; rui , t. ; lu , m. ; fu , l. ; liu , s. ; wang , s. ddl-slam : a robust rgb-d slam in dynamic environments combined with deep learning . ieee access 2020 ,8 , 162335–162342 . [ crossref ] 295 . liu , y. ; miura , j. rdmo-slam : real-time visual slam for dynamic environments using semantic label prediction with optical flow . ieee access 2021 ,9 , 106981–106997 . [ crossref ]']",https://doi.org/10.1109/WACV.2017.75
